{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "006de023",
   "metadata": {},
   "source": [
    "# RNN for Arithmetic Sequence from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b82c06",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46ba1195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481bcfbd",
   "metadata": {},
   "source": [
    "### Create the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b559ce7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets saved as 'Patterned_Arithmetic_Sequences.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Generate Arithmetic Operations sequences with two patterns\n",
    "arithmetic_sequences = []\n",
    "\n",
    "for i in range(100):\n",
    "    pattern_type = i % 2  # Define 2 types of patterns\n",
    "    sequence = []\n",
    "    if pattern_type == 0:  # Alternating differences (e.g., +5, -3, +5, -3)\n",
    "        start = random.randint(500, 1000)\n",
    "        diff1, diff2 = random.randint(1, 10), random.randint(-10, -1)\n",
    "        sequence = [start]\n",
    "        while len(sequence) < 100:\n",
    "            next_value = sequence[-1] + (diff1 if len(sequence) % 2 == 1 else diff2)\n",
    "            if next_value <= 0:\n",
    "                next_value = random.randint(1, 100)  # Reset to positive if needed\n",
    "            sequence.append(next_value)\n",
    "    elif pattern_type == 1:  # Repeating fixed difference (e.g., +7, +7, +7, ...)\n",
    "        start = random.randint(500, 1000)\n",
    "        diff = random.randint(1, 20)\n",
    "        sequence = [start]\n",
    "        for _ in range(99):\n",
    "            next_value = sequence[-1] + diff\n",
    "            if next_value <= 0:\n",
    "                next_value = random.randint(1, 100)  # Reset to positive if needed\n",
    "            sequence.append(next_value)\n",
    "    arithmetic_sequences.append(sequence)\n",
    "    \n",
    "# Create DataFrames\n",
    "arithmetic_df = pd.DataFrame(arithmetic_sequences).add_prefix(\"arithmetic_sequences_\")\n",
    "\n",
    "# Save to CSV\n",
    "arithmetic_df.to_csv(\"Patterned_Arithmetic_Sequences.csv\", index=False)\n",
    "print(\"Datasets saved as 'Patterned_Arithmetic_Sequences.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d9890d",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18c4eb89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arithmetic_sequences_file = \"Patterned_Arithmetic_Sequences.csv\"\n",
    "arithmetic_df = pd.read_csv(arithmetic_sequences_file)\n",
    "arithmetic_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0c228d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arithmetic_sequences_0</th>\n",
       "      <th>arithmetic_sequences_1</th>\n",
       "      <th>arithmetic_sequences_2</th>\n",
       "      <th>arithmetic_sequences_3</th>\n",
       "      <th>arithmetic_sequences_4</th>\n",
       "      <th>arithmetic_sequences_5</th>\n",
       "      <th>arithmetic_sequences_6</th>\n",
       "      <th>arithmetic_sequences_7</th>\n",
       "      <th>arithmetic_sequences_8</th>\n",
       "      <th>arithmetic_sequences_9</th>\n",
       "      <th>...</th>\n",
       "      <th>arithmetic_sequences_90</th>\n",
       "      <th>arithmetic_sequences_91</th>\n",
       "      <th>arithmetic_sequences_92</th>\n",
       "      <th>arithmetic_sequences_93</th>\n",
       "      <th>arithmetic_sequences_94</th>\n",
       "      <th>arithmetic_sequences_95</th>\n",
       "      <th>arithmetic_sequences_96</th>\n",
       "      <th>arithmetic_sequences_97</th>\n",
       "      <th>arithmetic_sequences_98</th>\n",
       "      <th>arithmetic_sequences_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>650</td>\n",
       "      <td>652</td>\n",
       "      <td>647</td>\n",
       "      <td>649</td>\n",
       "      <td>644</td>\n",
       "      <td>646</td>\n",
       "      <td>641</td>\n",
       "      <td>643</td>\n",
       "      <td>638</td>\n",
       "      <td>640</td>\n",
       "      <td>...</td>\n",
       "      <td>515</td>\n",
       "      <td>517</td>\n",
       "      <td>512</td>\n",
       "      <td>514</td>\n",
       "      <td>509</td>\n",
       "      <td>511</td>\n",
       "      <td>506</td>\n",
       "      <td>508</td>\n",
       "      <td>503</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>789</td>\n",
       "      <td>806</td>\n",
       "      <td>823</td>\n",
       "      <td>840</td>\n",
       "      <td>857</td>\n",
       "      <td>874</td>\n",
       "      <td>891</td>\n",
       "      <td>908</td>\n",
       "      <td>925</td>\n",
       "      <td>942</td>\n",
       "      <td>...</td>\n",
       "      <td>2319</td>\n",
       "      <td>2336</td>\n",
       "      <td>2353</td>\n",
       "      <td>2370</td>\n",
       "      <td>2387</td>\n",
       "      <td>2404</td>\n",
       "      <td>2421</td>\n",
       "      <td>2438</td>\n",
       "      <td>2455</td>\n",
       "      <td>2472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>563</td>\n",
       "      <td>564</td>\n",
       "      <td>560</td>\n",
       "      <td>561</td>\n",
       "      <td>557</td>\n",
       "      <td>558</td>\n",
       "      <td>554</td>\n",
       "      <td>555</td>\n",
       "      <td>551</td>\n",
       "      <td>552</td>\n",
       "      <td>...</td>\n",
       "      <td>428</td>\n",
       "      <td>429</td>\n",
       "      <td>425</td>\n",
       "      <td>426</td>\n",
       "      <td>422</td>\n",
       "      <td>423</td>\n",
       "      <td>419</td>\n",
       "      <td>420</td>\n",
       "      <td>416</td>\n",
       "      <td>417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>739</td>\n",
       "      <td>741</td>\n",
       "      <td>743</td>\n",
       "      <td>745</td>\n",
       "      <td>747</td>\n",
       "      <td>749</td>\n",
       "      <td>751</td>\n",
       "      <td>753</td>\n",
       "      <td>755</td>\n",
       "      <td>757</td>\n",
       "      <td>...</td>\n",
       "      <td>919</td>\n",
       "      <td>921</td>\n",
       "      <td>923</td>\n",
       "      <td>925</td>\n",
       "      <td>927</td>\n",
       "      <td>929</td>\n",
       "      <td>931</td>\n",
       "      <td>933</td>\n",
       "      <td>935</td>\n",
       "      <td>937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>803</td>\n",
       "      <td>805</td>\n",
       "      <td>797</td>\n",
       "      <td>799</td>\n",
       "      <td>791</td>\n",
       "      <td>793</td>\n",
       "      <td>785</td>\n",
       "      <td>787</td>\n",
       "      <td>779</td>\n",
       "      <td>781</td>\n",
       "      <td>...</td>\n",
       "      <td>533</td>\n",
       "      <td>535</td>\n",
       "      <td>527</td>\n",
       "      <td>529</td>\n",
       "      <td>521</td>\n",
       "      <td>523</td>\n",
       "      <td>515</td>\n",
       "      <td>517</td>\n",
       "      <td>509</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   arithmetic_sequences_0  arithmetic_sequences_1  arithmetic_sequences_2  \\\n",
       "0                     650                     652                     647   \n",
       "1                     789                     806                     823   \n",
       "2                     563                     564                     560   \n",
       "3                     739                     741                     743   \n",
       "4                     803                     805                     797   \n",
       "\n",
       "   arithmetic_sequences_3  arithmetic_sequences_4  arithmetic_sequences_5  \\\n",
       "0                     649                     644                     646   \n",
       "1                     840                     857                     874   \n",
       "2                     561                     557                     558   \n",
       "3                     745                     747                     749   \n",
       "4                     799                     791                     793   \n",
       "\n",
       "   arithmetic_sequences_6  arithmetic_sequences_7  arithmetic_sequences_8  \\\n",
       "0                     641                     643                     638   \n",
       "1                     891                     908                     925   \n",
       "2                     554                     555                     551   \n",
       "3                     751                     753                     755   \n",
       "4                     785                     787                     779   \n",
       "\n",
       "   arithmetic_sequences_9  ...  arithmetic_sequences_90  \\\n",
       "0                     640  ...                      515   \n",
       "1                     942  ...                     2319   \n",
       "2                     552  ...                      428   \n",
       "3                     757  ...                      919   \n",
       "4                     781  ...                      533   \n",
       "\n",
       "   arithmetic_sequences_91  arithmetic_sequences_92  arithmetic_sequences_93  \\\n",
       "0                      517                      512                      514   \n",
       "1                     2336                     2353                     2370   \n",
       "2                      429                      425                      426   \n",
       "3                      921                      923                      925   \n",
       "4                      535                      527                      529   \n",
       "\n",
       "   arithmetic_sequences_94  arithmetic_sequences_95  arithmetic_sequences_96  \\\n",
       "0                      509                      511                      506   \n",
       "1                     2387                     2404                     2421   \n",
       "2                      422                      423                      419   \n",
       "3                      927                      929                      931   \n",
       "4                      521                      523                      515   \n",
       "\n",
       "   arithmetic_sequences_97  arithmetic_sequences_98  arithmetic_sequences_99  \n",
       "0                      508                      503                      505  \n",
       "1                     2438                     2455                     2472  \n",
       "2                      420                      416                      417  \n",
       "3                      933                      935                      937  \n",
       "4                      517                      509                      511  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arithmetic_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93718360",
   "metadata": {},
   "source": [
    "### Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e98fabcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Normalize data column-wise\n",
    "normalized_data = scaler.fit_transform(arithmetic_df)\n",
    "\n",
    "# Convert back to a DataFrame for convenience\n",
    "arithmetic_df_normalized = pd.DataFrame(normalized_data, columns=arithmetic_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b40bb21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arithmetic_sequences_0</th>\n",
       "      <th>arithmetic_sequences_1</th>\n",
       "      <th>arithmetic_sequences_2</th>\n",
       "      <th>arithmetic_sequences_3</th>\n",
       "      <th>arithmetic_sequences_4</th>\n",
       "      <th>arithmetic_sequences_5</th>\n",
       "      <th>arithmetic_sequences_6</th>\n",
       "      <th>arithmetic_sequences_7</th>\n",
       "      <th>arithmetic_sequences_8</th>\n",
       "      <th>arithmetic_sequences_9</th>\n",
       "      <th>...</th>\n",
       "      <th>arithmetic_sequences_90</th>\n",
       "      <th>arithmetic_sequences_91</th>\n",
       "      <th>arithmetic_sequences_92</th>\n",
       "      <th>arithmetic_sequences_93</th>\n",
       "      <th>arithmetic_sequences_94</th>\n",
       "      <th>arithmetic_sequences_95</th>\n",
       "      <th>arithmetic_sequences_96</th>\n",
       "      <th>arithmetic_sequences_97</th>\n",
       "      <th>arithmetic_sequences_98</th>\n",
       "      <th>arithmetic_sequences_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.300</td>\n",
       "      <td>0.289683</td>\n",
       "      <td>0.268627</td>\n",
       "      <td>0.261628</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.234848</td>\n",
       "      <td>0.231193</td>\n",
       "      <td>0.225045</td>\n",
       "      <td>0.223199</td>\n",
       "      <td>0.214041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094495</td>\n",
       "      <td>0.093431</td>\n",
       "      <td>0.094317</td>\n",
       "      <td>0.093275</td>\n",
       "      <td>0.094146</td>\n",
       "      <td>0.093124</td>\n",
       "      <td>0.093981</td>\n",
       "      <td>0.092978</td>\n",
       "      <td>0.093822</td>\n",
       "      <td>0.092838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.578</td>\n",
       "      <td>0.595238</td>\n",
       "      <td>0.613725</td>\n",
       "      <td>0.631783</td>\n",
       "      <td>0.649425</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.689908</td>\n",
       "      <td>0.705989</td>\n",
       "      <td>0.727592</td>\n",
       "      <td>0.731164</td>\n",
       "      <td>...</td>\n",
       "      <td>0.835661</td>\n",
       "      <td>0.835577</td>\n",
       "      <td>0.836356</td>\n",
       "      <td>0.836269</td>\n",
       "      <td>0.837025</td>\n",
       "      <td>0.836935</td>\n",
       "      <td>0.837670</td>\n",
       "      <td>0.837577</td>\n",
       "      <td>0.838291</td>\n",
       "      <td>0.838196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.126</td>\n",
       "      <td>0.115079</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.091085</td>\n",
       "      <td>0.074713</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.071560</td>\n",
       "      <td>0.065336</td>\n",
       "      <td>0.070299</td>\n",
       "      <td>0.063356</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058751</td>\n",
       "      <td>0.057528</td>\n",
       "      <td>0.059250</td>\n",
       "      <td>0.058046</td>\n",
       "      <td>0.059731</td>\n",
       "      <td>0.058546</td>\n",
       "      <td>0.060194</td>\n",
       "      <td>0.059028</td>\n",
       "      <td>0.060641</td>\n",
       "      <td>0.059492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.478</td>\n",
       "      <td>0.466270</td>\n",
       "      <td>0.456863</td>\n",
       "      <td>0.447674</td>\n",
       "      <td>0.438697</td>\n",
       "      <td>0.429924</td>\n",
       "      <td>0.433028</td>\n",
       "      <td>0.424682</td>\n",
       "      <td>0.428822</td>\n",
       "      <td>0.414384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260477</td>\n",
       "      <td>0.258262</td>\n",
       "      <td>0.259976</td>\n",
       "      <td>0.257806</td>\n",
       "      <td>0.259494</td>\n",
       "      <td>0.257367</td>\n",
       "      <td>0.259029</td>\n",
       "      <td>0.256944</td>\n",
       "      <td>0.258581</td>\n",
       "      <td>0.256537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.606</td>\n",
       "      <td>0.593254</td>\n",
       "      <td>0.562745</td>\n",
       "      <td>0.552326</td>\n",
       "      <td>0.522989</td>\n",
       "      <td>0.513258</td>\n",
       "      <td>0.495413</td>\n",
       "      <td>0.486388</td>\n",
       "      <td>0.471002</td>\n",
       "      <td>0.455479</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101890</td>\n",
       "      <td>0.100775</td>\n",
       "      <td>0.100363</td>\n",
       "      <td>0.099279</td>\n",
       "      <td>0.098892</td>\n",
       "      <td>0.097839</td>\n",
       "      <td>0.097476</td>\n",
       "      <td>0.096451</td>\n",
       "      <td>0.096110</td>\n",
       "      <td>0.095112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   arithmetic_sequences_0  arithmetic_sequences_1  arithmetic_sequences_2  \\\n",
       "0                   0.300                0.289683                0.268627   \n",
       "1                   0.578                0.595238                0.613725   \n",
       "2                   0.126                0.115079                0.098039   \n",
       "3                   0.478                0.466270                0.456863   \n",
       "4                   0.606                0.593254                0.562745   \n",
       "\n",
       "   arithmetic_sequences_3  arithmetic_sequences_4  arithmetic_sequences_5  \\\n",
       "0                0.261628                0.241379                0.234848   \n",
       "1                0.631783                0.649425                0.666667   \n",
       "2                0.091085                0.074713                0.068182   \n",
       "3                0.447674                0.438697                0.429924   \n",
       "4                0.552326                0.522989                0.513258   \n",
       "\n",
       "   arithmetic_sequences_6  arithmetic_sequences_7  arithmetic_sequences_8  \\\n",
       "0                0.231193                0.225045                0.223199   \n",
       "1                0.689908                0.705989                0.727592   \n",
       "2                0.071560                0.065336                0.070299   \n",
       "3                0.433028                0.424682                0.428822   \n",
       "4                0.495413                0.486388                0.471002   \n",
       "\n",
       "   arithmetic_sequences_9  ...  arithmetic_sequences_90  \\\n",
       "0                0.214041  ...                 0.094495   \n",
       "1                0.731164  ...                 0.835661   \n",
       "2                0.063356  ...                 0.058751   \n",
       "3                0.414384  ...                 0.260477   \n",
       "4                0.455479  ...                 0.101890   \n",
       "\n",
       "   arithmetic_sequences_91  arithmetic_sequences_92  arithmetic_sequences_93  \\\n",
       "0                 0.093431                 0.094317                 0.093275   \n",
       "1                 0.835577                 0.836356                 0.836269   \n",
       "2                 0.057528                 0.059250                 0.058046   \n",
       "3                 0.258262                 0.259976                 0.257806   \n",
       "4                 0.100775                 0.100363                 0.099279   \n",
       "\n",
       "   arithmetic_sequences_94  arithmetic_sequences_95  arithmetic_sequences_96  \\\n",
       "0                 0.094146                 0.093124                 0.093981   \n",
       "1                 0.837025                 0.836935                 0.837670   \n",
       "2                 0.059731                 0.058546                 0.060194   \n",
       "3                 0.259494                 0.257367                 0.259029   \n",
       "4                 0.098892                 0.097839                 0.097476   \n",
       "\n",
       "   arithmetic_sequences_97  arithmetic_sequences_98  arithmetic_sequences_99  \n",
       "0                 0.092978                 0.093822                 0.092838  \n",
       "1                 0.837577                 0.838291                 0.838196  \n",
       "2                 0.059028                 0.060641                 0.059492  \n",
       "3                 0.256944                 0.258581                 0.256537  \n",
       "4                 0.096451                 0.096110                 0.095112  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arithmetic_df_normalized.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0da7282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = np.array(arithmetic_df_normalized)\n",
    "# Reshape the data for the RNN\n",
    "inputs = np.expand_dims(np.array(arithmetic_df_normalized), axis=-1)  # Add a dimension for features\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e645335",
   "metadata": {},
   "source": [
    "## Arithemetic Sequence RNN Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1794db96",
   "metadata": {},
   "source": [
    "### Input Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72dd7590",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputLayer:\n",
    "    def __init__(self, inputs: np.ndarray, hidden_size: int) -> None:\n",
    "        self.inputs = inputs  # Shape: [sequence_length, num_features, 1]\n",
    "        self.U = np.random.randn(hidden_size, inputs.shape[1]) * np.sqrt(1 / inputs.shape[1])\n",
    "        self.delta_U = np.zeros_like(self.U)\n",
    "\n",
    "    def get_input(self, time_step: int) -> np.ndarray:\n",
    "        return self.inputs[time_step].reshape(1, -1)  # Shape: [1, num_features]\n",
    "\n",
    "    def weighted_sum(self, time_step: int) -> np.ndarray:\n",
    "        return self.U @ self.get_input(time_step)\n",
    "\n",
    "    def calculate_deltas_per_step(self, time_step: int, delta_weighted_sum: np.ndarray) -> None:\n",
    "        self.delta_U += delta_weighted_sum @ self.get_input(time_step).T\n",
    "        max_norm = 1.0\n",
    "        grad_norm = np.linalg.norm(self.delta_U)\n",
    "        if grad_norm > max_norm:\n",
    "            self.delta_U *= max_norm / grad_norm\n",
    "\n",
    "    def update_weights_and_bias(self, learning_rate: float) -> None:\n",
    "        self.U -= learning_rate * self.delta_U\n",
    "        self.delta_U = np.zeros_like(self.U)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b803dd6f",
   "metadata": {},
   "source": [
    "### Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d13df2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer:\n",
    "    def __init__(self, sequence_length: int, size: int, dropout_rate: float = 0.0) -> None:\n",
    "        self.W = np.random.randn(size, size) * np.sqrt(1 / size)\n",
    "        self.bias = np.zeros((size, 1))\n",
    "        self.states = np.zeros((sequence_length, size, 1))\n",
    "        self.delta_W = np.zeros_like(self.W)\n",
    "        self.delta_bias = np.zeros_like(self.bias)\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def get_state(self, time_step: int) -> np.ndarray:\n",
    "        if time_step < 0:\n",
    "            return np.zeros_like(self.states[0])\n",
    "        return self.states[time_step]\n",
    "\n",
    "    def set_state(self, time_step: int, state: np.ndarray) -> None:\n",
    "        self.states[time_step] = state\n",
    "\n",
    "    def activate(self, weighted_input: np.ndarray, time_step: int, training: bool = True) -> np.ndarray:\n",
    "        previous_hidden_state = self.get_state(time_step - 1)\n",
    "        activation = np.tanh(weighted_input + self.W @ previous_hidden_state + self.bias)\n",
    "\n",
    "        if self.dropout_rate > 0 and training:\n",
    "            dropout_mask = np.random.binomial(1, 1 - self.dropout_rate, activation.shape)\n",
    "            activation *= dropout_mask\n",
    "\n",
    "        self.set_state(time_step, activation)\n",
    "        return activation\n",
    "\n",
    "    def calculate_deltas_per_step(self, time_step: int, delta_output: np.ndarray) -> np.ndarray:\n",
    "        delta_activation = delta_output\n",
    "        delta_weighted_sum = delta_activation * (1 - self.get_state(time_step) ** 2)\n",
    "        \n",
    "        # Ensure proper shape for weight updates\n",
    "        prev_hidden_state = self.get_state(time_step - 1).reshape(-1, 1)\n",
    "        self.delta_W += delta_weighted_sum @ prev_hidden_state.T  # Update W with outer product\n",
    "        self.delta_bias += delta_weighted_sum\n",
    "        return self.W.T @ delta_weighted_sum\n",
    "\n",
    "    def update_weights_and_bias(self, learning_rate: float) -> None:\n",
    "        self.W -= learning_rate * self.delta_W\n",
    "        self.bias -= learning_rate * self.delta_bias\n",
    "        self.delta_W = np.zeros_like(self.W)\n",
    "        self.delta_bias = np.zeros_like(self.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e163bd3f",
   "metadata": {},
   "source": [
    "### Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98795048",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputLayer:\n",
    "    def __init__(self, size: int, hidden_size: int, sequence_length: int) -> None:\n",
    "        # Weight initialization (Xavier initialization)\n",
    "        self.V = np.random.randn(size, hidden_size) * np.sqrt(1 / hidden_size)\n",
    "        self.bias = np.zeros((size, 1))\n",
    "        self.states = np.zeros((sequence_length, size, 1))\n",
    "        self.delta_V = np.zeros_like(self.V)\n",
    "        self.delta_bias = np.zeros_like(self.bias)\n",
    "\n",
    "    def get_state(self, time_step: int) -> np.ndarray:\n",
    "        return self.states[time_step]\n",
    "\n",
    "    def set_state(self, time_step: int, prediction: np.ndarray) -> None:\n",
    "        self.states[time_step] = prediction\n",
    "\n",
    "    def predict(self, hidden_state: np.ndarray, time_step: int) -> np.ndarray:\n",
    "        # Linear activation (no softmax for regression)\n",
    "        output = self.V @ hidden_state + self.bias\n",
    "        self.set_state(time_step, output)  # Directly store linear output\n",
    "        return output\n",
    "\n",
    "    def calculate_deltas_per_step(\n",
    "        self, expected: np.ndarray, hidden_state: np.ndarray, time_step: int\n",
    "    ) -> np.ndarray:\n",
    "        # Compute the delta for regression (MSE derivative)\n",
    "        delta_output = 2 * (self.get_state(time_step) - expected) / expected.size\n",
    "\n",
    "        # Accumulate gradients for weights and biases\n",
    "        self.delta_V += delta_output @ hidden_state.T\n",
    "        self.delta_bias += delta_output\n",
    "\n",
    "        # Return the gradient with respect to the hidden state\n",
    "        return self.V.T @ delta_output\n",
    "\n",
    "    def update_weights_and_bias(self, learning_rate: float, clip_value: float = 5.0) -> None:\n",
    "        # Gradient clipping to avoid exploding gradients\n",
    "        grad_norm_V = np.linalg.norm(self.delta_V)\n",
    "        grad_norm_bias = np.linalg.norm(self.delta_bias)\n",
    "\n",
    "        if grad_norm_V > clip_value:\n",
    "            self.delta_V *= clip_value / grad_norm_V\n",
    "\n",
    "        if grad_norm_bias > clip_value:\n",
    "            self.delta_bias *= clip_value / grad_norm_bias\n",
    "\n",
    "        # Apply gradient descent\n",
    "        self.V -= learning_rate * self.delta_V\n",
    "        self.bias -= learning_rate * self.delta_bias\n",
    "\n",
    "        # Reset gradients after the update\n",
    "        self.delta_V = np.zeros_like(self.V)\n",
    "        self.delta_bias = np.zeros_like(self.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795ca683",
   "metadata": {},
   "source": [
    "### Arithemetic Sequence RNN Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5572d4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import time\n",
    "\n",
    "class ArithmeticSequenceRNN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        hidden_size: int,\n",
    "        sequence_length: int,\n",
    "        learning_rate: float,\n",
    "        num_layers: int = 1,\n",
    "        dropout_rate: float = 0.0,\n",
    "    ):\n",
    "        if num_layers <= 0:\n",
    "            raise ValueError(\"Number of layers must be greater than 0.\")\n",
    "\n",
    "        self.input_layer = None  # Initialized later during training\n",
    "        self.hidden_layers = [\n",
    "            HiddenLayer(sequence_length=sequence_length, size=hidden_size, dropout_rate=dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        self.output_layer = OutputLayer(size=vocab_size, hidden_size=hidden_size, sequence_length=sequence_length)\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def feed_forward(self, inputs: np.ndarray, training: bool = True) -> OutputLayer:\n",
    "        start_time = time.time()\n",
    "        self.input_layer = InputLayer(inputs, hidden_size=self.hidden_layers[0].W.shape[0])\n",
    "        \n",
    "        for time_step in range(inputs.shape[0]):\n",
    "            weighted_input = self.input_layer.weighted_sum(time_step)\n",
    "\n",
    "            # Pass through hidden layers\n",
    "            activation = self.hidden_layers[0].activate(weighted_input, time_step, training)\n",
    "            for layer_idx in range(1, len(self.hidden_layers)):\n",
    "                activation = self.hidden_layers[layer_idx].activate(activation, time_step, training)\n",
    "\n",
    "            # Pass through output layer\n",
    "            self.output_layer.predict(activation, time_step)\n",
    "        #print(f\"Feed Forward Time: {time.time() - start_time:.4f} seconds\")\n",
    "        return self.output_layer\n",
    "\n",
    "    def backpropagation(self, expected: np.ndarray) -> None:\n",
    "        for time_step in reversed(range(expected.shape[0])):\n",
    "            # Backpropagation through output layer\n",
    "            delta_output = self.output_layer.calculate_deltas_per_step(\n",
    "                expected[time_step], self.hidden_layers[-1].get_state(time_step), time_step\n",
    "            )\n",
    "\n",
    "            # Backpropagate through hidden layers\n",
    "            delta_hidden = delta_output\n",
    "            for layer_idx in reversed(range(len(self.hidden_layers))):\n",
    "                delta_hidden = self.hidden_layers[layer_idx].calculate_deltas_per_step(time_step, delta_hidden)\n",
    "\n",
    "            # Backpropagation through input layer\n",
    "            self.input_layer.calculate_deltas_per_step(time_step, delta_hidden)\n",
    "        \n",
    "        # Update all weights and biases\n",
    "        self.output_layer.update_weights_and_bias(self.learning_rate)\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            hidden_layer.update_weights_and_bias(self.learning_rate)\n",
    "        self.input_layer.update_weights_and_bias(self.learning_rate)\n",
    "\n",
    "    def validate_input_shapes(self, inputs: np.ndarray, expected: np.ndarray) -> None:\n",
    "        if len(inputs) != len(expected):\n",
    "            raise ValueError(\"The number of input sequences must match the number of expected outputs.\")\n",
    "\n",
    "    def train(self, inputs: np.ndarray, expected: np.ndarray, epochs: int, validation_data=None) -> tuple:\n",
    "        self.validate_input_shapes(inputs, expected)\n",
    "        training_losses = []\n",
    "        validation_losses = []\n",
    "\n",
    "        batch_size = 10  # Adjust for efficiency\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "            epoch_loss = 0\n",
    "\n",
    "            for i in range(0, len(inputs), batch_size):\n",
    "                batch_inputs = inputs[i:i + batch_size]\n",
    "                batch_expected = expected[i:i + batch_size]\n",
    "\n",
    "                for input_seq, expected_seq in zip(batch_inputs, batch_expected):\n",
    "                    self.feed_forward(input_seq, training=True)\n",
    "                    self.backpropagation(expected_seq)\n",
    "\n",
    "                    # Print predictions after training\n",
    "                    predictions = [self.output_layer.get_state(t) for t in range(input_seq.shape[0])]\n",
    "                    #print(f\"Predictions for input sequence {i}: {predictions}\")\n",
    "\n",
    "                    # Calculate loss for the sequence (MSE for regression)\n",
    "                    loss = self.loss(predictions, expected_seq)\n",
    "                    epoch_loss += loss\n",
    "\n",
    "            avg_train_loss = epoch_loss / len(inputs)\n",
    "            training_losses.append(avg_train_loss)\n",
    "            print(f\"Training Loss: {avg_train_loss}\")\n",
    "\n",
    "            if validation_data is not None:\n",
    "                val_loss = self.evaluate(*validation_data)\n",
    "                validation_losses.append(val_loss)\n",
    "                print(f\"Validation Loss: {val_loss}\")\n",
    "\n",
    "        return training_losses, validation_losses\n",
    "\n",
    "    def evaluate(self, inputs: np.ndarray, expected: np.ndarray) -> float:\n",
    "        self.validate_input_shapes(inputs, expected)\n",
    "        total_loss = 0\n",
    "\n",
    "        for sequence_idx, input_seq in enumerate(inputs):\n",
    "            self.feed_forward(input_seq, training=False)\n",
    "            predicted = [self.output_layer.get_state(t) for t in range(input_seq.shape[0])]\n",
    "            total_loss += self.loss(predicted, expected[sequence_idx])\n",
    "\n",
    "        return total_loss / len(inputs)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss(predicted: List[np.ndarray], expected: List[np.ndarray]) -> float:\n",
    "        # Mean Squared Error for regression\n",
    "        return sum(np.mean((y_pred - y_true) ** 2) for y_pred, y_true in zip(predicted, expected)) / len(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96af048",
   "metadata": {},
   "source": [
    "### Split the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b6c5a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (70, 100, 1)\n",
      "Validation Data Shape: (10, 100, 1)\n",
      "Test Data Shape: (20, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "train_data, temp_data = train_test_split(inputs, test_size=0.3, random_state=42)\n",
    "validation_data, test_data = train_test_split(temp_data, test_size=0.66, random_state=42)\n",
    "\n",
    "print(f\"Training Data Shape: {train_data.shape}\")\n",
    "print(f\"Validation Data Shape: {validation_data.shape}\")\n",
    "print(f\"Test Data Shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5375a15",
   "metadata": {},
   "source": [
    "### Grid Search to Find Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09628edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing combination 1/48: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'sequence_length': 6, 'dropout_rate': 0.1}\n",
      "Truncated Train Data Shape: (70, 6, 1)\n",
      "Truncated Validation Data Shape: (10, 6, 1)\n",
      "    Trial 1/2 for combination 1/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.5769867639653317\n",
      "Validation Loss: 0.46069467001073877\n",
      "Epoch 2/100\n",
      "Training Loss: 0.4338450274135116\n",
      "Validation Loss: 0.3740409180552511\n",
      "Epoch 3/100\n",
      "Training Loss: 0.4703743054631974\n",
      "Validation Loss: 0.3717537255742585\n",
      "Epoch 4/100\n",
      "Training Loss: 0.42752104035214133\n",
      "Validation Loss: 0.2596660271536561\n",
      "Epoch 5/100\n",
      "Training Loss: 0.3709120913098654\n",
      "Validation Loss: 0.22227741753869545\n",
      "Epoch 6/100\n",
      "Training Loss: 0.3900725705993953\n",
      "Validation Loss: 0.23059572405826168\n",
      "Epoch 7/100\n",
      "Training Loss: 0.3931309353765472\n",
      "Validation Loss: 0.23489674933642637\n",
      "Epoch 8/100\n",
      "Training Loss: 0.34140540627929744\n",
      "Validation Loss: 0.0863957839002165\n",
      "Epoch 9/100\n",
      "Training Loss: 0.2560686424757877\n",
      "Validation Loss: 0.2085324015432357\n",
      "Epoch 10/100\n",
      "Training Loss: 0.3203912925760855\n",
      "Validation Loss: 0.17846250573123723\n",
      "Epoch 11/100\n",
      "Training Loss: 0.3073842668140531\n",
      "Validation Loss: 0.21244096994804157\n",
      "Epoch 12/100\n",
      "Training Loss: 0.30955151310959217\n",
      "Validation Loss: 0.16510955432195643\n",
      "Epoch 13/100\n",
      "Training Loss: 0.24385311279619099\n",
      "Validation Loss: 0.17016995857498435\n",
      "Epoch 14/100\n",
      "Training Loss: 0.2632163960821637\n",
      "Validation Loss: 0.3194772435570778\n",
      "Epoch 15/100\n",
      "Training Loss: 0.20171226397071104\n",
      "Validation Loss: 0.19218516830181517\n",
      "Epoch 16/100\n",
      "Training Loss: 0.23965040772076018\n",
      "Validation Loss: 0.3024423305617605\n",
      "Epoch 17/100\n",
      "Training Loss: 0.2519866567548223\n",
      "Validation Loss: 0.4462218179336827\n",
      "Epoch 18/100\n",
      "Training Loss: 0.2689496864341638\n",
      "Validation Loss: 0.21072924334088308\n",
      "Epoch 19/100\n",
      "Training Loss: 0.1852098029142263\n",
      "Validation Loss: 0.20557412993432683\n",
      "Epoch 20/100\n",
      "Training Loss: 0.2532569731806943\n",
      "Validation Loss: 0.2088862082017579\n",
      "Epoch 21/100\n",
      "Training Loss: 0.27895942442292143\n",
      "Validation Loss: 0.15201962074427292\n",
      "Epoch 22/100\n",
      "Training Loss: 0.30524832099155524\n",
      "Validation Loss: 0.3465856086799674\n",
      "Epoch 23/100\n",
      "Training Loss: 0.1812235356300492\n",
      "Validation Loss: 0.1288015074568122\n",
      "Epoch 24/100\n",
      "Training Loss: 0.20361154409548837\n",
      "Validation Loss: 0.20818504157914314\n",
      "Epoch 25/100\n",
      "Training Loss: 0.21348402450435952\n",
      "Validation Loss: 0.16856085033828244\n",
      "Epoch 26/100\n",
      "Training Loss: 0.21419961354991293\n",
      "Validation Loss: 0.08709602242938681\n",
      "Epoch 27/100\n",
      "Training Loss: 0.23637062437804773\n",
      "Validation Loss: 0.14869456017996244\n",
      "Epoch 28/100\n",
      "Training Loss: 0.25077177305321646\n",
      "Validation Loss: 0.18943178863161586\n",
      "Epoch 29/100\n",
      "Training Loss: 0.21041184529233198\n",
      "Validation Loss: 0.17579964842335966\n",
      "Epoch 30/100\n",
      "Training Loss: 0.17708307775489848\n",
      "Validation Loss: 0.11178796220046261\n",
      "Epoch 31/100\n",
      "Training Loss: 0.18706431374774826\n",
      "Validation Loss: 0.1307824635560414\n",
      "Epoch 32/100\n",
      "Training Loss: 0.21138472357643528\n",
      "Validation Loss: 0.1911261063347508\n",
      "Epoch 33/100\n",
      "Training Loss: 0.24873835616438145\n",
      "Validation Loss: 0.1360139602501687\n",
      "Epoch 34/100\n",
      "Training Loss: 0.22150131542714055\n",
      "Validation Loss: 0.17317148376590275\n",
      "Epoch 35/100\n",
      "Training Loss: 0.21252384598271057\n",
      "Validation Loss: 0.22238646326448577\n",
      "Epoch 36/100\n",
      "Training Loss: 0.1792697619816832\n",
      "Validation Loss: 0.19100474225465838\n",
      "Epoch 37/100\n",
      "Training Loss: 0.1778318400999268\n",
      "Validation Loss: 0.21367939300689603\n",
      "Epoch 38/100\n",
      "Training Loss: 0.21369083515173753\n",
      "Validation Loss: 0.16807846308495772\n",
      "Epoch 39/100\n",
      "Training Loss: 0.18531230239869081\n",
      "Validation Loss: 0.14125534348885238\n",
      "Epoch 40/100\n",
      "Training Loss: 0.2131892105641156\n",
      "Validation Loss: 0.09013319221697738\n",
      "Epoch 41/100\n",
      "Training Loss: 0.18208138859564513\n",
      "Validation Loss: 0.18354655268524472\n",
      "Epoch 42/100\n",
      "Training Loss: 0.16196816553574456\n",
      "Validation Loss: 0.24612018162660038\n",
      "Epoch 43/100\n",
      "Training Loss: 0.14472714969233333\n",
      "Validation Loss: 0.10380740995972271\n",
      "Epoch 44/100\n",
      "Training Loss: 0.1974697708517144\n",
      "Validation Loss: 0.08959572334897739\n",
      "Epoch 45/100\n",
      "Training Loss: 0.169125381201737\n",
      "Validation Loss: 0.13075208815209977\n",
      "Epoch 46/100\n",
      "Training Loss: 0.1631980804357038\n",
      "Validation Loss: 0.11804038923897642\n",
      "Epoch 47/100\n",
      "Training Loss: 0.18403655337885794\n",
      "Validation Loss: 0.0878510006295899\n",
      "Epoch 48/100\n",
      "Training Loss: 0.18564748753945962\n",
      "Validation Loss: 0.14157866285057946\n",
      "Epoch 49/100\n",
      "Training Loss: 0.17827835428835484\n",
      "Validation Loss: 0.1598060964389805\n",
      "Epoch 50/100\n",
      "Training Loss: 0.1817756628456979\n",
      "Validation Loss: 0.11596413383788425\n",
      "Epoch 51/100\n",
      "Training Loss: 0.14492716144658416\n",
      "Validation Loss: 0.11754490151937262\n",
      "Epoch 52/100\n",
      "Training Loss: 0.16233555848590117\n",
      "Validation Loss: 0.16014898734587352\n",
      "Epoch 53/100\n",
      "Training Loss: 0.14668251987528655\n",
      "Validation Loss: 0.09616738178737727\n",
      "Epoch 54/100\n",
      "Training Loss: 0.1607752098900453\n",
      "Validation Loss: 0.1588050913560034\n",
      "Epoch 55/100\n",
      "Training Loss: 0.16372093013249442\n",
      "Validation Loss: 0.13797936490436755\n",
      "Epoch 56/100\n",
      "Training Loss: 0.1892257346694619\n",
      "Validation Loss: 0.14957966882573961\n",
      "Epoch 57/100\n",
      "Training Loss: 0.16487968330581104\n",
      "Validation Loss: 0.10867081494923918\n",
      "Epoch 58/100\n",
      "Training Loss: 0.16050706192399275\n",
      "Validation Loss: 0.17066052503723453\n",
      "Epoch 59/100\n",
      "Training Loss: 0.12265120495346066\n",
      "Validation Loss: 0.08734632769663894\n",
      "Epoch 60/100\n",
      "Training Loss: 0.1551832187877394\n",
      "Validation Loss: 0.09314956234142885\n",
      "Epoch 61/100\n",
      "Training Loss: 0.12772906099237893\n",
      "Validation Loss: 0.22137136045658673\n",
      "Epoch 62/100\n",
      "Training Loss: 0.14750082453205177\n",
      "Validation Loss: 0.13872581427433478\n",
      "Epoch 63/100\n",
      "Training Loss: 0.14484189981953752\n",
      "Validation Loss: 0.12864826450701633\n",
      "Epoch 64/100\n",
      "Training Loss: 0.15194091219490594\n",
      "Validation Loss: 0.1370196210560728\n",
      "Epoch 65/100\n",
      "Training Loss: 0.14768677167799238\n",
      "Validation Loss: 0.20113939084938667\n",
      "Epoch 66/100\n",
      "Training Loss: 0.15616030038905843\n",
      "Validation Loss: 0.12289311358927639\n",
      "Epoch 67/100\n",
      "Training Loss: 0.17386836748990556\n",
      "Validation Loss: 0.0894567292527707\n",
      "Epoch 68/100\n",
      "Training Loss: 0.13956008253174781\n",
      "Validation Loss: 0.17974521712624286\n",
      "Epoch 69/100\n",
      "Training Loss: 0.1785464197823495\n",
      "Validation Loss: 0.1362663086806105\n",
      "Epoch 70/100\n",
      "Training Loss: 0.15100575194340127\n",
      "Validation Loss: 0.10513823359048222\n",
      "Epoch 71/100\n",
      "Training Loss: 0.16199196381981135\n",
      "Validation Loss: 0.09925682874290329\n",
      "Epoch 72/100\n",
      "Training Loss: 0.1840789481746191\n",
      "Validation Loss: 0.09695027257594549\n",
      "Epoch 73/100\n",
      "Training Loss: 0.1730741220959721\n",
      "Validation Loss: 0.10737063533478175\n",
      "Epoch 74/100\n",
      "Training Loss: 0.14586191631934026\n",
      "Validation Loss: 0.07165226586439852\n",
      "Epoch 75/100\n",
      "Training Loss: 0.1231108408083739\n",
      "Validation Loss: 0.1028952183919459\n",
      "Epoch 76/100\n",
      "Training Loss: 0.17624250875883027\n",
      "Validation Loss: 0.1748618641964618\n",
      "Epoch 77/100\n",
      "Training Loss: 0.14507500263558765\n",
      "Validation Loss: 0.16607840427244555\n",
      "Epoch 78/100\n",
      "Training Loss: 0.1580867422881652\n",
      "Validation Loss: 0.08959388220852695\n",
      "Epoch 79/100\n",
      "Training Loss: 0.17047742648477893\n",
      "Validation Loss: 0.13238496247417456\n",
      "Epoch 80/100\n",
      "Training Loss: 0.11264539492618496\n",
      "Validation Loss: 0.07995536408708728\n",
      "Epoch 81/100\n",
      "Training Loss: 0.15799960571830282\n",
      "Validation Loss: 0.062249758347694174\n",
      "Epoch 82/100\n",
      "Training Loss: 0.13532112618825043\n",
      "Validation Loss: 0.11051268082073398\n",
      "Epoch 83/100\n",
      "Training Loss: 0.1409173458628031\n",
      "Validation Loss: 0.1276685147190677\n",
      "Epoch 84/100\n",
      "Training Loss: 0.1562955084986283\n",
      "Validation Loss: 0.09689033394477009\n",
      "Epoch 85/100\n",
      "Training Loss: 0.13362193576639347\n",
      "Validation Loss: 0.10227922256068689\n",
      "Epoch 86/100\n",
      "Training Loss: 0.12392077357654026\n",
      "Validation Loss: 0.07658688402385772\n",
      "Epoch 87/100\n",
      "Training Loss: 0.1173551531944441\n",
      "Validation Loss: 0.18428699066488607\n",
      "Epoch 88/100\n",
      "Training Loss: 0.10645719876502141\n",
      "Validation Loss: 0.13789794798827665\n",
      "Epoch 89/100\n",
      "Training Loss: 0.15646487542047124\n",
      "Validation Loss: 0.07831298455997884\n",
      "Epoch 90/100\n",
      "Training Loss: 0.13694531231795468\n",
      "Validation Loss: 0.06473868887496884\n",
      "Epoch 91/100\n",
      "Training Loss: 0.12392119873857979\n",
      "Validation Loss: 0.123113030038233\n",
      "Epoch 92/100\n",
      "Training Loss: 0.14619040377216283\n",
      "Validation Loss: 0.07982802800858038\n",
      "Epoch 93/100\n",
      "Training Loss: 0.1362686702289757\n",
      "Validation Loss: 0.12723592707738657\n",
      "Epoch 94/100\n",
      "Training Loss: 0.14217826627300484\n",
      "Validation Loss: 0.06289105826664881\n",
      "Epoch 95/100\n",
      "Training Loss: 0.14804296552213092\n",
      "Validation Loss: 0.11596545707111675\n",
      "Epoch 96/100\n",
      "Training Loss: 0.13036076933762544\n",
      "Validation Loss: 0.08780622196595796\n",
      "Epoch 97/100\n",
      "Training Loss: 0.10911435711223018\n",
      "Validation Loss: 0.059922725456773865\n",
      "Epoch 98/100\n",
      "Training Loss: 0.12548530457663223\n",
      "Validation Loss: 0.08497379384348278\n",
      "Epoch 99/100\n",
      "Training Loss: 0.13474784999078382\n",
      "Validation Loss: 0.06574578527291602\n",
      "Epoch 100/100\n",
      "Training Loss: 0.13729547695175184\n",
      "Validation Loss: 0.1564011128277789\n",
      "    Trial 2/2 for combination 1/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.5556368203705528\n",
      "Validation Loss: 0.738937356703488\n",
      "Epoch 2/100\n",
      "Training Loss: 0.5053774878209414\n",
      "Validation Loss: 0.3093245864847064\n",
      "Epoch 3/100\n",
      "Training Loss: 0.39887329999375265\n",
      "Validation Loss: 0.2800703138934038\n",
      "Epoch 4/100\n",
      "Training Loss: 0.40864062269170837\n",
      "Validation Loss: 0.35292204137969657\n",
      "Epoch 5/100\n",
      "Training Loss: 0.530276901520559\n",
      "Validation Loss: 0.5345527420427764\n",
      "Epoch 6/100\n",
      "Training Loss: 0.3581409234594643\n",
      "Validation Loss: 0.16573472831235103\n",
      "Epoch 7/100\n",
      "Training Loss: 0.44179329443471854\n",
      "Validation Loss: 0.2303211054627671\n",
      "Epoch 8/100\n",
      "Training Loss: 0.365972336903743\n",
      "Validation Loss: 0.22495043359958614\n",
      "Epoch 9/100\n",
      "Training Loss: 0.42828677771088647\n",
      "Validation Loss: 0.3478113199312451\n",
      "Epoch 10/100\n",
      "Training Loss: 0.35812123533576495\n",
      "Validation Loss: 0.1711006416919186\n",
      "Epoch 11/100\n",
      "Training Loss: 0.33751600166061607\n",
      "Validation Loss: 0.31242856591018764\n",
      "Epoch 12/100\n",
      "Training Loss: 0.382830455323216\n",
      "Validation Loss: 0.3727670809522713\n",
      "Epoch 13/100\n",
      "Training Loss: 0.29731178014060866\n",
      "Validation Loss: 0.28367388182278386\n",
      "Epoch 14/100\n",
      "Training Loss: 0.3166042430319325\n",
      "Validation Loss: 0.13591000029451158\n",
      "Epoch 15/100\n",
      "Training Loss: 0.31146900303383124\n",
      "Validation Loss: 0.3432204981474224\n",
      "Epoch 16/100\n",
      "Training Loss: 0.17819534005703008\n",
      "Validation Loss: 0.12263774860970775\n",
      "Epoch 17/100\n",
      "Training Loss: 0.30778974471239906\n",
      "Validation Loss: 0.10548698433752299\n",
      "Epoch 18/100\n",
      "Training Loss: 0.333853848066764\n",
      "Validation Loss: 0.14140116221404364\n",
      "Epoch 19/100\n",
      "Training Loss: 0.27105595312467845\n",
      "Validation Loss: 0.22765368120759727\n",
      "Epoch 20/100\n",
      "Training Loss: 0.2879599907855494\n",
      "Validation Loss: 0.16108426037345963\n",
      "Epoch 21/100\n",
      "Training Loss: 0.28089662185594944\n",
      "Validation Loss: 0.19707879903371825\n",
      "Epoch 22/100\n",
      "Training Loss: 0.27755320590197063\n",
      "Validation Loss: 0.3261734140916591\n",
      "Epoch 23/100\n",
      "Training Loss: 0.27268406625590147\n",
      "Validation Loss: 0.3086346170380957\n",
      "Epoch 24/100\n",
      "Training Loss: 0.21466405973753855\n",
      "Validation Loss: 0.06836489465328667\n",
      "Epoch 25/100\n",
      "Training Loss: 0.1691583625930322\n",
      "Validation Loss: 0.18821817649612183\n",
      "Epoch 26/100\n",
      "Training Loss: 0.21023491116495316\n",
      "Validation Loss: 0.21840224605077713\n",
      "Epoch 27/100\n",
      "Training Loss: 0.264742905963226\n",
      "Validation Loss: 0.27015836907320867\n",
      "Epoch 28/100\n",
      "Training Loss: 0.18900178924532782\n",
      "Validation Loss: 0.2665802878600196\n",
      "Epoch 29/100\n",
      "Training Loss: 0.2001963548120259\n",
      "Validation Loss: 0.08842710079108708\n",
      "Epoch 30/100\n",
      "Training Loss: 0.22020558781033922\n",
      "Validation Loss: 0.22528096203590303\n",
      "Epoch 31/100\n",
      "Training Loss: 0.3077044924935586\n",
      "Validation Loss: 0.08077863625653584\n",
      "Epoch 32/100\n",
      "Training Loss: 0.24075195263672558\n",
      "Validation Loss: 0.14917513054306064\n",
      "Epoch 33/100\n",
      "Training Loss: 0.1893211017924575\n",
      "Validation Loss: 0.2138705956228936\n",
      "Epoch 34/100\n",
      "Training Loss: 0.14873323034086763\n",
      "Validation Loss: 0.099179989654307\n",
      "Epoch 35/100\n",
      "Training Loss: 0.20988298560640894\n",
      "Validation Loss: 0.11115883175548544\n",
      "Epoch 36/100\n",
      "Training Loss: 0.20220548636216804\n",
      "Validation Loss: 0.13064623257381847\n",
      "Epoch 37/100\n",
      "Training Loss: 0.16999958895902864\n",
      "Validation Loss: 0.19224818593055554\n",
      "Epoch 38/100\n",
      "Training Loss: 0.16071570631955753\n",
      "Validation Loss: 0.16771370702835295\n",
      "Epoch 39/100\n",
      "Training Loss: 0.18483648136647257\n",
      "Validation Loss: 0.1468165892203769\n",
      "Epoch 40/100\n",
      "Training Loss: 0.20028737563511\n",
      "Validation Loss: 0.08681077834473792\n",
      "Epoch 41/100\n",
      "Training Loss: 0.16597608520036977\n",
      "Validation Loss: 0.10240085759693729\n",
      "Epoch 42/100\n",
      "Training Loss: 0.1618370732897827\n",
      "Validation Loss: 0.19608525707379015\n",
      "Epoch 43/100\n",
      "Training Loss: 0.21764428785194323\n",
      "Validation Loss: 0.2801597448157148\n",
      "Epoch 44/100\n",
      "Training Loss: 0.1684586198580981\n",
      "Validation Loss: 0.19279105894630824\n",
      "Epoch 45/100\n",
      "Training Loss: 0.2017108596538022\n",
      "Validation Loss: 0.1393526369295383\n",
      "Epoch 46/100\n",
      "Training Loss: 0.2587034210854678\n",
      "Validation Loss: 0.14506058615305953\n",
      "Epoch 47/100\n",
      "Training Loss: 0.2225897589921303\n",
      "Validation Loss: 0.17874954806328452\n",
      "Epoch 48/100\n",
      "Training Loss: 0.20601735061678922\n",
      "Validation Loss: 0.10378047930625996\n",
      "Epoch 49/100\n",
      "Training Loss: 0.20805282263803992\n",
      "Validation Loss: 0.1480672603902819\n",
      "Epoch 50/100\n",
      "Training Loss: 0.20736423321739483\n",
      "Validation Loss: 0.13504384744186942\n",
      "Epoch 51/100\n",
      "Training Loss: 0.17949388339956354\n",
      "Validation Loss: 0.12068835353620003\n",
      "Epoch 52/100\n",
      "Training Loss: 0.19127949389010043\n",
      "Validation Loss: 0.10611196086089472\n",
      "Epoch 53/100\n",
      "Training Loss: 0.203085401574909\n",
      "Validation Loss: 0.10846394029954773\n",
      "Epoch 54/100\n",
      "Training Loss: 0.13127840486371084\n",
      "Validation Loss: 0.21130287010691595\n",
      "Epoch 55/100\n",
      "Training Loss: 0.15832344899604262\n",
      "Validation Loss: 0.14897107065677154\n",
      "Epoch 56/100\n",
      "Training Loss: 0.175052698985274\n",
      "Validation Loss: 0.13449794597126322\n",
      "Epoch 57/100\n",
      "Training Loss: 0.18010394480331013\n",
      "Validation Loss: 0.09984366382290273\n",
      "Epoch 58/100\n",
      "Training Loss: 0.13730916175522306\n",
      "Validation Loss: 0.07618299584246094\n",
      "Epoch 59/100\n",
      "Training Loss: 0.14661224748906543\n",
      "Validation Loss: 0.11422751710443613\n",
      "Epoch 60/100\n",
      "Training Loss: 0.20365691154514756\n",
      "Validation Loss: 0.14999208068218106\n",
      "Epoch 61/100\n",
      "Training Loss: 0.1412448604285483\n",
      "Validation Loss: 0.14654509374993638\n",
      "Epoch 62/100\n",
      "Training Loss: 0.15098573374727234\n",
      "Validation Loss: 0.08614621271502285\n",
      "Epoch 63/100\n",
      "Training Loss: 0.1450694006568074\n",
      "Validation Loss: 0.1265571946910034\n",
      "Epoch 64/100\n",
      "Training Loss: 0.1829025178490953\n",
      "Validation Loss: 0.1940552377056523\n",
      "Epoch 65/100\n",
      "Training Loss: 0.18859531236563765\n",
      "Validation Loss: 0.17333611225780463\n",
      "Epoch 66/100\n",
      "Training Loss: 0.19401715950933338\n",
      "Validation Loss: 0.09171643930860027\n",
      "Epoch 67/100\n",
      "Training Loss: 0.18729253809983923\n",
      "Validation Loss: 0.15095241006245305\n",
      "Epoch 68/100\n",
      "Training Loss: 0.17737003707161555\n",
      "Validation Loss: 0.11379402777471274\n",
      "Epoch 69/100\n",
      "Training Loss: 0.1979218085510091\n",
      "Validation Loss: 0.143104945140149\n",
      "Epoch 70/100\n",
      "Training Loss: 0.14273186048854644\n",
      "Validation Loss: 0.13252528312648412\n",
      "Epoch 71/100\n",
      "Training Loss: 0.154858394987633\n",
      "Validation Loss: 0.14310792708825323\n",
      "Epoch 72/100\n",
      "Training Loss: 0.14996769792420955\n",
      "Validation Loss: 0.1310383948698834\n",
      "Epoch 73/100\n",
      "Training Loss: 0.13929748105554438\n",
      "Validation Loss: 0.11662763158501592\n",
      "Epoch 74/100\n",
      "Training Loss: 0.1366062211840611\n",
      "Validation Loss: 0.12628023413067874\n",
      "Epoch 75/100\n",
      "Training Loss: 0.15976926966005978\n",
      "Validation Loss: 0.127751889660325\n",
      "Epoch 76/100\n",
      "Training Loss: 0.1455124631155011\n",
      "Validation Loss: 0.08153893874538268\n",
      "Epoch 77/100\n",
      "Training Loss: 0.13084244298462874\n",
      "Validation Loss: 0.24248809967551416\n",
      "Epoch 78/100\n",
      "Training Loss: 0.16824674385684094\n",
      "Validation Loss: 0.16780587657894003\n",
      "Epoch 79/100\n",
      "Training Loss: 0.14938742872366254\n",
      "Validation Loss: 0.09512798841072843\n",
      "Epoch 80/100\n",
      "Training Loss: 0.1501006870057911\n",
      "Validation Loss: 0.20025445611775097\n",
      "Epoch 81/100\n",
      "Training Loss: 0.13408700481276195\n",
      "Validation Loss: 0.15718419040766035\n",
      "Epoch 82/100\n",
      "Training Loss: 0.17784201225891563\n",
      "Validation Loss: 0.09810545501824322\n",
      "Epoch 83/100\n",
      "Training Loss: 0.11798327944128421\n",
      "Validation Loss: 0.11114775258931213\n",
      "Epoch 84/100\n",
      "Training Loss: 0.1403132793006396\n",
      "Validation Loss: 0.11776598949671524\n",
      "Epoch 85/100\n",
      "Training Loss: 0.13441311468383266\n",
      "Validation Loss: 0.07874161136827544\n",
      "Epoch 86/100\n",
      "Training Loss: 0.13905576791673133\n",
      "Validation Loss: 0.08080524984587759\n",
      "Epoch 87/100\n",
      "Training Loss: 0.13026441876675332\n",
      "Validation Loss: 0.08547333659331054\n",
      "Epoch 88/100\n",
      "Training Loss: 0.11385253830527886\n",
      "Validation Loss: 0.05087736878013677\n",
      "Epoch 89/100\n",
      "Training Loss: 0.15528571690428952\n",
      "Validation Loss: 0.04393761500625187\n",
      "Epoch 90/100\n",
      "Training Loss: 0.1601306202151095\n",
      "Validation Loss: 0.14242657798586827\n",
      "Epoch 91/100\n",
      "Training Loss: 0.15205903618677447\n",
      "Validation Loss: 0.06468187054708965\n",
      "Epoch 92/100\n",
      "Training Loss: 0.16239577546856748\n",
      "Validation Loss: 0.08325036837423375\n",
      "Epoch 93/100\n",
      "Training Loss: 0.1847316420956385\n",
      "Validation Loss: 0.09924795868901615\n",
      "Epoch 94/100\n",
      "Training Loss: 0.13309649857168665\n",
      "Validation Loss: 0.06827541043317184\n",
      "Epoch 95/100\n",
      "Training Loss: 0.1514610357012782\n",
      "Validation Loss: 0.08362351089401117\n",
      "Epoch 96/100\n",
      "Training Loss: 0.17390959347608367\n",
      "Validation Loss: 0.06027695643606994\n",
      "Epoch 97/100\n",
      "Training Loss: 0.13683701921374203\n",
      "Validation Loss: 0.16843903181715808\n",
      "Epoch 98/100\n",
      "Training Loss: 0.14581992414692924\n",
      "Validation Loss: 0.07421756606919587\n",
      "Epoch 99/100\n",
      "Training Loss: 0.1828547455251249\n",
      "Validation Loss: 0.06416769434532768\n",
      "Epoch 100/100\n",
      "Training Loss: 0.14153143793469297\n",
      "Validation Loss: 0.12771995462419047\n",
      "Combination 1: Avg Training Loss = 0.20744806017299428, Avg Validation Loss = 0.16159786373801938\n",
      "Testing combination 2/48: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'sequence_length': 6, 'dropout_rate': 0.2}\n",
      "Truncated Train Data Shape: (70, 6, 1)\n",
      "Truncated Validation Data Shape: (10, 6, 1)\n",
      "    Trial 1/2 for combination 2/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.5186451827524163\n",
      "Validation Loss: 0.33408757543871304\n",
      "Epoch 2/100\n",
      "Training Loss: 0.41155471275564914\n",
      "Validation Loss: 0.29278563469509633\n",
      "Epoch 3/100\n",
      "Training Loss: 0.44350636366156165\n",
      "Validation Loss: 0.38612099592738086\n",
      "Epoch 4/100\n",
      "Training Loss: 0.44871852926919564\n",
      "Validation Loss: 0.11192095543954106\n",
      "Epoch 5/100\n",
      "Training Loss: 0.34164075156640283\n",
      "Validation Loss: 0.36687529410896014\n",
      "Epoch 6/100\n",
      "Training Loss: 0.3176571057941004\n",
      "Validation Loss: 0.2788184005528548\n",
      "Epoch 7/100\n",
      "Training Loss: 0.341181373081276\n",
      "Validation Loss: 0.15803357977607907\n",
      "Epoch 8/100\n",
      "Training Loss: 0.31452782902068305\n",
      "Validation Loss: 0.2751165229981804\n",
      "Epoch 9/100\n",
      "Training Loss: 0.2771240567738295\n",
      "Validation Loss: 0.2692731135399121\n",
      "Epoch 10/100\n",
      "Training Loss: 0.2597611572246308\n",
      "Validation Loss: 0.19290436896986912\n",
      "Epoch 11/100\n",
      "Training Loss: 0.22595009569499758\n",
      "Validation Loss: 0.39281831707052983\n",
      "Epoch 12/100\n",
      "Training Loss: 0.32516258499064793\n",
      "Validation Loss: 0.11637076398007325\n",
      "Epoch 13/100\n",
      "Training Loss: 0.31555015500142247\n",
      "Validation Loss: 0.2043162151212151\n",
      "Epoch 14/100\n",
      "Training Loss: 0.2362886927007671\n",
      "Validation Loss: 0.2416433444307149\n",
      "Epoch 15/100\n",
      "Training Loss: 0.25317383268403176\n",
      "Validation Loss: 0.183141986832338\n",
      "Epoch 16/100\n",
      "Training Loss: 0.24741930440576054\n",
      "Validation Loss: 0.22699398415850042\n",
      "Epoch 17/100\n",
      "Training Loss: 0.21213882699950778\n",
      "Validation Loss: 0.18023915090360393\n",
      "Epoch 18/100\n",
      "Training Loss: 0.24048309535236692\n",
      "Validation Loss: 0.21602519534446146\n",
      "Epoch 19/100\n",
      "Training Loss: 0.2575291842811079\n",
      "Validation Loss: 0.4357735346247069\n",
      "Epoch 20/100\n",
      "Training Loss: 0.1895326332540357\n",
      "Validation Loss: 0.2267856977431269\n",
      "Epoch 21/100\n",
      "Training Loss: 0.26286994981561573\n",
      "Validation Loss: 0.09099117169693788\n",
      "Epoch 22/100\n",
      "Training Loss: 0.19941866235346076\n",
      "Validation Loss: 0.13817470880599877\n",
      "Epoch 23/100\n",
      "Training Loss: 0.21076956518554837\n",
      "Validation Loss: 0.12011786348023153\n",
      "Epoch 24/100\n",
      "Training Loss: 0.19226637941734703\n",
      "Validation Loss: 0.1851088453764233\n",
      "Epoch 25/100\n",
      "Training Loss: 0.21601530747720576\n",
      "Validation Loss: 0.20010220313674024\n",
      "Epoch 26/100\n",
      "Training Loss: 0.23098508676537627\n",
      "Validation Loss: 0.21483732078791368\n",
      "Epoch 27/100\n",
      "Training Loss: 0.2037978774740529\n",
      "Validation Loss: 0.12649733009914146\n",
      "Epoch 28/100\n",
      "Training Loss: 0.18228489546804003\n",
      "Validation Loss: 0.09163928841569406\n",
      "Epoch 29/100\n",
      "Training Loss: 0.21217251640478915\n",
      "Validation Loss: 0.12561199703989281\n",
      "Epoch 30/100\n",
      "Training Loss: 0.18713863244296006\n",
      "Validation Loss: 0.09073387392197249\n",
      "Epoch 31/100\n",
      "Training Loss: 0.2579865288516966\n",
      "Validation Loss: 0.13210130541034662\n",
      "Epoch 32/100\n",
      "Training Loss: 0.1600159859680814\n",
      "Validation Loss: 0.15000153722015583\n",
      "Epoch 33/100\n",
      "Training Loss: 0.17499868310234903\n",
      "Validation Loss: 0.3268700334403895\n",
      "Epoch 34/100\n",
      "Training Loss: 0.21150231731352884\n",
      "Validation Loss: 0.2138152721154846\n",
      "Epoch 35/100\n",
      "Training Loss: 0.17333327679028387\n",
      "Validation Loss: 0.10373105442380823\n",
      "Epoch 36/100\n",
      "Training Loss: 0.20796437301652534\n",
      "Validation Loss: 0.226044195381903\n",
      "Epoch 37/100\n",
      "Training Loss: 0.16619703746107872\n",
      "Validation Loss: 0.20447971510862462\n",
      "Epoch 38/100\n",
      "Training Loss: 0.19533036340371263\n",
      "Validation Loss: 0.13903662647072218\n",
      "Epoch 39/100\n",
      "Training Loss: 0.16020546434154617\n",
      "Validation Loss: 0.11339685112179047\n",
      "Epoch 40/100\n",
      "Training Loss: 0.16291956339445085\n",
      "Validation Loss: 0.14747978292570782\n",
      "Epoch 41/100\n",
      "Training Loss: 0.1655917867065176\n",
      "Validation Loss: 0.09244004208103745\n",
      "Epoch 42/100\n",
      "Training Loss: 0.16506914176081947\n",
      "Validation Loss: 0.08393285376099395\n",
      "Epoch 43/100\n",
      "Training Loss: 0.2067456376780853\n",
      "Validation Loss: 0.12014381964545226\n",
      "Epoch 44/100\n",
      "Training Loss: 0.1758100698881709\n",
      "Validation Loss: 0.06904059293227086\n",
      "Epoch 45/100\n",
      "Training Loss: 0.19145396594872888\n",
      "Validation Loss: 0.13065286708045507\n",
      "Epoch 46/100\n",
      "Training Loss: 0.20858637811355019\n",
      "Validation Loss: 0.1014648857206708\n",
      "Epoch 47/100\n",
      "Training Loss: 0.18186612849367137\n",
      "Validation Loss: 0.10926708319801112\n",
      "Epoch 48/100\n",
      "Training Loss: 0.1491984216185241\n",
      "Validation Loss: 0.11781630248169246\n",
      "Epoch 49/100\n",
      "Training Loss: 0.1617169081210894\n",
      "Validation Loss: 0.06580835120707512\n",
      "Epoch 50/100\n",
      "Training Loss: 0.1718077756200474\n",
      "Validation Loss: 0.0827905330157483\n",
      "Epoch 51/100\n",
      "Training Loss: 0.1868017829327052\n",
      "Validation Loss: 0.09272695709257081\n",
      "Epoch 52/100\n",
      "Training Loss: 0.1498942172071784\n",
      "Validation Loss: 0.13477744221955373\n",
      "Epoch 53/100\n",
      "Training Loss: 0.15825912579324858\n",
      "Validation Loss: 0.13698784358998953\n",
      "Epoch 54/100\n",
      "Training Loss: 0.14921860124417127\n",
      "Validation Loss: 0.15188070730654066\n",
      "Epoch 55/100\n",
      "Training Loss: 0.12879611772367366\n",
      "Validation Loss: 0.1680599912229383\n",
      "Epoch 56/100\n",
      "Training Loss: 0.1596223559002091\n",
      "Validation Loss: 0.10852671074834497\n",
      "Epoch 57/100\n",
      "Training Loss: 0.17584126719796211\n",
      "Validation Loss: 0.21875818834433333\n",
      "Epoch 58/100\n",
      "Training Loss: 0.16458701634412914\n",
      "Validation Loss: 0.0995248479906116\n",
      "Epoch 59/100\n",
      "Training Loss: 0.17820979132304013\n",
      "Validation Loss: 0.130192574907966\n",
      "Epoch 60/100\n",
      "Training Loss: 0.17372816584636253\n",
      "Validation Loss: 0.11175728382945307\n",
      "Epoch 61/100\n",
      "Training Loss: 0.15059127839736583\n",
      "Validation Loss: 0.13369796914989676\n",
      "Epoch 62/100\n",
      "Training Loss: 0.13645497440305515\n",
      "Validation Loss: 0.10759401590300657\n",
      "Epoch 63/100\n",
      "Training Loss: 0.15647612496548774\n",
      "Validation Loss: 0.13845841153712912\n",
      "Epoch 64/100\n",
      "Training Loss: 0.1728872212758046\n",
      "Validation Loss: 0.12011613926808114\n",
      "Epoch 65/100\n",
      "Training Loss: 0.14929917267269485\n",
      "Validation Loss: 0.10542826677133636\n",
      "Epoch 66/100\n",
      "Training Loss: 0.188123024850019\n",
      "Validation Loss: 0.08555053289196902\n",
      "Epoch 67/100\n",
      "Training Loss: 0.13607165176304242\n",
      "Validation Loss: 0.07783504250598257\n",
      "Epoch 68/100\n",
      "Training Loss: 0.15848990055368528\n",
      "Validation Loss: 0.07773223788330226\n",
      "Epoch 69/100\n",
      "Training Loss: 0.17212075669627852\n",
      "Validation Loss: 0.12900528875723202\n",
      "Epoch 70/100\n",
      "Training Loss: 0.153889816921685\n",
      "Validation Loss: 0.06814885637955878\n",
      "Epoch 71/100\n",
      "Training Loss: 0.13102999209754213\n",
      "Validation Loss: 0.09524559726047055\n",
      "Epoch 72/100\n",
      "Training Loss: 0.14155513443974246\n",
      "Validation Loss: 0.0882731631392517\n",
      "Epoch 73/100\n",
      "Training Loss: 0.14384602692930099\n",
      "Validation Loss: 0.08941344874824039\n",
      "Epoch 74/100\n",
      "Training Loss: 0.12364210092961504\n",
      "Validation Loss: 0.0817278867183744\n",
      "Epoch 75/100\n",
      "Training Loss: 0.12937057968133847\n",
      "Validation Loss: 0.1514876478399192\n",
      "Epoch 76/100\n",
      "Training Loss: 0.1601694586949123\n",
      "Validation Loss: 0.12715891751660321\n",
      "Epoch 77/100\n",
      "Training Loss: 0.12871056898138308\n",
      "Validation Loss: 0.0771893566965664\n",
      "Epoch 78/100\n",
      "Training Loss: 0.1673748568086618\n",
      "Validation Loss: 0.07867701701530691\n",
      "Epoch 79/100\n",
      "Training Loss: 0.16225675154369285\n",
      "Validation Loss: 0.10422968438788996\n",
      "Epoch 80/100\n",
      "Training Loss: 0.14419739979743856\n",
      "Validation Loss: 0.060554627631311086\n",
      "Epoch 81/100\n",
      "Training Loss: 0.13622234636302197\n",
      "Validation Loss: 0.11993261287677384\n",
      "Epoch 82/100\n",
      "Training Loss: 0.157593027192812\n",
      "Validation Loss: 0.11889555433003207\n",
      "Epoch 83/100\n",
      "Training Loss: 0.1543864455319916\n",
      "Validation Loss: 0.1000768591513674\n",
      "Epoch 84/100\n",
      "Training Loss: 0.13607507102421157\n",
      "Validation Loss: 0.1277991072518119\n",
      "Epoch 85/100\n",
      "Training Loss: 0.12691359309296965\n",
      "Validation Loss: 0.10624824445383085\n",
      "Epoch 86/100\n",
      "Training Loss: 0.14577173552567949\n",
      "Validation Loss: 0.059744744897778035\n",
      "Epoch 87/100\n",
      "Training Loss: 0.12016483587364264\n",
      "Validation Loss: 0.07094999659245789\n",
      "Epoch 88/100\n",
      "Training Loss: 0.14136854152989475\n",
      "Validation Loss: 0.08400291563356213\n",
      "Epoch 89/100\n",
      "Training Loss: 0.13388582782364478\n",
      "Validation Loss: 0.05903339652870275\n",
      "Epoch 90/100\n",
      "Training Loss: 0.13496279895948401\n",
      "Validation Loss: 0.05337150443223809\n",
      "Epoch 91/100\n",
      "Training Loss: 0.1413142118769136\n",
      "Validation Loss: 0.07647801821198599\n",
      "Epoch 92/100\n",
      "Training Loss: 0.15528577537792648\n",
      "Validation Loss: 0.10602881551275076\n",
      "Epoch 93/100\n",
      "Training Loss: 0.11223537221145262\n",
      "Validation Loss: 0.10736288234223894\n",
      "Epoch 94/100\n",
      "Training Loss: 0.1407646581617071\n",
      "Validation Loss: 0.09798837487491967\n",
      "Epoch 95/100\n",
      "Training Loss: 0.14068572997685094\n",
      "Validation Loss: 0.05397439139845497\n",
      "Epoch 96/100\n",
      "Training Loss: 0.12047311007204096\n",
      "Validation Loss: 0.0788769226902285\n",
      "Epoch 97/100\n",
      "Training Loss: 0.11579273309426556\n",
      "Validation Loss: 0.0669771609742565\n",
      "Epoch 98/100\n",
      "Training Loss: 0.15039020033879943\n",
      "Validation Loss: 0.09460288605473166\n",
      "Epoch 99/100\n",
      "Training Loss: 0.11366185951437473\n",
      "Validation Loss: 0.08128826705901333\n",
      "Epoch 100/100\n",
      "Training Loss: 0.11908076984433999\n",
      "Validation Loss: 0.07680567889437952\n",
      "    Trial 2/2 for combination 2/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.5156404468901344\n",
      "Validation Loss: 0.7926146476719718\n",
      "Epoch 2/100\n",
      "Training Loss: 0.5420518953287178\n",
      "Validation Loss: 0.3431106853173197\n",
      "Epoch 3/100\n",
      "Training Loss: 0.4584561209956157\n",
      "Validation Loss: 0.6938533935750463\n",
      "Epoch 4/100\n",
      "Training Loss: 0.499668754225599\n",
      "Validation Loss: 0.49576999658519527\n",
      "Epoch 5/100\n",
      "Training Loss: 0.47017147761729705\n",
      "Validation Loss: 0.37331134374854136\n",
      "Epoch 6/100\n",
      "Training Loss: 0.453424356993079\n",
      "Validation Loss: 0.40734617649448257\n",
      "Epoch 7/100\n",
      "Training Loss: 0.4637723022036953\n",
      "Validation Loss: 0.18906078527337464\n",
      "Epoch 8/100\n",
      "Training Loss: 0.3673582164351509\n",
      "Validation Loss: 0.2109053831182724\n",
      "Epoch 9/100\n",
      "Training Loss: 0.3037843710134463\n",
      "Validation Loss: 0.8078482341473787\n",
      "Epoch 10/100\n",
      "Training Loss: 0.41419651470489904\n",
      "Validation Loss: 0.18415446012051628\n",
      "Epoch 11/100\n",
      "Training Loss: 0.32692849638753235\n",
      "Validation Loss: 0.6634051557338048\n",
      "Epoch 12/100\n",
      "Training Loss: 0.2985994163674364\n",
      "Validation Loss: 0.18401128899404484\n",
      "Epoch 13/100\n",
      "Training Loss: 0.2861261271617422\n",
      "Validation Loss: 0.4895189047646782\n",
      "Epoch 14/100\n",
      "Training Loss: 0.3061721901529144\n",
      "Validation Loss: 0.28427016448736947\n",
      "Epoch 15/100\n",
      "Training Loss: 0.3202425803185954\n",
      "Validation Loss: 0.5051462714497205\n",
      "Epoch 16/100\n",
      "Training Loss: 0.24941201033648847\n",
      "Validation Loss: 0.20439922602370553\n",
      "Epoch 17/100\n",
      "Training Loss: 0.34726436457686116\n",
      "Validation Loss: 0.20897547407242706\n",
      "Epoch 18/100\n",
      "Training Loss: 0.2782916537352925\n",
      "Validation Loss: 0.22733679767249884\n",
      "Epoch 19/100\n",
      "Training Loss: 0.2549568639252713\n",
      "Validation Loss: 0.35048409105616346\n",
      "Epoch 20/100\n",
      "Training Loss: 0.22993791666348024\n",
      "Validation Loss: 0.25281476523774893\n",
      "Epoch 21/100\n",
      "Training Loss: 0.3022203750365062\n",
      "Validation Loss: 0.2708268486956842\n",
      "Epoch 22/100\n",
      "Training Loss: 0.29704240431667334\n",
      "Validation Loss: 0.3023939316578621\n",
      "Epoch 23/100\n",
      "Training Loss: 0.2798125021892524\n",
      "Validation Loss: 0.24325514140781607\n",
      "Epoch 24/100\n",
      "Training Loss: 0.30065645098345345\n",
      "Validation Loss: 0.09543213351918339\n",
      "Epoch 25/100\n",
      "Training Loss: 0.29829388075130236\n",
      "Validation Loss: 0.6352074529719215\n",
      "Epoch 26/100\n",
      "Training Loss: 0.3153066438785427\n",
      "Validation Loss: 0.22393175286649206\n",
      "Epoch 27/100\n",
      "Training Loss: 0.23901319014201186\n",
      "Validation Loss: 0.23106328660363312\n",
      "Epoch 28/100\n",
      "Training Loss: 0.2832996994478515\n",
      "Validation Loss: 0.19486591137678824\n",
      "Epoch 29/100\n",
      "Training Loss: 0.2590075875805035\n",
      "Validation Loss: 0.1633456750640061\n",
      "Epoch 30/100\n",
      "Training Loss: 0.29271249785428477\n",
      "Validation Loss: 0.32337670685609304\n",
      "Epoch 31/100\n",
      "Training Loss: 0.2322246683828786\n",
      "Validation Loss: 0.1275565959898643\n",
      "Epoch 32/100\n",
      "Training Loss: 0.2804236330526829\n",
      "Validation Loss: 0.14885306226355588\n",
      "Epoch 33/100\n",
      "Training Loss: 0.33847921001511055\n",
      "Validation Loss: 0.19911882218129004\n",
      "Epoch 34/100\n",
      "Training Loss: 0.28468174241737637\n",
      "Validation Loss: 0.3659550933390351\n",
      "Epoch 35/100\n",
      "Training Loss: 0.22393980566621452\n",
      "Validation Loss: 0.1519692526746141\n",
      "Epoch 36/100\n",
      "Training Loss: 0.18628051799347967\n",
      "Validation Loss: 0.284414574766873\n",
      "Epoch 37/100\n",
      "Training Loss: 0.2446571830278386\n",
      "Validation Loss: 0.10585474979035743\n",
      "Epoch 38/100\n",
      "Training Loss: 0.2700526930034893\n",
      "Validation Loss: 0.16400611258500025\n",
      "Epoch 39/100\n",
      "Training Loss: 0.21475280954435627\n",
      "Validation Loss: 0.1804994771707137\n",
      "Epoch 40/100\n",
      "Training Loss: 0.2284493361395701\n",
      "Validation Loss: 0.174268276282797\n",
      "Epoch 41/100\n",
      "Training Loss: 0.20480127644376808\n",
      "Validation Loss: 0.171042564011197\n",
      "Epoch 42/100\n",
      "Training Loss: 0.24816289228991784\n",
      "Validation Loss: 0.18613249194908327\n",
      "Epoch 43/100\n",
      "Training Loss: 0.21095667320284178\n",
      "Validation Loss: 0.14007596422506047\n",
      "Epoch 44/100\n",
      "Training Loss: 0.21178705168413184\n",
      "Validation Loss: 0.24347468306858425\n",
      "Epoch 45/100\n",
      "Training Loss: 0.2074883881150618\n",
      "Validation Loss: 0.09548234563410665\n",
      "Epoch 46/100\n",
      "Training Loss: 0.20725180868048698\n",
      "Validation Loss: 0.16123683964976993\n",
      "Epoch 47/100\n",
      "Training Loss: 0.24279289348077226\n",
      "Validation Loss: 0.15487852268342273\n",
      "Epoch 48/100\n",
      "Training Loss: 0.19739937496675516\n",
      "Validation Loss: 0.09465775389856601\n",
      "Epoch 49/100\n",
      "Training Loss: 0.1858429634980542\n",
      "Validation Loss: 0.15626100566178827\n",
      "Epoch 50/100\n",
      "Training Loss: 0.2754428066748604\n",
      "Validation Loss: 0.22431122982597693\n",
      "Epoch 51/100\n",
      "Training Loss: 0.24572902699873247\n",
      "Validation Loss: 0.24200707562562246\n",
      "Epoch 52/100\n",
      "Training Loss: 0.15715234891979565\n",
      "Validation Loss: 0.15656505614026656\n",
      "Epoch 53/100\n",
      "Training Loss: 0.1980377571078026\n",
      "Validation Loss: 0.10418901625195473\n",
      "Epoch 54/100\n",
      "Training Loss: 0.19647417682480994\n",
      "Validation Loss: 0.14521016688132754\n",
      "Epoch 55/100\n",
      "Training Loss: 0.18512650013406381\n",
      "Validation Loss: 0.1296660188027577\n",
      "Epoch 56/100\n",
      "Training Loss: 0.17024855278277218\n",
      "Validation Loss: 0.069280039354863\n",
      "Epoch 57/100\n",
      "Training Loss: 0.18920816476165692\n",
      "Validation Loss: 0.14089842876599118\n",
      "Epoch 58/100\n",
      "Training Loss: 0.19294135520586794\n",
      "Validation Loss: 0.17457248997538496\n",
      "Epoch 59/100\n",
      "Training Loss: 0.22006474756753272\n",
      "Validation Loss: 0.15373592857294655\n",
      "Epoch 60/100\n",
      "Training Loss: 0.17214043048924552\n",
      "Validation Loss: 0.15395622214505655\n",
      "Epoch 61/100\n",
      "Training Loss: 0.255285561246578\n",
      "Validation Loss: 0.12405093110379103\n",
      "Epoch 62/100\n",
      "Training Loss: 0.19408041078370367\n",
      "Validation Loss: 0.20377630090745616\n",
      "Epoch 63/100\n",
      "Training Loss: 0.15364874264951447\n",
      "Validation Loss: 0.14268176899049884\n",
      "Epoch 64/100\n",
      "Training Loss: 0.1808853514785869\n",
      "Validation Loss: 0.09457917883114876\n",
      "Epoch 65/100\n",
      "Training Loss: 0.18549563843682762\n",
      "Validation Loss: 0.1720612972578075\n",
      "Epoch 66/100\n",
      "Training Loss: 0.1455240968866356\n",
      "Validation Loss: 0.09293160070311586\n",
      "Epoch 67/100\n",
      "Training Loss: 0.16318202618562472\n",
      "Validation Loss: 0.1377523900292783\n",
      "Epoch 68/100\n",
      "Training Loss: 0.20205373514644004\n",
      "Validation Loss: 0.17918213975623498\n",
      "Epoch 69/100\n",
      "Training Loss: 0.15442609079423886\n",
      "Validation Loss: 0.11172621527146967\n",
      "Epoch 70/100\n",
      "Training Loss: 0.1438279437778508\n",
      "Validation Loss: 0.12142828971803525\n",
      "Epoch 71/100\n",
      "Training Loss: 0.15853817025275652\n",
      "Validation Loss: 0.11515307845505465\n",
      "Epoch 72/100\n",
      "Training Loss: 0.14699589414622843\n",
      "Validation Loss: 0.24079981965622071\n",
      "Epoch 73/100\n",
      "Training Loss: 0.17847418134642867\n",
      "Validation Loss: 0.12409099304378127\n",
      "Epoch 74/100\n",
      "Training Loss: 0.1761390378414839\n",
      "Validation Loss: 0.11619028950709213\n",
      "Epoch 75/100\n",
      "Training Loss: 0.18588082047342644\n",
      "Validation Loss: 0.08019099256549647\n",
      "Epoch 76/100\n",
      "Training Loss: 0.17252349694249913\n",
      "Validation Loss: 0.11917314979990849\n",
      "Epoch 77/100\n",
      "Training Loss: 0.18513041722470638\n",
      "Validation Loss: 0.07091456829933343\n",
      "Epoch 78/100\n",
      "Training Loss: 0.1440900998626979\n",
      "Validation Loss: 0.20542498974837847\n",
      "Epoch 79/100\n",
      "Training Loss: 0.1649399940056392\n",
      "Validation Loss: 0.18950820639723776\n",
      "Epoch 80/100\n",
      "Training Loss: 0.16722252823658995\n",
      "Validation Loss: 0.1660499797296375\n",
      "Epoch 81/100\n",
      "Training Loss: 0.15126077723259046\n",
      "Validation Loss: 0.14938636244463122\n",
      "Epoch 82/100\n",
      "Training Loss: 0.1465232472837439\n",
      "Validation Loss: 0.08782498042717639\n",
      "Epoch 83/100\n",
      "Training Loss: 0.14842551669600268\n",
      "Validation Loss: 0.07176393873901402\n",
      "Epoch 84/100\n",
      "Training Loss: 0.167053057497943\n",
      "Validation Loss: 0.09568643014863956\n",
      "Epoch 85/100\n",
      "Training Loss: 0.14074128753469572\n",
      "Validation Loss: 0.11752353679671387\n",
      "Epoch 86/100\n",
      "Training Loss: 0.182980134429626\n",
      "Validation Loss: 0.12569565784393352\n",
      "Epoch 87/100\n",
      "Training Loss: 0.16023944736252446\n",
      "Validation Loss: 0.11241893250760153\n",
      "Epoch 88/100\n",
      "Training Loss: 0.15307866362359615\n",
      "Validation Loss: 0.10483754035483332\n",
      "Epoch 89/100\n",
      "Training Loss: 0.15701217663591535\n",
      "Validation Loss: 0.10004889764756839\n",
      "Epoch 90/100\n",
      "Training Loss: 0.14786663430194705\n",
      "Validation Loss: 0.08785320079768391\n",
      "Epoch 91/100\n",
      "Training Loss: 0.16180829170622588\n",
      "Validation Loss: 0.09267403732101585\n",
      "Epoch 92/100\n",
      "Training Loss: 0.1309181427352745\n",
      "Validation Loss: 0.09363419353582829\n",
      "Epoch 93/100\n",
      "Training Loss: 0.1748317153084394\n",
      "Validation Loss: 0.09387892653738496\n",
      "Epoch 94/100\n",
      "Training Loss: 0.13864043333434453\n",
      "Validation Loss: 0.219457985884116\n",
      "Epoch 95/100\n",
      "Training Loss: 0.1993370075573867\n",
      "Validation Loss: 0.12603936448202463\n",
      "Epoch 96/100\n",
      "Training Loss: 0.1467784711549244\n",
      "Validation Loss: 0.10471226837870995\n",
      "Epoch 97/100\n",
      "Training Loss: 0.15068492514686502\n",
      "Validation Loss: 0.08819250868902395\n",
      "Epoch 98/100\n",
      "Training Loss: 0.1511810888872099\n",
      "Validation Loss: 0.10805116751979224\n",
      "Epoch 99/100\n",
      "Training Loss: 0.1591797326267693\n",
      "Validation Loss: 0.09447945828654844\n",
      "Epoch 100/100\n",
      "Training Loss: 0.13766080402098976\n",
      "Validation Loss: 0.09733919363846641\n",
      "Combination 2: Avg Training Loss = 0.2134673295752186, Avg Validation Loss = 0.1741332931852882\n",
      "Testing combination 3/48: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'sequence_length': 20, 'dropout_rate': 0.1}\n",
      "Truncated Train Data Shape: (70, 20, 1)\n",
      "Truncated Validation Data Shape: (10, 20, 1)\n",
      "    Trial 1/2 for combination 3/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.5853686196010953\n",
      "Validation Loss: 0.10147968675129544\n",
      "Epoch 2/100\n",
      "Training Loss: 0.4219993994301103\n",
      "Validation Loss: 0.24364840803540103\n",
      "Epoch 3/100\n",
      "Training Loss: 0.3577674277454835\n",
      "Validation Loss: 0.4406482822813458\n",
      "Epoch 4/100\n",
      "Training Loss: 0.3511071498088259\n",
      "Validation Loss: 0.21848443618015537\n",
      "Epoch 5/100\n",
      "Training Loss: 0.302906092839633\n",
      "Validation Loss: 0.20773648894273467\n",
      "Epoch 6/100\n",
      "Training Loss: 0.2769133317776506\n",
      "Validation Loss: 0.20351982343625458\n",
      "Epoch 7/100\n",
      "Training Loss: 0.34073237911911974\n",
      "Validation Loss: 0.12360697203989249\n",
      "Epoch 8/100\n",
      "Training Loss: 0.2694716482871829\n",
      "Validation Loss: 0.2293774407893247\n",
      "Epoch 9/100\n",
      "Training Loss: 0.2006371930147854\n",
      "Validation Loss: 0.18381529203429653\n",
      "Epoch 10/100\n",
      "Training Loss: 0.16352665268427063\n",
      "Validation Loss: 0.2844423676177672\n",
      "Epoch 11/100\n",
      "Training Loss: 0.22355602993150597\n",
      "Validation Loss: 0.10112192486917589\n",
      "Epoch 12/100\n",
      "Training Loss: 0.20126830936158127\n",
      "Validation Loss: 0.20196363085334687\n",
      "Epoch 13/100\n",
      "Training Loss: 0.22621663227367406\n",
      "Validation Loss: 0.23918264329859745\n",
      "Epoch 14/100\n",
      "Training Loss: 0.17740769033222525\n",
      "Validation Loss: 0.061707393085251784\n",
      "Epoch 15/100\n",
      "Training Loss: 0.22193790869871938\n",
      "Validation Loss: 0.17438895498268275\n",
      "Epoch 16/100\n",
      "Training Loss: 0.20654791429143288\n",
      "Validation Loss: 0.13273904540402728\n",
      "Epoch 17/100\n",
      "Training Loss: 0.14587755373983088\n",
      "Validation Loss: 0.2532608683808354\n",
      "Epoch 18/100\n",
      "Training Loss: 0.1871563222208219\n",
      "Validation Loss: 0.12450010905296786\n",
      "Epoch 19/100\n",
      "Training Loss: 0.22247883690112902\n",
      "Validation Loss: 0.09728226259982561\n",
      "Epoch 20/100\n",
      "Training Loss: 0.19901355113172017\n",
      "Validation Loss: 0.11607115858380315\n",
      "Epoch 21/100\n",
      "Training Loss: 0.19678557228560783\n",
      "Validation Loss: 0.10713786742935923\n",
      "Epoch 22/100\n",
      "Training Loss: 0.20340395718448295\n",
      "Validation Loss: 0.10098594369436786\n",
      "Epoch 23/100\n",
      "Training Loss: 0.14126672564589177\n",
      "Validation Loss: 0.06276825229441504\n",
      "Epoch 24/100\n",
      "Training Loss: 0.19853100251940717\n",
      "Validation Loss: 0.05924532907512682\n",
      "Epoch 25/100\n",
      "Training Loss: 0.18999448800369284\n",
      "Validation Loss: 0.1738644713283422\n",
      "Epoch 26/100\n",
      "Training Loss: 0.17638546878738806\n",
      "Validation Loss: 0.10991194494563042\n",
      "Epoch 27/100\n",
      "Training Loss: 0.16780192116332784\n",
      "Validation Loss: 0.07463122731668531\n",
      "Epoch 28/100\n",
      "Training Loss: 0.12950889622554676\n",
      "Validation Loss: 0.08112322675470884\n",
      "Epoch 29/100\n",
      "Training Loss: 0.19193739892337866\n",
      "Validation Loss: 0.0810161115138361\n",
      "Epoch 30/100\n",
      "Training Loss: 0.16693037707274902\n",
      "Validation Loss: 0.12115323714961126\n",
      "Epoch 31/100\n",
      "Training Loss: 0.17950287247123092\n",
      "Validation Loss: 0.0564714879860813\n",
      "Epoch 32/100\n",
      "Training Loss: 0.1574157590987908\n",
      "Validation Loss: 0.10067741012635312\n",
      "Epoch 33/100\n",
      "Training Loss: 0.14167818246626762\n",
      "Validation Loss: 0.07553715955186141\n",
      "Epoch 34/100\n",
      "Training Loss: 0.1426776434823554\n",
      "Validation Loss: 0.09964941796090364\n",
      "Epoch 35/100\n",
      "Training Loss: 0.13657596332806193\n",
      "Validation Loss: 0.07619872885745156\n",
      "Epoch 36/100\n",
      "Training Loss: 0.1495873914265921\n",
      "Validation Loss: 0.08988958895091281\n",
      "Epoch 37/100\n",
      "Training Loss: 0.12771815411954376\n",
      "Validation Loss: 0.07959318272779818\n",
      "Epoch 38/100\n",
      "Training Loss: 0.13167236112658626\n",
      "Validation Loss: 0.09332189606329791\n",
      "Epoch 39/100\n",
      "Training Loss: 0.16349872667818924\n",
      "Validation Loss: 0.055219202979101746\n",
      "Epoch 40/100\n",
      "Training Loss: 0.1741441862956688\n",
      "Validation Loss: 0.10807888058146857\n",
      "Epoch 41/100\n",
      "Training Loss: 0.13736331085003028\n",
      "Validation Loss: 0.06629837942711976\n",
      "Epoch 42/100\n",
      "Training Loss: 0.1644358264974598\n",
      "Validation Loss: 0.10765354481620423\n",
      "Epoch 43/100\n",
      "Training Loss: 0.13104072081876125\n",
      "Validation Loss: 0.06662817473231386\n",
      "Epoch 44/100\n",
      "Training Loss: 0.12867348196935857\n",
      "Validation Loss: 0.0909993545110094\n",
      "Epoch 45/100\n",
      "Training Loss: 0.14518366665614657\n",
      "Validation Loss: 0.0705298807219\n",
      "Epoch 46/100\n",
      "Training Loss: 0.15926802243229923\n",
      "Validation Loss: 0.08786329041068439\n",
      "Epoch 47/100\n",
      "Training Loss: 0.12755233566184715\n",
      "Validation Loss: 0.16077397371159688\n",
      "Epoch 48/100\n",
      "Training Loss: 0.10731830613199031\n",
      "Validation Loss: 0.0830630644814904\n",
      "Epoch 49/100\n",
      "Training Loss: 0.1572963482564086\n",
      "Validation Loss: 0.05721523886069639\n",
      "Epoch 50/100\n",
      "Training Loss: 0.15249846867169592\n",
      "Validation Loss: 0.13792748322813728\n",
      "Epoch 51/100\n",
      "Training Loss: 0.13337931596875038\n",
      "Validation Loss: 0.09674688731005675\n",
      "Epoch 52/100\n",
      "Training Loss: 0.14035339839977237\n",
      "Validation Loss: 0.07943138159726844\n",
      "Epoch 53/100\n",
      "Training Loss: 0.1400861007487592\n",
      "Validation Loss: 0.08901182640878208\n",
      "Epoch 54/100\n",
      "Training Loss: 0.13075687631417024\n",
      "Validation Loss: 0.06977648446948989\n",
      "Epoch 55/100\n",
      "Training Loss: 0.12706643087393835\n",
      "Validation Loss: 0.05501506084760719\n",
      "Epoch 56/100\n",
      "Training Loss: 0.13981444640210627\n",
      "Validation Loss: 0.1039313740928435\n",
      "Epoch 57/100\n",
      "Training Loss: 0.13924992069742329\n",
      "Validation Loss: 0.08914639928496701\n",
      "Epoch 58/100\n",
      "Training Loss: 0.13092888927787366\n",
      "Validation Loss: 0.08524659311535306\n",
      "Epoch 59/100\n",
      "Training Loss: 0.11434988715053296\n",
      "Validation Loss: 0.08123098878907029\n",
      "Epoch 60/100\n",
      "Training Loss: 0.13477850219805704\n",
      "Validation Loss: 0.059503500738276635\n",
      "Epoch 61/100\n",
      "Training Loss: 0.11415432884336468\n",
      "Validation Loss: 0.06491857342845987\n",
      "Epoch 62/100\n",
      "Training Loss: 0.114238921549579\n",
      "Validation Loss: 0.05972668214250697\n",
      "Epoch 63/100\n",
      "Training Loss: 0.13097380707133252\n",
      "Validation Loss: 0.06518463724350967\n",
      "Epoch 64/100\n",
      "Training Loss: 0.1171637438213301\n",
      "Validation Loss: 0.09799135849433885\n",
      "Epoch 65/100\n",
      "Training Loss: 0.11216023041125779\n",
      "Validation Loss: 0.07456877681438416\n",
      "Epoch 66/100\n",
      "Training Loss: 0.14874612026173453\n",
      "Validation Loss: 0.06490627695295476\n",
      "Epoch 67/100\n",
      "Training Loss: 0.1256536004561256\n",
      "Validation Loss: 0.0849343817218012\n",
      "Epoch 68/100\n",
      "Training Loss: 0.12177875989121102\n",
      "Validation Loss: 0.05501294472213829\n",
      "Epoch 69/100\n",
      "Training Loss: 0.11818755989859712\n",
      "Validation Loss: 0.06085298078480188\n",
      "Epoch 70/100\n",
      "Training Loss: 0.11850183301814382\n",
      "Validation Loss: 0.047957551149392215\n",
      "Epoch 71/100\n",
      "Training Loss: 0.11987099117061875\n",
      "Validation Loss: 0.06547497809824275\n",
      "Epoch 72/100\n",
      "Training Loss: 0.1141888052434461\n",
      "Validation Loss: 0.08096796616809579\n",
      "Epoch 73/100\n",
      "Training Loss: 0.1024777800065178\n",
      "Validation Loss: 0.09209347764444906\n",
      "Epoch 74/100\n",
      "Training Loss: 0.12537319415404413\n",
      "Validation Loss: 0.058410678472691505\n",
      "Epoch 75/100\n",
      "Training Loss: 0.13556991669572274\n",
      "Validation Loss: 0.08401255061053489\n",
      "Epoch 76/100\n",
      "Training Loss: 0.10889942850132368\n",
      "Validation Loss: 0.09616416791807972\n",
      "Epoch 77/100\n",
      "Training Loss: 0.108305929251636\n",
      "Validation Loss: 0.03772052510651692\n",
      "Epoch 78/100\n",
      "Training Loss: 0.12144280945943899\n",
      "Validation Loss: 0.1017167911166964\n",
      "Epoch 79/100\n",
      "Training Loss: 0.10545276977627718\n",
      "Validation Loss: 0.051260475110780766\n",
      "Epoch 80/100\n",
      "Training Loss: 0.09539640910020918\n",
      "Validation Loss: 0.03676053362329016\n",
      "Epoch 81/100\n",
      "Training Loss: 0.10791531264512264\n",
      "Validation Loss: 0.07703099654428607\n",
      "Epoch 82/100\n",
      "Training Loss: 0.11658655364781598\n",
      "Validation Loss: 0.04237382884547894\n",
      "Epoch 83/100\n",
      "Training Loss: 0.1190020441636362\n",
      "Validation Loss: 0.056827087597983994\n",
      "Epoch 84/100\n",
      "Training Loss: 0.11096330885387883\n",
      "Validation Loss: 0.05488572729906092\n",
      "Epoch 85/100\n",
      "Training Loss: 0.11624666272690864\n",
      "Validation Loss: 0.04319559394841126\n",
      "Epoch 86/100\n",
      "Training Loss: 0.09824246974519799\n",
      "Validation Loss: 0.10838420551771615\n",
      "Epoch 87/100\n",
      "Training Loss: 0.10407378978827331\n",
      "Validation Loss: 0.09428991386584237\n",
      "Epoch 88/100\n",
      "Training Loss: 0.118733914981642\n",
      "Validation Loss: 0.08539376622812582\n",
      "Epoch 89/100\n",
      "Training Loss: 0.10698484886318564\n",
      "Validation Loss: 0.06954745179999164\n",
      "Epoch 90/100\n",
      "Training Loss: 0.1014841299771898\n",
      "Validation Loss: 0.06736552938441462\n",
      "Epoch 91/100\n",
      "Training Loss: 0.11783550894940102\n",
      "Validation Loss: 0.06893449804177627\n",
      "Epoch 92/100\n",
      "Training Loss: 0.10533307722560394\n",
      "Validation Loss: 0.07308163877839627\n",
      "Epoch 93/100\n",
      "Training Loss: 0.09114223677948051\n",
      "Validation Loss: 0.09412376876413092\n",
      "Epoch 94/100\n",
      "Training Loss: 0.09984050029895448\n",
      "Validation Loss: 0.06751919183766084\n",
      "Epoch 95/100\n",
      "Training Loss: 0.09945602206331641\n",
      "Validation Loss: 0.06991943701334852\n",
      "Epoch 96/100\n",
      "Training Loss: 0.10585647516413638\n",
      "Validation Loss: 0.048707709296723974\n",
      "Epoch 97/100\n",
      "Training Loss: 0.09332652187296299\n",
      "Validation Loss: 0.06987131893954504\n",
      "Epoch 98/100\n",
      "Training Loss: 0.10436749887894468\n",
      "Validation Loss: 0.07099641731836658\n",
      "Epoch 99/100\n",
      "Training Loss: 0.09441824763421011\n",
      "Validation Loss: 0.06381391051827993\n",
      "Epoch 100/100\n",
      "Training Loss: 0.10262717824911666\n",
      "Validation Loss: 0.07108004616167456\n",
      "    Trial 2/2 for combination 3/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.5504075551230921\n",
      "Validation Loss: 0.4004346173028721\n",
      "Epoch 2/100\n",
      "Training Loss: 0.4124155472787839\n",
      "Validation Loss: 0.3190818592920299\n",
      "Epoch 3/100\n",
      "Training Loss: 0.41730723862311186\n",
      "Validation Loss: 0.2633922679158497\n",
      "Epoch 4/100\n",
      "Training Loss: 0.3686149515676654\n",
      "Validation Loss: 0.14818153127667336\n",
      "Epoch 5/100\n",
      "Training Loss: 0.35535416369169787\n",
      "Validation Loss: 0.1380009312719834\n",
      "Epoch 6/100\n",
      "Training Loss: 0.42515155006412975\n",
      "Validation Loss: 0.15064748476489964\n",
      "Epoch 7/100\n",
      "Training Loss: 0.24043960104511325\n",
      "Validation Loss: 0.10686867735960696\n",
      "Epoch 8/100\n",
      "Training Loss: 0.2568392669054759\n",
      "Validation Loss: 0.0749450754565537\n",
      "Epoch 9/100\n",
      "Training Loss: 0.2453448104762193\n",
      "Validation Loss: 0.161143178480251\n",
      "Epoch 10/100\n",
      "Training Loss: 0.32593412312635395\n",
      "Validation Loss: 0.18728988036233413\n",
      "Epoch 11/100\n",
      "Training Loss: 0.2717334744068453\n",
      "Validation Loss: 0.25568972961420144\n",
      "Epoch 12/100\n",
      "Training Loss: 0.21867869856134559\n",
      "Validation Loss: 0.0812366597504884\n",
      "Epoch 13/100\n",
      "Training Loss: 0.17891651549955476\n",
      "Validation Loss: 0.08676152060536417\n",
      "Epoch 14/100\n",
      "Training Loss: 0.2157295873841495\n",
      "Validation Loss: 0.08956570546226827\n",
      "Epoch 15/100\n",
      "Training Loss: 0.25403421140488\n",
      "Validation Loss: 0.12192923987506991\n",
      "Epoch 16/100\n",
      "Training Loss: 0.22079410792437437\n",
      "Validation Loss: 0.20083972481836052\n",
      "Epoch 17/100\n",
      "Training Loss: 0.24208496859607978\n",
      "Validation Loss: 0.10881278781186585\n",
      "Epoch 18/100\n",
      "Training Loss: 0.21388132260115866\n",
      "Validation Loss: 0.13439284382986955\n",
      "Epoch 19/100\n",
      "Training Loss: 0.2677855739836888\n",
      "Validation Loss: 0.1600636686739006\n",
      "Epoch 20/100\n",
      "Training Loss: 0.1927253456213085\n",
      "Validation Loss: 0.16240727572223254\n",
      "Epoch 21/100\n",
      "Training Loss: 0.20503188159652586\n",
      "Validation Loss: 0.125228539779178\n",
      "Epoch 22/100\n",
      "Training Loss: 0.18676555912432058\n",
      "Validation Loss: 0.08383437681336309\n",
      "Epoch 23/100\n",
      "Training Loss: 0.16453542417904615\n",
      "Validation Loss: 0.11522017808965021\n",
      "Epoch 24/100\n",
      "Training Loss: 0.1705001704482933\n",
      "Validation Loss: 0.14217902552285908\n",
      "Epoch 25/100\n",
      "Training Loss: 0.17598277005684307\n",
      "Validation Loss: 0.15209739035415012\n",
      "Epoch 26/100\n",
      "Training Loss: 0.23066235656705422\n",
      "Validation Loss: 0.12708152056999839\n",
      "Epoch 27/100\n",
      "Training Loss: 0.19400012360598162\n",
      "Validation Loss: 0.1153612140971461\n",
      "Epoch 28/100\n",
      "Training Loss: 0.1848986491725939\n",
      "Validation Loss: 0.07669141260869264\n",
      "Epoch 29/100\n",
      "Training Loss: 0.15848413211702436\n",
      "Validation Loss: 0.05756535182174436\n",
      "Epoch 30/100\n",
      "Training Loss: 0.1594273117093377\n",
      "Validation Loss: 0.09874471249406917\n",
      "Epoch 31/100\n",
      "Training Loss: 0.17366630580499473\n",
      "Validation Loss: 0.08255256350718246\n",
      "Epoch 32/100\n",
      "Training Loss: 0.19261156914372224\n",
      "Validation Loss: 0.10274283768310792\n",
      "Epoch 33/100\n",
      "Training Loss: 0.20418160278192707\n",
      "Validation Loss: 0.0863206789942321\n",
      "Epoch 34/100\n",
      "Training Loss: 0.18362225931987589\n",
      "Validation Loss: 0.10263267443133343\n",
      "Epoch 35/100\n",
      "Training Loss: 0.1509227551890964\n",
      "Validation Loss: 0.12249662200577498\n",
      "Epoch 36/100\n",
      "Training Loss: 0.18653831023631307\n",
      "Validation Loss: 0.07590870077154063\n",
      "Epoch 37/100\n",
      "Training Loss: 0.18934031499107995\n",
      "Validation Loss: 0.13349049302785163\n",
      "Epoch 38/100\n",
      "Training Loss: 0.1494332375904693\n",
      "Validation Loss: 0.1765214661970162\n",
      "Epoch 39/100\n",
      "Training Loss: 0.19052496838127372\n",
      "Validation Loss: 0.08665050578019332\n",
      "Epoch 40/100\n",
      "Training Loss: 0.18316584903467953\n",
      "Validation Loss: 0.12573453049570987\n",
      "Epoch 41/100\n",
      "Training Loss: 0.1608158301554532\n",
      "Validation Loss: 0.08382697863575253\n",
      "Epoch 42/100\n",
      "Training Loss: 0.14838426737624288\n",
      "Validation Loss: 0.12423346665388915\n",
      "Epoch 43/100\n",
      "Training Loss: 0.13194453477253\n",
      "Validation Loss: 0.12023154094203507\n",
      "Epoch 44/100\n",
      "Training Loss: 0.13280100577228765\n",
      "Validation Loss: 0.05826494282244241\n",
      "Epoch 45/100\n",
      "Training Loss: 0.13709415714735107\n",
      "Validation Loss: 0.09069252202368763\n",
      "Epoch 46/100\n",
      "Training Loss: 0.1444473498336849\n",
      "Validation Loss: 0.08818946254250967\n",
      "Epoch 47/100\n",
      "Training Loss: 0.14589930257676065\n",
      "Validation Loss: 0.08247082250346165\n",
      "Epoch 48/100\n",
      "Training Loss: 0.1583313482265753\n",
      "Validation Loss: 0.09095888637717413\n",
      "Epoch 49/100\n",
      "Training Loss: 0.162967772664389\n",
      "Validation Loss: 0.04725042360582497\n",
      "Epoch 50/100\n",
      "Training Loss: 0.15815870855752212\n",
      "Validation Loss: 0.10588380315453602\n",
      "Epoch 51/100\n",
      "Training Loss: 0.13068761125678474\n",
      "Validation Loss: 0.21888970439620783\n",
      "Epoch 52/100\n",
      "Training Loss: 0.16480651204752034\n",
      "Validation Loss: 0.10356288638569992\n",
      "Epoch 53/100\n",
      "Training Loss: 0.15793522870134868\n",
      "Validation Loss: 0.09131882191750466\n",
      "Epoch 54/100\n",
      "Training Loss: 0.15456444143588102\n",
      "Validation Loss: 0.07367508060135661\n",
      "Epoch 55/100\n",
      "Training Loss: 0.13517203694647534\n",
      "Validation Loss: 0.14490772933015217\n",
      "Epoch 56/100\n",
      "Training Loss: 0.14781079624983523\n",
      "Validation Loss: 0.06294278317523336\n",
      "Epoch 57/100\n",
      "Training Loss: 0.12771870828472745\n",
      "Validation Loss: 0.06721993274706226\n",
      "Epoch 58/100\n",
      "Training Loss: 0.18755657461642686\n",
      "Validation Loss: 0.08791179909044573\n",
      "Epoch 59/100\n",
      "Training Loss: 0.15670433587517088\n",
      "Validation Loss: 0.16089054444286058\n",
      "Epoch 60/100\n",
      "Training Loss: 0.14245156255781385\n",
      "Validation Loss: 0.11053210163790254\n",
      "Epoch 61/100\n",
      "Training Loss: 0.12690465066295817\n",
      "Validation Loss: 0.08155195402152558\n",
      "Epoch 62/100\n",
      "Training Loss: 0.12758481431009483\n",
      "Validation Loss: 0.07163265015550829\n",
      "Epoch 63/100\n",
      "Training Loss: 0.16802843209831575\n",
      "Validation Loss: 0.0676962262165424\n",
      "Epoch 64/100\n",
      "Training Loss: 0.13205269339562978\n",
      "Validation Loss: 0.06082321667624847\n",
      "Epoch 65/100\n",
      "Training Loss: 0.13081663713986044\n",
      "Validation Loss: 0.06493933699365131\n",
      "Epoch 66/100\n",
      "Training Loss: 0.11374163491999296\n",
      "Validation Loss: 0.10158293211592448\n",
      "Epoch 67/100\n",
      "Training Loss: 0.13971780843100468\n",
      "Validation Loss: 0.04186125714742707\n",
      "Epoch 68/100\n",
      "Training Loss: 0.12610940924259956\n",
      "Validation Loss: 0.0992206991568045\n",
      "Epoch 69/100\n",
      "Training Loss: 0.12193006455628999\n",
      "Validation Loss: 0.04073159363135614\n",
      "Epoch 70/100\n",
      "Training Loss: 0.10441039032880382\n",
      "Validation Loss: 0.12535522371892632\n",
      "Epoch 71/100\n",
      "Training Loss: 0.13232488291202965\n",
      "Validation Loss: 0.08946623586856706\n",
      "Epoch 72/100\n",
      "Training Loss: 0.13919091856560042\n",
      "Validation Loss: 0.11033139721825533\n",
      "Epoch 73/100\n",
      "Training Loss: 0.13197225682808025\n",
      "Validation Loss: 0.07783627920941119\n",
      "Epoch 74/100\n",
      "Training Loss: 0.14299942284837036\n",
      "Validation Loss: 0.06607064228399542\n",
      "Epoch 75/100\n",
      "Training Loss: 0.13186531449568298\n",
      "Validation Loss: 0.08892429797144656\n",
      "Epoch 76/100\n",
      "Training Loss: 0.1344518821024787\n",
      "Validation Loss: 0.05921892271021645\n",
      "Epoch 77/100\n",
      "Training Loss: 0.16417861545297005\n",
      "Validation Loss: 0.0864178784460819\n",
      "Epoch 78/100\n",
      "Training Loss: 0.12486455051953119\n",
      "Validation Loss: 0.05685705685241534\n",
      "Epoch 79/100\n",
      "Training Loss: 0.12364680244644959\n",
      "Validation Loss: 0.07295481984923316\n",
      "Epoch 80/100\n",
      "Training Loss: 0.17352177760515072\n",
      "Validation Loss: 0.04278763623362291\n",
      "Epoch 81/100\n",
      "Training Loss: 0.14436004177736197\n",
      "Validation Loss: 0.062250432287667634\n",
      "Epoch 82/100\n",
      "Training Loss: 0.14008658082582626\n",
      "Validation Loss: 0.07635659262372325\n",
      "Epoch 83/100\n",
      "Training Loss: 0.13492083067507243\n",
      "Validation Loss: 0.07470936670781203\n",
      "Epoch 84/100\n",
      "Training Loss: 0.13169013302555058\n",
      "Validation Loss: 0.08487544349443214\n",
      "Epoch 85/100\n",
      "Training Loss: 0.12465909373000235\n",
      "Validation Loss: 0.07805784385677132\n",
      "Epoch 86/100\n",
      "Training Loss: 0.11804435142610435\n",
      "Validation Loss: 0.06848507446485803\n",
      "Epoch 87/100\n",
      "Training Loss: 0.1412073773989726\n",
      "Validation Loss: 0.08069289784852546\n",
      "Epoch 88/100\n",
      "Training Loss: 0.12524403451776894\n",
      "Validation Loss: 0.045803057555542946\n",
      "Epoch 89/100\n",
      "Training Loss: 0.11329917911503916\n",
      "Validation Loss: 0.055404858985628656\n",
      "Epoch 90/100\n",
      "Training Loss: 0.11651369754049241\n",
      "Validation Loss: 0.06326240346744916\n",
      "Epoch 91/100\n",
      "Training Loss: 0.11568691557628549\n",
      "Validation Loss: 0.08927759841164176\n",
      "Epoch 92/100\n",
      "Training Loss: 0.11082302666880121\n",
      "Validation Loss: 0.053642601326014415\n",
      "Epoch 93/100\n",
      "Training Loss: 0.12806624085108337\n",
      "Validation Loss: 0.07684933827902943\n",
      "Epoch 94/100\n",
      "Training Loss: 0.1258681965946747\n",
      "Validation Loss: 0.10955972011248113\n",
      "Epoch 95/100\n",
      "Training Loss: 0.1250453950110862\n",
      "Validation Loss: 0.07073140678545738\n",
      "Epoch 96/100\n",
      "Training Loss: 0.13276666637276704\n",
      "Validation Loss: 0.0572074368318938\n",
      "Epoch 97/100\n",
      "Training Loss: 0.10352279937060979\n",
      "Validation Loss: 0.07303132450970422\n",
      "Epoch 98/100\n",
      "Training Loss: 0.1260859608718199\n",
      "Validation Loss: 0.11797238349691581\n",
      "Epoch 99/100\n",
      "Training Loss: 0.13820555309930543\n",
      "Validation Loss: 0.1328975880309435\n",
      "Epoch 100/100\n",
      "Training Loss: 0.11994615585811165\n",
      "Validation Loss: 0.07896726242038903\n",
      "Combination 3: Avg Training Loss = 0.16732175313861358, Avg Validation Loss = 0.10336155779368646\n",
      "Testing combination 4/48: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'sequence_length': 20, 'dropout_rate': 0.2}\n",
      "Truncated Train Data Shape: (70, 20, 1)\n",
      "Truncated Validation Data Shape: (10, 20, 1)\n",
      "    Trial 1/2 for combination 4/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.5344140891526639\n",
      "Validation Loss: 0.2928139702599991\n",
      "Epoch 2/100\n",
      "Training Loss: 0.5222985953367324\n",
      "Validation Loss: 0.23558933160256407\n",
      "Epoch 3/100\n",
      "Training Loss: 0.35673875791521875\n",
      "Validation Loss: 0.2698776027946181\n",
      "Epoch 4/100\n",
      "Training Loss: 0.3480192244604918\n",
      "Validation Loss: 0.23244830779792217\n",
      "Epoch 5/100\n",
      "Training Loss: 0.3231185881872787\n",
      "Validation Loss: 0.302269006721132\n",
      "Epoch 6/100\n",
      "Training Loss: 0.3378484287500448\n",
      "Validation Loss: 0.19101102738089176\n",
      "Epoch 7/100\n",
      "Training Loss: 0.3424633640802673\n",
      "Validation Loss: 0.147011576033991\n",
      "Epoch 8/100\n",
      "Training Loss: 0.2568306682299425\n",
      "Validation Loss: 0.13863630738203775\n",
      "Epoch 9/100\n",
      "Training Loss: 0.2110122325182812\n",
      "Validation Loss: 0.17339520353732424\n",
      "Epoch 10/100\n",
      "Training Loss: 0.2873045601758716\n",
      "Validation Loss: 0.11041093077927497\n",
      "Epoch 11/100\n",
      "Training Loss: 0.24465592079380905\n",
      "Validation Loss: 0.10604128810784381\n",
      "Epoch 12/100\n",
      "Training Loss: 0.32282842569318526\n",
      "Validation Loss: 0.06487340738688595\n",
      "Epoch 13/100\n",
      "Training Loss: 0.28939019788258763\n",
      "Validation Loss: 0.25185009168131617\n",
      "Epoch 14/100\n",
      "Training Loss: 0.259095751848601\n",
      "Validation Loss: 0.126988225149746\n",
      "Epoch 15/100\n",
      "Training Loss: 0.23586077831670263\n",
      "Validation Loss: 0.21899784814753218\n",
      "Epoch 16/100\n",
      "Training Loss: 0.2558861245975041\n",
      "Validation Loss: 0.110912602189837\n",
      "Epoch 17/100\n",
      "Training Loss: 0.27968840056753413\n",
      "Validation Loss: 0.1234514374770727\n",
      "Epoch 18/100\n",
      "Training Loss: 0.23409710386626875\n",
      "Validation Loss: 0.11179245257477333\n",
      "Epoch 19/100\n",
      "Training Loss: 0.2885858334494135\n",
      "Validation Loss: 0.09929026320596135\n",
      "Epoch 20/100\n",
      "Training Loss: 0.23082611646393722\n",
      "Validation Loss: 0.14880083147038042\n",
      "Epoch 21/100\n",
      "Training Loss: 0.2105170530948315\n",
      "Validation Loss: 0.14061922532832968\n",
      "Epoch 22/100\n",
      "Training Loss: 0.19958806852063765\n",
      "Validation Loss: 0.30784778355992964\n",
      "Epoch 23/100\n",
      "Training Loss: 0.2119007991608887\n",
      "Validation Loss: 0.09714118560331823\n",
      "Epoch 24/100\n",
      "Training Loss: 0.19217034545756145\n",
      "Validation Loss: 0.11600145874348548\n",
      "Epoch 25/100\n",
      "Training Loss: 0.18068621871304494\n",
      "Validation Loss: 0.13726160176300245\n",
      "Epoch 26/100\n",
      "Training Loss: 0.19116729986845582\n",
      "Validation Loss: 0.15812796816546978\n",
      "Epoch 27/100\n",
      "Training Loss: 0.23499694544987498\n",
      "Validation Loss: 0.10954412198790435\n",
      "Epoch 28/100\n",
      "Training Loss: 0.17482624304872046\n",
      "Validation Loss: 0.12840375989593358\n",
      "Epoch 29/100\n",
      "Training Loss: 0.20175346080192955\n",
      "Validation Loss: 0.0972223341578555\n",
      "Epoch 30/100\n",
      "Training Loss: 0.17252353385357563\n",
      "Validation Loss: 0.11551569267595752\n",
      "Epoch 31/100\n",
      "Training Loss: 0.1922139136627676\n",
      "Validation Loss: 0.10644758417290372\n",
      "Epoch 32/100\n",
      "Training Loss: 0.1615882955473827\n",
      "Validation Loss: 0.05809022537110663\n",
      "Epoch 33/100\n",
      "Training Loss: 0.16548472305434023\n",
      "Validation Loss: 0.10233260467571981\n",
      "Epoch 34/100\n",
      "Training Loss: 0.17356516698999522\n",
      "Validation Loss: 0.17957663245848302\n",
      "Epoch 35/100\n",
      "Training Loss: 0.16816733634651684\n",
      "Validation Loss: 0.07952235728470572\n",
      "Epoch 36/100\n",
      "Training Loss: 0.1618442376230935\n",
      "Validation Loss: 0.06456782408879883\n",
      "Epoch 37/100\n",
      "Training Loss: 0.1935749623815723\n",
      "Validation Loss: 0.107964398399743\n",
      "Epoch 38/100\n",
      "Training Loss: 0.16928763137895717\n",
      "Validation Loss: 0.1033797793923192\n",
      "Epoch 39/100\n",
      "Training Loss: 0.16366195748086082\n",
      "Validation Loss: 0.08015094627896531\n",
      "Epoch 40/100\n",
      "Training Loss: 0.18683762499056136\n",
      "Validation Loss: 0.11780022528213471\n",
      "Epoch 41/100\n",
      "Training Loss: 0.1943578867505842\n",
      "Validation Loss: 0.05834667503778892\n",
      "Epoch 42/100\n",
      "Training Loss: 0.18097551083958716\n",
      "Validation Loss: 0.07685980160614155\n",
      "Epoch 43/100\n",
      "Training Loss: 0.18423997717719834\n",
      "Validation Loss: 0.1432923069617393\n",
      "Epoch 44/100\n",
      "Training Loss: 0.16820005972360136\n",
      "Validation Loss: 0.10886392717756929\n",
      "Epoch 45/100\n",
      "Training Loss: 0.14703910108544047\n",
      "Validation Loss: 0.06759050016413168\n",
      "Epoch 46/100\n",
      "Training Loss: 0.15502506082940887\n",
      "Validation Loss: 0.13794358658113445\n",
      "Epoch 47/100\n",
      "Training Loss: 0.1834772652844945\n",
      "Validation Loss: 0.09532815932178941\n",
      "Epoch 48/100\n",
      "Training Loss: 0.15051133747298032\n",
      "Validation Loss: 0.08464031472965021\n",
      "Epoch 49/100\n",
      "Training Loss: 0.17010770366751138\n",
      "Validation Loss: 0.10269357130795635\n",
      "Epoch 50/100\n",
      "Training Loss: 0.15398656356181672\n",
      "Validation Loss: 0.08652814264979979\n",
      "Epoch 51/100\n",
      "Training Loss: 0.14957565976180165\n",
      "Validation Loss: 0.0585586058400687\n",
      "Epoch 52/100\n",
      "Training Loss: 0.13178396340143742\n",
      "Validation Loss: 0.1453499524738609\n",
      "Epoch 53/100\n",
      "Training Loss: 0.17340542122034316\n",
      "Validation Loss: 0.09585633105049143\n",
      "Epoch 54/100\n",
      "Training Loss: 0.1582825569317097\n",
      "Validation Loss: 0.06898045949550817\n",
      "Epoch 55/100\n",
      "Training Loss: 0.13555324215309655\n",
      "Validation Loss: 0.0839133906522169\n",
      "Epoch 56/100\n",
      "Training Loss: 0.1458510710383806\n",
      "Validation Loss: 0.06115219793524572\n",
      "Epoch 57/100\n",
      "Training Loss: 0.16026201706910384\n",
      "Validation Loss: 0.18333330518020416\n",
      "Epoch 58/100\n",
      "Training Loss: 0.15484070348430898\n",
      "Validation Loss: 0.06839532682327334\n",
      "Epoch 59/100\n",
      "Training Loss: 0.16394074304379083\n",
      "Validation Loss: 0.12211536771737178\n",
      "Epoch 60/100\n",
      "Training Loss: 0.13578366492791424\n",
      "Validation Loss: 0.10871435127146611\n",
      "Epoch 61/100\n",
      "Training Loss: 0.1528603284963454\n",
      "Validation Loss: 0.10652291768831992\n",
      "Epoch 62/100\n",
      "Training Loss: 0.16565350702218867\n",
      "Validation Loss: 0.12792013356035384\n",
      "Epoch 63/100\n",
      "Training Loss: 0.13619624427361438\n",
      "Validation Loss: 0.08516531965028669\n",
      "Epoch 64/100\n",
      "Training Loss: 0.18784448972077403\n",
      "Validation Loss: 0.06213302571752176\n",
      "Epoch 65/100\n",
      "Training Loss: 0.19201620064742003\n",
      "Validation Loss: 0.06520850039146862\n",
      "Epoch 66/100\n",
      "Training Loss: 0.16658989123870646\n",
      "Validation Loss: 0.08132003782073792\n",
      "Epoch 67/100\n",
      "Training Loss: 0.12535677649702034\n",
      "Validation Loss: 0.09014430860410338\n",
      "Epoch 68/100\n",
      "Training Loss: 0.14334376319157255\n",
      "Validation Loss: 0.08107451579567851\n",
      "Epoch 69/100\n",
      "Training Loss: 0.12445032287772942\n",
      "Validation Loss: 0.06756231599737132\n",
      "Epoch 70/100\n",
      "Training Loss: 0.15170749282642534\n",
      "Validation Loss: 0.08202834978647475\n",
      "Epoch 71/100\n",
      "Training Loss: 0.13606320793506385\n",
      "Validation Loss: 0.08232872295774914\n",
      "Epoch 72/100\n",
      "Training Loss: 0.13870722466238988\n",
      "Validation Loss: 0.12711419500507143\n",
      "Epoch 73/100\n",
      "Training Loss: 0.13930257552935488\n",
      "Validation Loss: 0.052268244375080944\n",
      "Epoch 74/100\n",
      "Training Loss: 0.1338088829636624\n",
      "Validation Loss: 0.06896117135593818\n",
      "Epoch 75/100\n",
      "Training Loss: 0.15646287151060198\n",
      "Validation Loss: 0.06456082092846681\n",
      "Epoch 76/100\n",
      "Training Loss: 0.1444155412145796\n",
      "Validation Loss: 0.04682294438126913\n",
      "Epoch 77/100\n",
      "Training Loss: 0.14404106911404962\n",
      "Validation Loss: 0.07817857683992274\n",
      "Epoch 78/100\n",
      "Training Loss: 0.12005319402369367\n",
      "Validation Loss: 0.0737158932567462\n",
      "Epoch 79/100\n",
      "Training Loss: 0.11178126823394718\n",
      "Validation Loss: 0.055160501966303156\n",
      "Epoch 80/100\n",
      "Training Loss: 0.12651934309378485\n",
      "Validation Loss: 0.09309018575518818\n",
      "Epoch 81/100\n",
      "Training Loss: 0.14420943375067555\n",
      "Validation Loss: 0.11680068447181235\n",
      "Epoch 82/100\n",
      "Training Loss: 0.1374006693566747\n",
      "Validation Loss: 0.0676866279051496\n",
      "Epoch 83/100\n",
      "Training Loss: 0.13126920417882854\n",
      "Validation Loss: 0.07346626295990617\n",
      "Epoch 84/100\n",
      "Training Loss: 0.1191343414003532\n",
      "Validation Loss: 0.09692520289996549\n",
      "Epoch 85/100\n",
      "Training Loss: 0.13058134507898786\n",
      "Validation Loss: 0.09328083846186144\n",
      "Epoch 86/100\n",
      "Training Loss: 0.10429387237691867\n",
      "Validation Loss: 0.09608977585264422\n",
      "Epoch 87/100\n",
      "Training Loss: 0.11794975018291355\n",
      "Validation Loss: 0.061790554504486096\n",
      "Epoch 88/100\n",
      "Training Loss: 0.12224618143451557\n",
      "Validation Loss: 0.06139538116195231\n",
      "Epoch 89/100\n",
      "Training Loss: 0.12569421091929436\n",
      "Validation Loss: 0.07368317920111664\n",
      "Epoch 90/100\n",
      "Training Loss: 0.11185004983313886\n",
      "Validation Loss: 0.07806439090218995\n",
      "Epoch 91/100\n",
      "Training Loss: 0.10666194508383087\n",
      "Validation Loss: 0.06749838641601567\n",
      "Epoch 92/100\n",
      "Training Loss: 0.13261383183273612\n",
      "Validation Loss: 0.09602136677500211\n",
      "Epoch 93/100\n",
      "Training Loss: 0.13044492987269474\n",
      "Validation Loss: 0.061490618594057876\n",
      "Epoch 94/100\n",
      "Training Loss: 0.12010633874115817\n",
      "Validation Loss: 0.0962093092623864\n",
      "Epoch 95/100\n",
      "Training Loss: 0.1167578313424351\n",
      "Validation Loss: 0.05160614027946907\n",
      "Epoch 96/100\n",
      "Training Loss: 0.12239279887582276\n",
      "Validation Loss: 0.07491579468295231\n",
      "Epoch 97/100\n",
      "Training Loss: 0.11387644979976393\n",
      "Validation Loss: 0.08440809938142856\n",
      "Epoch 98/100\n",
      "Training Loss: 0.1036829165644662\n",
      "Validation Loss: 0.10270250637278777\n",
      "Epoch 99/100\n",
      "Training Loss: 0.10298848516666899\n",
      "Validation Loss: 0.04412391254546906\n",
      "Epoch 100/100\n",
      "Training Loss: 0.14183104840902938\n",
      "Validation Loss: 0.06461411644389545\n",
      "    Trial 2/2 for combination 4/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.46418511829970605\n",
      "Validation Loss: 0.2354648547887767\n",
      "Epoch 2/100\n",
      "Training Loss: 0.4097043993602912\n",
      "Validation Loss: 0.24731906564342915\n",
      "Epoch 3/100\n",
      "Training Loss: 0.3645901858167163\n",
      "Validation Loss: 0.19709719911093418\n",
      "Epoch 4/100\n",
      "Training Loss: 0.34760122074178423\n",
      "Validation Loss: 0.2812171580813049\n",
      "Epoch 5/100\n",
      "Training Loss: 0.36678049002888935\n",
      "Validation Loss: 0.22053593397170584\n",
      "Epoch 6/100\n",
      "Training Loss: 0.285254539827347\n",
      "Validation Loss: 0.16401086019346178\n",
      "Epoch 7/100\n",
      "Training Loss: 0.2770585670693661\n",
      "Validation Loss: 0.08693011018501057\n",
      "Epoch 8/100\n",
      "Training Loss: 0.3104524874124677\n",
      "Validation Loss: 0.2959707310836648\n",
      "Epoch 9/100\n",
      "Training Loss: 0.2898385362809615\n",
      "Validation Loss: 0.26577347695901077\n",
      "Epoch 10/100\n",
      "Training Loss: 0.2909584205285015\n",
      "Validation Loss: 0.18173136731628284\n",
      "Epoch 11/100\n",
      "Training Loss: 0.20713108964061466\n",
      "Validation Loss: 0.28451429938802875\n",
      "Epoch 12/100\n",
      "Training Loss: 0.17569635841528133\n",
      "Validation Loss: 0.09293361296410094\n",
      "Epoch 13/100\n",
      "Training Loss: 0.20748808703268787\n",
      "Validation Loss: 0.17066471506075992\n",
      "Epoch 14/100\n",
      "Training Loss: 0.22564853217636052\n",
      "Validation Loss: 0.1615074748101377\n",
      "Epoch 15/100\n",
      "Training Loss: 0.19354409341582354\n",
      "Validation Loss: 0.14022521275962266\n",
      "Epoch 16/100\n",
      "Training Loss: 0.17452534163991976\n",
      "Validation Loss: 0.1339675558830275\n",
      "Epoch 17/100\n",
      "Training Loss: 0.1967134492760794\n",
      "Validation Loss: 0.18113484740296226\n",
      "Epoch 18/100\n",
      "Training Loss: 0.19130108393208212\n",
      "Validation Loss: 0.12634178548036196\n",
      "Epoch 19/100\n",
      "Training Loss: 0.18345917901092415\n",
      "Validation Loss: 0.1288410264273065\n",
      "Epoch 20/100\n",
      "Training Loss: 0.1611333398617238\n",
      "Validation Loss: 0.12718679435277408\n",
      "Epoch 21/100\n",
      "Training Loss: 0.1758332991597001\n",
      "Validation Loss: 0.0727292025063846\n",
      "Epoch 22/100\n",
      "Training Loss: 0.18708043375242597\n",
      "Validation Loss: 0.06160821862717063\n",
      "Epoch 23/100\n",
      "Training Loss: 0.2139926714067979\n",
      "Validation Loss: 0.09339676604075359\n",
      "Epoch 24/100\n",
      "Training Loss: 0.18174285806026128\n",
      "Validation Loss: 0.0623676671565669\n",
      "Epoch 25/100\n",
      "Training Loss: 0.17286775411007005\n",
      "Validation Loss: 0.17105397743489795\n",
      "Epoch 26/100\n",
      "Training Loss: 0.1470032926809674\n",
      "Validation Loss: 0.05781320025449922\n",
      "Epoch 27/100\n",
      "Training Loss: 0.1710951856049531\n",
      "Validation Loss: 0.11186628636173288\n",
      "Epoch 28/100\n",
      "Training Loss: 0.1996733914327369\n",
      "Validation Loss: 0.08920887401406263\n",
      "Epoch 29/100\n",
      "Training Loss: 0.19915426159393124\n",
      "Validation Loss: 0.06552381055406882\n",
      "Epoch 30/100\n",
      "Training Loss: 0.19668815913984572\n",
      "Validation Loss: 0.1431100594664901\n",
      "Epoch 31/100\n",
      "Training Loss: 0.14066292230795208\n",
      "Validation Loss: 0.05772327389169905\n",
      "Epoch 32/100\n",
      "Training Loss: 0.16557644791208484\n",
      "Validation Loss: 0.1146320626820309\n",
      "Epoch 33/100\n",
      "Training Loss: 0.1580657636710998\n",
      "Validation Loss: 0.11657256367917061\n",
      "Epoch 34/100\n",
      "Training Loss: 0.1388181639717523\n",
      "Validation Loss: 0.1290933603920859\n",
      "Epoch 35/100\n",
      "Training Loss: 0.15806075869202427\n",
      "Validation Loss: 0.0668013138011808\n",
      "Epoch 36/100\n",
      "Training Loss: 0.14241615949279757\n",
      "Validation Loss: 0.06143126538160576\n",
      "Epoch 37/100\n",
      "Training Loss: 0.1614902217507382\n",
      "Validation Loss: 0.06316519850051025\n",
      "Epoch 38/100\n",
      "Training Loss: 0.16272380227737573\n",
      "Validation Loss: 0.1265950082041873\n",
      "Epoch 39/100\n",
      "Training Loss: 0.13687610434517047\n",
      "Validation Loss: 0.10406564725122178\n",
      "Epoch 40/100\n",
      "Training Loss: 0.1782140643209908\n",
      "Validation Loss: 0.08488998362902869\n",
      "Epoch 41/100\n",
      "Training Loss: 0.17445963683741006\n",
      "Validation Loss: 0.16496692257244533\n",
      "Epoch 42/100\n",
      "Training Loss: 0.15247388046615215\n",
      "Validation Loss: 0.07854406048946554\n",
      "Epoch 43/100\n",
      "Training Loss: 0.1519660949687484\n",
      "Validation Loss: 0.08277960817208405\n",
      "Epoch 44/100\n",
      "Training Loss: 0.1589963006959896\n",
      "Validation Loss: 0.10548097974745856\n",
      "Epoch 45/100\n",
      "Training Loss: 0.1665742750941395\n",
      "Validation Loss: 0.08158804001430457\n",
      "Epoch 46/100\n",
      "Training Loss: 0.1214966806304752\n",
      "Validation Loss: 0.08549530533042668\n",
      "Epoch 47/100\n",
      "Training Loss: 0.13570789151432966\n",
      "Validation Loss: 0.04666674937939029\n",
      "Epoch 48/100\n",
      "Training Loss: 0.14141059795658864\n",
      "Validation Loss: 0.1279079971710551\n",
      "Epoch 49/100\n",
      "Training Loss: 0.12976712281803138\n",
      "Validation Loss: 0.08485196574606524\n",
      "Epoch 50/100\n",
      "Training Loss: 0.12488077730864222\n",
      "Validation Loss: 0.07579992266694136\n",
      "Epoch 51/100\n",
      "Training Loss: 0.11249163386644614\n",
      "Validation Loss: 0.07729543549462606\n",
      "Epoch 52/100\n",
      "Training Loss: 0.1252621572269495\n",
      "Validation Loss: 0.05639185785912364\n",
      "Epoch 53/100\n",
      "Training Loss: 0.14386907394901285\n",
      "Validation Loss: 0.06722580753515907\n",
      "Epoch 54/100\n",
      "Training Loss: 0.1326032791435484\n",
      "Validation Loss: 0.03890099347169865\n",
      "Epoch 55/100\n",
      "Training Loss: 0.1421044936184053\n",
      "Validation Loss: 0.06004769925367813\n",
      "Epoch 56/100\n",
      "Training Loss: 0.12023593534698528\n",
      "Validation Loss: 0.06157043507186102\n",
      "Epoch 57/100\n",
      "Training Loss: 0.12018406391263\n",
      "Validation Loss: 0.047155365344000486\n",
      "Epoch 58/100\n",
      "Training Loss: 0.14072297030639153\n",
      "Validation Loss: 0.12638842695952965\n",
      "Epoch 59/100\n",
      "Training Loss: 0.13262687013797128\n",
      "Validation Loss: 0.13754364653994328\n",
      "Epoch 60/100\n",
      "Training Loss: 0.13600486914164436\n",
      "Validation Loss: 0.05992239854201503\n",
      "Epoch 61/100\n",
      "Training Loss: 0.13267045507129854\n",
      "Validation Loss: 0.09345977347508017\n",
      "Epoch 62/100\n",
      "Training Loss: 0.13195739238892015\n",
      "Validation Loss: 0.10089778150846188\n",
      "Epoch 63/100\n",
      "Training Loss: 0.12580601358299218\n",
      "Validation Loss: 0.07568594559087209\n",
      "Epoch 64/100\n",
      "Training Loss: 0.13157101707280278\n",
      "Validation Loss: 0.05499290922065046\n",
      "Epoch 65/100\n",
      "Training Loss: 0.13410133788920983\n",
      "Validation Loss: 0.10097856284351114\n",
      "Epoch 66/100\n",
      "Training Loss: 0.11327715327408745\n",
      "Validation Loss: 0.09786638673761125\n",
      "Epoch 67/100\n",
      "Training Loss: 0.1038122825936991\n",
      "Validation Loss: 0.06376271258957097\n",
      "Epoch 68/100\n",
      "Training Loss: 0.11462039861089447\n",
      "Validation Loss: 0.07448917425419242\n",
      "Epoch 69/100\n",
      "Training Loss: 0.11327001559204103\n",
      "Validation Loss: 0.07030289707749032\n",
      "Epoch 70/100\n",
      "Training Loss: 0.13447722240961976\n",
      "Validation Loss: 0.06808252729848158\n",
      "Epoch 71/100\n",
      "Training Loss: 0.11109991951997647\n",
      "Validation Loss: 0.057963143901816915\n",
      "Epoch 72/100\n",
      "Training Loss: 0.12269646879360736\n",
      "Validation Loss: 0.09006920029589698\n",
      "Epoch 73/100\n",
      "Training Loss: 0.12265557855206785\n",
      "Validation Loss: 0.09421280885784704\n",
      "Epoch 74/100\n",
      "Training Loss: 0.09926088334362286\n",
      "Validation Loss: 0.068425944821477\n",
      "Epoch 75/100\n",
      "Training Loss: 0.11241227493481319\n",
      "Validation Loss: 0.054633571481980094\n",
      "Epoch 76/100\n",
      "Training Loss: 0.1125201871323154\n",
      "Validation Loss: 0.056201931920282854\n",
      "Epoch 77/100\n",
      "Training Loss: 0.12256421294051835\n",
      "Validation Loss: 0.06951695750015871\n",
      "Epoch 78/100\n",
      "Training Loss: 0.11307914985899531\n",
      "Validation Loss: 0.058685447595629256\n",
      "Epoch 79/100\n",
      "Training Loss: 0.12030207206773798\n",
      "Validation Loss: 0.12269802987612159\n",
      "Epoch 80/100\n",
      "Training Loss: 0.11398064996124982\n",
      "Validation Loss: 0.05053514755398919\n",
      "Epoch 81/100\n",
      "Training Loss: 0.13412746792802588\n",
      "Validation Loss: 0.05191629891738152\n",
      "Epoch 82/100\n",
      "Training Loss: 0.11814645806608125\n",
      "Validation Loss: 0.05408712321804167\n",
      "Epoch 83/100\n",
      "Training Loss: 0.10775397526878427\n",
      "Validation Loss: 0.07040286957444683\n",
      "Epoch 84/100\n",
      "Training Loss: 0.10552842242035743\n",
      "Validation Loss: 0.06003174531933024\n",
      "Epoch 85/100\n",
      "Training Loss: 0.09728106998682906\n",
      "Validation Loss: 0.0423334861522776\n",
      "Epoch 86/100\n",
      "Training Loss: 0.10739961222443707\n",
      "Validation Loss: 0.05049202624337891\n",
      "Epoch 87/100\n",
      "Training Loss: 0.10053597306813443\n",
      "Validation Loss: 0.06901566212325702\n",
      "Epoch 88/100\n",
      "Training Loss: 0.10370379908009036\n",
      "Validation Loss: 0.047657088921410524\n",
      "Epoch 89/100\n",
      "Training Loss: 0.12024464917312853\n",
      "Validation Loss: 0.09213780733751772\n",
      "Epoch 90/100\n",
      "Training Loss: 0.1029721779362566\n",
      "Validation Loss: 0.07400792990916073\n",
      "Epoch 91/100\n",
      "Training Loss: 0.10606121216342268\n",
      "Validation Loss: 0.05144606243267156\n",
      "Epoch 92/100\n",
      "Training Loss: 0.09510501092847605\n",
      "Validation Loss: 0.06908926037081584\n",
      "Epoch 93/100\n",
      "Training Loss: 0.12412876852954478\n",
      "Validation Loss: 0.06503401735288954\n",
      "Epoch 94/100\n",
      "Training Loss: 0.09991670391090983\n",
      "Validation Loss: 0.08031416048184438\n",
      "Epoch 95/100\n",
      "Training Loss: 0.09194247515088325\n",
      "Validation Loss: 0.05523535277464996\n",
      "Epoch 96/100\n",
      "Training Loss: 0.1082389428674483\n",
      "Validation Loss: 0.04954555068985534\n",
      "Epoch 97/100\n",
      "Training Loss: 0.09822587793586844\n",
      "Validation Loss: 0.06376372672245623\n",
      "Epoch 98/100\n",
      "Training Loss: 0.09332027518335946\n",
      "Validation Loss: 0.07022001007574745\n",
      "Epoch 99/100\n",
      "Training Loss: 0.09771590255342642\n",
      "Validation Loss: 0.059261658330111044\n",
      "Epoch 100/100\n",
      "Training Loss: 0.08396293170404794\n",
      "Validation Loss: 0.09226460488609982\n",
      "Combination 4: Avg Training Loss = 0.17177531775235963, Avg Validation Loss = 0.10505873130570466\n",
      "Testing combination 5/48: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'sequence_length': 50, 'dropout_rate': 0.1}\n",
      "Truncated Train Data Shape: (70, 50, 1)\n",
      "Truncated Validation Data Shape: (10, 50, 1)\n",
      "    Trial 1/2 for combination 5/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.48024313140986785\n",
      "Validation Loss: 0.1549162613319845\n",
      "Epoch 2/100\n",
      "Training Loss: 0.2727189138637394\n",
      "Validation Loss: 0.3010643565685266\n",
      "Epoch 3/100\n",
      "Training Loss: 0.3324203371302658\n",
      "Validation Loss: 0.18465147787170827\n",
      "Epoch 4/100\n",
      "Training Loss: 0.21652320185209353\n",
      "Validation Loss: 0.06803678876964346\n",
      "Epoch 5/100\n",
      "Training Loss: 0.17286818083863684\n",
      "Validation Loss: 0.12831504123986692\n",
      "Epoch 6/100\n",
      "Training Loss: 0.18251731992873324\n",
      "Validation Loss: 0.1626643610965053\n",
      "Epoch 7/100\n",
      "Training Loss: 0.19252958344088408\n",
      "Validation Loss: 0.09533136141840323\n",
      "Epoch 8/100\n",
      "Training Loss: 0.17919979473947678\n",
      "Validation Loss: 0.07361398075978201\n",
      "Epoch 9/100\n",
      "Training Loss: 0.1549901347672012\n",
      "Validation Loss: 0.1662206157881863\n",
      "Epoch 10/100\n",
      "Training Loss: 0.18094433273636137\n",
      "Validation Loss: 0.03836885408920011\n",
      "Epoch 11/100\n",
      "Training Loss: 0.17041927363636214\n",
      "Validation Loss: 0.0614598531556062\n",
      "Epoch 12/100\n",
      "Training Loss: 0.20047908584064744\n",
      "Validation Loss: 0.05310779561269713\n",
      "Epoch 13/100\n",
      "Training Loss: 0.18242617879503195\n",
      "Validation Loss: 0.13009125819384365\n",
      "Epoch 14/100\n",
      "Training Loss: 0.1793158709278215\n",
      "Validation Loss: 0.0899701606395318\n",
      "Epoch 15/100\n",
      "Training Loss: 0.1588305219386439\n",
      "Validation Loss: 0.16066107352670614\n",
      "Epoch 16/100\n",
      "Training Loss: 0.18004479802996423\n",
      "Validation Loss: 0.05363553387892631\n",
      "Epoch 17/100\n",
      "Training Loss: 0.13952762176182754\n",
      "Validation Loss: 0.0689772485238227\n",
      "Epoch 18/100\n",
      "Training Loss: 0.18156964802562556\n",
      "Validation Loss: 0.0770594987036377\n",
      "Epoch 19/100\n",
      "Training Loss: 0.14428885138893832\n",
      "Validation Loss: 0.07896757321596248\n",
      "Epoch 20/100\n",
      "Training Loss: 0.14203170023868275\n",
      "Validation Loss: 0.05662529602034226\n",
      "Epoch 21/100\n",
      "Training Loss: 0.12794607128443422\n",
      "Validation Loss: 0.13053934832811492\n",
      "Epoch 22/100\n",
      "Training Loss: 0.1243648627094528\n",
      "Validation Loss: 0.06720960400499285\n",
      "Epoch 23/100\n",
      "Training Loss: 0.14326759295321206\n",
      "Validation Loss: 0.055875657075182696\n",
      "Epoch 24/100\n",
      "Training Loss: 0.12989993638895597\n",
      "Validation Loss: 0.0867200253361645\n",
      "Epoch 25/100\n",
      "Training Loss: 0.14494523297784112\n",
      "Validation Loss: 0.15537527117100441\n",
      "Epoch 26/100\n",
      "Training Loss: 0.115580548602541\n",
      "Validation Loss: 0.10009682953205228\n",
      "Epoch 27/100\n",
      "Training Loss: 0.11062640487154761\n",
      "Validation Loss: 0.13793898005060917\n",
      "Epoch 28/100\n",
      "Training Loss: 0.12744402134949495\n",
      "Validation Loss: 0.16419534416358286\n",
      "Epoch 29/100\n",
      "Training Loss: 0.12320125622869241\n",
      "Validation Loss: 0.05620539369385209\n",
      "Epoch 30/100\n",
      "Training Loss: 0.1464642691129205\n",
      "Validation Loss: 0.08699109356959629\n",
      "Epoch 31/100\n",
      "Training Loss: 0.12210486167063728\n",
      "Validation Loss: 0.06734633432522222\n",
      "Epoch 32/100\n",
      "Training Loss: 0.12340404659424153\n",
      "Validation Loss: 0.060439093846455696\n",
      "Epoch 33/100\n",
      "Training Loss: 0.13288284817090323\n",
      "Validation Loss: 0.04826106793038644\n",
      "Epoch 34/100\n",
      "Training Loss: 0.11330207184866077\n",
      "Validation Loss: 0.08675835593793854\n",
      "Epoch 35/100\n",
      "Training Loss: 0.1241618096031496\n",
      "Validation Loss: 0.07343547860674543\n",
      "Epoch 36/100\n",
      "Training Loss: 0.15853009748094984\n",
      "Validation Loss: 0.07382355857848161\n",
      "Epoch 37/100\n",
      "Training Loss: 0.14753048514592318\n",
      "Validation Loss: 0.048989456012534856\n",
      "Epoch 38/100\n",
      "Training Loss: 0.11694556612123248\n",
      "Validation Loss: 0.05777245442679775\n",
      "Epoch 39/100\n",
      "Training Loss: 0.13205682529100918\n",
      "Validation Loss: 0.09558689115917915\n",
      "Epoch 40/100\n",
      "Training Loss: 0.13756910173794082\n",
      "Validation Loss: 0.07691070522510358\n",
      "Epoch 41/100\n",
      "Training Loss: 0.13153700975001792\n",
      "Validation Loss: 0.07101780692580581\n",
      "Epoch 42/100\n",
      "Training Loss: 0.10346420544304145\n",
      "Validation Loss: 0.05864422712901333\n",
      "Epoch 43/100\n",
      "Training Loss: 0.13467438676345722\n",
      "Validation Loss: 0.06150015939852449\n",
      "Epoch 44/100\n",
      "Training Loss: 0.14011921894536514\n",
      "Validation Loss: 0.07919423940066386\n",
      "Epoch 45/100\n",
      "Training Loss: 0.1272180066442083\n",
      "Validation Loss: 0.05790002979048501\n",
      "Epoch 46/100\n",
      "Training Loss: 0.09623704075738702\n",
      "Validation Loss: 0.04927979965165324\n",
      "Epoch 47/100\n",
      "Training Loss: 0.10176675728129493\n",
      "Validation Loss: 0.08965899041283533\n",
      "Epoch 48/100\n",
      "Training Loss: 0.12389796451522214\n",
      "Validation Loss: 0.08890174069707142\n",
      "Epoch 49/100\n",
      "Training Loss: 0.11160551960867893\n",
      "Validation Loss: 0.12789563547731558\n",
      "Epoch 50/100\n",
      "Training Loss: 0.10110247816943836\n",
      "Validation Loss: 0.0602765056869062\n",
      "Epoch 51/100\n",
      "Training Loss: 0.11183304898162688\n",
      "Validation Loss: 0.07140087938804132\n",
      "Epoch 52/100\n",
      "Training Loss: 0.1095578730719735\n",
      "Validation Loss: 0.04830980320632441\n",
      "Epoch 53/100\n",
      "Training Loss: 0.11827952428362042\n",
      "Validation Loss: 0.08784269767386786\n",
      "Epoch 54/100\n",
      "Training Loss: 0.13156155983048806\n",
      "Validation Loss: 0.09508680327521418\n",
      "Epoch 55/100\n",
      "Training Loss: 0.12569669636354497\n",
      "Validation Loss: 0.07835176207360281\n",
      "Epoch 56/100\n",
      "Training Loss: 0.10541874466763398\n",
      "Validation Loss: 0.06440132606351699\n",
      "Epoch 57/100\n",
      "Training Loss: 0.12658471887870656\n",
      "Validation Loss: 0.06490942176602424\n",
      "Epoch 58/100\n",
      "Training Loss: 0.11021800839176728\n",
      "Validation Loss: 0.06255859613721779\n",
      "Epoch 59/100\n",
      "Training Loss: 0.12345714263716445\n",
      "Validation Loss: 0.0622677720157668\n",
      "Epoch 60/100\n",
      "Training Loss: 0.10663768889463869\n",
      "Validation Loss: 0.06356431700703213\n",
      "Epoch 61/100\n",
      "Training Loss: 0.11652059307999786\n",
      "Validation Loss: 0.11127040328341031\n",
      "Epoch 62/100\n",
      "Training Loss: 0.09897514184615643\n",
      "Validation Loss: 0.09827186248031815\n",
      "Epoch 63/100\n",
      "Training Loss: 0.09815391071686963\n",
      "Validation Loss: 0.09103546299007503\n",
      "Epoch 64/100\n",
      "Training Loss: 0.10861396149157324\n",
      "Validation Loss: 0.11448489679010967\n",
      "Epoch 65/100\n",
      "Training Loss: 0.1156352325370022\n",
      "Validation Loss: 0.10248603318306262\n",
      "Epoch 66/100\n",
      "Training Loss: 0.10239919677333252\n",
      "Validation Loss: 0.08616327478785475\n",
      "Epoch 67/100\n",
      "Training Loss: 0.11528164955098252\n",
      "Validation Loss: 0.06271213013896337\n",
      "Epoch 68/100\n",
      "Training Loss: 0.11928066750664905\n",
      "Validation Loss: 0.062072540135655885\n",
      "Epoch 69/100\n",
      "Training Loss: 0.1038929745723346\n",
      "Validation Loss: 0.06250041995067787\n",
      "Epoch 70/100\n",
      "Training Loss: 0.11048310843596171\n",
      "Validation Loss: 0.05449658598557455\n",
      "Epoch 71/100\n",
      "Training Loss: 0.09608378014557277\n",
      "Validation Loss: 0.06721145343918873\n",
      "Epoch 72/100\n",
      "Training Loss: 0.10323959012673765\n",
      "Validation Loss: 0.08774681288962399\n",
      "Epoch 73/100\n",
      "Training Loss: 0.10674512070226168\n",
      "Validation Loss: 0.051715269291568736\n",
      "Epoch 74/100\n",
      "Training Loss: 0.08666794000223611\n",
      "Validation Loss: 0.11127964725872071\n",
      "Epoch 75/100\n",
      "Training Loss: 0.1099593434725652\n",
      "Validation Loss: 0.07390742264182606\n",
      "Epoch 76/100\n",
      "Training Loss: 0.10943698007205184\n",
      "Validation Loss: 0.07116068094343041\n",
      "Epoch 77/100\n",
      "Training Loss: 0.09168212855461977\n",
      "Validation Loss: 0.056455437308879565\n",
      "Epoch 78/100\n",
      "Training Loss: 0.10442823900384283\n",
      "Validation Loss: 0.05550155121176612\n",
      "Epoch 79/100\n",
      "Training Loss: 0.11107172529017341\n",
      "Validation Loss: 0.06739146977962703\n",
      "Epoch 80/100\n",
      "Training Loss: 0.09851878438094179\n",
      "Validation Loss: 0.06865823183375339\n",
      "Epoch 81/100\n",
      "Training Loss: 0.10274500774747704\n",
      "Validation Loss: 0.05502531200968368\n",
      "Epoch 82/100\n",
      "Training Loss: 0.09185858621654266\n",
      "Validation Loss: 0.07270982815079166\n",
      "Epoch 83/100\n",
      "Training Loss: 0.11330583623385998\n",
      "Validation Loss: 0.05979168307580377\n",
      "Epoch 84/100\n",
      "Training Loss: 0.08937390088253419\n",
      "Validation Loss: 0.0764474392212681\n",
      "Epoch 85/100\n",
      "Training Loss: 0.10799850333953093\n",
      "Validation Loss: 0.04729766141041694\n",
      "Epoch 86/100\n",
      "Training Loss: 0.09298161871217551\n",
      "Validation Loss: 0.07158900638252529\n",
      "Epoch 87/100\n",
      "Training Loss: 0.0939687907365263\n",
      "Validation Loss: 0.09958682143195992\n",
      "Epoch 88/100\n",
      "Training Loss: 0.11568569888871828\n",
      "Validation Loss: 0.05374409946694344\n",
      "Epoch 89/100\n",
      "Training Loss: 0.09760920763291289\n",
      "Validation Loss: 0.04796020230508778\n",
      "Epoch 90/100\n",
      "Training Loss: 0.10390642898612004\n",
      "Validation Loss: 0.04893986671789134\n",
      "Epoch 91/100\n",
      "Training Loss: 0.09066816818650969\n",
      "Validation Loss: 0.05990388848810817\n",
      "Epoch 92/100\n",
      "Training Loss: 0.10063106748713478\n",
      "Validation Loss: 0.08372763092739813\n",
      "Epoch 93/100\n",
      "Training Loss: 0.09809968403588387\n",
      "Validation Loss: 0.04464167435131675\n",
      "Epoch 94/100\n",
      "Training Loss: 0.10235086551153016\n",
      "Validation Loss: 0.05185056036151927\n",
      "Epoch 95/100\n",
      "Training Loss: 0.10716027840557622\n",
      "Validation Loss: 0.06942025716922565\n",
      "Epoch 96/100\n",
      "Training Loss: 0.10567369870481969\n",
      "Validation Loss: 0.04632673234206165\n",
      "Epoch 97/100\n",
      "Training Loss: 0.10069297674601368\n",
      "Validation Loss: 0.04819773335679543\n",
      "Epoch 98/100\n",
      "Training Loss: 0.10254423824102943\n",
      "Validation Loss: 0.062495989996184095\n",
      "Epoch 99/100\n",
      "Training Loss: 0.09126291215149014\n",
      "Validation Loss: 0.04843254933918849\n",
      "Epoch 100/100\n",
      "Training Loss: 0.10660365581505811\n",
      "Validation Loss: 0.07872429595960463\n",
      "    Trial 2/2 for combination 5/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.5063239554624981\n",
      "Validation Loss: 0.1741444410403473\n",
      "Epoch 2/100\n",
      "Training Loss: 0.3217112179147123\n",
      "Validation Loss: 0.10061770752917332\n",
      "Epoch 3/100\n",
      "Training Loss: 0.28920506260793183\n",
      "Validation Loss: 0.35840895598634376\n",
      "Epoch 4/100\n",
      "Training Loss: 0.22060146989849072\n",
      "Validation Loss: 0.13044450374616298\n",
      "Epoch 5/100\n",
      "Training Loss: 0.2400369229024404\n",
      "Validation Loss: 0.10453946953954407\n",
      "Epoch 6/100\n",
      "Training Loss: 0.21865081750708995\n",
      "Validation Loss: 0.08456278557942812\n",
      "Epoch 7/100\n",
      "Training Loss: 0.23115534093384832\n",
      "Validation Loss: 0.17993192334566555\n",
      "Epoch 8/100\n",
      "Training Loss: 0.17629533356496277\n",
      "Validation Loss: 0.17390247118045404\n",
      "Epoch 9/100\n",
      "Training Loss: 0.14745718571346617\n",
      "Validation Loss: 0.12482856315648552\n",
      "Epoch 10/100\n",
      "Training Loss: 0.2398145645267956\n",
      "Validation Loss: 0.1443213594173462\n",
      "Epoch 11/100\n",
      "Training Loss: 0.21521651536031883\n",
      "Validation Loss: 0.1013024885910164\n",
      "Epoch 12/100\n",
      "Training Loss: 0.1548660123786608\n",
      "Validation Loss: 0.11022233735174866\n",
      "Epoch 13/100\n",
      "Training Loss: 0.17131334725873323\n",
      "Validation Loss: 0.12243471482057992\n",
      "Epoch 14/100\n",
      "Training Loss: 0.18154231758699138\n",
      "Validation Loss: 0.13191588993718958\n",
      "Epoch 15/100\n",
      "Training Loss: 0.15950330170298707\n",
      "Validation Loss: 0.07010422183936121\n",
      "Epoch 16/100\n",
      "Training Loss: 0.20714221376159314\n",
      "Validation Loss: 0.2114748843554069\n",
      "Epoch 17/100\n",
      "Training Loss: 0.140239324215284\n",
      "Validation Loss: 0.08451026297740778\n",
      "Epoch 18/100\n",
      "Training Loss: 0.14914025280564994\n",
      "Validation Loss: 0.06367210984151825\n",
      "Epoch 19/100\n",
      "Training Loss: 0.15155367474675255\n",
      "Validation Loss: 0.08533188952772956\n",
      "Epoch 20/100\n",
      "Training Loss: 0.12445429634573504\n",
      "Validation Loss: 0.06920082296568315\n",
      "Epoch 21/100\n",
      "Training Loss: 0.17400691109101138\n",
      "Validation Loss: 0.1317187755396055\n",
      "Epoch 22/100\n",
      "Training Loss: 0.15461072769150688\n",
      "Validation Loss: 0.057733973181860986\n",
      "Epoch 23/100\n",
      "Training Loss: 0.14667894155772132\n",
      "Validation Loss: 0.10547696850692115\n",
      "Epoch 24/100\n",
      "Training Loss: 0.17577851467355318\n",
      "Validation Loss: 0.11085129524630555\n",
      "Epoch 25/100\n",
      "Training Loss: 0.13101451076484558\n",
      "Validation Loss: 0.09634498174194633\n",
      "Epoch 26/100\n",
      "Training Loss: 0.16570627595298934\n",
      "Validation Loss: 0.07622712308575604\n",
      "Epoch 27/100\n",
      "Training Loss: 0.13144665454567475\n",
      "Validation Loss: 0.08008173969948892\n",
      "Epoch 28/100\n",
      "Training Loss: 0.17182056824251019\n",
      "Validation Loss: 0.11411561974705275\n",
      "Epoch 29/100\n",
      "Training Loss: 0.17801361527173917\n",
      "Validation Loss: 0.0739843204643138\n",
      "Epoch 30/100\n",
      "Training Loss: 0.16824175734927466\n",
      "Validation Loss: 0.10925561911377571\n",
      "Epoch 31/100\n",
      "Training Loss: 0.17487902131523345\n",
      "Validation Loss: 0.0781926484195502\n",
      "Epoch 32/100\n",
      "Training Loss: 0.18988384974186723\n",
      "Validation Loss: 0.12479690049965539\n",
      "Epoch 33/100\n",
      "Training Loss: 0.1309910871404271\n",
      "Validation Loss: 0.14202386532081313\n",
      "Epoch 34/100\n",
      "Training Loss: 0.14200919489508437\n",
      "Validation Loss: 0.11677673301721023\n",
      "Epoch 35/100\n",
      "Training Loss: 0.14435865161069955\n",
      "Validation Loss: 0.0680492561610525\n",
      "Epoch 36/100\n",
      "Training Loss: 0.10970425924508374\n",
      "Validation Loss: 0.10860536047823063\n",
      "Epoch 37/100\n",
      "Training Loss: 0.11461988354468708\n",
      "Validation Loss: 0.07811405142852725\n",
      "Epoch 38/100\n",
      "Training Loss: 0.10935083867172894\n",
      "Validation Loss: 0.05496925995321037\n",
      "Epoch 39/100\n",
      "Training Loss: 0.12939839729712152\n",
      "Validation Loss: 0.08230878346666644\n",
      "Epoch 40/100\n",
      "Training Loss: 0.17756428380296768\n",
      "Validation Loss: 0.07893036478572493\n",
      "Epoch 41/100\n",
      "Training Loss: 0.1463106427790001\n",
      "Validation Loss: 0.09383226186221064\n",
      "Epoch 42/100\n",
      "Training Loss: 0.13342438016040617\n",
      "Validation Loss: 0.06816544258459112\n",
      "Epoch 43/100\n",
      "Training Loss: 0.11432970069551747\n",
      "Validation Loss: 0.0725285918926852\n",
      "Epoch 44/100\n",
      "Training Loss: 0.1351620070146959\n",
      "Validation Loss: 0.06545027357234041\n",
      "Epoch 45/100\n",
      "Training Loss: 0.1559070127702469\n",
      "Validation Loss: 0.10524972343967035\n",
      "Epoch 46/100\n",
      "Training Loss: 0.14631076384405609\n",
      "Validation Loss: 0.04008028978168452\n",
      "Epoch 47/100\n",
      "Training Loss: 0.1208487583573681\n",
      "Validation Loss: 0.07599497353623405\n",
      "Epoch 48/100\n",
      "Training Loss: 0.13722809738645475\n",
      "Validation Loss: 0.1196137431986565\n",
      "Epoch 49/100\n",
      "Training Loss: 0.1344818566808484\n",
      "Validation Loss: 0.04428138686204192\n",
      "Epoch 50/100\n",
      "Training Loss: 0.12269005979141373\n",
      "Validation Loss: 0.05343946952972363\n",
      "Epoch 51/100\n",
      "Training Loss: 0.12654238668582737\n",
      "Validation Loss: 0.07974147423520489\n",
      "Epoch 52/100\n",
      "Training Loss: 0.11120143005896982\n",
      "Validation Loss: 0.051395172317960716\n",
      "Epoch 53/100\n",
      "Training Loss: 0.11906403314680909\n",
      "Validation Loss: 0.07370252826073052\n",
      "Epoch 54/100\n",
      "Training Loss: 0.13530923130769\n",
      "Validation Loss: 0.08605604503817067\n",
      "Epoch 55/100\n",
      "Training Loss: 0.12524058755380718\n",
      "Validation Loss: 0.07067384987570977\n",
      "Epoch 56/100\n",
      "Training Loss: 0.12262845042351374\n",
      "Validation Loss: 0.1018171534351642\n",
      "Epoch 57/100\n",
      "Training Loss: 0.12549046084367277\n",
      "Validation Loss: 0.06524263968354573\n",
      "Epoch 58/100\n",
      "Training Loss: 0.11124249324105176\n",
      "Validation Loss: 0.06643980860320192\n",
      "Epoch 59/100\n",
      "Training Loss: 0.13154618270715104\n",
      "Validation Loss: 0.06999369036891336\n",
      "Epoch 60/100\n",
      "Training Loss: 0.1208787434946718\n",
      "Validation Loss: 0.07055879685871533\n",
      "Epoch 61/100\n",
      "Training Loss: 0.1157259769140753\n",
      "Validation Loss: 0.061760854261614415\n",
      "Epoch 62/100\n",
      "Training Loss: 0.12104818800852588\n",
      "Validation Loss: 0.12161566249252535\n",
      "Epoch 63/100\n",
      "Training Loss: 0.13913672566775492\n",
      "Validation Loss: 0.09505320823390524\n",
      "Epoch 64/100\n",
      "Training Loss: 0.11108028844411579\n",
      "Validation Loss: 0.09939515086086838\n",
      "Epoch 65/100\n",
      "Training Loss: 0.11706753510379368\n",
      "Validation Loss: 0.06345183770075423\n",
      "Epoch 66/100\n",
      "Training Loss: 0.1135900533645992\n",
      "Validation Loss: 0.056685998136106155\n",
      "Epoch 67/100\n",
      "Training Loss: 0.11553611900290758\n",
      "Validation Loss: 0.07541338867640322\n",
      "Epoch 68/100\n",
      "Training Loss: 0.12875754495304953\n",
      "Validation Loss: 0.051217146196447495\n",
      "Epoch 69/100\n",
      "Training Loss: 0.13022524110240855\n",
      "Validation Loss: 0.06454541721561853\n",
      "Epoch 70/100\n",
      "Training Loss: 0.12029355535695567\n",
      "Validation Loss: 0.06453160313645631\n",
      "Epoch 71/100\n",
      "Training Loss: 0.12061706901282199\n",
      "Validation Loss: 0.06604437625915785\n",
      "Epoch 72/100\n",
      "Training Loss: 0.0971962217471686\n",
      "Validation Loss: 0.04527705273013712\n",
      "Epoch 73/100\n",
      "Training Loss: 0.12331699790610551\n",
      "Validation Loss: 0.08766873681744267\n",
      "Epoch 74/100\n",
      "Training Loss: 0.12507689062947477\n",
      "Validation Loss: 0.05819680021606678\n",
      "Epoch 75/100\n",
      "Training Loss: 0.11121153127002875\n",
      "Validation Loss: 0.07267326384840149\n",
      "Epoch 76/100\n",
      "Training Loss: 0.11423412872008698\n",
      "Validation Loss: 0.06703270815700932\n",
      "Epoch 77/100\n",
      "Training Loss: 0.10655926977423033\n",
      "Validation Loss: 0.04629405818813969\n",
      "Epoch 78/100\n",
      "Training Loss: 0.11228909374737989\n",
      "Validation Loss: 0.04959152175754879\n",
      "Epoch 79/100\n",
      "Training Loss: 0.11576300404243983\n",
      "Validation Loss: 0.09793548101829236\n",
      "Epoch 80/100\n",
      "Training Loss: 0.1256343595245609\n",
      "Validation Loss: 0.08321459676631193\n",
      "Epoch 81/100\n",
      "Training Loss: 0.11529571722331958\n",
      "Validation Loss: 0.08109844916938501\n",
      "Epoch 82/100\n",
      "Training Loss: 0.11881555819768294\n",
      "Validation Loss: 0.09092538863066871\n",
      "Epoch 83/100\n",
      "Training Loss: 0.10916064304539273\n",
      "Validation Loss: 0.05944695006770383\n",
      "Epoch 84/100\n",
      "Training Loss: 0.11032989938206929\n",
      "Validation Loss: 0.0985639659370238\n",
      "Epoch 85/100\n",
      "Training Loss: 0.10893444951561755\n",
      "Validation Loss: 0.06531707639018751\n",
      "Epoch 86/100\n",
      "Training Loss: 0.10717345977652479\n",
      "Validation Loss: 0.08615473095625414\n",
      "Epoch 87/100\n",
      "Training Loss: 0.10520639489239436\n",
      "Validation Loss: 0.04995807791027791\n",
      "Epoch 88/100\n",
      "Training Loss: 0.12416382634866\n",
      "Validation Loss: 0.04927405578370289\n",
      "Epoch 89/100\n",
      "Training Loss: 0.09495581395323008\n",
      "Validation Loss: 0.09616411328754636\n",
      "Epoch 90/100\n",
      "Training Loss: 0.10566214302859571\n",
      "Validation Loss: 0.12442070026621735\n",
      "Epoch 91/100\n",
      "Training Loss: 0.08838085528947194\n",
      "Validation Loss: 0.06083412653246234\n",
      "Epoch 92/100\n",
      "Training Loss: 0.09686882645518016\n",
      "Validation Loss: 0.06123548285670651\n",
      "Epoch 93/100\n",
      "Training Loss: 0.12358873320355739\n",
      "Validation Loss: 0.08091797656702449\n",
      "Epoch 94/100\n",
      "Training Loss: 0.09741987288729001\n",
      "Validation Loss: 0.04632510512504793\n",
      "Epoch 95/100\n",
      "Training Loss: 0.10637906072767996\n",
      "Validation Loss: 0.08804020029885042\n",
      "Epoch 96/100\n",
      "Training Loss: 0.10529481256623881\n",
      "Validation Loss: 0.06641417844929065\n",
      "Epoch 97/100\n",
      "Training Loss: 0.10646519710054399\n",
      "Validation Loss: 0.1083953063678047\n",
      "Epoch 98/100\n",
      "Training Loss: 0.1013919870895661\n",
      "Validation Loss: 0.0739722039859172\n",
      "Epoch 99/100\n",
      "Training Loss: 0.0997658951245545\n",
      "Validation Loss: 0.04499566575746312\n",
      "Epoch 100/100\n",
      "Training Loss: 0.1081679393587596\n",
      "Validation Loss: 0.05495795919422861\n",
      "Combination 5: Avg Training Loss = 0.1372454720411989, Avg Validation Loss = 0.08532101999535005\n",
      "Testing combination 6/48: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'sequence_length': 50, 'dropout_rate': 0.2}\n",
      "Truncated Train Data Shape: (70, 50, 1)\n",
      "Truncated Validation Data Shape: (10, 50, 1)\n",
      "    Trial 1/2 for combination 6/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.3838390039246525\n",
      "Validation Loss: 0.13567451344083015\n",
      "Epoch 2/100\n",
      "Training Loss: 0.43131013256022455\n",
      "Validation Loss: 0.20237777168264062\n",
      "Epoch 3/100\n",
      "Training Loss: 0.25543511314735573\n",
      "Validation Loss: 0.11304098354892722\n",
      "Epoch 4/100\n",
      "Training Loss: 0.2544505106268226\n",
      "Validation Loss: 0.19213484291544644\n",
      "Epoch 5/100\n",
      "Training Loss: 0.1904309721055451\n",
      "Validation Loss: 0.0669445711246306\n",
      "Epoch 6/100\n",
      "Training Loss: 0.22990429431268267\n",
      "Validation Loss: 0.12206481717908133\n",
      "Epoch 7/100\n",
      "Training Loss: 0.19984486742567625\n",
      "Validation Loss: 0.1512800606006965\n",
      "Epoch 8/100\n",
      "Training Loss: 0.19979009857812544\n",
      "Validation Loss: 0.10147520360930536\n",
      "Epoch 9/100\n",
      "Training Loss: 0.2144089145446077\n",
      "Validation Loss: 0.1371645639159832\n",
      "Epoch 10/100\n",
      "Training Loss: 0.19745805310569903\n",
      "Validation Loss: 0.0657657368383607\n",
      "Epoch 11/100\n",
      "Training Loss: 0.16964389014347633\n",
      "Validation Loss: 0.06081250336164623\n",
      "Epoch 12/100\n",
      "Training Loss: 0.19584808550510596\n",
      "Validation Loss: 0.1717657206962892\n",
      "Epoch 13/100\n",
      "Training Loss: 0.18067667897753295\n",
      "Validation Loss: 0.1436782987606518\n",
      "Epoch 14/100\n",
      "Training Loss: 0.17144877792519683\n",
      "Validation Loss: 0.10522667146672668\n",
      "Epoch 15/100\n",
      "Training Loss: 0.21115888010924216\n",
      "Validation Loss: 0.1009618417666844\n",
      "Epoch 16/100\n",
      "Training Loss: 0.1695917506632515\n",
      "Validation Loss: 0.07109623949006048\n",
      "Epoch 17/100\n",
      "Training Loss: 0.1826866676314341\n",
      "Validation Loss: 0.06047785424400287\n",
      "Epoch 18/100\n",
      "Training Loss: 0.12975028839795752\n",
      "Validation Loss: 0.09901351891678115\n",
      "Epoch 19/100\n",
      "Training Loss: 0.16552657144895536\n",
      "Validation Loss: 0.09135543252750769\n",
      "Epoch 20/100\n",
      "Training Loss: 0.1877623422120907\n",
      "Validation Loss: 0.11841204979657476\n",
      "Epoch 21/100\n",
      "Training Loss: 0.18221810453362866\n",
      "Validation Loss: 0.12505779852589347\n",
      "Epoch 22/100\n",
      "Training Loss: 0.16658926147217593\n",
      "Validation Loss: 0.13330169882957413\n",
      "Epoch 23/100\n",
      "Training Loss: 0.17792065407542995\n",
      "Validation Loss: 0.14405423134206724\n",
      "Epoch 24/100\n",
      "Training Loss: 0.1504683789888001\n",
      "Validation Loss: 0.04929740296898069\n",
      "Epoch 25/100\n",
      "Training Loss: 0.13336251223580625\n",
      "Validation Loss: 0.06862725893628716\n",
      "Epoch 26/100\n",
      "Training Loss: 0.1656569712801321\n",
      "Validation Loss: 0.07979585933374218\n",
      "Epoch 27/100\n",
      "Training Loss: 0.15569920804702203\n",
      "Validation Loss: 0.08557829193180712\n",
      "Epoch 28/100\n",
      "Training Loss: 0.1483538293835092\n",
      "Validation Loss: 0.07228161711311765\n",
      "Epoch 29/100\n",
      "Training Loss: 0.15956643277261684\n",
      "Validation Loss: 0.07652400189627487\n",
      "Epoch 30/100\n",
      "Training Loss: 0.13035928096377558\n",
      "Validation Loss: 0.06397735495139392\n",
      "Epoch 31/100\n",
      "Training Loss: 0.16841016428534242\n",
      "Validation Loss: 0.10230745469814491\n",
      "Epoch 32/100\n",
      "Training Loss: 0.14909043700358193\n",
      "Validation Loss: 0.12179588249890279\n",
      "Epoch 33/100\n",
      "Training Loss: 0.15088649037702798\n",
      "Validation Loss: 0.05946616255409457\n",
      "Epoch 34/100\n",
      "Training Loss: 0.15194950733752177\n",
      "Validation Loss: 0.05348444326135456\n",
      "Epoch 35/100\n",
      "Training Loss: 0.15379547056981704\n",
      "Validation Loss: 0.0874606446439585\n",
      "Epoch 36/100\n",
      "Training Loss: 0.14357587328351465\n",
      "Validation Loss: 0.08396212790288528\n",
      "Epoch 37/100\n",
      "Training Loss: 0.15862823764363895\n",
      "Validation Loss: 0.05914733243657536\n",
      "Epoch 38/100\n",
      "Training Loss: 0.14401483628152315\n",
      "Validation Loss: 0.047401973906042685\n",
      "Epoch 39/100\n",
      "Training Loss: 0.1502120819561363\n",
      "Validation Loss: 0.08540306224119273\n",
      "Epoch 40/100\n",
      "Training Loss: 0.1477308101902879\n",
      "Validation Loss: 0.088172478490383\n",
      "Epoch 41/100\n",
      "Training Loss: 0.1379891236471612\n",
      "Validation Loss: 0.04325061848243507\n",
      "Epoch 42/100\n",
      "Training Loss: 0.11741409643134225\n",
      "Validation Loss: 0.17019940664186445\n",
      "Epoch 43/100\n",
      "Training Loss: 0.129569056824907\n",
      "Validation Loss: 0.11005547799030595\n",
      "Epoch 44/100\n",
      "Training Loss: 0.13160559622447554\n",
      "Validation Loss: 0.05187239403413234\n",
      "Epoch 45/100\n",
      "Training Loss: 0.16637090592078999\n",
      "Validation Loss: 0.2679368990930529\n",
      "Epoch 46/100\n",
      "Training Loss: 0.1371761524105943\n",
      "Validation Loss: 0.07794836884159197\n",
      "Epoch 47/100\n",
      "Training Loss: 0.13065927079290576\n",
      "Validation Loss: 0.06067500696763885\n",
      "Epoch 48/100\n",
      "Training Loss: 0.120303259415861\n",
      "Validation Loss: 0.053949664109616526\n",
      "Epoch 49/100\n",
      "Training Loss: 0.14217683098564624\n",
      "Validation Loss: 0.09399368854221107\n",
      "Epoch 50/100\n",
      "Training Loss: 0.12020234358704814\n",
      "Validation Loss: 0.08809173000413416\n",
      "Epoch 51/100\n",
      "Training Loss: 0.12303678956415173\n",
      "Validation Loss: 0.052216864265841045\n",
      "Epoch 52/100\n",
      "Training Loss: 0.15638632018688167\n",
      "Validation Loss: 0.035108445739959734\n",
      "Epoch 53/100\n",
      "Training Loss: 0.11790750022653208\n",
      "Validation Loss: 0.13424313697814816\n",
      "Epoch 54/100\n",
      "Training Loss: 0.14896479195907963\n",
      "Validation Loss: 0.055260684394829916\n",
      "Epoch 55/100\n",
      "Training Loss: 0.11079304148563683\n",
      "Validation Loss: 0.060524076733349454\n",
      "Epoch 56/100\n",
      "Training Loss: 0.1178911281591895\n",
      "Validation Loss: 0.036491334765209396\n",
      "Epoch 57/100\n",
      "Training Loss: 0.13107917028386984\n",
      "Validation Loss: 0.09576063017740248\n",
      "Epoch 58/100\n",
      "Training Loss: 0.12216053104257762\n",
      "Validation Loss: 0.09594361142096831\n",
      "Epoch 59/100\n",
      "Training Loss: 0.11232501818323122\n",
      "Validation Loss: 0.03930239762544193\n",
      "Epoch 60/100\n",
      "Training Loss: 0.11678558648626836\n",
      "Validation Loss: 0.045616546173287434\n",
      "Epoch 61/100\n",
      "Training Loss: 0.14249337144943414\n",
      "Validation Loss: 0.21477056993877075\n",
      "Epoch 62/100\n",
      "Training Loss: 0.1219987340685075\n",
      "Validation Loss: 0.08362163093475135\n",
      "Epoch 63/100\n",
      "Training Loss: 0.11218495808841078\n",
      "Validation Loss: 0.0995952497073911\n",
      "Epoch 64/100\n",
      "Training Loss: 0.14255785192773748\n",
      "Validation Loss: 0.060580492299265175\n",
      "Epoch 65/100\n",
      "Training Loss: 0.10311514295316236\n",
      "Validation Loss: 0.053779631297127074\n",
      "Epoch 66/100\n",
      "Training Loss: 0.12172580466016499\n",
      "Validation Loss: 0.08216832062738166\n",
      "Epoch 67/100\n",
      "Training Loss: 0.12270481173888424\n",
      "Validation Loss: 0.06475649717141067\n",
      "Epoch 68/100\n",
      "Training Loss: 0.11783559242377059\n",
      "Validation Loss: 0.08059916830752237\n",
      "Epoch 69/100\n",
      "Training Loss: 0.11450808387592556\n",
      "Validation Loss: 0.10889901513225748\n",
      "Epoch 70/100\n",
      "Training Loss: 0.11151969228632058\n",
      "Validation Loss: 0.04993460746218011\n",
      "Epoch 71/100\n",
      "Training Loss: 0.12519539638060223\n",
      "Validation Loss: 0.0654541410105145\n",
      "Epoch 72/100\n",
      "Training Loss: 0.11716633308543753\n",
      "Validation Loss: 0.12174879875656548\n",
      "Epoch 73/100\n",
      "Training Loss: 0.11521091825819203\n",
      "Validation Loss: 0.031144243396905018\n",
      "Epoch 74/100\n",
      "Training Loss: 0.11046661341753064\n",
      "Validation Loss: 0.05956118130817763\n",
      "Epoch 75/100\n",
      "Training Loss: 0.1195287337273292\n",
      "Validation Loss: 0.14364379974693586\n",
      "Epoch 76/100\n",
      "Training Loss: 0.10507868058649339\n",
      "Validation Loss: 0.09103528677664587\n",
      "Epoch 77/100\n",
      "Training Loss: 0.1086558680073956\n",
      "Validation Loss: 0.045564526077372124\n",
      "Epoch 78/100\n",
      "Training Loss: 0.11268383321047531\n",
      "Validation Loss: 0.05100785449887901\n",
      "Epoch 79/100\n",
      "Training Loss: 0.10606088301740121\n",
      "Validation Loss: 0.06778545982607934\n",
      "Epoch 80/100\n",
      "Training Loss: 0.11013395664084392\n",
      "Validation Loss: 0.03471145398257505\n",
      "Epoch 81/100\n",
      "Training Loss: 0.09497354048116746\n",
      "Validation Loss: 0.03964442465686755\n",
      "Epoch 82/100\n",
      "Training Loss: 0.10829921030347306\n",
      "Validation Loss: 0.04572737076580736\n",
      "Epoch 83/100\n",
      "Training Loss: 0.11647132574977889\n",
      "Validation Loss: 0.0726383959584539\n",
      "Epoch 84/100\n",
      "Training Loss: 0.11440405376477858\n",
      "Validation Loss: 0.0699837978357699\n",
      "Epoch 85/100\n",
      "Training Loss: 0.11365222744695545\n",
      "Validation Loss: 0.03633372221736436\n",
      "Epoch 86/100\n",
      "Training Loss: 0.0979806766012675\n",
      "Validation Loss: 0.06546853671407178\n",
      "Epoch 87/100\n",
      "Training Loss: 0.09839138241907243\n",
      "Validation Loss: 0.05204125395813951\n",
      "Epoch 88/100\n",
      "Training Loss: 0.1145391440792081\n",
      "Validation Loss: 0.06270322156021094\n",
      "Epoch 89/100\n",
      "Training Loss: 0.10287431775681435\n",
      "Validation Loss: 0.03864692873360197\n",
      "Epoch 90/100\n",
      "Training Loss: 0.11163138561575292\n",
      "Validation Loss: 0.06785634175185026\n",
      "Epoch 91/100\n",
      "Training Loss: 0.11074057154752931\n",
      "Validation Loss: 0.033633904347213875\n",
      "Epoch 92/100\n",
      "Training Loss: 0.09514447364797475\n",
      "Validation Loss: 0.041611418788253696\n",
      "Epoch 93/100\n",
      "Training Loss: 0.09875618200220862\n",
      "Validation Loss: 0.04322377719949233\n",
      "Epoch 94/100\n",
      "Training Loss: 0.10795098025314492\n",
      "Validation Loss: 0.0990458487789941\n",
      "Epoch 95/100\n",
      "Training Loss: 0.09208788965357319\n",
      "Validation Loss: 0.12371049508499299\n",
      "Epoch 96/100\n",
      "Training Loss: 0.09381040758873346\n",
      "Validation Loss: 0.038951380707946896\n",
      "Epoch 97/100\n",
      "Training Loss: 0.09795723883070521\n",
      "Validation Loss: 0.04983061443594822\n",
      "Epoch 98/100\n",
      "Training Loss: 0.1068044161457334\n",
      "Validation Loss: 0.04501021103128494\n",
      "Epoch 99/100\n",
      "Training Loss: 0.10219748491521084\n",
      "Validation Loss: 0.05417027625226957\n",
      "Epoch 100/100\n",
      "Training Loss: 0.08808312900798415\n",
      "Validation Loss: 0.05072888674204977\n",
      "    Trial 2/2 for combination 6/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.5381869318237666\n",
      "Validation Loss: 0.2743736220326873\n",
      "Epoch 2/100\n",
      "Training Loss: 0.3834854286965895\n",
      "Validation Loss: 0.10773288088229205\n",
      "Epoch 3/100\n",
      "Training Loss: 0.29092923889749134\n",
      "Validation Loss: 0.24665579722530837\n",
      "Epoch 4/100\n",
      "Training Loss: 0.2535623469081036\n",
      "Validation Loss: 0.1201036046207146\n",
      "Epoch 5/100\n",
      "Training Loss: 0.273064006475886\n",
      "Validation Loss: 0.2130148583320493\n",
      "Epoch 6/100\n",
      "Training Loss: 0.29520670779009506\n",
      "Validation Loss: 0.15997639798356272\n",
      "Epoch 7/100\n",
      "Training Loss: 0.2970799516013569\n",
      "Validation Loss: 0.07375668553471453\n",
      "Epoch 8/100\n",
      "Training Loss: 0.17887334871784347\n",
      "Validation Loss: 0.105566213086883\n",
      "Epoch 9/100\n",
      "Training Loss: 0.2035379757767815\n",
      "Validation Loss: 0.12917000419969613\n",
      "Epoch 10/100\n",
      "Training Loss: 0.20761749022451909\n",
      "Validation Loss: 0.15646201314970484\n",
      "Epoch 11/100\n",
      "Training Loss: 0.26379504908007306\n",
      "Validation Loss: 0.1531793132600589\n",
      "Epoch 12/100\n",
      "Training Loss: 0.1634552907168426\n",
      "Validation Loss: 0.09763790490829827\n",
      "Epoch 13/100\n",
      "Training Loss: 0.23511344131229275\n",
      "Validation Loss: 0.07534335080732701\n",
      "Epoch 14/100\n",
      "Training Loss: 0.21646803466147269\n",
      "Validation Loss: 0.12206105474735776\n",
      "Epoch 15/100\n",
      "Training Loss: 0.19220168786278558\n",
      "Validation Loss: 0.12789947369798077\n",
      "Epoch 16/100\n",
      "Training Loss: 0.2080407210351822\n",
      "Validation Loss: 0.11797424685069806\n",
      "Epoch 17/100\n",
      "Training Loss: 0.23182784811420984\n",
      "Validation Loss: 0.13904759300348277\n",
      "Epoch 18/100\n",
      "Training Loss: 0.1788791014790918\n",
      "Validation Loss: 0.09413112010556723\n",
      "Epoch 19/100\n",
      "Training Loss: 0.16526393332772601\n",
      "Validation Loss: 0.13770214418664223\n",
      "Epoch 20/100\n",
      "Training Loss: 0.17622404478767315\n",
      "Validation Loss: 0.095420200847838\n",
      "Epoch 21/100\n",
      "Training Loss: 0.17512696448460935\n",
      "Validation Loss: 0.1264572682170574\n",
      "Epoch 22/100\n",
      "Training Loss: 0.16896485464539154\n",
      "Validation Loss: 0.10189863638300152\n",
      "Epoch 23/100\n",
      "Training Loss: 0.18293812683959276\n",
      "Validation Loss: 0.13839501808158997\n",
      "Epoch 24/100\n",
      "Training Loss: 0.167992984518235\n",
      "Validation Loss: 0.10923402384209621\n",
      "Epoch 25/100\n",
      "Training Loss: 0.17474465952248314\n",
      "Validation Loss: 0.07729667148725422\n",
      "Epoch 26/100\n",
      "Training Loss: 0.17746831073678512\n",
      "Validation Loss: 0.15006070990991766\n",
      "Epoch 27/100\n",
      "Training Loss: 0.19057683139535625\n",
      "Validation Loss: 0.04707764791865419\n",
      "Epoch 28/100\n",
      "Training Loss: 0.19965261706551424\n",
      "Validation Loss: 0.1298464387847068\n",
      "Epoch 29/100\n",
      "Training Loss: 0.17127299796509385\n",
      "Validation Loss: 0.04866851565732539\n",
      "Epoch 30/100\n",
      "Training Loss: 0.16507182336147086\n",
      "Validation Loss: 0.16320872529250985\n",
      "Epoch 31/100\n",
      "Training Loss: 0.17780955138508275\n",
      "Validation Loss: 0.06517717858506332\n",
      "Epoch 32/100\n",
      "Training Loss: 0.1708580552921664\n",
      "Validation Loss: 0.09196610768604263\n",
      "Epoch 33/100\n",
      "Training Loss: 0.16448682973587564\n",
      "Validation Loss: 0.13196370331206797\n",
      "Epoch 34/100\n",
      "Training Loss: 0.15995762203942024\n",
      "Validation Loss: 0.1028379074739902\n",
      "Epoch 35/100\n",
      "Training Loss: 0.1783049796471795\n",
      "Validation Loss: 0.05381457211923918\n",
      "Epoch 36/100\n",
      "Training Loss: 0.16309295404223448\n",
      "Validation Loss: 0.1244050884362257\n",
      "Epoch 37/100\n",
      "Training Loss: 0.17539719105245014\n",
      "Validation Loss: 0.0788332787143316\n",
      "Epoch 38/100\n",
      "Training Loss: 0.15037120273223364\n",
      "Validation Loss: 0.09566107141125083\n",
      "Epoch 39/100\n",
      "Training Loss: 0.16709252070092664\n",
      "Validation Loss: 0.08284519582473579\n",
      "Epoch 40/100\n",
      "Training Loss: 0.16685486787717874\n",
      "Validation Loss: 0.08840867788963022\n",
      "Epoch 41/100\n",
      "Training Loss: 0.15167124763824485\n",
      "Validation Loss: 0.09364815534942875\n",
      "Epoch 42/100\n",
      "Training Loss: 0.13852224543165695\n",
      "Validation Loss: 0.06094843470422273\n",
      "Epoch 43/100\n",
      "Training Loss: 0.13659045644450532\n",
      "Validation Loss: 0.06366789587702046\n",
      "Epoch 44/100\n",
      "Training Loss: 0.14857845626731303\n",
      "Validation Loss: 0.07027717541772403\n",
      "Epoch 45/100\n",
      "Training Loss: 0.13952138581532011\n",
      "Validation Loss: 0.06951109873930886\n",
      "Epoch 46/100\n",
      "Training Loss: 0.16055731078158605\n",
      "Validation Loss: 0.09302562341302636\n",
      "Epoch 47/100\n",
      "Training Loss: 0.14176643854511436\n",
      "Validation Loss: 0.06510906354526864\n",
      "Epoch 48/100\n",
      "Training Loss: 0.17342247218910528\n",
      "Validation Loss: 0.05995547625476775\n",
      "Epoch 49/100\n",
      "Training Loss: 0.1457515113285936\n",
      "Validation Loss: 0.050109405783052195\n",
      "Epoch 50/100\n",
      "Training Loss: 0.11521751095671899\n",
      "Validation Loss: 0.11278082061104906\n",
      "Epoch 51/100\n",
      "Training Loss: 0.13635144694893495\n",
      "Validation Loss: 0.0656876581328366\n",
      "Epoch 52/100\n",
      "Training Loss: 0.14111925734062358\n",
      "Validation Loss: 0.055527674351415514\n",
      "Epoch 53/100\n",
      "Training Loss: 0.1584485310171433\n",
      "Validation Loss: 0.07421153375863879\n",
      "Epoch 54/100\n",
      "Training Loss: 0.13292083611483477\n",
      "Validation Loss: 0.062426671862188435\n",
      "Epoch 55/100\n",
      "Training Loss: 0.1344840559918962\n",
      "Validation Loss: 0.060458581843330014\n",
      "Epoch 56/100\n",
      "Training Loss: 0.15114887379123162\n",
      "Validation Loss: 0.048202246269300884\n",
      "Epoch 57/100\n",
      "Training Loss: 0.15115187600427318\n",
      "Validation Loss: 0.08114548556307485\n",
      "Epoch 58/100\n",
      "Training Loss: 0.1362833084707498\n",
      "Validation Loss: 0.06218581955084973\n",
      "Epoch 59/100\n",
      "Training Loss: 0.12768924560681236\n",
      "Validation Loss: 0.06184862231613062\n",
      "Epoch 60/100\n",
      "Training Loss: 0.1259463380012137\n",
      "Validation Loss: 0.10217434019973781\n",
      "Epoch 61/100\n",
      "Training Loss: 0.15645648425358988\n",
      "Validation Loss: 0.05846276798459728\n",
      "Epoch 62/100\n",
      "Training Loss: 0.12226643983982272\n",
      "Validation Loss: 0.09055135477656767\n",
      "Epoch 63/100\n",
      "Training Loss: 0.13657473730041245\n",
      "Validation Loss: 0.06705461240069951\n",
      "Epoch 64/100\n",
      "Training Loss: 0.12493855233742328\n",
      "Validation Loss: 0.12266932934852112\n",
      "Epoch 65/100\n",
      "Training Loss: 0.14001630014967742\n",
      "Validation Loss: 0.09707804026392355\n",
      "Epoch 66/100\n",
      "Training Loss: 0.12604192323694802\n",
      "Validation Loss: 0.05266353073982767\n",
      "Epoch 67/100\n",
      "Training Loss: 0.13193459137357064\n",
      "Validation Loss: 0.06479746096335556\n",
      "Epoch 68/100\n",
      "Training Loss: 0.09680285709831254\n",
      "Validation Loss: 0.05384234933567687\n",
      "Epoch 69/100\n",
      "Training Loss: 0.12699127194230095\n",
      "Validation Loss: 0.0802758219936744\n",
      "Epoch 70/100\n",
      "Training Loss: 0.10953687160569452\n",
      "Validation Loss: 0.05732501681649273\n",
      "Epoch 71/100\n",
      "Training Loss: 0.1224814489264945\n",
      "Validation Loss: 0.06056150606759826\n",
      "Epoch 72/100\n",
      "Training Loss: 0.12181750849486488\n",
      "Validation Loss: 0.0649972465410392\n",
      "Epoch 73/100\n",
      "Training Loss: 0.14244407543345838\n",
      "Validation Loss: 0.046325958309944765\n",
      "Epoch 74/100\n",
      "Training Loss: 0.122408074968333\n",
      "Validation Loss: 0.06956741511119093\n",
      "Epoch 75/100\n",
      "Training Loss: 0.12006389462874219\n",
      "Validation Loss: 0.06281322641414953\n",
      "Epoch 76/100\n",
      "Training Loss: 0.12193307395299041\n",
      "Validation Loss: 0.05865034813408075\n",
      "Epoch 77/100\n",
      "Training Loss: 0.1156391095972393\n",
      "Validation Loss: 0.07731662670113867\n",
      "Epoch 78/100\n",
      "Training Loss: 0.12258367805458967\n",
      "Validation Loss: 0.12427039564420397\n",
      "Epoch 79/100\n",
      "Training Loss: 0.13068402869277018\n",
      "Validation Loss: 0.05659482033775824\n",
      "Epoch 80/100\n",
      "Training Loss: 0.11218932739216518\n",
      "Validation Loss: 0.07470434634456058\n",
      "Epoch 81/100\n",
      "Training Loss: 0.1089879078370019\n",
      "Validation Loss: 0.06314376558115865\n",
      "Epoch 82/100\n",
      "Training Loss: 0.12988413924733067\n",
      "Validation Loss: 0.07341564778803188\n",
      "Epoch 83/100\n",
      "Training Loss: 0.10043662538182604\n",
      "Validation Loss: 0.09864624942852082\n",
      "Epoch 84/100\n",
      "Training Loss: 0.1071957460612682\n",
      "Validation Loss: 0.033436041039319284\n",
      "Epoch 85/100\n",
      "Training Loss: 0.1037389097949275\n",
      "Validation Loss: 0.09261748545765489\n",
      "Epoch 86/100\n",
      "Training Loss: 0.1172521329276429\n",
      "Validation Loss: 0.0736510811614022\n",
      "Epoch 87/100\n",
      "Training Loss: 0.11756992015211079\n",
      "Validation Loss: 0.06521166836089952\n",
      "Epoch 88/100\n",
      "Training Loss: 0.13177392093784818\n",
      "Validation Loss: 0.03676802619628548\n",
      "Epoch 89/100\n",
      "Training Loss: 0.10623913015072683\n",
      "Validation Loss: 0.06261121222331631\n",
      "Epoch 90/100\n",
      "Training Loss: 0.11138813683142222\n",
      "Validation Loss: 0.07243136613898915\n",
      "Epoch 91/100\n",
      "Training Loss: 0.11791685685094055\n",
      "Validation Loss: 0.06946818352828492\n",
      "Epoch 92/100\n",
      "Training Loss: 0.09651637136233163\n",
      "Validation Loss: 0.06388614075421507\n",
      "Epoch 93/100\n",
      "Training Loss: 0.11941542935311526\n",
      "Validation Loss: 0.05584673604953281\n",
      "Epoch 94/100\n",
      "Training Loss: 0.10181347155279952\n",
      "Validation Loss: 0.06002959918670564\n",
      "Epoch 95/100\n",
      "Training Loss: 0.11576514262411905\n",
      "Validation Loss: 0.07760120968413382\n",
      "Epoch 96/100\n",
      "Training Loss: 0.11962004588487127\n",
      "Validation Loss: 0.03549879691418988\n",
      "Epoch 97/100\n",
      "Training Loss: 0.11082769002139335\n",
      "Validation Loss: 0.07339143755606412\n",
      "Epoch 98/100\n",
      "Training Loss: 0.10001552665235007\n",
      "Validation Loss: 0.04011504531896038\n",
      "Epoch 99/100\n",
      "Training Loss: 0.12106187223054839\n",
      "Validation Loss: 0.0468315144915432\n",
      "Epoch 100/100\n",
      "Training Loss: 0.111341496828262\n",
      "Validation Loss: 0.07109861706364048\n",
      "Combination 6: Avg Training Loss = 0.15204191149218665, Avg Validation Loss = 0.08640986345644994\n",
      "Testing combination 7/48: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.001, 'sequence_length': 6, 'dropout_rate': 0.1}\n",
      "Truncated Train Data Shape: (70, 6, 1)\n",
      "Truncated Validation Data Shape: (10, 6, 1)\n",
      "    Trial 1/2 for combination 7/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.4540760782662763\n",
      "Validation Loss: 0.13746815484014907\n",
      "Epoch 2/100\n",
      "Training Loss: 0.3924484398780214\n",
      "Validation Loss: 0.10054304990419538\n",
      "Epoch 3/100\n",
      "Training Loss: 0.23019757433058782\n",
      "Validation Loss: 0.17200880850170752\n",
      "Epoch 4/100\n",
      "Training Loss: 0.24408379769524582\n",
      "Validation Loss: 0.17349804124082865\n",
      "Epoch 5/100\n",
      "Training Loss: 0.24974234131938974\n",
      "Validation Loss: 0.061874761827316396\n",
      "Epoch 6/100\n",
      "Training Loss: 0.21850233861337\n",
      "Validation Loss: 0.09547121976282923\n",
      "Epoch 7/100\n",
      "Training Loss: 0.16428764385987207\n",
      "Validation Loss: 0.0968764506140252\n",
      "Epoch 8/100\n",
      "Training Loss: 0.19560712468764144\n",
      "Validation Loss: 0.10425222145352173\n",
      "Epoch 9/100\n",
      "Training Loss: 0.14991893922052463\n",
      "Validation Loss: 0.07860852424488862\n",
      "Epoch 10/100\n",
      "Training Loss: 0.15838631279466314\n",
      "Validation Loss: 0.08503001757698787\n",
      "Epoch 11/100\n",
      "Training Loss: 0.13674332771796777\n",
      "Validation Loss: 0.06555127781963267\n",
      "Epoch 12/100\n",
      "Training Loss: 0.13974844748994655\n",
      "Validation Loss: 0.13646673805249662\n",
      "Epoch 13/100\n",
      "Training Loss: 0.11910853239060802\n",
      "Validation Loss: 0.07494469114848587\n",
      "Epoch 14/100\n",
      "Training Loss: 0.11873935047743088\n",
      "Validation Loss: 0.0633300785196417\n",
      "Epoch 15/100\n",
      "Training Loss: 0.09359887263311235\n",
      "Validation Loss: 0.06154656554474104\n",
      "Epoch 16/100\n",
      "Training Loss: 0.12822877353162218\n",
      "Validation Loss: 0.08951824029814862\n",
      "Epoch 17/100\n",
      "Training Loss: 0.13237097454455768\n",
      "Validation Loss: 0.06470049119118051\n",
      "Epoch 18/100\n",
      "Training Loss: 0.09843400270449017\n",
      "Validation Loss: 0.04776170272544837\n",
      "Epoch 19/100\n",
      "Training Loss: 0.09361804913463609\n",
      "Validation Loss: 0.08211498823906763\n",
      "Epoch 20/100\n",
      "Training Loss: 0.09651750173533931\n",
      "Validation Loss: 0.06615216182727233\n",
      "Epoch 21/100\n",
      "Training Loss: 0.10432040372975392\n",
      "Validation Loss: 0.10131381554562172\n",
      "Epoch 22/100\n",
      "Training Loss: 0.09386705622393333\n",
      "Validation Loss: 0.05552915233949964\n",
      "Epoch 23/100\n",
      "Training Loss: 0.09510839388153801\n",
      "Validation Loss: 0.06245876504601755\n",
      "Epoch 24/100\n",
      "Training Loss: 0.09401694978163279\n",
      "Validation Loss: 0.05218691538433625\n",
      "Epoch 25/100\n",
      "Training Loss: 0.09871616148693983\n",
      "Validation Loss: 0.0490196454074459\n",
      "Epoch 26/100\n",
      "Training Loss: 0.0964873751387036\n",
      "Validation Loss: 0.06253656262782263\n",
      "Epoch 27/100\n",
      "Training Loss: 0.09691709391723403\n",
      "Validation Loss: 0.046695876188296175\n",
      "Epoch 28/100\n",
      "Training Loss: 0.0940265525229397\n",
      "Validation Loss: 0.08210671921516416\n",
      "Epoch 29/100\n",
      "Training Loss: 0.08766745379863584\n",
      "Validation Loss: 0.0664542969883808\n",
      "Epoch 30/100\n",
      "Training Loss: 0.09221075677466464\n",
      "Validation Loss: 0.04847285390182746\n",
      "Epoch 31/100\n",
      "Training Loss: 0.09727649888177528\n",
      "Validation Loss: 0.045928941460354894\n",
      "Epoch 32/100\n",
      "Training Loss: 0.0892297451205052\n",
      "Validation Loss: 0.04989746428077897\n",
      "Epoch 33/100\n",
      "Training Loss: 0.07273434189485904\n",
      "Validation Loss: 0.04907525782946978\n",
      "Epoch 34/100\n",
      "Training Loss: 0.07752667635035114\n",
      "Validation Loss: 0.03796515698446572\n",
      "Epoch 35/100\n",
      "Training Loss: 0.08949399047283653\n",
      "Validation Loss: 0.06446913270964641\n",
      "Epoch 36/100\n",
      "Training Loss: 0.08264801850174118\n",
      "Validation Loss: 0.04368295317277469\n",
      "Epoch 37/100\n",
      "Training Loss: 0.07596443709688498\n",
      "Validation Loss: 0.054512455829752435\n",
      "Epoch 38/100\n",
      "Training Loss: 0.0771757405753826\n",
      "Validation Loss: 0.04902124934789191\n",
      "Epoch 39/100\n",
      "Training Loss: 0.0888480177838702\n",
      "Validation Loss: 0.039797320007870195\n",
      "Epoch 40/100\n",
      "Training Loss: 0.06988238018442909\n",
      "Validation Loss: 0.039293495562240086\n",
      "Epoch 41/100\n",
      "Training Loss: 0.08846796152681141\n",
      "Validation Loss: 0.054866005354172534\n",
      "Epoch 42/100\n",
      "Training Loss: 0.07596883469024725\n",
      "Validation Loss: 0.03846140803573903\n",
      "Epoch 43/100\n",
      "Training Loss: 0.07863664856225173\n",
      "Validation Loss: 0.057758537129039965\n",
      "Epoch 44/100\n",
      "Training Loss: 0.08126061386336979\n",
      "Validation Loss: 0.04521971100833394\n",
      "Epoch 45/100\n",
      "Training Loss: 0.08408832366279669\n",
      "Validation Loss: 0.03591234975784492\n",
      "Epoch 46/100\n",
      "Training Loss: 0.073696160737238\n",
      "Validation Loss: 0.04082468435508918\n",
      "Epoch 47/100\n",
      "Training Loss: 0.07159134235198242\n",
      "Validation Loss: 0.05057867226451927\n",
      "Epoch 48/100\n",
      "Training Loss: 0.07229786494411468\n",
      "Validation Loss: 0.04564505358247693\n",
      "Epoch 49/100\n",
      "Training Loss: 0.07029059881981714\n",
      "Validation Loss: 0.07844389319301669\n",
      "Epoch 50/100\n",
      "Training Loss: 0.08312181510186077\n",
      "Validation Loss: 0.04350456358587136\n",
      "Epoch 51/100\n",
      "Training Loss: 0.07266810414673218\n",
      "Validation Loss: 0.04234663051882455\n",
      "Epoch 52/100\n",
      "Training Loss: 0.07629858763134874\n",
      "Validation Loss: 0.06194336441435897\n",
      "Epoch 53/100\n",
      "Training Loss: 0.07728321443693364\n",
      "Validation Loss: 0.04172933810749534\n",
      "Epoch 54/100\n",
      "Training Loss: 0.07839552131216228\n",
      "Validation Loss: 0.07560118541040689\n",
      "Epoch 55/100\n",
      "Training Loss: 0.07965950681793192\n",
      "Validation Loss: 0.06535842341635509\n",
      "Epoch 56/100\n",
      "Training Loss: 0.0779571937310682\n",
      "Validation Loss: 0.03918300683820414\n",
      "Epoch 57/100\n",
      "Training Loss: 0.0639788049781201\n",
      "Validation Loss: 0.04268447172873659\n",
      "Epoch 58/100\n",
      "Training Loss: 0.08194329722575555\n",
      "Validation Loss: 0.04132188518211473\n",
      "Epoch 59/100\n",
      "Training Loss: 0.0774294961050938\n",
      "Validation Loss: 0.05283820823361636\n",
      "Epoch 60/100\n",
      "Training Loss: 0.08596438800807238\n",
      "Validation Loss: 0.03806949230603688\n",
      "Epoch 61/100\n",
      "Training Loss: 0.07227670098761142\n",
      "Validation Loss: 0.04567364410919112\n",
      "Epoch 62/100\n",
      "Training Loss: 0.07933382482363209\n",
      "Validation Loss: 0.04193111659507167\n",
      "Epoch 63/100\n",
      "Training Loss: 0.07768202021733421\n",
      "Validation Loss: 0.061435191369106024\n",
      "Epoch 64/100\n",
      "Training Loss: 0.07172753206417197\n",
      "Validation Loss: 0.04617804540147437\n",
      "Epoch 65/100\n",
      "Training Loss: 0.06990164757354705\n",
      "Validation Loss: 0.04004492896222596\n",
      "Epoch 66/100\n",
      "Training Loss: 0.06893644504062367\n",
      "Validation Loss: 0.04969368893865574\n",
      "Epoch 67/100\n",
      "Training Loss: 0.07524655690368234\n",
      "Validation Loss: 0.04355717797110712\n",
      "Epoch 68/100\n",
      "Training Loss: 0.0795681587359453\n",
      "Validation Loss: 0.04972589661369037\n",
      "Epoch 69/100\n",
      "Training Loss: 0.0656367129172722\n",
      "Validation Loss: 0.045377889890126956\n",
      "Epoch 70/100\n",
      "Training Loss: 0.08294288219058037\n",
      "Validation Loss: 0.05172017593467896\n",
      "Epoch 71/100\n",
      "Training Loss: 0.08861213940726187\n",
      "Validation Loss: 0.05211682536027148\n",
      "Epoch 72/100\n",
      "Training Loss: 0.07165665226695112\n",
      "Validation Loss: 0.060453057493092856\n",
      "Epoch 73/100\n",
      "Training Loss: 0.0831492752502959\n",
      "Validation Loss: 0.05684147432230906\n",
      "Epoch 74/100\n",
      "Training Loss: 0.08824725084188426\n",
      "Validation Loss: 0.04933657816876864\n",
      "Epoch 75/100\n",
      "Training Loss: 0.07475209258861064\n",
      "Validation Loss: 0.055364439836837\n",
      "Epoch 76/100\n",
      "Training Loss: 0.07965263799132291\n",
      "Validation Loss: 0.05111688723416004\n",
      "Epoch 77/100\n",
      "Training Loss: 0.07818235053742703\n",
      "Validation Loss: 0.047272369327086235\n",
      "Epoch 78/100\n",
      "Training Loss: 0.07131302531838933\n",
      "Validation Loss: 0.060584932798145585\n",
      "Epoch 79/100\n",
      "Training Loss: 0.07762980665798541\n",
      "Validation Loss: 0.04490211880988677\n",
      "Epoch 80/100\n",
      "Training Loss: 0.06592829032357563\n",
      "Validation Loss: 0.05322306097587972\n",
      "Epoch 81/100\n",
      "Training Loss: 0.08131759426830784\n",
      "Validation Loss: 0.051163203512813483\n",
      "Epoch 82/100\n",
      "Training Loss: 0.09352179265414534\n",
      "Validation Loss: 0.049792215466037966\n",
      "Epoch 83/100\n",
      "Training Loss: 0.07989309829663774\n",
      "Validation Loss: 0.05540922447133086\n",
      "Epoch 84/100\n",
      "Training Loss: 0.08605546506364434\n",
      "Validation Loss: 0.04258928798572058\n",
      "Epoch 85/100\n",
      "Training Loss: 0.07170739258619625\n",
      "Validation Loss: 0.03504311905072104\n",
      "Epoch 86/100\n",
      "Training Loss: 0.07785304090432918\n",
      "Validation Loss: 0.0503058956299277\n",
      "Epoch 87/100\n",
      "Training Loss: 0.08416456374322319\n",
      "Validation Loss: 0.05490714832495062\n",
      "Epoch 88/100\n",
      "Training Loss: 0.07540050868501359\n",
      "Validation Loss: 0.04487093415491052\n",
      "Epoch 89/100\n",
      "Training Loss: 0.07880676751619813\n",
      "Validation Loss: 0.036162024619489204\n",
      "Epoch 90/100\n",
      "Training Loss: 0.07890614649696576\n",
      "Validation Loss: 0.05406507717039115\n",
      "Epoch 91/100\n",
      "Training Loss: 0.09886379636592471\n",
      "Validation Loss: 0.060212335133231584\n",
      "Epoch 92/100\n",
      "Training Loss: 0.08756126657441828\n",
      "Validation Loss: 0.044718617211721885\n",
      "Epoch 93/100\n",
      "Training Loss: 0.06809048880478831\n",
      "Validation Loss: 0.07387144059154996\n",
      "Epoch 94/100\n",
      "Training Loss: 0.07725252535051748\n",
      "Validation Loss: 0.04974374412148564\n",
      "Epoch 95/100\n",
      "Training Loss: 0.07231153615891031\n",
      "Validation Loss: 0.03187558460408198\n",
      "Epoch 96/100\n",
      "Training Loss: 0.07992084881445159\n",
      "Validation Loss: 0.050408973088985495\n",
      "Epoch 97/100\n",
      "Training Loss: 0.0843166850136227\n",
      "Validation Loss: 0.05862505044451418\n",
      "Epoch 98/100\n",
      "Training Loss: 0.08050691292324533\n",
      "Validation Loss: 0.03726496187976376\n",
      "Epoch 99/100\n",
      "Training Loss: 0.09937265702466126\n",
      "Validation Loss: 0.053566907122644336\n",
      "Epoch 100/100\n",
      "Training Loss: 0.0899743753182276\n",
      "Validation Loss: 0.056044583716550654\n",
      "    Trial 2/2 for combination 7/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.4300938042626333\n",
      "Validation Loss: 0.2544036747979618\n",
      "Epoch 2/100\n",
      "Training Loss: 0.22530807861129376\n",
      "Validation Loss: 0.16588342011885443\n",
      "Epoch 3/100\n",
      "Training Loss: 0.23064165608701032\n",
      "Validation Loss: 0.2718707304446851\n",
      "Epoch 4/100\n",
      "Training Loss: 0.25947418400141004\n",
      "Validation Loss: 0.1644721730060449\n",
      "Epoch 5/100\n",
      "Training Loss: 0.18231810123643657\n",
      "Validation Loss: 0.11211047219265917\n",
      "Epoch 6/100\n",
      "Training Loss: 0.15068020374155855\n",
      "Validation Loss: 0.170187866094597\n",
      "Epoch 7/100\n",
      "Training Loss: 0.15555125351697757\n",
      "Validation Loss: 0.09900043618993973\n",
      "Epoch 8/100\n",
      "Training Loss: 0.17737993146690295\n",
      "Validation Loss: 0.1266188942325189\n",
      "Epoch 9/100\n",
      "Training Loss: 0.1860128694101406\n",
      "Validation Loss: 0.10912155678284656\n",
      "Epoch 10/100\n",
      "Training Loss: 0.13529582877957225\n",
      "Validation Loss: 0.11365221943687334\n",
      "Epoch 11/100\n",
      "Training Loss: 0.13790896860482169\n",
      "Validation Loss: 0.05754214440040413\n",
      "Epoch 12/100\n",
      "Training Loss: 0.14877484643487734\n",
      "Validation Loss: 0.1254027052456636\n",
      "Epoch 13/100\n",
      "Training Loss: 0.12973251343444922\n",
      "Validation Loss: 0.03923921412172869\n",
      "Epoch 14/100\n",
      "Training Loss: 0.11334672827213256\n",
      "Validation Loss: 0.08329355121293872\n",
      "Epoch 15/100\n",
      "Training Loss: 0.13259924937277523\n",
      "Validation Loss: 0.05379836766892408\n",
      "Epoch 16/100\n",
      "Training Loss: 0.11259891891282625\n",
      "Validation Loss: 0.10912546024858376\n",
      "Epoch 17/100\n",
      "Training Loss: 0.12566678105703766\n",
      "Validation Loss: 0.08025535146069468\n",
      "Epoch 18/100\n",
      "Training Loss: 0.08662844982917314\n",
      "Validation Loss: 0.05768608910775672\n",
      "Epoch 19/100\n",
      "Training Loss: 0.1022255255177597\n",
      "Validation Loss: 0.06007703576417598\n",
      "Epoch 20/100\n",
      "Training Loss: 0.08193487211031088\n",
      "Validation Loss: 0.04946147998822499\n",
      "Epoch 21/100\n",
      "Training Loss: 0.10518091946511748\n",
      "Validation Loss: 0.06419609052004005\n",
      "Epoch 22/100\n",
      "Training Loss: 0.1102057790175289\n",
      "Validation Loss: 0.06569389233112474\n",
      "Epoch 23/100\n",
      "Training Loss: 0.09763874478638511\n",
      "Validation Loss: 0.05952696070389708\n",
      "Epoch 24/100\n",
      "Training Loss: 0.08912237834628566\n",
      "Validation Loss: 0.06061083167074823\n",
      "Epoch 25/100\n",
      "Training Loss: 0.08774993173255435\n",
      "Validation Loss: 0.061234291434502375\n",
      "Epoch 26/100\n",
      "Training Loss: 0.10543882527018655\n",
      "Validation Loss: 0.04503669343866677\n",
      "Epoch 27/100\n",
      "Training Loss: 0.0943209212340826\n",
      "Validation Loss: 0.0456648723162842\n",
      "Epoch 28/100\n",
      "Training Loss: 0.08382736506263122\n",
      "Validation Loss: 0.04208806673302473\n",
      "Epoch 29/100\n",
      "Training Loss: 0.08338188687982985\n",
      "Validation Loss: 0.05049333802236063\n",
      "Epoch 30/100\n",
      "Training Loss: 0.08422611997324919\n",
      "Validation Loss: 0.05538708430801696\n",
      "Epoch 31/100\n",
      "Training Loss: 0.08532061672384218\n",
      "Validation Loss: 0.06489473352376926\n",
      "Epoch 32/100\n",
      "Training Loss: 0.08332338510707185\n",
      "Validation Loss: 0.05308904353315299\n",
      "Epoch 33/100\n",
      "Training Loss: 0.08602757227917963\n",
      "Validation Loss: 0.04817395029076314\n",
      "Epoch 34/100\n",
      "Training Loss: 0.0780575001868262\n",
      "Validation Loss: 0.05096003340980595\n",
      "Epoch 35/100\n",
      "Training Loss: 0.08292478463613788\n",
      "Validation Loss: 0.05569785122336975\n",
      "Epoch 36/100\n",
      "Training Loss: 0.07099689324733448\n",
      "Validation Loss: 0.0604802433644569\n",
      "Epoch 37/100\n",
      "Training Loss: 0.07686011227341631\n",
      "Validation Loss: 0.047344500569802724\n",
      "Epoch 38/100\n",
      "Training Loss: 0.0804468185686131\n",
      "Validation Loss: 0.05372962454264626\n",
      "Epoch 39/100\n",
      "Training Loss: 0.07437709882934188\n",
      "Validation Loss: 0.0476886166061272\n",
      "Epoch 40/100\n",
      "Training Loss: 0.06889944465941203\n",
      "Validation Loss: 0.046363129893915694\n",
      "Epoch 41/100\n",
      "Training Loss: 0.07509464374735512\n",
      "Validation Loss: 0.060929606590047716\n",
      "Epoch 42/100\n",
      "Training Loss: 0.07369987109448348\n",
      "Validation Loss: 0.036002449127314956\n",
      "Epoch 43/100\n",
      "Training Loss: 0.06904526445465031\n",
      "Validation Loss: 0.0541844671022801\n",
      "Epoch 44/100\n",
      "Training Loss: 0.08237939416691635\n",
      "Validation Loss: 0.04655195435708568\n",
      "Epoch 45/100\n",
      "Training Loss: 0.0785992195472724\n",
      "Validation Loss: 0.06740816802591047\n",
      "Epoch 46/100\n",
      "Training Loss: 0.08721766538551123\n",
      "Validation Loss: 0.06871766493721876\n",
      "Epoch 47/100\n",
      "Training Loss: 0.07756916445230838\n",
      "Validation Loss: 0.05353181288570026\n",
      "Epoch 48/100\n",
      "Training Loss: 0.07714759868274469\n",
      "Validation Loss: 0.05771366592254769\n",
      "Epoch 49/100\n",
      "Training Loss: 0.08023201072081378\n",
      "Validation Loss: 0.06815071993301887\n",
      "Epoch 50/100\n",
      "Training Loss: 0.07799145749051921\n",
      "Validation Loss: 0.04415691497785634\n",
      "Epoch 51/100\n",
      "Training Loss: 0.07287945247422398\n",
      "Validation Loss: 0.05709834039607552\n",
      "Epoch 52/100\n",
      "Training Loss: 0.08864992067391186\n",
      "Validation Loss: 0.051408374596295214\n",
      "Epoch 53/100\n",
      "Training Loss: 0.08224125415511781\n",
      "Validation Loss: 0.042144742642202956\n",
      "Epoch 54/100\n",
      "Training Loss: 0.0737058038725337\n",
      "Validation Loss: 0.04833373840811399\n",
      "Epoch 55/100\n",
      "Training Loss: 0.08059168656278692\n",
      "Validation Loss: 0.05509772780029827\n",
      "Epoch 56/100\n",
      "Training Loss: 0.076224959192704\n",
      "Validation Loss: 0.04079952597392567\n",
      "Epoch 57/100\n",
      "Training Loss: 0.07238925185903722\n",
      "Validation Loss: 0.04597045737282573\n",
      "Epoch 58/100\n",
      "Training Loss: 0.0701962984282801\n",
      "Validation Loss: 0.041804963412806324\n",
      "Epoch 59/100\n",
      "Training Loss: 0.0829710048330198\n",
      "Validation Loss: 0.056483000835233846\n",
      "Epoch 60/100\n",
      "Training Loss: 0.07553082337718647\n",
      "Validation Loss: 0.06112794377979699\n",
      "Epoch 61/100\n",
      "Training Loss: 0.0789899063714127\n",
      "Validation Loss: 0.05158742489754213\n",
      "Epoch 62/100\n",
      "Training Loss: 0.07203086190911337\n",
      "Validation Loss: 0.03509798795590699\n",
      "Epoch 63/100\n",
      "Training Loss: 0.08374967734440875\n",
      "Validation Loss: 0.05853535888560686\n",
      "Epoch 64/100\n",
      "Training Loss: 0.07249233978994005\n",
      "Validation Loss: 0.05696089213404489\n",
      "Epoch 65/100\n",
      "Training Loss: 0.07296206948884824\n",
      "Validation Loss: 0.04017219338148888\n",
      "Epoch 66/100\n",
      "Training Loss: 0.08225186286256488\n",
      "Validation Loss: 0.0427088965294891\n",
      "Epoch 67/100\n",
      "Training Loss: 0.06938030388786849\n",
      "Validation Loss: 0.05881642642900031\n",
      "Epoch 68/100\n",
      "Training Loss: 0.07602467734451072\n",
      "Validation Loss: 0.06631485758215051\n",
      "Epoch 69/100\n",
      "Training Loss: 0.07570403591095069\n",
      "Validation Loss: 0.04110833921459308\n",
      "Epoch 70/100\n",
      "Training Loss: 0.07664409597415749\n",
      "Validation Loss: 0.04862573055690424\n",
      "Epoch 71/100\n",
      "Training Loss: 0.07748930876065105\n",
      "Validation Loss: 0.036745560587835714\n",
      "Epoch 72/100\n",
      "Training Loss: 0.07119419846472595\n",
      "Validation Loss: 0.04271892916924272\n",
      "Epoch 73/100\n",
      "Training Loss: 0.07855023274546603\n",
      "Validation Loss: 0.029599699349670917\n",
      "Epoch 74/100\n",
      "Training Loss: 0.07311081122468188\n",
      "Validation Loss: 0.04982157106433526\n",
      "Epoch 75/100\n",
      "Training Loss: 0.08630723276404255\n",
      "Validation Loss: 0.03582540655582993\n",
      "Epoch 76/100\n",
      "Training Loss: 0.07266384816039757\n",
      "Validation Loss: 0.06272771729179795\n",
      "Epoch 77/100\n",
      "Training Loss: 0.07549014091318547\n",
      "Validation Loss: 0.05528288922720551\n",
      "Epoch 78/100\n",
      "Training Loss: 0.08309504889355608\n",
      "Validation Loss: 0.04383497407644495\n",
      "Epoch 79/100\n",
      "Training Loss: 0.075034497698251\n",
      "Validation Loss: 0.048300500929344715\n",
      "Epoch 80/100\n",
      "Training Loss: 0.07122245729934783\n",
      "Validation Loss: 0.051638937876017185\n",
      "Epoch 81/100\n",
      "Training Loss: 0.07492566323354902\n",
      "Validation Loss: 0.054300684211594806\n",
      "Epoch 82/100\n",
      "Training Loss: 0.0787997902913648\n",
      "Validation Loss: 0.04345874578004328\n",
      "Epoch 83/100\n",
      "Training Loss: 0.07871171261363058\n",
      "Validation Loss: 0.06613431650644083\n",
      "Epoch 84/100\n",
      "Training Loss: 0.08314053654113399\n",
      "Validation Loss: 0.06354548805736797\n",
      "Epoch 85/100\n",
      "Training Loss: 0.08173320061147428\n",
      "Validation Loss: 0.061429906413474775\n",
      "Epoch 86/100\n",
      "Training Loss: 0.08194612788743816\n",
      "Validation Loss: 0.04215065293658648\n",
      "Epoch 87/100\n",
      "Training Loss: 0.07719148140899686\n",
      "Validation Loss: 0.03298381109026959\n",
      "Epoch 88/100\n",
      "Training Loss: 0.07588334841177762\n",
      "Validation Loss: 0.03879153598502432\n",
      "Epoch 89/100\n",
      "Training Loss: 0.09110664619921553\n",
      "Validation Loss: 0.049463516245453684\n",
      "Epoch 90/100\n",
      "Training Loss: 0.07980643967978325\n",
      "Validation Loss: 0.04799674727137597\n",
      "Epoch 91/100\n",
      "Training Loss: 0.08264505695768785\n",
      "Validation Loss: 0.06352223930072717\n",
      "Epoch 92/100\n",
      "Training Loss: 0.07125455066405774\n",
      "Validation Loss: 0.05201218151811118\n",
      "Epoch 93/100\n",
      "Training Loss: 0.07350690378704043\n",
      "Validation Loss: 0.058462716367423485\n",
      "Epoch 94/100\n",
      "Training Loss: 0.08693514463614349\n",
      "Validation Loss: 0.046922683735042096\n",
      "Epoch 95/100\n",
      "Training Loss: 0.07443188931592741\n",
      "Validation Loss: 0.035228336690386074\n",
      "Epoch 96/100\n",
      "Training Loss: 0.07290579630235845\n",
      "Validation Loss: 0.055731844993330684\n",
      "Epoch 97/100\n",
      "Training Loss: 0.08316649867868676\n",
      "Validation Loss: 0.06515689523863796\n",
      "Epoch 98/100\n",
      "Training Loss: 0.07632479437705425\n",
      "Validation Loss: 0.07118220928569877\n",
      "Epoch 99/100\n",
      "Training Loss: 0.08556519121166044\n",
      "Validation Loss: 0.046394775074411136\n",
      "Epoch 100/100\n",
      "Training Loss: 0.07930112238840864\n",
      "Validation Loss: 0.0345642883713822\n",
      "Combination 7: Avg Training Loss = 0.09904087161057565, Avg Validation Loss = 0.06186323863396703\n",
      "Testing combination 8/48: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.001, 'sequence_length': 6, 'dropout_rate': 0.2}\n",
      "Truncated Train Data Shape: (70, 6, 1)\n",
      "Truncated Validation Data Shape: (10, 6, 1)\n",
      "    Trial 1/2 for combination 8/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.49148620607508675\n",
      "Validation Loss: 0.4216810242468364\n",
      "Epoch 2/100\n",
      "Training Loss: 0.2847334698245882\n",
      "Validation Loss: 0.15062027990292462\n",
      "Epoch 3/100\n",
      "Training Loss: 0.23890069294868652\n",
      "Validation Loss: 0.30373836658258224\n",
      "Epoch 4/100\n",
      "Training Loss: 0.19380933869044528\n",
      "Validation Loss: 0.30989128958837997\n",
      "Epoch 5/100\n",
      "Training Loss: 0.2656459233063161\n",
      "Validation Loss: 0.13947693482632603\n",
      "Epoch 6/100\n",
      "Training Loss: 0.1626652466862487\n",
      "Validation Loss: 0.09847645726932645\n",
      "Epoch 7/100\n",
      "Training Loss: 0.17526854375165163\n",
      "Validation Loss: 0.12213682473227808\n",
      "Epoch 8/100\n",
      "Training Loss: 0.17321684196707868\n",
      "Validation Loss: 0.16076844825988088\n",
      "Epoch 9/100\n",
      "Training Loss: 0.1440041749737163\n",
      "Validation Loss: 0.10487544535582556\n",
      "Epoch 10/100\n",
      "Training Loss: 0.16691222462926544\n",
      "Validation Loss: 0.14946666565832384\n",
      "Epoch 11/100\n",
      "Training Loss: 0.14068331539282541\n",
      "Validation Loss: 0.0746938686430283\n",
      "Epoch 12/100\n",
      "Training Loss: 0.13915324471014548\n",
      "Validation Loss: 0.07743331145975828\n",
      "Epoch 13/100\n",
      "Training Loss: 0.13212059782713784\n",
      "Validation Loss: 0.07434000702081445\n",
      "Epoch 14/100\n",
      "Training Loss: 0.12168406382887041\n",
      "Validation Loss: 0.12159670065565105\n",
      "Epoch 15/100\n",
      "Training Loss: 0.12099940470127647\n",
      "Validation Loss: 0.06279158619363857\n",
      "Epoch 16/100\n",
      "Training Loss: 0.12026819337369651\n",
      "Validation Loss: 0.08530990769494545\n",
      "Epoch 17/100\n",
      "Training Loss: 0.13493089732854735\n",
      "Validation Loss: 0.09086169556641745\n",
      "Epoch 18/100\n",
      "Training Loss: 0.121757591648734\n",
      "Validation Loss: 0.05837051574679096\n",
      "Epoch 19/100\n",
      "Training Loss: 0.12655413370163954\n",
      "Validation Loss: 0.054866501920258805\n",
      "Epoch 20/100\n",
      "Training Loss: 0.09527859785124256\n",
      "Validation Loss: 0.06877872868557502\n",
      "Epoch 21/100\n",
      "Training Loss: 0.09013676195833646\n",
      "Validation Loss: 0.057020755000651124\n",
      "Epoch 22/100\n",
      "Training Loss: 0.10167586262504695\n",
      "Validation Loss: 0.04670553706018806\n",
      "Epoch 23/100\n",
      "Training Loss: 0.10296907976990594\n",
      "Validation Loss: 0.04974948185466778\n",
      "Epoch 24/100\n",
      "Training Loss: 0.0901084856818012\n",
      "Validation Loss: 0.07501970756570271\n",
      "Epoch 25/100\n",
      "Training Loss: 0.08721519443272342\n",
      "Validation Loss: 0.061034128648677986\n",
      "Epoch 26/100\n",
      "Training Loss: 0.0762316827832265\n",
      "Validation Loss: 0.055234133672864526\n",
      "Epoch 27/100\n",
      "Training Loss: 0.08507469476551267\n",
      "Validation Loss: 0.0528572409491063\n",
      "Epoch 28/100\n",
      "Training Loss: 0.09158481314302457\n",
      "Validation Loss: 0.054303891463629926\n",
      "Epoch 29/100\n",
      "Training Loss: 0.07834439029106859\n",
      "Validation Loss: 0.0568741160902362\n",
      "Epoch 30/100\n",
      "Training Loss: 0.08059365749703129\n",
      "Validation Loss: 0.049882390995628974\n",
      "Epoch 31/100\n",
      "Training Loss: 0.0823644739056053\n",
      "Validation Loss: 0.04896054044576946\n",
      "Epoch 32/100\n",
      "Training Loss: 0.06546084102209655\n",
      "Validation Loss: 0.04825752514272056\n",
      "Epoch 33/100\n",
      "Training Loss: 0.07790871741153406\n",
      "Validation Loss: 0.046326372993426235\n",
      "Epoch 34/100\n",
      "Training Loss: 0.07666706908331881\n",
      "Validation Loss: 0.04641865901377592\n",
      "Epoch 35/100\n",
      "Training Loss: 0.07740043636447506\n",
      "Validation Loss: 0.03247134297092062\n",
      "Epoch 36/100\n",
      "Training Loss: 0.07761098805413008\n",
      "Validation Loss: 0.04194068110290514\n",
      "Epoch 37/100\n",
      "Training Loss: 0.07638695734008194\n",
      "Validation Loss: 0.0507984025549086\n",
      "Epoch 38/100\n",
      "Training Loss: 0.07364401332179127\n",
      "Validation Loss: 0.055750260648962935\n",
      "Epoch 39/100\n",
      "Training Loss: 0.07779809263214071\n",
      "Validation Loss: 0.04812251293174179\n",
      "Epoch 40/100\n",
      "Training Loss: 0.07489769426760691\n",
      "Validation Loss: 0.05442085219235393\n",
      "Epoch 41/100\n",
      "Training Loss: 0.07003754402447782\n",
      "Validation Loss: 0.039122613112261076\n",
      "Epoch 42/100\n",
      "Training Loss: 0.07938391746256931\n",
      "Validation Loss: 0.041481985952619274\n",
      "Epoch 43/100\n",
      "Training Loss: 0.07042290073768738\n",
      "Validation Loss: 0.03765411066169447\n",
      "Epoch 44/100\n",
      "Training Loss: 0.07105772022773861\n",
      "Validation Loss: 0.04649840728160603\n",
      "Epoch 45/100\n",
      "Training Loss: 0.07302226448589036\n",
      "Validation Loss: 0.06203230392459329\n",
      "Epoch 46/100\n",
      "Training Loss: 0.07426725268609423\n",
      "Validation Loss: 0.042026665773965445\n",
      "Epoch 47/100\n",
      "Training Loss: 0.0786570755437834\n",
      "Validation Loss: 0.05465375673553535\n",
      "Epoch 48/100\n",
      "Training Loss: 0.07680660175774387\n",
      "Validation Loss: 0.051159446816260855\n",
      "Epoch 49/100\n",
      "Training Loss: 0.07181651011119261\n",
      "Validation Loss: 0.048682608035950245\n",
      "Epoch 50/100\n",
      "Training Loss: 0.07667582165414151\n",
      "Validation Loss: 0.03917896367737024\n",
      "Epoch 51/100\n",
      "Training Loss: 0.07122704116299923\n",
      "Validation Loss: 0.03346932090587683\n",
      "Epoch 52/100\n",
      "Training Loss: 0.07584400274960465\n",
      "Validation Loss: 0.05091248985562892\n",
      "Epoch 53/100\n",
      "Training Loss: 0.0867247132961043\n",
      "Validation Loss: 0.04117030064260304\n",
      "Epoch 54/100\n",
      "Training Loss: 0.07871422980899626\n",
      "Validation Loss: 0.039903928759538076\n",
      "Epoch 55/100\n",
      "Training Loss: 0.07002149884086777\n",
      "Validation Loss: 0.04943847419062479\n",
      "Epoch 56/100\n",
      "Training Loss: 0.07796968608419524\n",
      "Validation Loss: 0.033652205039921904\n",
      "Epoch 57/100\n",
      "Training Loss: 0.07503311183218907\n",
      "Validation Loss: 0.04861637472047269\n",
      "Epoch 58/100\n",
      "Training Loss: 0.0780284805691486\n",
      "Validation Loss: 0.047993645224684384\n",
      "Epoch 59/100\n",
      "Training Loss: 0.0740922636644999\n",
      "Validation Loss: 0.04685596790141255\n",
      "Epoch 60/100\n",
      "Training Loss: 0.06714742240436675\n",
      "Validation Loss: 0.039110616546653175\n",
      "Epoch 61/100\n",
      "Training Loss: 0.07891009231212769\n",
      "Validation Loss: 0.05451319238510131\n",
      "Epoch 62/100\n",
      "Training Loss: 0.07822587546631614\n",
      "Validation Loss: 0.0787847068676525\n",
      "Epoch 63/100\n",
      "Training Loss: 0.07715882847135931\n",
      "Validation Loss: 0.06647077442661325\n",
      "Epoch 64/100\n",
      "Training Loss: 0.07639408605846316\n",
      "Validation Loss: 0.06400568167396556\n",
      "Epoch 65/100\n",
      "Training Loss: 0.08487490198209734\n",
      "Validation Loss: 0.044254087408552814\n",
      "Epoch 66/100\n",
      "Training Loss: 0.08374265265779751\n",
      "Validation Loss: 0.04282515611569192\n",
      "Epoch 67/100\n",
      "Training Loss: 0.07770728756679318\n",
      "Validation Loss: 0.044610258141651565\n",
      "Epoch 68/100\n",
      "Training Loss: 0.08774649327068612\n",
      "Validation Loss: 0.03726954820543296\n",
      "Epoch 69/100\n",
      "Training Loss: 0.07185817192154895\n",
      "Validation Loss: 0.043050947677922766\n",
      "Epoch 70/100\n",
      "Training Loss: 0.09361213509209863\n",
      "Validation Loss: 0.041359459311342904\n",
      "Epoch 71/100\n",
      "Training Loss: 0.08163914174176093\n",
      "Validation Loss: 0.042834076634933146\n",
      "Epoch 72/100\n",
      "Training Loss: 0.08094019817833628\n",
      "Validation Loss: 0.07218090185848561\n",
      "Epoch 73/100\n",
      "Training Loss: 0.06917788376607897\n",
      "Validation Loss: 0.05506480056587861\n",
      "Epoch 74/100\n",
      "Training Loss: 0.07183688521418982\n",
      "Validation Loss: 0.042635532670749085\n",
      "Epoch 75/100\n",
      "Training Loss: 0.06358160795410563\n",
      "Validation Loss: 0.05344988117294831\n",
      "Epoch 76/100\n",
      "Training Loss: 0.0739389114238654\n",
      "Validation Loss: 0.048277774297855916\n",
      "Epoch 77/100\n",
      "Training Loss: 0.07367600131923312\n",
      "Validation Loss: 0.0434583723396781\n",
      "Epoch 78/100\n",
      "Training Loss: 0.06245681784745923\n",
      "Validation Loss: 0.05158563377808042\n",
      "Epoch 79/100\n",
      "Training Loss: 0.08854122907957475\n",
      "Validation Loss: 0.043241949326017645\n",
      "Epoch 80/100\n",
      "Training Loss: 0.0755848126030292\n",
      "Validation Loss: 0.04368097761964426\n",
      "Epoch 81/100\n",
      "Training Loss: 0.08244237612478829\n",
      "Validation Loss: 0.05039356364884675\n",
      "Epoch 82/100\n",
      "Training Loss: 0.07796817964091235\n",
      "Validation Loss: 0.07557385250092954\n",
      "Epoch 83/100\n",
      "Training Loss: 0.09096814221622022\n",
      "Validation Loss: 0.039364363995482446\n",
      "Epoch 84/100\n",
      "Training Loss: 0.08092765770641445\n",
      "Validation Loss: 0.04788905641392413\n",
      "Epoch 85/100\n",
      "Training Loss: 0.09183905739966818\n",
      "Validation Loss: 0.052463344334179665\n",
      "Epoch 86/100\n",
      "Training Loss: 0.08245883793408636\n",
      "Validation Loss: 0.05947579829142158\n",
      "Epoch 87/100\n",
      "Training Loss: 0.07774748286674356\n",
      "Validation Loss: 0.03397396337788282\n",
      "Epoch 88/100\n",
      "Training Loss: 0.07388401516570078\n",
      "Validation Loss: 0.0688389647143575\n",
      "Epoch 89/100\n",
      "Training Loss: 0.07647455698773012\n",
      "Validation Loss: 0.04963041920751383\n",
      "Epoch 90/100\n",
      "Training Loss: 0.06626963854363438\n",
      "Validation Loss: 0.07855714455483716\n",
      "Epoch 91/100\n",
      "Training Loss: 0.08475393600035441\n",
      "Validation Loss: 0.06082042185986329\n",
      "Epoch 92/100\n",
      "Training Loss: 0.07997231658486954\n",
      "Validation Loss: 0.054949613102236394\n",
      "Epoch 93/100\n",
      "Training Loss: 0.09336366547118001\n",
      "Validation Loss: 0.048610890703984694\n",
      "Epoch 94/100\n",
      "Training Loss: 0.08269145214555881\n",
      "Validation Loss: 0.0807467920913614\n",
      "Epoch 95/100\n",
      "Training Loss: 0.08310249749186077\n",
      "Validation Loss: 0.03098360829919996\n",
      "Epoch 96/100\n",
      "Training Loss: 0.08971784351316618\n",
      "Validation Loss: 0.08631208355959816\n",
      "Epoch 97/100\n",
      "Training Loss: 0.07959973714555028\n",
      "Validation Loss: 0.05680174595204519\n",
      "Epoch 98/100\n",
      "Training Loss: 0.08448159389809906\n",
      "Validation Loss: 0.06747116396323108\n",
      "Epoch 99/100\n",
      "Training Loss: 0.09435888284231135\n",
      "Validation Loss: 0.054292392588090856\n",
      "Epoch 100/100\n",
      "Training Loss: 0.09539266669618679\n",
      "Validation Loss: 0.08085915259688087\n",
      "    Trial 2/2 for combination 8/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.37357842346711256\n",
      "Validation Loss: 0.5411486897195987\n",
      "Epoch 2/100\n",
      "Training Loss: 0.3599963869199987\n",
      "Validation Loss: 0.23397930192354788\n",
      "Epoch 3/100\n",
      "Training Loss: 0.25406301772231765\n",
      "Validation Loss: 0.15170598033175403\n",
      "Epoch 4/100\n",
      "Training Loss: 0.26888566353132426\n",
      "Validation Loss: 0.15675435231087556\n",
      "Epoch 5/100\n",
      "Training Loss: 0.19737233383480135\n",
      "Validation Loss: 0.11434271049573996\n",
      "Epoch 6/100\n",
      "Training Loss: 0.18003595830468241\n",
      "Validation Loss: 0.09302198463001303\n",
      "Epoch 7/100\n",
      "Training Loss: 0.17698141067407577\n",
      "Validation Loss: 0.14904063689518027\n",
      "Epoch 8/100\n",
      "Training Loss: 0.14571657950764122\n",
      "Validation Loss: 0.0783508462677153\n",
      "Epoch 9/100\n",
      "Training Loss: 0.15056186967223567\n",
      "Validation Loss: 0.07416457526651696\n",
      "Epoch 10/100\n",
      "Training Loss: 0.1480551000528516\n",
      "Validation Loss: 0.07222597525386663\n",
      "Epoch 11/100\n",
      "Training Loss: 0.11164674422425909\n",
      "Validation Loss: 0.14931601945541872\n",
      "Epoch 12/100\n",
      "Training Loss: 0.12913789935714912\n",
      "Validation Loss: 0.08652639364065806\n",
      "Epoch 13/100\n",
      "Training Loss: 0.1388870594628479\n",
      "Validation Loss: 0.11447939039044372\n",
      "Epoch 14/100\n",
      "Training Loss: 0.11914456025937005\n",
      "Validation Loss: 0.059991948904821665\n",
      "Epoch 15/100\n",
      "Training Loss: 0.1082961477812016\n",
      "Validation Loss: 0.0509703503416476\n",
      "Epoch 16/100\n",
      "Training Loss: 0.0985801653916696\n",
      "Validation Loss: 0.06773793455868338\n",
      "Epoch 17/100\n",
      "Training Loss: 0.10008632432462485\n",
      "Validation Loss: 0.05518766063736234\n",
      "Epoch 18/100\n",
      "Training Loss: 0.09190802859849702\n",
      "Validation Loss: 0.06699445920707567\n",
      "Epoch 19/100\n",
      "Training Loss: 0.09143749114958458\n",
      "Validation Loss: 0.056455455715238176\n",
      "Epoch 20/100\n",
      "Training Loss: 0.11011308982762769\n",
      "Validation Loss: 0.059623572863879704\n",
      "Epoch 21/100\n",
      "Training Loss: 0.09609306333949733\n",
      "Validation Loss: 0.052921216842985466\n",
      "Epoch 22/100\n",
      "Training Loss: 0.0934067041238899\n",
      "Validation Loss: 0.04164828367595409\n",
      "Epoch 23/100\n",
      "Training Loss: 0.08688019375763084\n",
      "Validation Loss: 0.08462495158621042\n",
      "Epoch 24/100\n",
      "Training Loss: 0.08090145561729753\n",
      "Validation Loss: 0.061705143242628435\n",
      "Epoch 25/100\n",
      "Training Loss: 0.09579869639238896\n",
      "Validation Loss: 0.047438522363255645\n",
      "Epoch 26/100\n",
      "Training Loss: 0.0844530977321812\n",
      "Validation Loss: 0.0516357410382046\n",
      "Epoch 27/100\n",
      "Training Loss: 0.07598543590970744\n",
      "Validation Loss: 0.041242760744082965\n",
      "Epoch 28/100\n",
      "Training Loss: 0.07936188855483825\n",
      "Validation Loss: 0.06697832610925761\n",
      "Epoch 29/100\n",
      "Training Loss: 0.0801898218878521\n",
      "Validation Loss: 0.04259352167613904\n",
      "Epoch 30/100\n",
      "Training Loss: 0.0864858658996301\n",
      "Validation Loss: 0.06305472156087225\n",
      "Epoch 31/100\n",
      "Training Loss: 0.07460200203416828\n",
      "Validation Loss: 0.0712685758365322\n",
      "Epoch 32/100\n",
      "Training Loss: 0.077491797054818\n",
      "Validation Loss: 0.06768069887081213\n",
      "Epoch 33/100\n",
      "Training Loss: 0.07808596627943519\n",
      "Validation Loss: 0.044582371270837716\n",
      "Epoch 34/100\n",
      "Training Loss: 0.07855458329932423\n",
      "Validation Loss: 0.04901347829874637\n",
      "Epoch 35/100\n",
      "Training Loss: 0.07580372458023028\n",
      "Validation Loss: 0.054351039325288106\n",
      "Epoch 36/100\n",
      "Training Loss: 0.07628386517765676\n",
      "Validation Loss: 0.048607989845644295\n",
      "Epoch 37/100\n",
      "Training Loss: 0.08138571307582096\n",
      "Validation Loss: 0.04406777208029765\n",
      "Epoch 38/100\n",
      "Training Loss: 0.07529621207348931\n",
      "Validation Loss: 0.03992552593714854\n",
      "Epoch 39/100\n",
      "Training Loss: 0.07649131323344452\n",
      "Validation Loss: 0.04156716240911154\n",
      "Epoch 40/100\n",
      "Training Loss: 0.07568039449598486\n",
      "Validation Loss: 0.047707078027007\n",
      "Epoch 41/100\n",
      "Training Loss: 0.063145343232129\n",
      "Validation Loss: 0.03992075510854527\n",
      "Epoch 42/100\n",
      "Training Loss: 0.08372106886459557\n",
      "Validation Loss: 0.05070649756479527\n",
      "Epoch 43/100\n",
      "Training Loss: 0.06870037293967157\n",
      "Validation Loss: 0.04909580881113566\n",
      "Epoch 44/100\n",
      "Training Loss: 0.08417746812709412\n",
      "Validation Loss: 0.03849726701269046\n",
      "Epoch 45/100\n",
      "Training Loss: 0.07399892126830379\n",
      "Validation Loss: 0.05314872130045747\n",
      "Epoch 46/100\n",
      "Training Loss: 0.08158595366926472\n",
      "Validation Loss: 0.036478581859311694\n",
      "Epoch 47/100\n",
      "Training Loss: 0.07260705015654471\n",
      "Validation Loss: 0.049732318596068845\n",
      "Epoch 48/100\n",
      "Training Loss: 0.07424036597359558\n",
      "Validation Loss: 0.054025572709415395\n",
      "Epoch 49/100\n",
      "Training Loss: 0.0756478040492748\n",
      "Validation Loss: 0.03985969624266755\n",
      "Epoch 50/100\n",
      "Training Loss: 0.0740363099485255\n",
      "Validation Loss: 0.04412913871028498\n",
      "Epoch 51/100\n",
      "Training Loss: 0.06943533560821354\n",
      "Validation Loss: 0.055521498449061304\n",
      "Epoch 52/100\n",
      "Training Loss: 0.06823167959407431\n",
      "Validation Loss: 0.03852230098925387\n",
      "Epoch 53/100\n",
      "Training Loss: 0.07804929298946969\n",
      "Validation Loss: 0.06469040109902831\n",
      "Epoch 54/100\n",
      "Training Loss: 0.07657164502436327\n",
      "Validation Loss: 0.04907719474141886\n",
      "Epoch 55/100\n",
      "Training Loss: 0.07447277871226758\n",
      "Validation Loss: 0.05949996425382058\n",
      "Epoch 56/100\n",
      "Training Loss: 0.07295136815198959\n",
      "Validation Loss: 0.04604013622402795\n",
      "Epoch 57/100\n",
      "Training Loss: 0.07984020310854138\n",
      "Validation Loss: 0.03658500249150691\n",
      "Epoch 58/100\n",
      "Training Loss: 0.07195456698565032\n",
      "Validation Loss: 0.06992542627996086\n",
      "Epoch 59/100\n",
      "Training Loss: 0.07745222202909424\n",
      "Validation Loss: 0.06208941115434021\n",
      "Epoch 60/100\n",
      "Training Loss: 0.07495599314260992\n",
      "Validation Loss: 0.05713206033125603\n",
      "Epoch 61/100\n",
      "Training Loss: 0.07475251386272475\n",
      "Validation Loss: 0.05440859932859695\n",
      "Epoch 62/100\n",
      "Training Loss: 0.07388055153116171\n",
      "Validation Loss: 0.048572567705300204\n",
      "Epoch 63/100\n",
      "Training Loss: 0.07527589582232082\n",
      "Validation Loss: 0.04430657886650599\n",
      "Epoch 64/100\n",
      "Training Loss: 0.0680979213445939\n",
      "Validation Loss: 0.046017368981108274\n",
      "Epoch 65/100\n",
      "Training Loss: 0.08236223619956773\n",
      "Validation Loss: 0.06337991046414947\n",
      "Epoch 66/100\n",
      "Training Loss: 0.08044958427999283\n",
      "Validation Loss: 0.07407710708340066\n",
      "Epoch 67/100\n",
      "Training Loss: 0.09028258179062067\n",
      "Validation Loss: 0.05828694938806789\n",
      "Epoch 68/100\n",
      "Training Loss: 0.07950546015977161\n",
      "Validation Loss: 0.04605021710888935\n",
      "Epoch 69/100\n",
      "Training Loss: 0.073147986049513\n",
      "Validation Loss: 0.027812832347481102\n",
      "Epoch 70/100\n",
      "Training Loss: 0.08178158644664742\n",
      "Validation Loss: 0.05674766222109773\n",
      "Epoch 71/100\n",
      "Training Loss: 0.07655009971404331\n",
      "Validation Loss: 0.06117569290131235\n",
      "Epoch 72/100\n",
      "Training Loss: 0.07540919519525398\n",
      "Validation Loss: 0.0561225197389104\n",
      "Epoch 73/100\n",
      "Training Loss: 0.07729044670098073\n",
      "Validation Loss: 0.05556896439959572\n",
      "Epoch 74/100\n",
      "Training Loss: 0.08311527318321887\n",
      "Validation Loss: 0.055757077982789206\n",
      "Epoch 75/100\n",
      "Training Loss: 0.06683557247737049\n",
      "Validation Loss: 0.04833200068404388\n",
      "Epoch 76/100\n",
      "Training Loss: 0.08035023289060052\n",
      "Validation Loss: 0.05820834591932365\n",
      "Epoch 77/100\n",
      "Training Loss: 0.09233269049083405\n",
      "Validation Loss: 0.06437912469603077\n",
      "Epoch 78/100\n",
      "Training Loss: 0.08086407482668095\n",
      "Validation Loss: 0.03834072390507266\n",
      "Epoch 79/100\n",
      "Training Loss: 0.08094254034945321\n",
      "Validation Loss: 0.043114349457851045\n",
      "Epoch 80/100\n",
      "Training Loss: 0.08777682299071088\n",
      "Validation Loss: 0.06816942789002882\n",
      "Epoch 81/100\n",
      "Training Loss: 0.08547951731148294\n",
      "Validation Loss: 0.06752487824201811\n",
      "Epoch 82/100\n",
      "Training Loss: 0.07570759364828687\n",
      "Validation Loss: 0.055977583391097305\n",
      "Epoch 83/100\n",
      "Training Loss: 0.07579546837462521\n",
      "Validation Loss: 0.07001389675983186\n",
      "Epoch 84/100\n",
      "Training Loss: 0.09015535187734357\n",
      "Validation Loss: 0.062174925398939505\n",
      "Epoch 85/100\n",
      "Training Loss: 0.07125921697454018\n",
      "Validation Loss: 0.07187463070905345\n",
      "Epoch 86/100\n",
      "Training Loss: 0.07599226266659255\n",
      "Validation Loss: 0.06046502120533727\n",
      "Epoch 87/100\n",
      "Training Loss: 0.08565606592964971\n",
      "Validation Loss: 0.09194492453350712\n",
      "Epoch 88/100\n",
      "Training Loss: 0.07445235572795336\n",
      "Validation Loss: 0.04727519719805233\n",
      "Epoch 89/100\n",
      "Training Loss: 0.08361937018099154\n",
      "Validation Loss: 0.0507027362166234\n",
      "Epoch 90/100\n",
      "Training Loss: 0.07202948966989774\n",
      "Validation Loss: 0.05019828482637891\n",
      "Epoch 91/100\n",
      "Training Loss: 0.06952491197672968\n",
      "Validation Loss: 0.04695945873822998\n",
      "Epoch 92/100\n",
      "Training Loss: 0.07366378503233191\n",
      "Validation Loss: 0.05616539510689787\n",
      "Epoch 93/100\n",
      "Training Loss: 0.09780551726821411\n",
      "Validation Loss: 0.06069573507929667\n",
      "Epoch 94/100\n",
      "Training Loss: 0.06826073187771152\n",
      "Validation Loss: 0.07592768158580822\n",
      "Epoch 95/100\n",
      "Training Loss: 0.07893815740163762\n",
      "Validation Loss: 0.048442694283813745\n",
      "Epoch 96/100\n",
      "Training Loss: 0.06776068697147598\n",
      "Validation Loss: 0.12314667075254104\n",
      "Epoch 97/100\n",
      "Training Loss: 0.09741242600651116\n",
      "Validation Loss: 0.04415942745935238\n",
      "Epoch 98/100\n",
      "Training Loss: 0.10081697575474298\n",
      "Validation Loss: 0.07814911793394397\n",
      "Epoch 99/100\n",
      "Training Loss: 0.08649022460388942\n",
      "Validation Loss: 0.049164558188792125\n",
      "Epoch 100/100\n",
      "Training Loss: 0.08982505828323595\n",
      "Validation Loss: 0.0353865075694992\n",
      "Combination 8: Avg Training Loss = 0.09796127722488118, Avg Validation Loss = 0.06825993773515206\n",
      "Testing combination 9/48: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.001, 'sequence_length': 20, 'dropout_rate': 0.1}\n",
      "Truncated Train Data Shape: (70, 20, 1)\n",
      "Truncated Validation Data Shape: (10, 20, 1)\n",
      "    Trial 1/2 for combination 9/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.37097727563901794\n",
      "Validation Loss: 0.3642768711978309\n",
      "Epoch 2/100\n",
      "Training Loss: 0.24659948217973102\n",
      "Validation Loss: 0.2853103226702197\n",
      "Epoch 3/100\n",
      "Training Loss: 0.18591666663011042\n",
      "Validation Loss: 0.13148276574921025\n",
      "Epoch 4/100\n",
      "Training Loss: 0.18724069877056887\n",
      "Validation Loss: 0.07074657956586741\n",
      "Epoch 5/100\n",
      "Training Loss: 0.16740527104167274\n",
      "Validation Loss: 0.10224014403509543\n",
      "Epoch 6/100\n",
      "Training Loss: 0.12736469139173548\n",
      "Validation Loss: 0.08222195246819052\n",
      "Epoch 7/100\n",
      "Training Loss: 0.12963695020007937\n",
      "Validation Loss: 0.07151999833589168\n",
      "Epoch 8/100\n",
      "Training Loss: 0.11591943473108657\n",
      "Validation Loss: 0.0776745128093048\n",
      "Epoch 9/100\n",
      "Training Loss: 0.12038200151435667\n",
      "Validation Loss: 0.1121097856326588\n",
      "Epoch 10/100\n",
      "Training Loss: 0.10908502374531943\n",
      "Validation Loss: 0.07285869378207377\n",
      "Epoch 11/100\n",
      "Training Loss: 0.12394352600535281\n",
      "Validation Loss: 0.056053348693860375\n",
      "Epoch 12/100\n",
      "Training Loss: 0.07753993439701089\n",
      "Validation Loss: 0.08369792873423955\n",
      "Epoch 13/100\n",
      "Training Loss: 0.10256502544679254\n",
      "Validation Loss: 0.07145919083763083\n",
      "Epoch 14/100\n",
      "Training Loss: 0.08678329539080656\n",
      "Validation Loss: 0.08621139313361247\n",
      "Epoch 15/100\n",
      "Training Loss: 0.09412085093073931\n",
      "Validation Loss: 0.05728756818679927\n",
      "Epoch 16/100\n",
      "Training Loss: 0.0841150840654757\n",
      "Validation Loss: 0.06255823455987139\n",
      "Epoch 17/100\n",
      "Training Loss: 0.08040444997257674\n",
      "Validation Loss: 0.03358220264767609\n",
      "Epoch 18/100\n",
      "Training Loss: 0.08263175031648952\n",
      "Validation Loss: 0.040129909443777395\n",
      "Epoch 19/100\n",
      "Training Loss: 0.09371923026529959\n",
      "Validation Loss: 0.04925551596068621\n",
      "Epoch 20/100\n",
      "Training Loss: 0.07292111774554318\n",
      "Validation Loss: 0.06005505741048724\n",
      "Epoch 21/100\n",
      "Training Loss: 0.08325224231351498\n",
      "Validation Loss: 0.05242842230024976\n",
      "Epoch 22/100\n",
      "Training Loss: 0.07147328261832155\n",
      "Validation Loss: 0.04876304424130644\n",
      "Epoch 23/100\n",
      "Training Loss: 0.07295232481886038\n",
      "Validation Loss: 0.03543972966161234\n",
      "Epoch 24/100\n",
      "Training Loss: 0.06772209411000447\n",
      "Validation Loss: 0.044091956247756525\n",
      "Epoch 25/100\n",
      "Training Loss: 0.07714226131973183\n",
      "Validation Loss: 0.05086529758653787\n",
      "Epoch 26/100\n",
      "Training Loss: 0.07916190019285223\n",
      "Validation Loss: 0.0466470254926666\n",
      "Epoch 27/100\n",
      "Training Loss: 0.06125651360846849\n",
      "Validation Loss: 0.04691228517051736\n",
      "Epoch 28/100\n",
      "Training Loss: 0.07192741570371317\n",
      "Validation Loss: 0.03722350789723251\n",
      "Epoch 29/100\n",
      "Training Loss: 0.08284724348082921\n",
      "Validation Loss: 0.03588436411784967\n",
      "Epoch 30/100\n",
      "Training Loss: 0.0725068342592584\n",
      "Validation Loss: 0.025271300461327757\n",
      "Epoch 31/100\n",
      "Training Loss: 0.06863651492831581\n",
      "Validation Loss: 0.05818531847363989\n",
      "Epoch 32/100\n",
      "Training Loss: 0.06129113742915224\n",
      "Validation Loss: 0.04962258375285831\n",
      "Epoch 33/100\n",
      "Training Loss: 0.07390232963455118\n",
      "Validation Loss: 0.04826632637436316\n",
      "Epoch 34/100\n",
      "Training Loss: 0.06265001562747845\n",
      "Validation Loss: 0.0320488716575721\n",
      "Epoch 35/100\n",
      "Training Loss: 0.06276957165597641\n",
      "Validation Loss: 0.04670270611532017\n",
      "Epoch 36/100\n",
      "Training Loss: 0.06879567964938071\n",
      "Validation Loss: 0.08708434493663157\n",
      "Epoch 37/100\n",
      "Training Loss: 0.07357997702247683\n",
      "Validation Loss: 0.03526124027226154\n",
      "Epoch 38/100\n",
      "Training Loss: 0.062046578085086374\n",
      "Validation Loss: 0.04286518700669435\n",
      "Epoch 39/100\n",
      "Training Loss: 0.06574715730741816\n",
      "Validation Loss: 0.04027989097346254\n",
      "Epoch 40/100\n",
      "Training Loss: 0.07627546340471704\n",
      "Validation Loss: 0.031027428383989426\n",
      "Epoch 41/100\n",
      "Training Loss: 0.06790247493070811\n",
      "Validation Loss: 0.03246823086416832\n",
      "Epoch 42/100\n",
      "Training Loss: 0.06558159646694134\n",
      "Validation Loss: 0.05105985600729458\n",
      "Epoch 43/100\n",
      "Training Loss: 0.06878834711586174\n",
      "Validation Loss: 0.04548206763458451\n",
      "Epoch 44/100\n",
      "Training Loss: 0.06397135367333745\n",
      "Validation Loss: 0.04299021433570628\n",
      "Epoch 45/100\n",
      "Training Loss: 0.06188982871754415\n",
      "Validation Loss: 0.012996595902869551\n",
      "Epoch 46/100\n",
      "Training Loss: 0.06522766122349556\n",
      "Validation Loss: 0.06045250587691877\n",
      "Epoch 47/100\n",
      "Training Loss: 0.06487145644341037\n",
      "Validation Loss: 0.055039019388488694\n",
      "Epoch 48/100\n",
      "Training Loss: 0.06756158710944689\n",
      "Validation Loss: 0.08183770091361228\n",
      "Epoch 49/100\n",
      "Training Loss: 0.07272308885070645\n",
      "Validation Loss: 0.04450500004367263\n",
      "Epoch 50/100\n",
      "Training Loss: 0.07271077518468358\n",
      "Validation Loss: 0.08791580791876834\n",
      "Epoch 51/100\n",
      "Training Loss: 0.06773892030046814\n",
      "Validation Loss: 0.060043181202539195\n",
      "Epoch 52/100\n",
      "Training Loss: 0.0751609658029661\n",
      "Validation Loss: 0.032199076329936585\n",
      "Epoch 53/100\n",
      "Training Loss: 0.07348310138038523\n",
      "Validation Loss: 0.028588295093618342\n",
      "Epoch 54/100\n",
      "Training Loss: 0.07270001255397285\n",
      "Validation Loss: 0.030275696598269057\n",
      "Epoch 55/100\n",
      "Training Loss: 0.07850960252384155\n",
      "Validation Loss: 0.044933067247087954\n",
      "Epoch 56/100\n",
      "Training Loss: 0.06854057003166858\n",
      "Validation Loss: 0.05754507967917925\n",
      "Epoch 57/100\n",
      "Training Loss: 0.0772489244713437\n",
      "Validation Loss: 0.09433021152655059\n",
      "Epoch 58/100\n",
      "Training Loss: 0.07254285713098417\n",
      "Validation Loss: 0.030979509605617118\n",
      "Epoch 59/100\n",
      "Training Loss: 0.07657286609916789\n",
      "Validation Loss: 0.041167126120411915\n",
      "Epoch 60/100\n",
      "Training Loss: 0.06522602511149461\n",
      "Validation Loss: 0.02778526623795821\n",
      "Epoch 61/100\n",
      "Training Loss: 0.07400036351390889\n",
      "Validation Loss: 0.05560033900158727\n",
      "Epoch 62/100\n",
      "Training Loss: 0.05901323593912721\n",
      "Validation Loss: 0.061535415970840654\n",
      "Epoch 63/100\n",
      "Training Loss: 0.05705380744765978\n",
      "Validation Loss: 0.060154212504943\n",
      "Epoch 64/100\n",
      "Training Loss: 0.07082330562499818\n",
      "Validation Loss: 0.05074886052184383\n",
      "Epoch 65/100\n",
      "Training Loss: 0.08663390713650895\n",
      "Validation Loss: 0.04077526566445277\n",
      "Epoch 66/100\n",
      "Training Loss: 0.08013153332855252\n",
      "Validation Loss: 0.04618574585969328\n",
      "Epoch 67/100\n",
      "Training Loss: 0.07243151361025892\n",
      "Validation Loss: 0.041758474439798056\n",
      "Epoch 68/100\n",
      "Training Loss: 0.0623022502639257\n",
      "Validation Loss: 0.04323328694411416\n",
      "Epoch 69/100\n",
      "Training Loss: 0.07969331840476335\n",
      "Validation Loss: 0.048737172959039524\n",
      "Epoch 70/100\n",
      "Training Loss: 0.0869911789518147\n",
      "Validation Loss: 0.06618892821432362\n",
      "Epoch 71/100\n",
      "Training Loss: 0.083319882770725\n",
      "Validation Loss: 0.0404512616385934\n",
      "Epoch 72/100\n",
      "Training Loss: 0.07039960692914198\n",
      "Validation Loss: 0.05657020724484757\n",
      "Epoch 73/100\n",
      "Training Loss: 0.0789254575468896\n",
      "Validation Loss: 0.03827929307833785\n",
      "Epoch 74/100\n",
      "Training Loss: 0.07379728126611748\n",
      "Validation Loss: 0.03923701838223852\n",
      "Epoch 75/100\n",
      "Training Loss: 0.07161716662908203\n",
      "Validation Loss: 0.04068961918404495\n",
      "Epoch 76/100\n",
      "Training Loss: 0.08209968785989977\n",
      "Validation Loss: 0.045753024658692264\n",
      "Epoch 77/100\n",
      "Training Loss: 0.07794927009896883\n",
      "Validation Loss: 0.0365935463475416\n",
      "Epoch 78/100\n",
      "Training Loss: 0.06931452461470591\n",
      "Validation Loss: 0.06395821529749249\n",
      "Epoch 79/100\n",
      "Training Loss: 0.055477461947096116\n",
      "Validation Loss: 0.03026444922716892\n",
      "Epoch 80/100\n",
      "Training Loss: 0.0774388331987026\n",
      "Validation Loss: 0.04872321396978056\n",
      "Epoch 81/100\n",
      "Training Loss: 0.08287454627041806\n",
      "Validation Loss: 0.05707401081130438\n",
      "Epoch 82/100\n",
      "Training Loss: 0.05955818136669144\n",
      "Validation Loss: 0.0482306771412784\n",
      "Epoch 83/100\n",
      "Training Loss: 0.06748173161951058\n",
      "Validation Loss: 0.04014377617851468\n",
      "Epoch 84/100\n",
      "Training Loss: 0.0895171462555429\n",
      "Validation Loss: 0.042260368005533594\n",
      "Epoch 85/100\n",
      "Training Loss: 0.07392423457705065\n",
      "Validation Loss: 0.03076365847600296\n",
      "Epoch 86/100\n",
      "Training Loss: 0.06267124720615073\n",
      "Validation Loss: 0.04543640802280425\n",
      "Epoch 87/100\n",
      "Training Loss: 0.07119264120414164\n",
      "Validation Loss: 0.018821891134658382\n",
      "Epoch 88/100\n",
      "Training Loss: 0.0666683535291508\n",
      "Validation Loss: 0.026067080173630974\n",
      "Epoch 89/100\n",
      "Training Loss: 0.07126884276554746\n",
      "Validation Loss: 0.03974511206389114\n",
      "Epoch 90/100\n",
      "Training Loss: 0.07695992598851281\n",
      "Validation Loss: 0.05055442865777382\n",
      "Epoch 91/100\n",
      "Training Loss: 0.060788980738570156\n",
      "Validation Loss: 0.05545551353764612\n",
      "Epoch 92/100\n",
      "Training Loss: 0.06781725024637913\n",
      "Validation Loss: 0.0448703468957759\n",
      "Epoch 93/100\n",
      "Training Loss: 0.09881308829925253\n",
      "Validation Loss: 0.050075252687991414\n",
      "Epoch 94/100\n",
      "Training Loss: 0.08380547642106256\n",
      "Validation Loss: 0.06812586870767466\n",
      "Epoch 95/100\n",
      "Training Loss: 0.07071465753706377\n",
      "Validation Loss: 0.0385447636847741\n",
      "Epoch 96/100\n",
      "Training Loss: 0.06747017897931236\n",
      "Validation Loss: 0.03850084479665391\n",
      "Epoch 97/100\n",
      "Training Loss: 0.08696139510213893\n",
      "Validation Loss: 0.08175140426220348\n",
      "Epoch 98/100\n",
      "Training Loss: 0.07854539038398554\n",
      "Validation Loss: 0.036371683506847635\n",
      "Epoch 99/100\n",
      "Training Loss: 0.0670296611023748\n",
      "Validation Loss: 0.033131596836764254\n",
      "Epoch 100/100\n",
      "Training Loss: 0.0783437188566871\n",
      "Validation Loss: 0.033740141630170896\n",
      "    Trial 2/2 for combination 9/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.25420519630387994\n",
      "Validation Loss: 0.15795159191146732\n",
      "Epoch 2/100\n",
      "Training Loss: 0.2338407639220176\n",
      "Validation Loss: 0.05854775642267883\n",
      "Epoch 3/100\n",
      "Training Loss: 0.24249810371113162\n",
      "Validation Loss: 0.0624975051242029\n",
      "Epoch 4/100\n",
      "Training Loss: 0.15738429806716273\n",
      "Validation Loss: 0.1860031279468836\n",
      "Epoch 5/100\n",
      "Training Loss: 0.1358158311531344\n",
      "Validation Loss: 0.15858061131833173\n",
      "Epoch 6/100\n",
      "Training Loss: 0.156214301426004\n",
      "Validation Loss: 0.08463490311706563\n",
      "Epoch 7/100\n",
      "Training Loss: 0.14115515296267941\n",
      "Validation Loss: 0.0906311040976004\n",
      "Epoch 8/100\n",
      "Training Loss: 0.10052516890970985\n",
      "Validation Loss: 0.054981470057276285\n",
      "Epoch 9/100\n",
      "Training Loss: 0.12507741873170367\n",
      "Validation Loss: 0.04382535888743156\n",
      "Epoch 10/100\n",
      "Training Loss: 0.09496511619975553\n",
      "Validation Loss: 0.05054930932917653\n",
      "Epoch 11/100\n",
      "Training Loss: 0.11076747069398232\n",
      "Validation Loss: 0.10446860898004062\n",
      "Epoch 12/100\n",
      "Training Loss: 0.09423782364814294\n",
      "Validation Loss: 0.07113035258543057\n",
      "Epoch 13/100\n",
      "Training Loss: 0.0898290189094106\n",
      "Validation Loss: 0.06883864902554003\n",
      "Epoch 14/100\n",
      "Training Loss: 0.10563459096973095\n",
      "Validation Loss: 0.07503476750875057\n",
      "Epoch 15/100\n",
      "Training Loss: 0.0866161073130032\n",
      "Validation Loss: 0.0684489055497296\n",
      "Epoch 16/100\n",
      "Training Loss: 0.08480400616516215\n",
      "Validation Loss: 0.04172527226281399\n",
      "Epoch 17/100\n",
      "Training Loss: 0.0775710302755536\n",
      "Validation Loss: 0.08628663213063806\n",
      "Epoch 18/100\n",
      "Training Loss: 0.07375829834233738\n",
      "Validation Loss: 0.06273787207033885\n",
      "Epoch 19/100\n",
      "Training Loss: 0.07866572348792393\n",
      "Validation Loss: 0.044697744761969097\n",
      "Epoch 20/100\n",
      "Training Loss: 0.08351824720267857\n",
      "Validation Loss: 0.0400559846572745\n",
      "Epoch 21/100\n",
      "Training Loss: 0.07240619826152206\n",
      "Validation Loss: 0.051457863359723376\n",
      "Epoch 22/100\n",
      "Training Loss: 0.07171007797677839\n",
      "Validation Loss: 0.058029600234197155\n",
      "Epoch 23/100\n",
      "Training Loss: 0.07350621328623558\n",
      "Validation Loss: 0.07794942477466243\n",
      "Epoch 24/100\n",
      "Training Loss: 0.07924418576623442\n",
      "Validation Loss: 0.07343258311023969\n",
      "Epoch 25/100\n",
      "Training Loss: 0.07466760086253761\n",
      "Validation Loss: 0.04751171053947776\n",
      "Epoch 26/100\n",
      "Training Loss: 0.0749316779249949\n",
      "Validation Loss: 0.062242598557447106\n",
      "Epoch 27/100\n",
      "Training Loss: 0.07073517434891981\n",
      "Validation Loss: 0.04401163730873959\n",
      "Epoch 28/100\n",
      "Training Loss: 0.0717517347903907\n",
      "Validation Loss: 0.03695882958429915\n",
      "Epoch 29/100\n",
      "Training Loss: 0.07603404823256642\n",
      "Validation Loss: 0.05736375179996184\n",
      "Epoch 30/100\n",
      "Training Loss: 0.07414953207528738\n",
      "Validation Loss: 0.05786419016450959\n",
      "Epoch 31/100\n",
      "Training Loss: 0.06623384367747304\n",
      "Validation Loss: 0.04091944218663645\n",
      "Epoch 32/100\n",
      "Training Loss: 0.06838669934697889\n",
      "Validation Loss: 0.05193035056253189\n",
      "Epoch 33/100\n",
      "Training Loss: 0.07536739342792376\n",
      "Validation Loss: 0.0301213376928119\n",
      "Epoch 34/100\n",
      "Training Loss: 0.06937048190950239\n",
      "Validation Loss: 0.05653286951813609\n",
      "Epoch 35/100\n",
      "Training Loss: 0.08316955659569224\n",
      "Validation Loss: 0.031259304416653665\n",
      "Epoch 36/100\n",
      "Training Loss: 0.06612615388352171\n",
      "Validation Loss: 0.05326318396537507\n",
      "Epoch 37/100\n",
      "Training Loss: 0.08560633177764998\n",
      "Validation Loss: 0.03830826026448831\n",
      "Epoch 38/100\n",
      "Training Loss: 0.06314998989729065\n",
      "Validation Loss: 0.05932706786690778\n",
      "Epoch 39/100\n",
      "Training Loss: 0.07480159845300115\n",
      "Validation Loss: 0.04469358817490379\n",
      "Epoch 40/100\n",
      "Training Loss: 0.06680320717850023\n",
      "Validation Loss: 0.04259740561849441\n",
      "Epoch 41/100\n",
      "Training Loss: 0.0740667397680414\n",
      "Validation Loss: 0.05083833337700161\n",
      "Epoch 42/100\n",
      "Training Loss: 0.07064759803902455\n",
      "Validation Loss: 0.06303074019243214\n",
      "Epoch 43/100\n",
      "Training Loss: 0.06553896234479675\n",
      "Validation Loss: 0.05199419060804224\n",
      "Epoch 44/100\n",
      "Training Loss: 0.06768527192411498\n",
      "Validation Loss: 0.05481811977195504\n",
      "Epoch 45/100\n",
      "Training Loss: 0.0746800300737735\n",
      "Validation Loss: 0.0903696773131355\n",
      "Epoch 46/100\n",
      "Training Loss: 0.07211598767893208\n",
      "Validation Loss: 0.031261764735324146\n",
      "Epoch 47/100\n",
      "Training Loss: 0.07742112870512073\n",
      "Validation Loss: 0.05981319299446264\n",
      "Epoch 48/100\n",
      "Training Loss: 0.06415075014897968\n",
      "Validation Loss: 0.06643687333957102\n",
      "Epoch 49/100\n",
      "Training Loss: 0.07328249583023487\n",
      "Validation Loss: 0.06184947202757916\n",
      "Epoch 50/100\n",
      "Training Loss: 0.06486375486172039\n",
      "Validation Loss: 0.06405823336877156\n",
      "Epoch 51/100\n",
      "Training Loss: 0.08037513187544226\n",
      "Validation Loss: 0.07266057261900064\n",
      "Epoch 52/100\n",
      "Training Loss: 0.07422361039002101\n",
      "Validation Loss: 0.04199446826293264\n",
      "Epoch 53/100\n",
      "Training Loss: 0.07693982758910707\n",
      "Validation Loss: 0.026577168945325032\n",
      "Epoch 54/100\n",
      "Training Loss: 0.07090135618585697\n",
      "Validation Loss: 0.051674517797126576\n",
      "Epoch 55/100\n",
      "Training Loss: 0.07519061132305027\n",
      "Validation Loss: 0.03207364917115775\n",
      "Epoch 56/100\n",
      "Training Loss: 0.059913729511517695\n",
      "Validation Loss: 0.029542512973802317\n",
      "Epoch 57/100\n",
      "Training Loss: 0.0760052613382568\n",
      "Validation Loss: 0.02286670785173351\n",
      "Epoch 58/100\n",
      "Training Loss: 0.06229260738766614\n",
      "Validation Loss: 0.055978551251307664\n",
      "Epoch 59/100\n",
      "Training Loss: 0.07989693514028133\n",
      "Validation Loss: 0.02471332314083302\n",
      "Epoch 60/100\n",
      "Training Loss: 0.08410258140478323\n",
      "Validation Loss: 0.04147479018802129\n",
      "Epoch 61/100\n",
      "Training Loss: 0.0709520255347652\n",
      "Validation Loss: 0.02437327808873833\n",
      "Epoch 62/100\n",
      "Training Loss: 0.06872427283532563\n",
      "Validation Loss: 0.029867347768203788\n",
      "Epoch 63/100\n",
      "Training Loss: 0.0793787268236914\n",
      "Validation Loss: 0.04660387932156079\n",
      "Epoch 64/100\n",
      "Training Loss: 0.07051311623298764\n",
      "Validation Loss: 0.04073100414695992\n",
      "Epoch 65/100\n",
      "Training Loss: 0.08094133965856469\n",
      "Validation Loss: 0.050387547097883865\n",
      "Epoch 66/100\n",
      "Training Loss: 0.07406391083323045\n",
      "Validation Loss: 0.0479684621364444\n",
      "Epoch 67/100\n",
      "Training Loss: 0.07535044013535104\n",
      "Validation Loss: 0.07046675883423817\n",
      "Epoch 68/100\n",
      "Training Loss: 0.06865011386680694\n",
      "Validation Loss: 0.03013221040680229\n",
      "Epoch 69/100\n",
      "Training Loss: 0.07928605937254728\n",
      "Validation Loss: 0.05460930241699885\n",
      "Epoch 70/100\n",
      "Training Loss: 0.06558465944943999\n",
      "Validation Loss: 0.05615744778407942\n",
      "Epoch 71/100\n",
      "Training Loss: 0.06750461162922987\n",
      "Validation Loss: 0.07977518152868006\n",
      "Epoch 72/100\n",
      "Training Loss: 0.07163161843080405\n",
      "Validation Loss: 0.0612236373809615\n",
      "Epoch 73/100\n",
      "Training Loss: 0.07588107839004671\n",
      "Validation Loss: 0.06232707367618132\n",
      "Epoch 74/100\n",
      "Training Loss: 0.0714823342953427\n",
      "Validation Loss: 0.09213872862464141\n",
      "Epoch 75/100\n",
      "Training Loss: 0.07378890846307218\n",
      "Validation Loss: 0.05950203647911802\n",
      "Epoch 76/100\n",
      "Training Loss: 0.0719743719729846\n",
      "Validation Loss: 0.027617809983677643\n",
      "Epoch 77/100\n",
      "Training Loss: 0.08053228847014596\n",
      "Validation Loss: 0.038004621427834694\n",
      "Epoch 78/100\n",
      "Training Loss: 0.0918815494129913\n",
      "Validation Loss: 0.06553066186030357\n",
      "Epoch 79/100\n",
      "Training Loss: 0.07742484843628367\n",
      "Validation Loss: 0.04519681264405626\n",
      "Epoch 80/100\n",
      "Training Loss: 0.09182144800637002\n",
      "Validation Loss: 0.04084579001710557\n",
      "Epoch 81/100\n",
      "Training Loss: 0.07998843765092996\n",
      "Validation Loss: 0.04090967170510902\n",
      "Epoch 82/100\n",
      "Training Loss: 0.09346273937707412\n",
      "Validation Loss: 0.0633281941271566\n",
      "Epoch 83/100\n",
      "Training Loss: 0.0746079052779919\n",
      "Validation Loss: 0.059788019205831267\n",
      "Epoch 84/100\n",
      "Training Loss: 0.08680163435965349\n",
      "Validation Loss: 0.023856265894980862\n",
      "Epoch 85/100\n",
      "Training Loss: 0.09745306734550918\n",
      "Validation Loss: 0.038939740500024626\n",
      "Epoch 86/100\n",
      "Training Loss: 0.07786829856488327\n",
      "Validation Loss: 0.03357299851135732\n",
      "Epoch 87/100\n",
      "Training Loss: 0.08522847223625896\n",
      "Validation Loss: 0.04551940644084188\n",
      "Epoch 88/100\n",
      "Training Loss: 0.07882433199233527\n",
      "Validation Loss: 0.04040804923615035\n",
      "Epoch 89/100\n",
      "Training Loss: 0.0680918522662626\n",
      "Validation Loss: 0.04457959345589595\n",
      "Epoch 90/100\n",
      "Training Loss: 0.09307994698592542\n",
      "Validation Loss: 0.04741030816413829\n",
      "Epoch 91/100\n",
      "Training Loss: 0.08235665614303579\n",
      "Validation Loss: 0.04181776658597757\n",
      "Epoch 92/100\n",
      "Training Loss: 0.06425285442257858\n",
      "Validation Loss: 0.050019126208307875\n",
      "Epoch 93/100\n",
      "Training Loss: 0.09602566767688621\n",
      "Validation Loss: 0.04537581229954912\n",
      "Epoch 94/100\n",
      "Training Loss: 0.07895218621303582\n",
      "Validation Loss: 0.035904536317989254\n",
      "Epoch 95/100\n",
      "Training Loss: 0.07395512307507146\n",
      "Validation Loss: 0.04336111406863762\n",
      "Epoch 96/100\n",
      "Training Loss: 0.06753161306189095\n",
      "Validation Loss: 0.06694023066328428\n",
      "Epoch 97/100\n",
      "Training Loss: 0.08536440128185692\n",
      "Validation Loss: 0.06113714759523271\n",
      "Epoch 98/100\n",
      "Training Loss: 0.09265013881073665\n",
      "Validation Loss: 0.07408796560637157\n",
      "Epoch 99/100\n",
      "Training Loss: 0.07911102903400656\n",
      "Validation Loss: 0.0659222844184269\n",
      "Epoch 100/100\n",
      "Training Loss: 0.08664839061823167\n",
      "Validation Loss: 0.0719921770778949\n",
      "Combination 9: Avg Training Loss = 0.08502566378218827, Avg Validation Loss = 0.056515290404546684\n",
      "Testing combination 10/48: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.001, 'sequence_length': 20, 'dropout_rate': 0.2}\n",
      "Truncated Train Data Shape: (70, 20, 1)\n",
      "Truncated Validation Data Shape: (10, 20, 1)\n",
      "    Trial 1/2 for combination 10/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.3535959173915126\n",
      "Validation Loss: 0.09025850485768243\n",
      "Epoch 2/100\n",
      "Training Loss: 0.20527689838088103\n",
      "Validation Loss: 0.0736393138484076\n",
      "Epoch 3/100\n",
      "Training Loss: 0.20764085747201375\n",
      "Validation Loss: 0.19521826711720686\n",
      "Epoch 4/100\n",
      "Training Loss: 0.18590589881024946\n",
      "Validation Loss: 0.10735758026625405\n",
      "Epoch 5/100\n",
      "Training Loss: 0.16180700414588978\n",
      "Validation Loss: 0.14156329716283247\n",
      "Epoch 6/100\n",
      "Training Loss: 0.13275761260252197\n",
      "Validation Loss: 0.0667619630830004\n",
      "Epoch 7/100\n",
      "Training Loss: 0.10493346560578462\n",
      "Validation Loss: 0.05652743384900642\n",
      "Epoch 8/100\n",
      "Training Loss: 0.11569476826077076\n",
      "Validation Loss: 0.061727403449037044\n",
      "Epoch 9/100\n",
      "Training Loss: 0.1160971756768613\n",
      "Validation Loss: 0.06423765642871382\n",
      "Epoch 10/100\n",
      "Training Loss: 0.11588080830270983\n",
      "Validation Loss: 0.09745142954651279\n",
      "Epoch 11/100\n",
      "Training Loss: 0.1092933794911496\n",
      "Validation Loss: 0.09192099200857899\n",
      "Epoch 12/100\n",
      "Training Loss: 0.09510765930057698\n",
      "Validation Loss: 0.06362739923215482\n",
      "Epoch 13/100\n",
      "Training Loss: 0.09311786184916249\n",
      "Validation Loss: 0.055385801108761804\n",
      "Epoch 14/100\n",
      "Training Loss: 0.08486350552537601\n",
      "Validation Loss: 0.06551318984707886\n",
      "Epoch 15/100\n",
      "Training Loss: 0.08669172621776557\n",
      "Validation Loss: 0.04133235643147053\n",
      "Epoch 16/100\n",
      "Training Loss: 0.08307274208561627\n",
      "Validation Loss: 0.04538086599486827\n",
      "Epoch 17/100\n",
      "Training Loss: 0.07772920318070924\n",
      "Validation Loss: 0.036423350026968435\n",
      "Epoch 18/100\n",
      "Training Loss: 0.08414691360871338\n",
      "Validation Loss: 0.07352678883960903\n",
      "Epoch 19/100\n",
      "Training Loss: 0.07848122565835641\n",
      "Validation Loss: 0.038631166018870844\n",
      "Epoch 20/100\n",
      "Training Loss: 0.07202744753204723\n",
      "Validation Loss: 0.0407022018418942\n",
      "Epoch 21/100\n",
      "Training Loss: 0.07067518029211617\n",
      "Validation Loss: 0.05085454581509889\n",
      "Epoch 22/100\n",
      "Training Loss: 0.07154174810406215\n",
      "Validation Loss: 0.04813520650011379\n",
      "Epoch 23/100\n",
      "Training Loss: 0.07070144959656492\n",
      "Validation Loss: 0.05166134297293943\n",
      "Epoch 24/100\n",
      "Training Loss: 0.0725226087571587\n",
      "Validation Loss: 0.05786049323987295\n",
      "Epoch 25/100\n",
      "Training Loss: 0.06570325148331325\n",
      "Validation Loss: 0.052172083085850626\n",
      "Epoch 26/100\n",
      "Training Loss: 0.06821196820204231\n",
      "Validation Loss: 0.04589568194606034\n",
      "Epoch 27/100\n",
      "Training Loss: 0.06107293457138657\n",
      "Validation Loss: 0.03375817737887045\n",
      "Epoch 28/100\n",
      "Training Loss: 0.07467862950276817\n",
      "Validation Loss: 0.06586113398874743\n",
      "Epoch 29/100\n",
      "Training Loss: 0.07065729254150871\n",
      "Validation Loss: 0.06287972520994863\n",
      "Epoch 30/100\n",
      "Training Loss: 0.07198932284560096\n",
      "Validation Loss: 0.07619422535045121\n",
      "Epoch 31/100\n",
      "Training Loss: 0.07247337260290108\n",
      "Validation Loss: 0.03977814829609862\n",
      "Epoch 32/100\n",
      "Training Loss: 0.06996116016825672\n",
      "Validation Loss: 0.06437459934129512\n",
      "Epoch 33/100\n",
      "Training Loss: 0.06235154347230177\n",
      "Validation Loss: 0.06898562863870147\n",
      "Epoch 34/100\n",
      "Training Loss: 0.08017592610214283\n",
      "Validation Loss: 0.02844034769635147\n",
      "Epoch 35/100\n",
      "Training Loss: 0.06851734979256453\n",
      "Validation Loss: 0.044391929528748195\n",
      "Epoch 36/100\n",
      "Training Loss: 0.07738552351841348\n",
      "Validation Loss: 0.0838923227749596\n",
      "Epoch 37/100\n",
      "Training Loss: 0.06284891245728512\n",
      "Validation Loss: 0.07997576353573246\n",
      "Epoch 38/100\n",
      "Training Loss: 0.07191272812958904\n",
      "Validation Loss: 0.05685180616669414\n",
      "Epoch 39/100\n",
      "Training Loss: 0.06076410281628315\n",
      "Validation Loss: 0.032248636974724555\n",
      "Epoch 40/100\n",
      "Training Loss: 0.06139385294644335\n",
      "Validation Loss: 0.0249878769857901\n",
      "Epoch 41/100\n",
      "Training Loss: 0.055605928178538544\n",
      "Validation Loss: 0.05989937295208185\n",
      "Epoch 42/100\n",
      "Training Loss: 0.0724715953152629\n",
      "Validation Loss: 0.05821553183175469\n",
      "Epoch 43/100\n",
      "Training Loss: 0.06357162659579925\n",
      "Validation Loss: 0.06996451331645374\n",
      "Epoch 44/100\n",
      "Training Loss: 0.07823792861868108\n",
      "Validation Loss: 0.04858152269373568\n",
      "Epoch 45/100\n",
      "Training Loss: 0.07426978668795714\n",
      "Validation Loss: 0.043453315547932976\n",
      "Epoch 46/100\n",
      "Training Loss: 0.07006571547648144\n",
      "Validation Loss: 0.06391344763286522\n",
      "Epoch 47/100\n",
      "Training Loss: 0.08266461287363752\n",
      "Validation Loss: 0.027043598387617628\n",
      "Epoch 48/100\n",
      "Training Loss: 0.062191811144851186\n",
      "Validation Loss: 0.06643429908870022\n",
      "Epoch 49/100\n",
      "Training Loss: 0.07762442309147045\n",
      "Validation Loss: 0.03309378202106872\n",
      "Epoch 50/100\n",
      "Training Loss: 0.0735335359284885\n",
      "Validation Loss: 0.05224166308458263\n",
      "Epoch 51/100\n",
      "Training Loss: 0.0730989110253888\n",
      "Validation Loss: 0.04476448181459698\n",
      "Epoch 52/100\n",
      "Training Loss: 0.0641841675935545\n",
      "Validation Loss: 0.0307706274017109\n",
      "Epoch 53/100\n",
      "Training Loss: 0.06972459333577022\n",
      "Validation Loss: 0.04101723244310855\n",
      "Epoch 54/100\n",
      "Training Loss: 0.07594584929699856\n",
      "Validation Loss: 0.02667113913235241\n",
      "Epoch 55/100\n",
      "Training Loss: 0.06886992248382975\n",
      "Validation Loss: 0.043762499637409344\n",
      "Epoch 56/100\n",
      "Training Loss: 0.08050659304129534\n",
      "Validation Loss: 0.03806613103156736\n",
      "Epoch 57/100\n",
      "Training Loss: 0.07282137038351195\n",
      "Validation Loss: 0.048537247948593945\n",
      "Epoch 58/100\n",
      "Training Loss: 0.07469820684020602\n",
      "Validation Loss: 0.06440560707532093\n",
      "Epoch 59/100\n",
      "Training Loss: 0.0636381392961462\n",
      "Validation Loss: 0.02676331061060176\n",
      "Epoch 60/100\n",
      "Training Loss: 0.07407449960100859\n",
      "Validation Loss: 0.0708518302110916\n",
      "Epoch 61/100\n",
      "Training Loss: 0.07735238239083549\n",
      "Validation Loss: 0.06429959466058344\n",
      "Epoch 62/100\n",
      "Training Loss: 0.06512050807028281\n",
      "Validation Loss: 0.1108619684003942\n",
      "Epoch 63/100\n",
      "Training Loss: 0.0769325382959214\n",
      "Validation Loss: 0.03378334926944375\n",
      "Epoch 64/100\n",
      "Training Loss: 0.06975167314563185\n",
      "Validation Loss: 0.11816027083217237\n",
      "Epoch 65/100\n",
      "Training Loss: 0.0654241831649874\n",
      "Validation Loss: 0.05132397951304708\n",
      "Epoch 66/100\n",
      "Training Loss: 0.07832606268731501\n",
      "Validation Loss: 0.04051171895711479\n",
      "Epoch 67/100\n",
      "Training Loss: 0.07318013057469185\n",
      "Validation Loss: 0.0921397435230404\n",
      "Epoch 68/100\n",
      "Training Loss: 0.07982972048853207\n",
      "Validation Loss: 0.051861381404295345\n",
      "Epoch 69/100\n",
      "Training Loss: 0.06724253973922727\n",
      "Validation Loss: 0.05762668996484703\n",
      "Epoch 70/100\n",
      "Training Loss: 0.06877422382912204\n",
      "Validation Loss: 0.1447013781776681\n",
      "Epoch 71/100\n",
      "Training Loss: 0.077290500037628\n",
      "Validation Loss: 0.06632544933447111\n",
      "Epoch 72/100\n",
      "Training Loss: 0.0667916283199467\n",
      "Validation Loss: 0.041420377166703395\n",
      "Epoch 73/100\n",
      "Training Loss: 0.07482378559365628\n",
      "Validation Loss: 0.024961400608343072\n",
      "Epoch 74/100\n",
      "Training Loss: 0.08711563827311836\n",
      "Validation Loss: 0.06285756416248235\n",
      "Epoch 75/100\n",
      "Training Loss: 0.07064429034107934\n",
      "Validation Loss: 0.04240830363722438\n",
      "Epoch 76/100\n",
      "Training Loss: 0.0720726022943858\n",
      "Validation Loss: 0.0478855962879727\n",
      "Epoch 77/100\n",
      "Training Loss: 0.07189227430923394\n",
      "Validation Loss: 0.03529085961942926\n",
      "Epoch 78/100\n",
      "Training Loss: 0.07889574063592304\n",
      "Validation Loss: 0.0540452575245981\n",
      "Epoch 79/100\n",
      "Training Loss: 0.06808239622400501\n",
      "Validation Loss: 0.04252269568636515\n",
      "Epoch 80/100\n",
      "Training Loss: 0.06429858012651007\n",
      "Validation Loss: 0.05254241487979729\n",
      "Epoch 81/100\n",
      "Training Loss: 0.08050080466300494\n",
      "Validation Loss: 0.05587156740382816\n",
      "Epoch 82/100\n",
      "Training Loss: 0.08859947274928884\n",
      "Validation Loss: 0.05185991302864708\n",
      "Epoch 83/100\n",
      "Training Loss: 0.09088735098134368\n",
      "Validation Loss: 0.03306557376971945\n",
      "Epoch 84/100\n",
      "Training Loss: 0.09512250986756493\n",
      "Validation Loss: 0.05017756708279982\n",
      "Epoch 85/100\n",
      "Training Loss: 0.08055422497719861\n",
      "Validation Loss: 0.03607269086470316\n",
      "Epoch 86/100\n",
      "Training Loss: 0.064502609079349\n",
      "Validation Loss: 0.07980507448612453\n",
      "Epoch 87/100\n",
      "Training Loss: 0.07133141492180223\n",
      "Validation Loss: 0.10116380952078022\n",
      "Epoch 88/100\n",
      "Training Loss: 0.07197179592933838\n",
      "Validation Loss: 0.1125485125340286\n",
      "Epoch 89/100\n",
      "Training Loss: 0.07204827285629035\n",
      "Validation Loss: 0.0483511097198286\n",
      "Epoch 90/100\n",
      "Training Loss: 0.06885978593910742\n",
      "Validation Loss: 0.0690196577197758\n",
      "Epoch 91/100\n",
      "Training Loss: 0.08303661128643862\n",
      "Validation Loss: 0.0438187556140984\n",
      "Epoch 92/100\n",
      "Training Loss: 0.06784886512489578\n",
      "Validation Loss: 0.035794004234059304\n",
      "Epoch 93/100\n",
      "Training Loss: 0.06817772501584958\n",
      "Validation Loss: 0.028785557439121486\n",
      "Epoch 94/100\n",
      "Training Loss: 0.08320227071698584\n",
      "Validation Loss: 0.06855174783735671\n",
      "Epoch 95/100\n",
      "Training Loss: 0.06206969034699316\n",
      "Validation Loss: 0.06065223047611068\n",
      "Epoch 96/100\n",
      "Training Loss: 0.07451323384456666\n",
      "Validation Loss: 0.03019198200001167\n",
      "Epoch 97/100\n",
      "Training Loss: 0.08208321635982463\n",
      "Validation Loss: 0.049752870411416614\n",
      "Epoch 98/100\n",
      "Training Loss: 0.10078271444774356\n",
      "Validation Loss: 0.03001020576168411\n",
      "Epoch 99/100\n",
      "Training Loss: 0.0812326852669589\n",
      "Validation Loss: 0.051104619874251925\n",
      "Epoch 100/100\n",
      "Training Loss: 0.06986434777867029\n",
      "Validation Loss: 0.06819749353022893\n",
      "    Trial 2/2 for combination 10/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.38803144004705076\n",
      "Validation Loss: 0.24265469272640075\n",
      "Epoch 2/100\n",
      "Training Loss: 0.2815522879367683\n",
      "Validation Loss: 0.15645854902807046\n",
      "Epoch 3/100\n",
      "Training Loss: 0.16395998181517218\n",
      "Validation Loss: 0.21004777988574114\n",
      "Epoch 4/100\n",
      "Training Loss: 0.17546003545100672\n",
      "Validation Loss: 0.09793902273750392\n",
      "Epoch 5/100\n",
      "Training Loss: 0.1764036489564531\n",
      "Validation Loss: 0.13328779200869492\n",
      "Epoch 6/100\n",
      "Training Loss: 0.15743526563121682\n",
      "Validation Loss: 0.11290319700489988\n",
      "Epoch 7/100\n",
      "Training Loss: 0.1309719958201936\n",
      "Validation Loss: 0.04421986448062809\n",
      "Epoch 8/100\n",
      "Training Loss: 0.11810127948652173\n",
      "Validation Loss: 0.0716359749281254\n",
      "Epoch 9/100\n",
      "Training Loss: 0.1257864057491456\n",
      "Validation Loss: 0.04040542077959134\n",
      "Epoch 10/100\n",
      "Training Loss: 0.1118477168270575\n",
      "Validation Loss: 0.05958658085719824\n",
      "Epoch 11/100\n",
      "Training Loss: 0.101050875888822\n",
      "Validation Loss: 0.04769984664435527\n",
      "Epoch 12/100\n",
      "Training Loss: 0.09965223860268987\n",
      "Validation Loss: 0.04766995480902831\n",
      "Epoch 13/100\n",
      "Training Loss: 0.10084378017065816\n",
      "Validation Loss: 0.06326479927354492\n",
      "Epoch 14/100\n",
      "Training Loss: 0.08900907247640942\n",
      "Validation Loss: 0.04473757366948743\n",
      "Epoch 15/100\n",
      "Training Loss: 0.08771208887510382\n",
      "Validation Loss: 0.10304231353350177\n",
      "Epoch 16/100\n",
      "Training Loss: 0.08447447015893746\n",
      "Validation Loss: 0.03420301634011608\n",
      "Epoch 17/100\n",
      "Training Loss: 0.07172396415615408\n",
      "Validation Loss: 0.060174657643769204\n",
      "Epoch 18/100\n",
      "Training Loss: 0.07733804376383203\n",
      "Validation Loss: 0.06531125121150602\n",
      "Epoch 19/100\n",
      "Training Loss: 0.07126189909177837\n",
      "Validation Loss: 0.038313876369112\n",
      "Epoch 20/100\n",
      "Training Loss: 0.06734621206393836\n",
      "Validation Loss: 0.05811934158035392\n",
      "Epoch 21/100\n",
      "Training Loss: 0.0666680413262066\n",
      "Validation Loss: 0.04561769407859566\n",
      "Epoch 22/100\n",
      "Training Loss: 0.07111314419620378\n",
      "Validation Loss: 0.05329488140398671\n",
      "Epoch 23/100\n",
      "Training Loss: 0.06585803276506776\n",
      "Validation Loss: 0.03531118994441948\n",
      "Epoch 24/100\n",
      "Training Loss: 0.06749674428507726\n",
      "Validation Loss: 0.06329664834626643\n",
      "Epoch 25/100\n",
      "Training Loss: 0.07144358511105373\n",
      "Validation Loss: 0.04345465327846034\n",
      "Epoch 26/100\n",
      "Training Loss: 0.0689353205411056\n",
      "Validation Loss: 0.046723371029526596\n",
      "Epoch 27/100\n",
      "Training Loss: 0.07075749360705379\n",
      "Validation Loss: 0.043655107752144\n",
      "Epoch 28/100\n",
      "Training Loss: 0.07043722326677027\n",
      "Validation Loss: 0.0456030730373004\n",
      "Epoch 29/100\n",
      "Training Loss: 0.05879739141586635\n",
      "Validation Loss: 0.03675326497046494\n",
      "Epoch 30/100\n",
      "Training Loss: 0.07208498374202929\n",
      "Validation Loss: 0.07114695898432172\n",
      "Epoch 31/100\n",
      "Training Loss: 0.06730185195480728\n",
      "Validation Loss: 0.03945937127266691\n",
      "Epoch 32/100\n",
      "Training Loss: 0.0681443276869526\n",
      "Validation Loss: 0.032289951710671\n",
      "Epoch 33/100\n",
      "Training Loss: 0.06312302630057112\n",
      "Validation Loss: 0.056131992199711246\n",
      "Epoch 34/100\n",
      "Training Loss: 0.06655309647901014\n",
      "Validation Loss: 0.06020196123098891\n",
      "Epoch 35/100\n",
      "Training Loss: 0.06216927565392051\n",
      "Validation Loss: 0.07016348115321538\n",
      "Epoch 36/100\n",
      "Training Loss: 0.06797919855334172\n",
      "Validation Loss: 0.0476772967212426\n",
      "Epoch 37/100\n",
      "Training Loss: 0.07123750176855882\n",
      "Validation Loss: 0.03644421126738211\n",
      "Epoch 38/100\n",
      "Training Loss: 0.07140993519184491\n",
      "Validation Loss: 0.06322269410816071\n",
      "Epoch 39/100\n",
      "Training Loss: 0.061989844559346315\n",
      "Validation Loss: 0.048194728910254656\n",
      "Epoch 40/100\n",
      "Training Loss: 0.07855995426613425\n",
      "Validation Loss: 0.04512872138327011\n",
      "Epoch 41/100\n",
      "Training Loss: 0.06857876183191813\n",
      "Validation Loss: 0.07838083000998708\n",
      "Epoch 42/100\n",
      "Training Loss: 0.06993853528917021\n",
      "Validation Loss: 0.04644883374876156\n",
      "Epoch 43/100\n",
      "Training Loss: 0.07708640393804023\n",
      "Validation Loss: 0.09618917334347765\n",
      "Epoch 44/100\n",
      "Training Loss: 0.08041665553850116\n",
      "Validation Loss: 0.03804739832329755\n",
      "Epoch 45/100\n",
      "Training Loss: 0.06934967585129342\n",
      "Validation Loss: 0.05326018298783265\n",
      "Epoch 46/100\n",
      "Training Loss: 0.08344965278831369\n",
      "Validation Loss: 0.038451328366350776\n",
      "Epoch 47/100\n",
      "Training Loss: 0.06498853356225112\n",
      "Validation Loss: 0.0757525301282126\n",
      "Epoch 48/100\n",
      "Training Loss: 0.06725627495496095\n",
      "Validation Loss: 0.07709979467030068\n",
      "Epoch 49/100\n",
      "Training Loss: 0.06729225462970094\n",
      "Validation Loss: 0.051011051259039555\n",
      "Epoch 50/100\n",
      "Training Loss: 0.06779059051342132\n",
      "Validation Loss: 0.06654369501905875\n",
      "Epoch 51/100\n",
      "Training Loss: 0.06633447235280744\n",
      "Validation Loss: 0.044885682084094644\n",
      "Epoch 52/100\n",
      "Training Loss: 0.0794203004602183\n",
      "Validation Loss: 0.06610699489501227\n",
      "Epoch 53/100\n",
      "Training Loss: 0.058887868456658256\n",
      "Validation Loss: 0.03846067382643497\n",
      "Epoch 54/100\n",
      "Training Loss: 0.077055161610319\n",
      "Validation Loss: 0.048713709941336024\n",
      "Epoch 55/100\n",
      "Training Loss: 0.05901359403957398\n",
      "Validation Loss: 0.0234416708045816\n",
      "Epoch 56/100\n",
      "Training Loss: 0.07573048296101546\n",
      "Validation Loss: 0.04990126991981501\n",
      "Epoch 57/100\n",
      "Training Loss: 0.07596064876053862\n",
      "Validation Loss: 0.09770361696164802\n",
      "Epoch 58/100\n",
      "Training Loss: 0.06904069155520787\n",
      "Validation Loss: 0.04133052648323249\n",
      "Epoch 59/100\n",
      "Training Loss: 0.06958880043439118\n",
      "Validation Loss: 0.02724614436996764\n",
      "Epoch 60/100\n",
      "Training Loss: 0.06432063106427147\n",
      "Validation Loss: 0.06174154510142549\n",
      "Epoch 61/100\n",
      "Training Loss: 0.07085763383569305\n",
      "Validation Loss: 0.04453333734272484\n",
      "Epoch 62/100\n",
      "Training Loss: 0.07336512846694097\n",
      "Validation Loss: 0.05191006116111763\n",
      "Epoch 63/100\n",
      "Training Loss: 0.07053261772598805\n",
      "Validation Loss: 0.03870551144882582\n",
      "Epoch 64/100\n",
      "Training Loss: 0.06945510576748644\n",
      "Validation Loss: 0.05523127983735711\n",
      "Epoch 65/100\n",
      "Training Loss: 0.07026798978287183\n",
      "Validation Loss: 0.021395741647524745\n",
      "Epoch 66/100\n",
      "Training Loss: 0.09023100797234429\n",
      "Validation Loss: 0.02627495499044492\n",
      "Epoch 67/100\n",
      "Training Loss: 0.08173744435302831\n",
      "Validation Loss: 0.04521995200666716\n",
      "Epoch 68/100\n",
      "Training Loss: 0.07924278439341426\n",
      "Validation Loss: 0.06742365926345391\n",
      "Epoch 69/100\n",
      "Training Loss: 0.08823489733884499\n",
      "Validation Loss: 0.07138884163989564\n",
      "Epoch 70/100\n",
      "Training Loss: 0.06252525446803148\n",
      "Validation Loss: 0.020188093158817148\n",
      "Epoch 71/100\n",
      "Training Loss: 0.07014553692208825\n",
      "Validation Loss: 0.06126637703662248\n",
      "Epoch 72/100\n",
      "Training Loss: 0.07153331778638801\n",
      "Validation Loss: 0.06515139980766889\n",
      "Epoch 73/100\n",
      "Training Loss: 0.07994632432140318\n",
      "Validation Loss: 0.034307062789085636\n",
      "Epoch 74/100\n",
      "Training Loss: 0.07717066534985537\n",
      "Validation Loss: 0.05943862111850793\n",
      "Epoch 75/100\n",
      "Training Loss: 0.07893287173308228\n",
      "Validation Loss: 0.01811057697294368\n",
      "Epoch 76/100\n",
      "Training Loss: 0.07407081654647471\n",
      "Validation Loss: 0.07422718995459962\n",
      "Epoch 77/100\n",
      "Training Loss: 0.06991186277291508\n",
      "Validation Loss: 0.02826982321534153\n",
      "Epoch 78/100\n",
      "Training Loss: 0.0734588510418847\n",
      "Validation Loss: 0.02984816356989743\n",
      "Epoch 79/100\n",
      "Training Loss: 0.07278924040507115\n",
      "Validation Loss: 0.03994933803152984\n",
      "Epoch 80/100\n",
      "Training Loss: 0.08657723734096409\n",
      "Validation Loss: 0.10511902096235268\n",
      "Epoch 81/100\n",
      "Training Loss: 0.06948129805028853\n",
      "Validation Loss: 0.11954276472807517\n",
      "Epoch 82/100\n",
      "Training Loss: 0.08032127772683864\n",
      "Validation Loss: 0.056827169449058135\n",
      "Epoch 83/100\n",
      "Training Loss: 0.08812589508353559\n",
      "Validation Loss: 0.11188757686486457\n",
      "Epoch 84/100\n",
      "Training Loss: 0.06726471985096003\n",
      "Validation Loss: 0.08124669631568017\n",
      "Epoch 85/100\n",
      "Training Loss: 0.07080640428541111\n",
      "Validation Loss: 0.05395074363117387\n",
      "Epoch 86/100\n",
      "Training Loss: 0.0793289156493012\n",
      "Validation Loss: 0.04151208823595583\n",
      "Epoch 87/100\n",
      "Training Loss: 0.07354500419509806\n",
      "Validation Loss: 0.058132259695758325\n",
      "Epoch 88/100\n",
      "Training Loss: 0.07182896516257925\n",
      "Validation Loss: 0.056883681654261664\n",
      "Epoch 89/100\n",
      "Training Loss: 0.09473731902260277\n",
      "Validation Loss: 0.06660955350547497\n",
      "Epoch 90/100\n",
      "Training Loss: 0.07793565191169798\n",
      "Validation Loss: 0.09009027686221444\n",
      "Epoch 91/100\n",
      "Training Loss: 0.06949183846546293\n",
      "Validation Loss: 0.051091086517816756\n",
      "Epoch 92/100\n",
      "Training Loss: 0.07301834694800283\n",
      "Validation Loss: 0.052743364735208666\n",
      "Epoch 93/100\n",
      "Training Loss: 0.08268115566346429\n",
      "Validation Loss: 0.04784588570784769\n",
      "Epoch 94/100\n",
      "Training Loss: 0.08426573346372583\n",
      "Validation Loss: 0.04004469815764093\n",
      "Epoch 95/100\n",
      "Training Loss: 0.07839332330764726\n",
      "Validation Loss: 0.0540479059663476\n",
      "Epoch 96/100\n",
      "Training Loss: 0.0863862000726882\n",
      "Validation Loss: 0.12782275064835813\n",
      "Epoch 97/100\n",
      "Training Loss: 0.08540011859351708\n",
      "Validation Loss: 0.049977172836664585\n",
      "Epoch 98/100\n",
      "Training Loss: 0.08045650164475089\n",
      "Validation Loss: 0.07231481905508497\n",
      "Epoch 99/100\n",
      "Training Loss: 0.08702016825681598\n",
      "Validation Loss: 0.03588333365155301\n",
      "Epoch 100/100\n",
      "Training Loss: 0.06300205979118534\n",
      "Validation Loss: 0.032245131802073945\n",
      "Combination 10: Avg Training Loss = 0.08458245649104125, Avg Validation Loss = 0.05984992051841356\n",
      "Testing combination 11/48: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.001, 'sequence_length': 50, 'dropout_rate': 0.1}\n",
      "Truncated Train Data Shape: (70, 50, 1)\n",
      "Truncated Validation Data Shape: (10, 50, 1)\n",
      "    Trial 1/2 for combination 11/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.30769651540042464\n",
      "Validation Loss: 0.17302989661391313\n",
      "Epoch 2/100\n",
      "Training Loss: 0.2177038784188538\n",
      "Validation Loss: 0.10110143758442107\n",
      "Epoch 3/100\n",
      "Training Loss: 0.18988012699612694\n",
      "Validation Loss: 0.0796201549682446\n",
      "Epoch 4/100\n",
      "Training Loss: 0.17201480179109585\n",
      "Validation Loss: 0.05526819480735225\n",
      "Epoch 5/100\n",
      "Training Loss: 0.15214144089685577\n",
      "Validation Loss: 0.08795452096575737\n",
      "Epoch 6/100\n",
      "Training Loss: 0.1473842763886527\n",
      "Validation Loss: 0.06543756083276173\n",
      "Epoch 7/100\n",
      "Training Loss: 0.13059486072478202\n",
      "Validation Loss: 0.0706876899157026\n",
      "Epoch 8/100\n",
      "Training Loss: 0.13591249791916082\n",
      "Validation Loss: 0.05078474124521043\n",
      "Epoch 9/100\n",
      "Training Loss: 0.12008390577999464\n",
      "Validation Loss: 0.05026603087674841\n",
      "Epoch 10/100\n",
      "Training Loss: 0.0997174376998962\n",
      "Validation Loss: 0.041762666127498724\n",
      "Epoch 11/100\n",
      "Training Loss: 0.11642096097428598\n",
      "Validation Loss: 0.07465749051944595\n",
      "Epoch 12/100\n",
      "Training Loss: 0.09249418323563481\n",
      "Validation Loss: 0.08109532885932079\n",
      "Epoch 13/100\n",
      "Training Loss: 0.08468956597278073\n",
      "Validation Loss: 0.05155112911446162\n",
      "Epoch 14/100\n",
      "Training Loss: 0.09050267638725315\n",
      "Validation Loss: 0.049807440499758084\n",
      "Epoch 15/100\n",
      "Training Loss: 0.08552314913705887\n",
      "Validation Loss: 0.0544028044694813\n",
      "Epoch 16/100\n",
      "Training Loss: 0.08097113913296486\n",
      "Validation Loss: 0.05595845241212662\n",
      "Epoch 17/100\n",
      "Training Loss: 0.07759820423353876\n",
      "Validation Loss: 0.07203078193762365\n",
      "Epoch 18/100\n",
      "Training Loss: 0.06759329763469933\n",
      "Validation Loss: 0.030466128744277694\n",
      "Epoch 19/100\n",
      "Training Loss: 0.07215177051285467\n",
      "Validation Loss: 0.03604590565620913\n",
      "Epoch 20/100\n",
      "Training Loss: 0.07747503128284738\n",
      "Validation Loss: 0.04574901303958524\n",
      "Epoch 21/100\n",
      "Training Loss: 0.07849607536409632\n",
      "Validation Loss: 0.06148610838840015\n",
      "Epoch 22/100\n",
      "Training Loss: 0.06680424977533371\n",
      "Validation Loss: 0.07957053053978436\n",
      "Epoch 23/100\n",
      "Training Loss: 0.07337526898611574\n",
      "Validation Loss: 0.07050325900721324\n",
      "Epoch 24/100\n",
      "Training Loss: 0.0816535994285328\n",
      "Validation Loss: 0.03314900864183237\n",
      "Epoch 25/100\n",
      "Training Loss: 0.06405654488611641\n",
      "Validation Loss: 0.04358581299110076\n",
      "Epoch 26/100\n",
      "Training Loss: 0.06585087553549827\n",
      "Validation Loss: 0.03266148314588468\n",
      "Epoch 27/100\n",
      "Training Loss: 0.07579062414905799\n",
      "Validation Loss: 0.036090195119933166\n",
      "Epoch 28/100\n",
      "Training Loss: 0.06450304737864258\n",
      "Validation Loss: 0.022645990758093433\n",
      "Epoch 29/100\n",
      "Training Loss: 0.057259162830939646\n",
      "Validation Loss: 0.06289396569613138\n",
      "Epoch 30/100\n",
      "Training Loss: 0.07447204258169174\n",
      "Validation Loss: 0.03308882133810895\n",
      "Epoch 31/100\n",
      "Training Loss: 0.05378009849475667\n",
      "Validation Loss: 0.0565171921891385\n",
      "Epoch 32/100\n",
      "Training Loss: 0.060669094311907125\n",
      "Validation Loss: 0.043069596687346286\n",
      "Epoch 33/100\n",
      "Training Loss: 0.07958230299609738\n",
      "Validation Loss: 0.030997940551661357\n",
      "Epoch 34/100\n",
      "Training Loss: 0.06825537602348611\n",
      "Validation Loss: 0.04769031831855099\n",
      "Epoch 35/100\n",
      "Training Loss: 0.06351281005081073\n",
      "Validation Loss: 0.06646973518046477\n",
      "Epoch 36/100\n",
      "Training Loss: 0.07433369696212852\n",
      "Validation Loss: 0.03613377151196353\n",
      "Epoch 37/100\n",
      "Training Loss: 0.06276714451253201\n",
      "Validation Loss: 0.03916626067554446\n",
      "Epoch 38/100\n",
      "Training Loss: 0.06090599983148838\n",
      "Validation Loss: 0.05475084716016422\n",
      "Epoch 39/100\n",
      "Training Loss: 0.06291536518024175\n",
      "Validation Loss: 0.05327097830769746\n",
      "Epoch 40/100\n",
      "Training Loss: 0.06579005867140224\n",
      "Validation Loss: 0.03904643476725567\n",
      "Epoch 41/100\n",
      "Training Loss: 0.07896313391721946\n",
      "Validation Loss: 0.0340363804044953\n",
      "Epoch 42/100\n",
      "Training Loss: 0.07414319982014336\n",
      "Validation Loss: 0.060471362374388306\n",
      "Epoch 43/100\n",
      "Training Loss: 0.06734694594588228\n",
      "Validation Loss: 0.07341525560563024\n",
      "Epoch 44/100\n",
      "Training Loss: 0.06911097989721292\n",
      "Validation Loss: 0.04890537423535189\n",
      "Epoch 45/100\n",
      "Training Loss: 0.06293951006991973\n",
      "Validation Loss: 0.07695030781440981\n",
      "Epoch 46/100\n",
      "Training Loss: 0.0540338610419463\n",
      "Validation Loss: 0.023243479904295226\n",
      "Epoch 47/100\n",
      "Training Loss: 0.07359324146207252\n",
      "Validation Loss: 0.04299247753480165\n",
      "Epoch 48/100\n",
      "Training Loss: 0.06422459615170474\n",
      "Validation Loss: 0.050708585982656464\n",
      "Epoch 49/100\n",
      "Training Loss: 0.07287208824213276\n",
      "Validation Loss: 0.057460689547542354\n",
      "Epoch 50/100\n",
      "Training Loss: 0.06924393738239346\n",
      "Validation Loss: 0.04805254765176548\n",
      "Epoch 51/100\n",
      "Training Loss: 0.0673963079621206\n",
      "Validation Loss: 0.04673846820534973\n",
      "Epoch 52/100\n",
      "Training Loss: 0.07250170012629265\n",
      "Validation Loss: 0.0642088392811347\n",
      "Epoch 53/100\n",
      "Training Loss: 0.069444104049249\n",
      "Validation Loss: 0.03513246556824483\n",
      "Epoch 54/100\n",
      "Training Loss: 0.06760065284379559\n",
      "Validation Loss: 0.04132429853449905\n",
      "Epoch 55/100\n",
      "Training Loss: 0.06811068112011036\n",
      "Validation Loss: 0.029280664468484034\n",
      "Epoch 56/100\n",
      "Training Loss: 0.07519761714422962\n",
      "Validation Loss: 0.019048925431642685\n",
      "Epoch 57/100\n",
      "Training Loss: 0.07153237054054777\n",
      "Validation Loss: 0.049836771727689425\n",
      "Epoch 58/100\n",
      "Training Loss: 0.06724808525061068\n",
      "Validation Loss: 0.05953283204205627\n",
      "Epoch 59/100\n",
      "Training Loss: 0.06578598666770476\n",
      "Validation Loss: 0.025947649629319308\n",
      "Epoch 60/100\n",
      "Training Loss: 0.06100595254328267\n",
      "Validation Loss: 0.054467112125071805\n",
      "Epoch 61/100\n",
      "Training Loss: 0.06409770530550861\n",
      "Validation Loss: 0.04588267279333441\n",
      "Epoch 62/100\n",
      "Training Loss: 0.060517857952846706\n",
      "Validation Loss: 0.047515041664513785\n",
      "Epoch 63/100\n",
      "Training Loss: 0.06945370810557942\n",
      "Validation Loss: 0.05441273114594518\n",
      "Epoch 64/100\n",
      "Training Loss: 0.06619219429568811\n",
      "Validation Loss: 0.05410823237692568\n",
      "Epoch 65/100\n",
      "Training Loss: 0.07587894242259474\n",
      "Validation Loss: 0.049442893346478285\n",
      "Epoch 66/100\n",
      "Training Loss: 0.058374953015806676\n",
      "Validation Loss: 0.05646095929699825\n",
      "Epoch 67/100\n",
      "Training Loss: 0.07570049964514598\n",
      "Validation Loss: 0.03670851792542597\n",
      "Epoch 68/100\n",
      "Training Loss: 0.07472125343060869\n",
      "Validation Loss: 0.04579262314566556\n",
      "Epoch 69/100\n",
      "Training Loss: 0.06864567886931576\n",
      "Validation Loss: 0.042299902483303606\n",
      "Epoch 70/100\n",
      "Training Loss: 0.05751244409152762\n",
      "Validation Loss: 0.04010991540543567\n",
      "Epoch 71/100\n",
      "Training Loss: 0.08831182269003723\n",
      "Validation Loss: 0.030929166653611784\n",
      "Epoch 72/100\n",
      "Training Loss: 0.06998818688173433\n",
      "Validation Loss: 0.05374885113038351\n",
      "Epoch 73/100\n",
      "Training Loss: 0.07539181630739654\n",
      "Validation Loss: 0.03731562258626194\n",
      "Epoch 74/100\n",
      "Training Loss: 0.057113086739588356\n",
      "Validation Loss: 0.036306596822059244\n",
      "Epoch 75/100\n",
      "Training Loss: 0.065821032220592\n",
      "Validation Loss: 0.06400714950351225\n",
      "Epoch 76/100\n",
      "Training Loss: 0.06184702504168524\n",
      "Validation Loss: 0.027976798533369464\n",
      "Epoch 77/100\n",
      "Training Loss: 0.07033974694062302\n",
      "Validation Loss: 0.09257596865138282\n",
      "Epoch 78/100\n",
      "Training Loss: 0.07270875905843938\n",
      "Validation Loss: 0.06847036796469123\n",
      "Epoch 79/100\n",
      "Training Loss: 0.07894672614619185\n",
      "Validation Loss: 0.04327240613795424\n",
      "Epoch 80/100\n",
      "Training Loss: 0.07120557688914572\n",
      "Validation Loss: 0.03529821296275533\n",
      "Epoch 81/100\n",
      "Training Loss: 0.062321060921323265\n",
      "Validation Loss: 0.044418994143732383\n",
      "Epoch 82/100\n",
      "Training Loss: 0.06456708525840596\n",
      "Validation Loss: 0.019748138673060262\n",
      "Epoch 83/100\n",
      "Training Loss: 0.07024161405858403\n",
      "Validation Loss: 0.040071204413931034\n",
      "Epoch 84/100\n",
      "Training Loss: 0.08990639568159146\n",
      "Validation Loss: 0.03155507811785521\n",
      "Epoch 85/100\n",
      "Training Loss: 0.05870947848698668\n",
      "Validation Loss: 0.07719025395767548\n",
      "Epoch 86/100\n",
      "Training Loss: 0.07692792038947753\n",
      "Validation Loss: 0.05383290018218545\n",
      "Epoch 87/100\n",
      "Training Loss: 0.06591105807498183\n",
      "Validation Loss: 0.034929646055103916\n",
      "Epoch 88/100\n",
      "Training Loss: 0.0628049775622602\n",
      "Validation Loss: 0.03624690891336534\n",
      "Epoch 89/100\n",
      "Training Loss: 0.07424390756713792\n",
      "Validation Loss: 0.043177594509317054\n",
      "Epoch 90/100\n",
      "Training Loss: 0.06580661561332145\n",
      "Validation Loss: 0.04567424394752503\n",
      "Epoch 91/100\n",
      "Training Loss: 0.0716679285030887\n",
      "Validation Loss: 0.06378926650904544\n",
      "Epoch 92/100\n",
      "Training Loss: 0.06601245467917542\n",
      "Validation Loss: 0.033950936405886645\n",
      "Epoch 93/100\n",
      "Training Loss: 0.08319635492492043\n",
      "Validation Loss: 0.04669989126505876\n",
      "Epoch 94/100\n",
      "Training Loss: 0.06063572665674318\n",
      "Validation Loss: 0.03643197429048854\n",
      "Epoch 95/100\n",
      "Training Loss: 0.07668187970685902\n",
      "Validation Loss: 0.037371973596777304\n",
      "Epoch 96/100\n",
      "Training Loss: 0.0675098917881657\n",
      "Validation Loss: 0.04522182062015766\n",
      "Epoch 97/100\n",
      "Training Loss: 0.0799772935487414\n",
      "Validation Loss: 0.05454661664795054\n",
      "Epoch 98/100\n",
      "Training Loss: 0.07289953831547066\n",
      "Validation Loss: 0.05493774216336074\n",
      "Epoch 99/100\n",
      "Training Loss: 0.05922910815012433\n",
      "Validation Loss: 0.045677960396242515\n",
      "Epoch 100/100\n",
      "Training Loss: 0.06545142904581526\n",
      "Validation Loss: 0.06281235265041749\n",
      "    Trial 2/2 for combination 11/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.3710603270631343\n",
      "Validation Loss: 0.2573009015492794\n",
      "Epoch 2/100\n",
      "Training Loss: 0.29660598406753447\n",
      "Validation Loss: 0.19356345732981461\n",
      "Epoch 3/100\n",
      "Training Loss: 0.19861767513776965\n",
      "Validation Loss: 0.11836764669357044\n",
      "Epoch 4/100\n",
      "Training Loss: 0.1960403325297002\n",
      "Validation Loss: 0.12537943035587112\n",
      "Epoch 5/100\n",
      "Training Loss: 0.14938110305029412\n",
      "Validation Loss: 0.11069290609192821\n",
      "Epoch 6/100\n",
      "Training Loss: 0.1548956568877584\n",
      "Validation Loss: 0.1594319036381756\n",
      "Epoch 7/100\n",
      "Training Loss: 0.1569485086377766\n",
      "Validation Loss: 0.04158541928778964\n",
      "Epoch 8/100\n",
      "Training Loss: 0.14854655236984013\n",
      "Validation Loss: 0.09509599502155693\n",
      "Epoch 9/100\n",
      "Training Loss: 0.11781741337673561\n",
      "Validation Loss: 0.046565461622351506\n",
      "Epoch 10/100\n",
      "Training Loss: 0.10666889848552136\n",
      "Validation Loss: 0.057258156069646196\n",
      "Epoch 11/100\n",
      "Training Loss: 0.09107994576703993\n",
      "Validation Loss: 0.08634039453069178\n",
      "Epoch 12/100\n",
      "Training Loss: 0.10224426597817428\n",
      "Validation Loss: 0.055018339690664234\n",
      "Epoch 13/100\n",
      "Training Loss: 0.08794659421878442\n",
      "Validation Loss: 0.054564984601483776\n",
      "Epoch 14/100\n",
      "Training Loss: 0.08747640931347524\n",
      "Validation Loss: 0.05582074145212618\n",
      "Epoch 15/100\n",
      "Training Loss: 0.0796541412576519\n",
      "Validation Loss: 0.03633660344534617\n",
      "Epoch 16/100\n",
      "Training Loss: 0.09020851265694702\n",
      "Validation Loss: 0.052068708057681025\n",
      "Epoch 17/100\n",
      "Training Loss: 0.0725724494561519\n",
      "Validation Loss: 0.053492510217283476\n",
      "Epoch 18/100\n",
      "Training Loss: 0.07129324412050828\n",
      "Validation Loss: 0.040309427421926405\n",
      "Epoch 19/100\n",
      "Training Loss: 0.0672344076481347\n",
      "Validation Loss: 0.04526912238850945\n",
      "Epoch 20/100\n",
      "Training Loss: 0.07281072033660221\n",
      "Validation Loss: 0.033352109830944235\n",
      "Epoch 21/100\n",
      "Training Loss: 0.06061155572923473\n",
      "Validation Loss: 0.05120219459511207\n",
      "Epoch 22/100\n",
      "Training Loss: 0.06958314086053154\n",
      "Validation Loss: 0.04488703237863553\n",
      "Epoch 23/100\n",
      "Training Loss: 0.0630363053178013\n",
      "Validation Loss: 0.03094029471600166\n",
      "Epoch 24/100\n",
      "Training Loss: 0.0637792580735748\n",
      "Validation Loss: 0.032165472092113324\n",
      "Epoch 25/100\n",
      "Training Loss: 0.06545497420439858\n",
      "Validation Loss: 0.03209580464400461\n",
      "Epoch 26/100\n",
      "Training Loss: 0.06188401722477008\n",
      "Validation Loss: 0.04672067911821051\n",
      "Epoch 27/100\n",
      "Training Loss: 0.05666354464366584\n",
      "Validation Loss: 0.06423523129607478\n",
      "Epoch 28/100\n",
      "Training Loss: 0.05402383578358897\n",
      "Validation Loss: 0.05737156667876718\n",
      "Epoch 29/100\n",
      "Training Loss: 0.06714943227442395\n",
      "Validation Loss: 0.05763598003971332\n",
      "Epoch 30/100\n",
      "Training Loss: 0.06321497867910168\n",
      "Validation Loss: 0.05053481242458051\n",
      "Epoch 31/100\n",
      "Training Loss: 0.07175903402685595\n",
      "Validation Loss: 0.03667694825055841\n",
      "Epoch 32/100\n",
      "Training Loss: 0.07225656401923974\n",
      "Validation Loss: 0.04288410516103584\n",
      "Epoch 33/100\n",
      "Training Loss: 0.06182679819282276\n",
      "Validation Loss: 0.04436205489335517\n",
      "Epoch 34/100\n",
      "Training Loss: 0.05650748650736668\n",
      "Validation Loss: 0.04199202218358154\n",
      "Epoch 35/100\n",
      "Training Loss: 0.06565056884908875\n",
      "Validation Loss: 0.024160118616963835\n",
      "Epoch 36/100\n",
      "Training Loss: 0.06191471937237403\n",
      "Validation Loss: 0.06250643643958145\n",
      "Epoch 37/100\n",
      "Training Loss: 0.06928890965059745\n",
      "Validation Loss: 0.04561095127651324\n",
      "Epoch 38/100\n",
      "Training Loss: 0.06875325339156844\n",
      "Validation Loss: 0.03448829921802059\n",
      "Epoch 39/100\n",
      "Training Loss: 0.06953247841444056\n",
      "Validation Loss: 0.06124485544718814\n",
      "Epoch 40/100\n",
      "Training Loss: 0.06927871794931566\n",
      "Validation Loss: 0.046531673833504025\n",
      "Epoch 41/100\n",
      "Training Loss: 0.07303864401064002\n",
      "Validation Loss: 0.06089394080559002\n",
      "Epoch 42/100\n",
      "Training Loss: 0.0686160874151133\n",
      "Validation Loss: 0.033074765284503954\n",
      "Epoch 43/100\n",
      "Training Loss: 0.061110546319636645\n",
      "Validation Loss: 0.07139635058176894\n",
      "Epoch 44/100\n",
      "Training Loss: 0.0677565055625688\n",
      "Validation Loss: 0.04663153982079797\n",
      "Epoch 45/100\n",
      "Training Loss: 0.07938641545168956\n",
      "Validation Loss: 0.08672931399979342\n",
      "Epoch 46/100\n",
      "Training Loss: 0.05916242788045071\n",
      "Validation Loss: 0.05351837997212931\n",
      "Epoch 47/100\n",
      "Training Loss: 0.0545143142947253\n",
      "Validation Loss: 0.03198842136231411\n",
      "Epoch 48/100\n",
      "Training Loss: 0.07206384389797112\n",
      "Validation Loss: 0.04892963394858316\n",
      "Epoch 49/100\n",
      "Training Loss: 0.06626931929964537\n",
      "Validation Loss: 0.08976663778955885\n",
      "Epoch 50/100\n",
      "Training Loss: 0.07175123812056032\n",
      "Validation Loss: 0.045779625151900295\n",
      "Epoch 51/100\n",
      "Training Loss: 0.07147712137265061\n",
      "Validation Loss: 0.05673598975275289\n",
      "Epoch 52/100\n",
      "Training Loss: 0.06204925232264222\n",
      "Validation Loss: 0.03407469282030652\n",
      "Epoch 53/100\n",
      "Training Loss: 0.07723112228212327\n",
      "Validation Loss: 0.030497154422428296\n",
      "Epoch 54/100\n",
      "Training Loss: 0.0605349715832362\n",
      "Validation Loss: 0.043238241022943845\n",
      "Epoch 55/100\n",
      "Training Loss: 0.07215435763363111\n",
      "Validation Loss: 0.044439541800218134\n",
      "Epoch 56/100\n",
      "Training Loss: 0.07495380925186937\n",
      "Validation Loss: 0.04754112047287181\n",
      "Epoch 57/100\n",
      "Training Loss: 0.06904806166131523\n",
      "Validation Loss: 0.054769890718430816\n",
      "Epoch 58/100\n",
      "Training Loss: 0.07706599046636656\n",
      "Validation Loss: 0.027514100981700575\n",
      "Epoch 59/100\n",
      "Training Loss: 0.07235656179738263\n",
      "Validation Loss: 0.05176241966733015\n",
      "Epoch 60/100\n",
      "Training Loss: 0.08894710912093273\n",
      "Validation Loss: 0.07155852803760615\n",
      "Epoch 61/100\n",
      "Training Loss: 0.06447623292899086\n",
      "Validation Loss: 0.036207118628139315\n",
      "Epoch 62/100\n",
      "Training Loss: 0.07075411658535895\n",
      "Validation Loss: 0.052955054893193076\n",
      "Epoch 63/100\n",
      "Training Loss: 0.062196878583303174\n",
      "Validation Loss: 0.05028639420003265\n",
      "Epoch 64/100\n",
      "Training Loss: 0.05418288294976973\n",
      "Validation Loss: 0.05938765681685104\n",
      "Epoch 65/100\n",
      "Training Loss: 0.07225461738434162\n",
      "Validation Loss: 0.04422874508970735\n",
      "Epoch 66/100\n",
      "Training Loss: 0.07638729521160101\n",
      "Validation Loss: 0.031329969170365124\n",
      "Epoch 67/100\n",
      "Training Loss: 0.08307902692175699\n",
      "Validation Loss: 0.031053517524490638\n",
      "Epoch 68/100\n",
      "Training Loss: 0.07168612002349795\n",
      "Validation Loss: 0.06116519140546023\n",
      "Epoch 69/100\n",
      "Training Loss: 0.08653312363867478\n",
      "Validation Loss: 0.06673334001740874\n",
      "Epoch 70/100\n",
      "Training Loss: 0.08582481755557096\n",
      "Validation Loss: 0.04481277370143773\n",
      "Epoch 71/100\n",
      "Training Loss: 0.06538891268923686\n",
      "Validation Loss: 0.05386232469087028\n",
      "Epoch 72/100\n",
      "Training Loss: 0.0644703424235795\n",
      "Validation Loss: 0.08277559348599738\n",
      "Epoch 73/100\n",
      "Training Loss: 0.07586525734395264\n",
      "Validation Loss: 0.0505591278196383\n",
      "Epoch 74/100\n",
      "Training Loss: 0.070095526760637\n",
      "Validation Loss: 0.03605776525761383\n",
      "Epoch 75/100\n",
      "Training Loss: 0.08158935024329142\n",
      "Validation Loss: 0.033577875968069285\n",
      "Epoch 76/100\n",
      "Training Loss: 0.06831166045660436\n",
      "Validation Loss: 0.05968932713321153\n",
      "Epoch 77/100\n",
      "Training Loss: 0.0849452042411946\n",
      "Validation Loss: 0.06390014626548636\n",
      "Epoch 78/100\n",
      "Training Loss: 0.07072494675306985\n",
      "Validation Loss: 0.0706369260235884\n",
      "Epoch 79/100\n",
      "Training Loss: 0.0867347441873382\n",
      "Validation Loss: 0.042402633643952886\n",
      "Epoch 80/100\n",
      "Training Loss: 0.07584150473820638\n",
      "Validation Loss: 0.03387363542340104\n",
      "Epoch 81/100\n",
      "Training Loss: 0.06675814705133107\n",
      "Validation Loss: 0.04091641179470935\n",
      "Epoch 82/100\n",
      "Training Loss: 0.06522008714651918\n",
      "Validation Loss: 0.04776640540885925\n",
      "Epoch 83/100\n",
      "Training Loss: 0.0768185125154113\n",
      "Validation Loss: 0.03567340124878073\n",
      "Epoch 84/100\n",
      "Training Loss: 0.05732021078128635\n",
      "Validation Loss: 0.053677680724592815\n",
      "Epoch 85/100\n",
      "Training Loss: 0.07124963284778503\n",
      "Validation Loss: 0.038556180223999995\n",
      "Epoch 86/100\n",
      "Training Loss: 0.07079408220479934\n",
      "Validation Loss: 0.032492209740690704\n",
      "Epoch 87/100\n",
      "Training Loss: 0.06571505931251913\n",
      "Validation Loss: 0.06765005978993843\n",
      "Epoch 88/100\n",
      "Training Loss: 0.06309598581314237\n",
      "Validation Loss: 0.05500985354927925\n",
      "Epoch 89/100\n",
      "Training Loss: 0.08152432065625193\n",
      "Validation Loss: 0.06586159994495622\n",
      "Epoch 90/100\n",
      "Training Loss: 0.06607461210991421\n",
      "Validation Loss: 0.03436860073310223\n",
      "Epoch 91/100\n",
      "Training Loss: 0.059738740600686374\n",
      "Validation Loss: 0.04111955344372414\n",
      "Epoch 92/100\n",
      "Training Loss: 0.08313797267498653\n",
      "Validation Loss: 0.08065558701346091\n",
      "Epoch 93/100\n",
      "Training Loss: 0.07673920823460684\n",
      "Validation Loss: 0.04950562411229943\n",
      "Epoch 94/100\n",
      "Training Loss: 0.08315519556699297\n",
      "Validation Loss: 0.0638137604778163\n",
      "Epoch 95/100\n",
      "Training Loss: 0.07327930753961998\n",
      "Validation Loss: 0.033166720559793604\n",
      "Epoch 96/100\n",
      "Training Loss: 0.07059498539209039\n",
      "Validation Loss: 0.037470228087874785\n",
      "Epoch 97/100\n",
      "Training Loss: 0.07214600223917265\n",
      "Validation Loss: 0.04075984616589033\n",
      "Epoch 98/100\n",
      "Training Loss: 0.06215921003882816\n",
      "Validation Loss: 0.04502465000432279\n",
      "Epoch 99/100\n",
      "Training Loss: 0.07490935804472801\n",
      "Validation Loss: 0.04551649100601081\n",
      "Epoch 100/100\n",
      "Training Loss: 0.0708399244568565\n",
      "Validation Loss: 0.03352948479804282\n",
      "Combination 11: Avg Training Loss = 0.08158231188874697, Avg Validation Loss = 0.053440353716124454\n",
      "Testing combination 12/48: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.001, 'sequence_length': 50, 'dropout_rate': 0.2}\n",
      "Truncated Train Data Shape: (70, 50, 1)\n",
      "Truncated Validation Data Shape: (10, 50, 1)\n",
      "    Trial 1/2 for combination 12/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.34373573666499574\n",
      "Validation Loss: 0.1557862027347495\n",
      "Epoch 2/100\n",
      "Training Loss: 0.24612440651724035\n",
      "Validation Loss: 0.15930682374664712\n",
      "Epoch 3/100\n",
      "Training Loss: 0.2132776826725024\n",
      "Validation Loss: 0.1438256671677683\n",
      "Epoch 4/100\n",
      "Training Loss: 0.20323863735546147\n",
      "Validation Loss: 0.1038342372440952\n",
      "Epoch 5/100\n",
      "Training Loss: 0.148440689259454\n",
      "Validation Loss: 0.12191181031109141\n",
      "Epoch 6/100\n",
      "Training Loss: 0.12300353036380611\n",
      "Validation Loss: 0.08125250107728318\n",
      "Epoch 7/100\n",
      "Training Loss: 0.1356085660418665\n",
      "Validation Loss: 0.07529993897194705\n",
      "Epoch 8/100\n",
      "Training Loss: 0.09683425133109606\n",
      "Validation Loss: 0.09640742417770778\n",
      "Epoch 9/100\n",
      "Training Loss: 0.10677377856552359\n",
      "Validation Loss: 0.08033570939507181\n",
      "Epoch 10/100\n",
      "Training Loss: 0.11133562787685189\n",
      "Validation Loss: 0.06719392084034428\n",
      "Epoch 11/100\n",
      "Training Loss: 0.10377070051264596\n",
      "Validation Loss: 0.058857450119135055\n",
      "Epoch 12/100\n",
      "Training Loss: 0.083641249282248\n",
      "Validation Loss: 0.05743520902334487\n",
      "Epoch 13/100\n",
      "Training Loss: 0.09809404651759385\n",
      "Validation Loss: 0.050416233197499716\n",
      "Epoch 14/100\n",
      "Training Loss: 0.07819389007305509\n",
      "Validation Loss: 0.0539682734905957\n",
      "Epoch 15/100\n",
      "Training Loss: 0.07115654952988458\n",
      "Validation Loss: 0.05086800146293417\n",
      "Epoch 16/100\n",
      "Training Loss: 0.06982560701450861\n",
      "Validation Loss: 0.049002137651383074\n",
      "Epoch 17/100\n",
      "Training Loss: 0.07057014844637509\n",
      "Validation Loss: 0.051086247443862455\n",
      "Epoch 18/100\n",
      "Training Loss: 0.05806438628022286\n",
      "Validation Loss: 0.03467773266107738\n",
      "Epoch 19/100\n",
      "Training Loss: 0.0689774759043496\n",
      "Validation Loss: 0.022683650712012473\n",
      "Epoch 20/100\n",
      "Training Loss: 0.07596531700621048\n",
      "Validation Loss: 0.0465102323682112\n",
      "Epoch 21/100\n",
      "Training Loss: 0.05876002281104245\n",
      "Validation Loss: 0.031043559691260908\n",
      "Epoch 22/100\n",
      "Training Loss: 0.06719955975843202\n",
      "Validation Loss: 0.04726632258070049\n",
      "Epoch 23/100\n",
      "Training Loss: 0.06798735458623449\n",
      "Validation Loss: 0.07268050299040156\n",
      "Epoch 24/100\n",
      "Training Loss: 0.0625463730730499\n",
      "Validation Loss: 0.034082676244215335\n",
      "Epoch 25/100\n",
      "Training Loss: 0.0749438898277079\n",
      "Validation Loss: 0.03842905784876438\n",
      "Epoch 26/100\n",
      "Training Loss: 0.06746444286081935\n",
      "Validation Loss: 0.044981580181632966\n",
      "Epoch 27/100\n",
      "Training Loss: 0.06414830252350372\n",
      "Validation Loss: 0.050535587358357524\n",
      "Epoch 28/100\n",
      "Training Loss: 0.05550895870474793\n",
      "Validation Loss: 0.026799007301523375\n",
      "Epoch 29/100\n",
      "Training Loss: 0.06072504310507023\n",
      "Validation Loss: 0.07654034952811384\n",
      "Epoch 30/100\n",
      "Training Loss: 0.0697615885755372\n",
      "Validation Loss: 0.08443940842573845\n",
      "Epoch 31/100\n",
      "Training Loss: 0.06410395645762502\n",
      "Validation Loss: 0.0687365353367018\n",
      "Epoch 32/100\n",
      "Training Loss: 0.06387359845112506\n",
      "Validation Loss: 0.05168583748570427\n",
      "Epoch 33/100\n",
      "Training Loss: 0.06967675575617438\n",
      "Validation Loss: 0.03500965705906029\n",
      "Epoch 34/100\n",
      "Training Loss: 0.07732941891185252\n",
      "Validation Loss: 0.04022549369527716\n",
      "Epoch 35/100\n",
      "Training Loss: 0.06831285189523635\n",
      "Validation Loss: 0.08508901923244114\n",
      "Epoch 36/100\n",
      "Training Loss: 0.061630950329587604\n",
      "Validation Loss: 0.04297228734000303\n",
      "Epoch 37/100\n",
      "Training Loss: 0.07929614790285222\n",
      "Validation Loss: 0.08758678845314968\n",
      "Epoch 38/100\n",
      "Training Loss: 0.07259328085105195\n",
      "Validation Loss: 0.05747613513164466\n",
      "Epoch 39/100\n",
      "Training Loss: 0.06544972650343535\n",
      "Validation Loss: 0.04785554133079779\n",
      "Epoch 40/100\n",
      "Training Loss: 0.07764422337400705\n",
      "Validation Loss: 0.057422208438055976\n",
      "Epoch 41/100\n",
      "Training Loss: 0.07078433286591408\n",
      "Validation Loss: 0.05015373778993042\n",
      "Epoch 42/100\n",
      "Training Loss: 0.07741063866463341\n",
      "Validation Loss: 0.0497693142947312\n",
      "Epoch 43/100\n",
      "Training Loss: 0.07168771079388321\n",
      "Validation Loss: 0.04116625380410756\n",
      "Epoch 44/100\n",
      "Training Loss: 0.07884317094125164\n",
      "Validation Loss: 0.024320484725865725\n",
      "Epoch 45/100\n",
      "Training Loss: 0.059201617784123035\n",
      "Validation Loss: 0.04002302950621035\n",
      "Epoch 46/100\n",
      "Training Loss: 0.07266753712854229\n",
      "Validation Loss: 0.05515184883150962\n",
      "Epoch 47/100\n",
      "Training Loss: 0.07268520537105656\n",
      "Validation Loss: 0.05144391872962591\n",
      "Epoch 48/100\n",
      "Training Loss: 0.06918680349655132\n",
      "Validation Loss: 0.052679298913767214\n",
      "Epoch 49/100\n",
      "Training Loss: 0.060701981171762245\n",
      "Validation Loss: 0.035146192691560434\n",
      "Epoch 50/100\n",
      "Training Loss: 0.07465719297204172\n",
      "Validation Loss: 0.04859229825780337\n",
      "Epoch 51/100\n",
      "Training Loss: 0.06787427816145565\n",
      "Validation Loss: 0.037396020981143946\n",
      "Epoch 52/100\n",
      "Training Loss: 0.07701796616613317\n",
      "Validation Loss: 0.02999282759178843\n",
      "Epoch 53/100\n",
      "Training Loss: 0.06544609296655676\n",
      "Validation Loss: 0.07751481138146878\n",
      "Epoch 54/100\n",
      "Training Loss: 0.07442582512049072\n",
      "Validation Loss: 0.05402921923167607\n",
      "Epoch 55/100\n",
      "Training Loss: 0.07265812459681209\n",
      "Validation Loss: 0.061604668356231705\n",
      "Epoch 56/100\n",
      "Training Loss: 0.07347518148981497\n",
      "Validation Loss: 0.05044164960566787\n",
      "Epoch 57/100\n",
      "Training Loss: 0.07079672647190892\n",
      "Validation Loss: 0.038944816352877665\n",
      "Epoch 58/100\n",
      "Training Loss: 0.0695222507157508\n",
      "Validation Loss: 0.04818094041897899\n",
      "Epoch 59/100\n",
      "Training Loss: 0.07121908159557659\n",
      "Validation Loss: 0.05835421637593152\n",
      "Epoch 60/100\n",
      "Training Loss: 0.06973804067998132\n",
      "Validation Loss: 0.05605285317273321\n",
      "Epoch 61/100\n",
      "Training Loss: 0.077517545221502\n",
      "Validation Loss: 0.04554841485194475\n",
      "Epoch 62/100\n",
      "Training Loss: 0.06849127285267541\n",
      "Validation Loss: 0.04734614727672038\n",
      "Epoch 63/100\n",
      "Training Loss: 0.07317254421390369\n",
      "Validation Loss: 0.02407172150777006\n",
      "Epoch 64/100\n",
      "Training Loss: 0.06663344022571097\n",
      "Validation Loss: 0.03732134073312332\n",
      "Epoch 65/100\n",
      "Training Loss: 0.07494928801916917\n",
      "Validation Loss: 0.03493171863695647\n",
      "Epoch 66/100\n",
      "Training Loss: 0.08951022877006803\n",
      "Validation Loss: 0.062427372989684274\n",
      "Epoch 67/100\n",
      "Training Loss: 0.06750690576739242\n",
      "Validation Loss: 0.03221775753659325\n",
      "Epoch 68/100\n",
      "Training Loss: 0.06728688644654685\n",
      "Validation Loss: 0.0605353916667248\n",
      "Epoch 69/100\n",
      "Training Loss: 0.07758991047944848\n",
      "Validation Loss: 0.048935213429625035\n",
      "Epoch 70/100\n",
      "Training Loss: 0.07684098271775393\n",
      "Validation Loss: 0.03561365894774584\n",
      "Epoch 71/100\n",
      "Training Loss: 0.07099328775890262\n",
      "Validation Loss: 0.05964402182278219\n",
      "Epoch 72/100\n",
      "Training Loss: 0.07128922492526897\n",
      "Validation Loss: 0.03416253866090162\n",
      "Epoch 73/100\n",
      "Training Loss: 0.07163508724650904\n",
      "Validation Loss: 0.05360029658025574\n",
      "Epoch 74/100\n",
      "Training Loss: 0.0721838147821758\n",
      "Validation Loss: 0.04784553102795372\n",
      "Epoch 75/100\n",
      "Training Loss: 0.06363675053338728\n",
      "Validation Loss: 0.06606095882286005\n",
      "Epoch 76/100\n",
      "Training Loss: 0.06601793082373117\n",
      "Validation Loss: 0.04535558793579868\n",
      "Epoch 77/100\n",
      "Training Loss: 0.06549968322597408\n",
      "Validation Loss: 0.041876616591264446\n",
      "Epoch 78/100\n",
      "Training Loss: 0.07315489035833973\n",
      "Validation Loss: 0.04477397876849033\n",
      "Epoch 79/100\n",
      "Training Loss: 0.0734971508167733\n",
      "Validation Loss: 0.10154187523773354\n",
      "Epoch 80/100\n",
      "Training Loss: 0.07323183797604742\n",
      "Validation Loss: 0.11534318011491713\n",
      "Epoch 81/100\n",
      "Training Loss: 0.08072534631672137\n",
      "Validation Loss: 0.054108372897170555\n",
      "Epoch 82/100\n",
      "Training Loss: 0.06408353245501647\n",
      "Validation Loss: 0.0891073094646784\n",
      "Epoch 83/100\n",
      "Training Loss: 0.07650833061261948\n",
      "Validation Loss: 0.09382661753110264\n",
      "Epoch 84/100\n",
      "Training Loss: 0.07175878584468046\n",
      "Validation Loss: 0.034203897903720915\n",
      "Epoch 85/100\n",
      "Training Loss: 0.06754296268249593\n",
      "Validation Loss: 0.04195348240931855\n",
      "Epoch 86/100\n",
      "Training Loss: 0.06361895987744856\n",
      "Validation Loss: 0.16443428731374904\n",
      "Epoch 87/100\n",
      "Training Loss: 0.06523950754161996\n",
      "Validation Loss: 0.038186184082041656\n",
      "Epoch 88/100\n",
      "Training Loss: 0.07595934464163497\n",
      "Validation Loss: 0.0375642723305603\n",
      "Epoch 89/100\n",
      "Training Loss: 0.08379371169183118\n",
      "Validation Loss: 0.0410096884460442\n",
      "Epoch 90/100\n",
      "Training Loss: 0.06605537911171021\n",
      "Validation Loss: 0.0637288266476563\n",
      "Epoch 91/100\n",
      "Training Loss: 0.08542254573305748\n",
      "Validation Loss: 0.03466294511319791\n",
      "Epoch 92/100\n",
      "Training Loss: 0.06776701045072038\n",
      "Validation Loss: 0.027368985140432185\n",
      "Epoch 93/100\n",
      "Training Loss: 0.06817052738364582\n",
      "Validation Loss: 0.036051871474313414\n",
      "Epoch 94/100\n",
      "Training Loss: 0.07919047240229313\n",
      "Validation Loss: 0.03435336904320204\n",
      "Epoch 95/100\n",
      "Training Loss: 0.07011609423874238\n",
      "Validation Loss: 0.0647081214246475\n",
      "Epoch 96/100\n",
      "Training Loss: 0.0762403281001233\n",
      "Validation Loss: 0.07537220832919275\n",
      "Epoch 97/100\n",
      "Training Loss: 0.07403032436390673\n",
      "Validation Loss: 0.02738300097082158\n",
      "Epoch 98/100\n",
      "Training Loss: 0.05912291503230466\n",
      "Validation Loss: 0.01863927143440667\n",
      "Epoch 99/100\n",
      "Training Loss: 0.0745479184611119\n",
      "Validation Loss: 0.05191814932215114\n",
      "Epoch 100/100\n",
      "Training Loss: 0.0621126886394489\n",
      "Validation Loss: 0.04123081347178758\n",
      "    Trial 2/2 for combination 12/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.33021333618629206\n",
      "Validation Loss: 0.11817369821204068\n",
      "Epoch 2/100\n",
      "Training Loss: 0.25048034552308823\n",
      "Validation Loss: 0.09428073622427802\n",
      "Epoch 3/100\n",
      "Training Loss: 0.17038952441517377\n",
      "Validation Loss: 0.10352982164330624\n",
      "Epoch 4/100\n",
      "Training Loss: 0.15869001526131166\n",
      "Validation Loss: 0.17753696099149852\n",
      "Epoch 5/100\n",
      "Training Loss: 0.17859023654252865\n",
      "Validation Loss: 0.1204596651419039\n",
      "Epoch 6/100\n",
      "Training Loss: 0.13578425510505912\n",
      "Validation Loss: 0.09751606549337363\n",
      "Epoch 7/100\n",
      "Training Loss: 0.1491605155490808\n",
      "Validation Loss: 0.07316535413805676\n",
      "Epoch 8/100\n",
      "Training Loss: 0.12116731500284546\n",
      "Validation Loss: 0.06043870285131479\n",
      "Epoch 9/100\n",
      "Training Loss: 0.10719527601273926\n",
      "Validation Loss: 0.0830114384951493\n",
      "Epoch 10/100\n",
      "Training Loss: 0.08390200873464906\n",
      "Validation Loss: 0.08428095456539175\n",
      "Epoch 11/100\n",
      "Training Loss: 0.09181043672172035\n",
      "Validation Loss: 0.09257313933056567\n",
      "Epoch 12/100\n",
      "Training Loss: 0.08227333678159508\n",
      "Validation Loss: 0.06492414412215369\n",
      "Epoch 13/100\n",
      "Training Loss: 0.07781120611877668\n",
      "Validation Loss: 0.07427190283091838\n",
      "Epoch 14/100\n",
      "Training Loss: 0.07374938632644927\n",
      "Validation Loss: 0.04449389836065928\n",
      "Epoch 15/100\n",
      "Training Loss: 0.07640241737261204\n",
      "Validation Loss: 0.057964674445570186\n",
      "Epoch 16/100\n",
      "Training Loss: 0.07405025308353046\n",
      "Validation Loss: 0.03640601867388406\n",
      "Epoch 17/100\n",
      "Training Loss: 0.06639338710443936\n",
      "Validation Loss: 0.0741543455760563\n",
      "Epoch 18/100\n",
      "Training Loss: 0.06614998176950601\n",
      "Validation Loss: 0.05401475298933359\n",
      "Epoch 19/100\n",
      "Training Loss: 0.07252070646468185\n",
      "Validation Loss: 0.05268256506166854\n",
      "Epoch 20/100\n",
      "Training Loss: 0.07181421125490947\n",
      "Validation Loss: 0.05717372450998211\n",
      "Epoch 21/100\n",
      "Training Loss: 0.062133212337538095\n",
      "Validation Loss: 0.03503287545950601\n",
      "Epoch 22/100\n",
      "Training Loss: 0.05360898968433167\n",
      "Validation Loss: 0.049755792525073034\n",
      "Epoch 23/100\n",
      "Training Loss: 0.06533376635414152\n",
      "Validation Loss: 0.06847608566721397\n",
      "Epoch 24/100\n",
      "Training Loss: 0.07288773527519911\n",
      "Validation Loss: 0.06741415520725472\n",
      "Epoch 25/100\n",
      "Training Loss: 0.06556182782361239\n",
      "Validation Loss: 0.058796705498882176\n",
      "Epoch 26/100\n",
      "Training Loss: 0.06294325792037186\n",
      "Validation Loss: 0.06153615150036858\n",
      "Epoch 27/100\n",
      "Training Loss: 0.0642597939880586\n",
      "Validation Loss: 0.03162976268544575\n",
      "Epoch 28/100\n",
      "Training Loss: 0.0662310649460584\n",
      "Validation Loss: 0.043898782194959074\n",
      "Epoch 29/100\n",
      "Training Loss: 0.06401096875469904\n",
      "Validation Loss: 0.049166845322338726\n",
      "Epoch 30/100\n",
      "Training Loss: 0.06976701989417106\n",
      "Validation Loss: 0.05279342844831035\n",
      "Epoch 31/100\n",
      "Training Loss: 0.07995215698936618\n",
      "Validation Loss: 0.10999043858987256\n",
      "Epoch 32/100\n",
      "Training Loss: 0.0688300303079339\n",
      "Validation Loss: 0.02587418010816178\n",
      "Epoch 33/100\n",
      "Training Loss: 0.060154503266563214\n",
      "Validation Loss: 0.03923652949152894\n",
      "Epoch 34/100\n",
      "Training Loss: 0.062159717781049596\n",
      "Validation Loss: 0.06934368077400993\n",
      "Epoch 35/100\n",
      "Training Loss: 0.058074704183893266\n",
      "Validation Loss: 0.0736324252599889\n",
      "Epoch 36/100\n",
      "Training Loss: 0.06595910620995518\n",
      "Validation Loss: 0.04674151705439771\n",
      "Epoch 37/100\n",
      "Training Loss: 0.05281936349932855\n",
      "Validation Loss: 0.05751073460939406\n",
      "Epoch 38/100\n",
      "Training Loss: 0.0706784119677934\n",
      "Validation Loss: 0.03985798295155828\n",
      "Epoch 39/100\n",
      "Training Loss: 0.06640645452115994\n",
      "Validation Loss: 0.06591394199425585\n",
      "Epoch 40/100\n",
      "Training Loss: 0.07573488721290013\n",
      "Validation Loss: 0.03428332184882336\n",
      "Epoch 41/100\n",
      "Training Loss: 0.06953774665859108\n",
      "Validation Loss: 0.05037674389953396\n",
      "Epoch 42/100\n",
      "Training Loss: 0.07328281033197082\n",
      "Validation Loss: 0.05276762894946781\n",
      "Epoch 43/100\n",
      "Training Loss: 0.06916205143698495\n",
      "Validation Loss: 0.03026355018355036\n",
      "Epoch 44/100\n",
      "Training Loss: 0.0727255621861043\n",
      "Validation Loss: 0.040159276119427254\n",
      "Epoch 45/100\n",
      "Training Loss: 0.07642526296650068\n",
      "Validation Loss: 0.04503400526397017\n",
      "Epoch 46/100\n",
      "Training Loss: 0.07614447582414598\n",
      "Validation Loss: 0.04824859258622034\n",
      "Epoch 47/100\n",
      "Training Loss: 0.07010423034620121\n",
      "Validation Loss: 0.044674750864858156\n",
      "Epoch 48/100\n",
      "Training Loss: 0.06146506011951912\n",
      "Validation Loss: 0.031141216756941113\n",
      "Epoch 49/100\n",
      "Training Loss: 0.060937302971291886\n",
      "Validation Loss: 0.029852620738082515\n",
      "Epoch 50/100\n",
      "Training Loss: 0.06782983811690498\n",
      "Validation Loss: 0.04453291561759598\n",
      "Epoch 51/100\n",
      "Training Loss: 0.07448554963423554\n",
      "Validation Loss: 0.04499435012799733\n",
      "Epoch 52/100\n",
      "Training Loss: 0.07093540334966879\n",
      "Validation Loss: 0.03107509299483513\n",
      "Epoch 53/100\n",
      "Training Loss: 0.06638992897895564\n",
      "Validation Loss: 0.055581754680622555\n",
      "Epoch 54/100\n",
      "Training Loss: 0.0673972273027787\n",
      "Validation Loss: 0.041245877291138745\n",
      "Epoch 55/100\n",
      "Training Loss: 0.06479547204314148\n",
      "Validation Loss: 0.04942447733656101\n",
      "Epoch 56/100\n",
      "Training Loss: 0.07088279723491496\n",
      "Validation Loss: 0.043596160353499426\n",
      "Epoch 57/100\n",
      "Training Loss: 0.08518348942094656\n",
      "Validation Loss: 0.05072928402546145\n",
      "Epoch 58/100\n",
      "Training Loss: 0.07166575581342449\n",
      "Validation Loss: 0.04768786999707937\n",
      "Epoch 59/100\n",
      "Training Loss: 0.06675483008451603\n",
      "Validation Loss: 0.09245522773855105\n",
      "Epoch 60/100\n",
      "Training Loss: 0.05630943263705532\n",
      "Validation Loss: 0.07552703598494799\n",
      "Epoch 61/100\n",
      "Training Loss: 0.06171920252654482\n",
      "Validation Loss: 0.05421348645989861\n",
      "Epoch 62/100\n",
      "Training Loss: 0.055877504671413225\n",
      "Validation Loss: 0.028402127829217583\n",
      "Epoch 63/100\n",
      "Training Loss: 0.07245941517728105\n",
      "Validation Loss: 0.062394062848596644\n",
      "Epoch 64/100\n",
      "Training Loss: 0.06002692044240061\n",
      "Validation Loss: 0.04312098436888072\n",
      "Epoch 65/100\n",
      "Training Loss: 0.07448586368879806\n",
      "Validation Loss: 0.05571534795794926\n",
      "Epoch 66/100\n",
      "Training Loss: 0.0657786793282783\n",
      "Validation Loss: 0.06454162429408737\n",
      "Epoch 67/100\n",
      "Training Loss: 0.07220664950298573\n",
      "Validation Loss: 0.055959025557748364\n",
      "Epoch 68/100\n",
      "Training Loss: 0.08265610102425008\n",
      "Validation Loss: 0.11020362373441676\n",
      "Epoch 69/100\n",
      "Training Loss: 0.06814587456588997\n",
      "Validation Loss: 0.02891651745184399\n",
      "Epoch 70/100\n",
      "Training Loss: 0.06458194246525133\n",
      "Validation Loss: 0.03649507905761461\n",
      "Epoch 71/100\n",
      "Training Loss: 0.08856578357395317\n",
      "Validation Loss: 0.021134918073393117\n",
      "Epoch 72/100\n",
      "Training Loss: 0.08598467467594818\n",
      "Validation Loss: 0.05377832387029826\n",
      "Epoch 73/100\n",
      "Training Loss: 0.08093093741613161\n",
      "Validation Loss: 0.04607913181739702\n",
      "Epoch 74/100\n",
      "Training Loss: 0.06540163122927214\n",
      "Validation Loss: 0.08194328050839501\n",
      "Epoch 75/100\n",
      "Training Loss: 0.06209107681972169\n",
      "Validation Loss: 0.06172107184247141\n",
      "Epoch 76/100\n",
      "Training Loss: 0.07304326671218517\n",
      "Validation Loss: 0.03431400215248574\n",
      "Epoch 77/100\n",
      "Training Loss: 0.0751972928901606\n",
      "Validation Loss: 0.03786516975571916\n",
      "Epoch 78/100\n",
      "Training Loss: 0.06940108229786725\n",
      "Validation Loss: 0.04066785912570074\n",
      "Epoch 79/100\n",
      "Training Loss: 0.07912843529132765\n",
      "Validation Loss: 0.07304518524265156\n",
      "Epoch 80/100\n",
      "Training Loss: 0.0641330985120709\n",
      "Validation Loss: 0.0394134743609223\n",
      "Epoch 81/100\n",
      "Training Loss: 0.06505237062681642\n",
      "Validation Loss: 0.052641825739687634\n",
      "Epoch 82/100\n",
      "Training Loss: 0.06596874874786902\n",
      "Validation Loss: 0.09048978182469529\n",
      "Epoch 83/100\n",
      "Training Loss: 0.07971065154642161\n",
      "Validation Loss: 0.06599445322896226\n",
      "Epoch 84/100\n",
      "Training Loss: 0.07730023618914203\n",
      "Validation Loss: 0.0527660372021295\n",
      "Epoch 85/100\n",
      "Training Loss: 0.07671751384868647\n",
      "Validation Loss: 0.054350369541628606\n",
      "Epoch 86/100\n",
      "Training Loss: 0.06704571324079453\n",
      "Validation Loss: 0.03106049465383077\n",
      "Epoch 87/100\n",
      "Training Loss: 0.0757048685744674\n",
      "Validation Loss: 0.046767110726219585\n",
      "Epoch 88/100\n",
      "Training Loss: 0.06361644667272323\n",
      "Validation Loss: 0.03708501298556564\n",
      "Epoch 89/100\n",
      "Training Loss: 0.06612840864827037\n",
      "Validation Loss: 0.043266757820174626\n",
      "Epoch 90/100\n",
      "Training Loss: 0.06639308134506652\n",
      "Validation Loss: 0.0861150664048704\n",
      "Epoch 91/100\n",
      "Training Loss: 0.07102755182630556\n",
      "Validation Loss: 0.051656491241187555\n",
      "Epoch 92/100\n",
      "Training Loss: 0.0728655698466593\n",
      "Validation Loss: 0.09657004736830072\n",
      "Epoch 93/100\n",
      "Training Loss: 0.06924003429660958\n",
      "Validation Loss: 0.07380514833938702\n",
      "Epoch 94/100\n",
      "Training Loss: 0.09715010486916284\n",
      "Validation Loss: 0.04124041939046151\n",
      "Epoch 95/100\n",
      "Training Loss: 0.06314929587852987\n",
      "Validation Loss: 0.04054647565759929\n",
      "Epoch 96/100\n",
      "Training Loss: 0.06811524296853613\n",
      "Validation Loss: 0.07596487581776432\n",
      "Epoch 97/100\n",
      "Training Loss: 0.06202664970048719\n",
      "Validation Loss: 0.057224225191239195\n",
      "Epoch 98/100\n",
      "Training Loss: 0.08142099722736448\n",
      "Validation Loss: 0.07052992585825008\n",
      "Epoch 99/100\n",
      "Training Loss: 0.08002343981287632\n",
      "Validation Loss: 0.04090410377578667\n",
      "Epoch 100/100\n",
      "Training Loss: 0.07380263439830681\n",
      "Validation Loss: 0.027481918218014496\n",
      "Combination 12: Avg Training Loss = 0.08075489088077717, Avg Validation Loss = 0.057645657640040954\n",
      "Testing combination 13/48: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0001, 'sequence_length': 6, 'dropout_rate': 0.1}\n",
      "Truncated Train Data Shape: (70, 6, 1)\n",
      "Truncated Validation Data Shape: (10, 6, 1)\n",
      "    Trial 1/2 for combination 13/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.6219530603720956\n",
      "Validation Loss: 0.5228083636544036\n",
      "Epoch 2/100\n",
      "Training Loss: 0.6219957332097271\n",
      "Validation Loss: 0.5334456092279296\n",
      "Epoch 3/100\n",
      "Training Loss: 0.4709924539649399\n",
      "Validation Loss: 0.4904143375420591\n",
      "Epoch 4/100\n",
      "Training Loss: 0.43930789439661555\n",
      "Validation Loss: 0.21742594897555995\n",
      "Epoch 5/100\n",
      "Training Loss: 0.3755402556536355\n",
      "Validation Loss: 0.39849766344782983\n",
      "Epoch 6/100\n",
      "Training Loss: 0.34902737727683125\n",
      "Validation Loss: 0.19286381862473062\n",
      "Epoch 7/100\n",
      "Training Loss: 0.35548690584595005\n",
      "Validation Loss: 0.2767967185024487\n",
      "Epoch 8/100\n",
      "Training Loss: 0.2714559535069994\n",
      "Validation Loss: 0.217019062968666\n",
      "Epoch 9/100\n",
      "Training Loss: 0.27829196216025287\n",
      "Validation Loss: 0.3320326380465509\n",
      "Epoch 10/100\n",
      "Training Loss: 0.29244452192090337\n",
      "Validation Loss: 0.22776044938621615\n",
      "Epoch 11/100\n",
      "Training Loss: 0.25438131894211613\n",
      "Validation Loss: 0.22030191750595313\n",
      "Epoch 12/100\n",
      "Training Loss: 0.2585373775390002\n",
      "Validation Loss: 0.19600714036050984\n",
      "Epoch 13/100\n",
      "Training Loss: 0.3174233764447662\n",
      "Validation Loss: 0.10263402975040199\n",
      "Epoch 14/100\n",
      "Training Loss: 0.30068894319346284\n",
      "Validation Loss: 0.30910833885409017\n",
      "Epoch 15/100\n",
      "Training Loss: 0.19928079869364032\n",
      "Validation Loss: 0.2903327527348559\n",
      "Epoch 16/100\n",
      "Training Loss: 0.23801256396534431\n",
      "Validation Loss: 0.35307559949634676\n",
      "Epoch 17/100\n",
      "Training Loss: 0.2620851794943026\n",
      "Validation Loss: 0.17854897788647786\n",
      "Epoch 18/100\n",
      "Training Loss: 0.22934699353738663\n",
      "Validation Loss: 0.14361272030701525\n",
      "Epoch 19/100\n",
      "Training Loss: 0.2401797250465932\n",
      "Validation Loss: 0.12539597338264444\n",
      "Epoch 20/100\n",
      "Training Loss: 0.20571375212819684\n",
      "Validation Loss: 0.15631604694315152\n",
      "Epoch 21/100\n",
      "Training Loss: 0.2393907264203275\n",
      "Validation Loss: 0.08857512127588604\n",
      "Epoch 22/100\n",
      "Training Loss: 0.20868831649576325\n",
      "Validation Loss: 0.18939235919021352\n",
      "Epoch 23/100\n",
      "Training Loss: 0.22387520923304788\n",
      "Validation Loss: 0.20564907411805006\n",
      "Epoch 24/100\n",
      "Training Loss: 0.2714073658061951\n",
      "Validation Loss: 0.1864240893474643\n",
      "Epoch 25/100\n",
      "Training Loss: 0.26315788182291855\n",
      "Validation Loss: 0.19346626495478697\n",
      "Epoch 26/100\n",
      "Training Loss: 0.20225111570577542\n",
      "Validation Loss: 0.13605731468305707\n",
      "Epoch 27/100\n",
      "Training Loss: 0.23244543824910174\n",
      "Validation Loss: 0.29314506417799857\n",
      "Epoch 28/100\n",
      "Training Loss: 0.19552304591518146\n",
      "Validation Loss: 0.1022925451864211\n",
      "Epoch 29/100\n",
      "Training Loss: 0.23139014352500067\n",
      "Validation Loss: 0.16517389671550659\n",
      "Epoch 30/100\n",
      "Training Loss: 0.2202143731328879\n",
      "Validation Loss: 0.19072248666795572\n",
      "Epoch 31/100\n",
      "Training Loss: 0.19178804206421524\n",
      "Validation Loss: 0.1265093376630255\n",
      "Epoch 32/100\n",
      "Training Loss: 0.1779000563777075\n",
      "Validation Loss: 0.14897504697298566\n",
      "Epoch 33/100\n",
      "Training Loss: 0.19084345645932418\n",
      "Validation Loss: 0.16744993883732612\n",
      "Epoch 34/100\n",
      "Training Loss: 0.1759069703132943\n",
      "Validation Loss: 0.16719625071240854\n",
      "Epoch 35/100\n",
      "Training Loss: 0.18192451209633248\n",
      "Validation Loss: 0.16129659880635425\n",
      "Epoch 36/100\n",
      "Training Loss: 0.1573795948534433\n",
      "Validation Loss: 0.1612432688165197\n",
      "Epoch 37/100\n",
      "Training Loss: 0.18713697273813776\n",
      "Validation Loss: 0.12407225608047687\n",
      "Epoch 38/100\n",
      "Training Loss: 0.17670208221275166\n",
      "Validation Loss: 0.1258228234147266\n",
      "Epoch 39/100\n",
      "Training Loss: 0.18084446409604718\n",
      "Validation Loss: 0.10816542992258704\n",
      "Epoch 40/100\n",
      "Training Loss: 0.1884209497240215\n",
      "Validation Loss: 0.11542698625762526\n",
      "Epoch 41/100\n",
      "Training Loss: 0.167820655047484\n",
      "Validation Loss: 0.13619252148390187\n",
      "Epoch 42/100\n",
      "Training Loss: 0.1584518358052604\n",
      "Validation Loss: 0.12102929287571407\n",
      "Epoch 43/100\n",
      "Training Loss: 0.1753879875199898\n",
      "Validation Loss: 0.17631149131876891\n",
      "Epoch 44/100\n",
      "Training Loss: 0.17786479467189578\n",
      "Validation Loss: 0.06821160162360809\n",
      "Epoch 45/100\n",
      "Training Loss: 0.18739423815729672\n",
      "Validation Loss: 0.1486790717445802\n",
      "Epoch 46/100\n",
      "Training Loss: 0.1373841983696459\n",
      "Validation Loss: 0.1851588273569943\n",
      "Epoch 47/100\n",
      "Training Loss: 0.15857187117250712\n",
      "Validation Loss: 0.12558941231436435\n",
      "Epoch 48/100\n",
      "Training Loss: 0.16134384343185046\n",
      "Validation Loss: 0.11999479660754367\n",
      "Epoch 49/100\n",
      "Training Loss: 0.1497460467530036\n",
      "Validation Loss: 0.10846812372830208\n",
      "Epoch 50/100\n",
      "Training Loss: 0.15706471509949957\n",
      "Validation Loss: 0.1638420270001721\n",
      "Epoch 51/100\n",
      "Training Loss: 0.18205625089447708\n",
      "Validation Loss: 0.16731194346758332\n",
      "Epoch 52/100\n",
      "Training Loss: 0.12532873008663248\n",
      "Validation Loss: 0.12187098061738648\n",
      "Epoch 53/100\n",
      "Training Loss: 0.15682166653371477\n",
      "Validation Loss: 0.13703243087110012\n",
      "Epoch 54/100\n",
      "Training Loss: 0.16741160422937162\n",
      "Validation Loss: 0.12689937461128828\n",
      "Epoch 55/100\n",
      "Training Loss: 0.18105225681266454\n",
      "Validation Loss: 0.13874927408245652\n",
      "Epoch 56/100\n",
      "Training Loss: 0.1725559126229254\n",
      "Validation Loss: 0.14080523027955863\n",
      "Epoch 57/100\n",
      "Training Loss: 0.1590956264106759\n",
      "Validation Loss: 0.14653138141233063\n",
      "Epoch 58/100\n",
      "Training Loss: 0.14901679045747637\n",
      "Validation Loss: 0.09487243184908171\n",
      "Epoch 59/100\n",
      "Training Loss: 0.16227605369505085\n",
      "Validation Loss: 0.12739968592245302\n",
      "Epoch 60/100\n",
      "Training Loss: 0.16904748233213374\n",
      "Validation Loss: 0.1690189218075139\n",
      "Epoch 61/100\n",
      "Training Loss: 0.16359069726871858\n",
      "Validation Loss: 0.11581759080399386\n",
      "Epoch 62/100\n",
      "Training Loss: 0.1379387333362182\n",
      "Validation Loss: 0.07202237074737967\n",
      "Epoch 63/100\n",
      "Training Loss: 0.16117824055762733\n",
      "Validation Loss: 0.11212746107656009\n",
      "Epoch 64/100\n",
      "Training Loss: 0.1569465490104636\n",
      "Validation Loss: 0.10480121287888114\n",
      "Epoch 65/100\n",
      "Training Loss: 0.15200417699298793\n",
      "Validation Loss: 0.07914673944431667\n",
      "Epoch 66/100\n",
      "Training Loss: 0.15037980065348402\n",
      "Validation Loss: 0.13886317024075995\n",
      "Epoch 67/100\n",
      "Training Loss: 0.13962381733732343\n",
      "Validation Loss: 0.12141783047261874\n",
      "Epoch 68/100\n",
      "Training Loss: 0.12459222088749082\n",
      "Validation Loss: 0.10213830781356029\n",
      "Epoch 69/100\n",
      "Training Loss: 0.1379175638043253\n",
      "Validation Loss: 0.08588143132451984\n",
      "Epoch 70/100\n",
      "Training Loss: 0.18068552132750426\n",
      "Validation Loss: 0.13868804333280538\n",
      "Epoch 71/100\n",
      "Training Loss: 0.1388549088345837\n",
      "Validation Loss: 0.11477223813725404\n",
      "Epoch 72/100\n",
      "Training Loss: 0.13497045288626808\n",
      "Validation Loss: 0.10553150265367502\n",
      "Epoch 73/100\n",
      "Training Loss: 0.13579294143272494\n",
      "Validation Loss: 0.09205251253417748\n",
      "Epoch 74/100\n",
      "Training Loss: 0.14227003413065545\n",
      "Validation Loss: 0.10777451866273129\n",
      "Epoch 75/100\n",
      "Training Loss: 0.1318250918136048\n",
      "Validation Loss: 0.11999110870092114\n",
      "Epoch 76/100\n",
      "Training Loss: 0.10954651431891788\n",
      "Validation Loss: 0.08262585454880576\n",
      "Epoch 77/100\n",
      "Training Loss: 0.1416039708032493\n",
      "Validation Loss: 0.10269677809389495\n",
      "Epoch 78/100\n",
      "Training Loss: 0.13278097092678823\n",
      "Validation Loss: 0.06850366819235391\n",
      "Epoch 79/100\n",
      "Training Loss: 0.15395604316230493\n",
      "Validation Loss: 0.12063487514362307\n",
      "Epoch 80/100\n",
      "Training Loss: 0.14717017592638468\n",
      "Validation Loss: 0.0680095639568691\n",
      "Epoch 81/100\n",
      "Training Loss: 0.14191146473264327\n",
      "Validation Loss: 0.1294960526622604\n",
      "Epoch 82/100\n",
      "Training Loss: 0.133109785085813\n",
      "Validation Loss: 0.07257671632590348\n",
      "Epoch 83/100\n",
      "Training Loss: 0.15382449332019968\n",
      "Validation Loss: 0.08355855717587543\n",
      "Epoch 84/100\n",
      "Training Loss: 0.12329984569562448\n",
      "Validation Loss: 0.07265967356489933\n",
      "Epoch 85/100\n",
      "Training Loss: 0.12561789376013816\n",
      "Validation Loss: 0.08963141436136368\n",
      "Epoch 86/100\n",
      "Training Loss: 0.12509389845267863\n",
      "Validation Loss: 0.06226187209111735\n",
      "Epoch 87/100\n",
      "Training Loss: 0.1479120440260169\n",
      "Validation Loss: 0.09056513595360025\n",
      "Epoch 88/100\n",
      "Training Loss: 0.11748678961753178\n",
      "Validation Loss: 0.12992102657524146\n",
      "Epoch 89/100\n",
      "Training Loss: 0.12524244648117028\n",
      "Validation Loss: 0.12432246158431708\n",
      "Epoch 90/100\n",
      "Training Loss: 0.12623306088357286\n",
      "Validation Loss: 0.06609348182394166\n",
      "Epoch 91/100\n",
      "Training Loss: 0.16013594269609907\n",
      "Validation Loss: 0.13272785406571688\n",
      "Epoch 92/100\n",
      "Training Loss: 0.11928601239288926\n",
      "Validation Loss: 0.1450473013784803\n",
      "Epoch 93/100\n",
      "Training Loss: 0.13692959010947847\n",
      "Validation Loss: 0.07492421343275721\n",
      "Epoch 94/100\n",
      "Training Loss: 0.13393937860117147\n",
      "Validation Loss: 0.15829737912933545\n",
      "Epoch 95/100\n",
      "Training Loss: 0.1360245349684156\n",
      "Validation Loss: 0.08786319922823833\n",
      "Epoch 96/100\n",
      "Training Loss: 0.11596481515409379\n",
      "Validation Loss: 0.058174930009905604\n",
      "Epoch 97/100\n",
      "Training Loss: 0.13614514132428676\n",
      "Validation Loss: 0.08239326260977323\n",
      "Epoch 98/100\n",
      "Training Loss: 0.1298667549625547\n",
      "Validation Loss: 0.09612085240255897\n",
      "Epoch 99/100\n",
      "Training Loss: 0.13503980636576954\n",
      "Validation Loss: 0.09128959925851628\n",
      "Epoch 100/100\n",
      "Training Loss: 0.1321651086833947\n",
      "Validation Loss: 0.06130659094354905\n",
      "    Trial 2/2 for combination 13/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.4761173056917407\n",
      "Validation Loss: 0.23929281012483578\n",
      "Epoch 2/100\n",
      "Training Loss: 0.5229801530177313\n",
      "Validation Loss: 0.33837544535530584\n",
      "Epoch 3/100\n",
      "Training Loss: 0.353091408150489\n",
      "Validation Loss: 0.4069490975383582\n",
      "Epoch 4/100\n",
      "Training Loss: 0.3821952415307222\n",
      "Validation Loss: 0.32812290962604845\n",
      "Epoch 5/100\n",
      "Training Loss: 0.3964254780675796\n",
      "Validation Loss: 0.46938026103550534\n",
      "Epoch 6/100\n",
      "Training Loss: 0.3700167136579202\n",
      "Validation Loss: 0.3280984596305748\n",
      "Epoch 7/100\n",
      "Training Loss: 0.38592161888317417\n",
      "Validation Loss: 0.3666721032483145\n",
      "Epoch 8/100\n",
      "Training Loss: 0.319146023449057\n",
      "Validation Loss: 0.2311129714638572\n",
      "Epoch 9/100\n",
      "Training Loss: 0.32514339007725324\n",
      "Validation Loss: 0.302096550533856\n",
      "Epoch 10/100\n",
      "Training Loss: 0.27735311844599914\n",
      "Validation Loss: 0.23899618636623704\n",
      "Epoch 11/100\n",
      "Training Loss: 0.2933533252390118\n",
      "Validation Loss: 0.29207964934603553\n",
      "Epoch 12/100\n",
      "Training Loss: 0.22742839560210948\n",
      "Validation Loss: 0.2219094506504963\n",
      "Epoch 13/100\n",
      "Training Loss: 0.2234164263619506\n",
      "Validation Loss: 0.19855346520859932\n",
      "Epoch 14/100\n",
      "Training Loss: 0.3219170413414606\n",
      "Validation Loss: 0.20914269664792345\n",
      "Epoch 15/100\n",
      "Training Loss: 0.26382454105512326\n",
      "Validation Loss: 0.15187137730047578\n",
      "Epoch 16/100\n",
      "Training Loss: 0.26188526473751034\n",
      "Validation Loss: 0.24957813627571598\n",
      "Epoch 17/100\n",
      "Training Loss: 0.3053798979286571\n",
      "Validation Loss: 0.3431904092515381\n",
      "Epoch 18/100\n",
      "Training Loss: 0.22957253329897606\n",
      "Validation Loss: 0.08241715902345563\n",
      "Epoch 19/100\n",
      "Training Loss: 0.23980464655061462\n",
      "Validation Loss: 0.21105082277378498\n",
      "Epoch 20/100\n",
      "Training Loss: 0.19047903263467095\n",
      "Validation Loss: 0.1827849288420989\n",
      "Epoch 21/100\n",
      "Training Loss: 0.2568231005862979\n",
      "Validation Loss: 0.21261665582221262\n",
      "Epoch 22/100\n",
      "Training Loss: 0.23291715697725468\n",
      "Validation Loss: 0.18612717099852955\n",
      "Epoch 23/100\n",
      "Training Loss: 0.19661607856629038\n",
      "Validation Loss: 0.25760037531208546\n",
      "Epoch 24/100\n",
      "Training Loss: 0.23759438793376017\n",
      "Validation Loss: 0.16958826290495183\n",
      "Epoch 25/100\n",
      "Training Loss: 0.23844070975035525\n",
      "Validation Loss: 0.13065036480859754\n",
      "Epoch 26/100\n",
      "Training Loss: 0.19933241310691768\n",
      "Validation Loss: 0.18174368489868215\n",
      "Epoch 27/100\n",
      "Training Loss: 0.25528810972059224\n",
      "Validation Loss: 0.12650732332582706\n",
      "Epoch 28/100\n",
      "Training Loss: 0.182303599219078\n",
      "Validation Loss: 0.18872008346765012\n",
      "Epoch 29/100\n",
      "Training Loss: 0.2007079728283775\n",
      "Validation Loss: 0.1373856628030353\n",
      "Epoch 30/100\n",
      "Training Loss: 0.18487564514435023\n",
      "Validation Loss: 0.19935407945895592\n",
      "Epoch 31/100\n",
      "Training Loss: 0.22632812542116895\n",
      "Validation Loss: 0.1207982665153039\n",
      "Epoch 32/100\n",
      "Training Loss: 0.19383329502754168\n",
      "Validation Loss: 0.1737103486714708\n",
      "Epoch 33/100\n",
      "Training Loss: 0.21380638753330444\n",
      "Validation Loss: 0.14151948352238403\n",
      "Epoch 34/100\n",
      "Training Loss: 0.1900252052145656\n",
      "Validation Loss: 0.06217692631905495\n",
      "Epoch 35/100\n",
      "Training Loss: 0.15984147128014628\n",
      "Validation Loss: 0.22133560159677718\n",
      "Epoch 36/100\n",
      "Training Loss: 0.2033384511629747\n",
      "Validation Loss: 0.08870260843020443\n",
      "Epoch 37/100\n",
      "Training Loss: 0.18216231013696216\n",
      "Validation Loss: 0.12223377582676505\n",
      "Epoch 38/100\n",
      "Training Loss: 0.22375868134946444\n",
      "Validation Loss: 0.21248067596340264\n",
      "Epoch 39/100\n",
      "Training Loss: 0.1952209881177687\n",
      "Validation Loss: 0.13934824418056352\n",
      "Epoch 40/100\n",
      "Training Loss: 0.19085339180280128\n",
      "Validation Loss: 0.10162211929352183\n",
      "Epoch 41/100\n",
      "Training Loss: 0.17637065972027574\n",
      "Validation Loss: 0.15706440054375098\n",
      "Epoch 42/100\n",
      "Training Loss: 0.20485824848963086\n",
      "Validation Loss: 0.23548939200589275\n",
      "Epoch 43/100\n",
      "Training Loss: 0.1518047268946291\n",
      "Validation Loss: 0.1692873046155195\n",
      "Epoch 44/100\n",
      "Training Loss: 0.1846404842511108\n",
      "Validation Loss: 0.12571097350718152\n",
      "Epoch 45/100\n",
      "Training Loss: 0.16305345102747051\n",
      "Validation Loss: 0.09703157968264023\n",
      "Epoch 46/100\n",
      "Training Loss: 0.16208600128841055\n",
      "Validation Loss: 0.15985947839484932\n",
      "Epoch 47/100\n",
      "Training Loss: 0.16645939829210518\n",
      "Validation Loss: 0.137412552742132\n",
      "Epoch 48/100\n",
      "Training Loss: 0.15981927786226413\n",
      "Validation Loss: 0.13109035859210363\n",
      "Epoch 49/100\n",
      "Training Loss: 0.14270755291826812\n",
      "Validation Loss: 0.12100072843679285\n",
      "Epoch 50/100\n",
      "Training Loss: 0.156813693633265\n",
      "Validation Loss: 0.1466250216094843\n",
      "Epoch 51/100\n",
      "Training Loss: 0.16190805246780438\n",
      "Validation Loss: 0.10029866731183079\n",
      "Epoch 52/100\n",
      "Training Loss: 0.1881080784504289\n",
      "Validation Loss: 0.13531480061436804\n",
      "Epoch 53/100\n",
      "Training Loss: 0.13223694712455458\n",
      "Validation Loss: 0.19055629324202378\n",
      "Epoch 54/100\n",
      "Training Loss: 0.15039288759642513\n",
      "Validation Loss: 0.0884151764364677\n",
      "Epoch 55/100\n",
      "Training Loss: 0.19828112670129402\n",
      "Validation Loss: 0.12404413864874535\n",
      "Epoch 56/100\n",
      "Training Loss: 0.15731986758811053\n",
      "Validation Loss: 0.15545763704446908\n",
      "Epoch 57/100\n",
      "Training Loss: 0.191283804624548\n",
      "Validation Loss: 0.09222011982956976\n",
      "Epoch 58/100\n",
      "Training Loss: 0.16614202494597644\n",
      "Validation Loss: 0.1377537149472877\n",
      "Epoch 59/100\n",
      "Training Loss: 0.14996745673681125\n",
      "Validation Loss: 0.17218237237613607\n",
      "Epoch 60/100\n",
      "Training Loss: 0.14615106086957108\n",
      "Validation Loss: 0.0771739483700433\n",
      "Epoch 61/100\n",
      "Training Loss: 0.17323088755614716\n",
      "Validation Loss: 0.1258506455879863\n",
      "Epoch 62/100\n",
      "Training Loss: 0.16770031238901037\n",
      "Validation Loss: 0.1337604465933052\n",
      "Epoch 63/100\n",
      "Training Loss: 0.19351271474545598\n",
      "Validation Loss: 0.09603777277885897\n",
      "Epoch 64/100\n",
      "Training Loss: 0.15438797236576898\n",
      "Validation Loss: 0.11618563397237747\n",
      "Epoch 65/100\n",
      "Training Loss: 0.16213847200461695\n",
      "Validation Loss: 0.1822875183319302\n",
      "Epoch 66/100\n",
      "Training Loss: 0.131058895102152\n",
      "Validation Loss: 0.13536321691054617\n",
      "Epoch 67/100\n",
      "Training Loss: 0.1545383367304862\n",
      "Validation Loss: 0.07380135912869207\n",
      "Epoch 68/100\n",
      "Training Loss: 0.14828985051563334\n",
      "Validation Loss: 0.14714682270800017\n",
      "Epoch 69/100\n",
      "Training Loss: 0.15745547065834994\n",
      "Validation Loss: 0.11343351103982328\n",
      "Epoch 70/100\n",
      "Training Loss: 0.15029442345101562\n",
      "Validation Loss: 0.09044400506701575\n",
      "Epoch 71/100\n",
      "Training Loss: 0.17293899081972236\n",
      "Validation Loss: 0.06283544204533974\n",
      "Epoch 72/100\n",
      "Training Loss: 0.14380998720013927\n",
      "Validation Loss: 0.08809772792234799\n",
      "Epoch 73/100\n",
      "Training Loss: 0.13285477071074928\n",
      "Validation Loss: 0.11347494286674745\n",
      "Epoch 74/100\n",
      "Training Loss: 0.14622279834731447\n",
      "Validation Loss: 0.1330239895059045\n",
      "Epoch 75/100\n",
      "Training Loss: 0.13347805029564386\n",
      "Validation Loss: 0.1696188958337783\n",
      "Epoch 76/100\n",
      "Training Loss: 0.1274842847757012\n",
      "Validation Loss: 0.08732557812432415\n",
      "Epoch 77/100\n",
      "Training Loss: 0.14260316720990956\n",
      "Validation Loss: 0.1346794875175352\n",
      "Epoch 78/100\n",
      "Training Loss: 0.13571249337304409\n",
      "Validation Loss: 0.07306024614785449\n",
      "Epoch 79/100\n",
      "Training Loss: 0.15116186592633968\n",
      "Validation Loss: 0.10932066671849985\n",
      "Epoch 80/100\n",
      "Training Loss: 0.15976808262507608\n",
      "Validation Loss: 0.134330422693058\n",
      "Epoch 81/100\n",
      "Training Loss: 0.15529767414737566\n",
      "Validation Loss: 0.07884978227800939\n",
      "Epoch 82/100\n",
      "Training Loss: 0.1539906567362077\n",
      "Validation Loss: 0.08319427822004874\n",
      "Epoch 83/100\n",
      "Training Loss: 0.13527276347411302\n",
      "Validation Loss: 0.10928760638773316\n",
      "Epoch 84/100\n",
      "Training Loss: 0.13022214213374803\n",
      "Validation Loss: 0.1005001478044879\n",
      "Epoch 85/100\n",
      "Training Loss: 0.11370279057069148\n",
      "Validation Loss: 0.062267008816966916\n",
      "Epoch 86/100\n",
      "Training Loss: 0.13886770608294888\n",
      "Validation Loss: 0.10529916143369403\n",
      "Epoch 87/100\n",
      "Training Loss: 0.14912492442799444\n",
      "Validation Loss: 0.07345953922742346\n",
      "Epoch 88/100\n",
      "Training Loss: 0.1095093139837055\n",
      "Validation Loss: 0.10388448373224497\n",
      "Epoch 89/100\n",
      "Training Loss: 0.16436843270796397\n",
      "Validation Loss: 0.15755874092057173\n",
      "Epoch 90/100\n",
      "Training Loss: 0.11873765044072004\n",
      "Validation Loss: 0.10582738911756102\n",
      "Epoch 91/100\n",
      "Training Loss: 0.12942181479848297\n",
      "Validation Loss: 0.1439779737370255\n",
      "Epoch 92/100\n",
      "Training Loss: 0.13143734212929864\n",
      "Validation Loss: 0.0940429709818095\n",
      "Epoch 93/100\n",
      "Training Loss: 0.11750987438857383\n",
      "Validation Loss: 0.10202881323646437\n",
      "Epoch 94/100\n",
      "Training Loss: 0.1135738715804206\n",
      "Validation Loss: 0.12552664657239426\n",
      "Epoch 95/100\n",
      "Training Loss: 0.13026299904526623\n",
      "Validation Loss: 0.10846053759281978\n",
      "Epoch 96/100\n",
      "Training Loss: 0.13410483782317148\n",
      "Validation Loss: 0.10293042861421438\n",
      "Epoch 97/100\n",
      "Training Loss: 0.1447336318233551\n",
      "Validation Loss: 0.06820996793511223\n",
      "Epoch 98/100\n",
      "Training Loss: 0.11335461078863389\n",
      "Validation Loss: 0.07622293172044949\n",
      "Epoch 99/100\n",
      "Training Loss: 0.12114935654597599\n",
      "Validation Loss: 0.06769503540733668\n",
      "Epoch 100/100\n",
      "Training Loss: 0.15334164338856776\n",
      "Validation Loss: 0.06017180956822753\n",
      "Combination 13: Avg Training Loss = 0.1950021817963357, Avg Validation Loss = 0.15448755141911885\n",
      "Testing combination 14/48: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0001, 'sequence_length': 6, 'dropout_rate': 0.2}\n",
      "Truncated Train Data Shape: (70, 6, 1)\n",
      "Truncated Validation Data Shape: (10, 6, 1)\n",
      "    Trial 1/2 for combination 14/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.437762964556659\n",
      "Validation Loss: 0.32932813295704144\n",
      "Epoch 2/100\n",
      "Training Loss: 0.4134964253283978\n",
      "Validation Loss: 0.4790725385242152\n",
      "Epoch 3/100\n",
      "Training Loss: 0.4025326340682293\n",
      "Validation Loss: 0.22865366772282178\n",
      "Epoch 4/100\n",
      "Training Loss: 0.32303945853368604\n",
      "Validation Loss: 0.2904169776729434\n",
      "Epoch 5/100\n",
      "Training Loss: 0.33352971789688396\n",
      "Validation Loss: 0.37022088833234784\n",
      "Epoch 6/100\n",
      "Training Loss: 0.2923572598956688\n",
      "Validation Loss: 0.22610275077724418\n",
      "Epoch 7/100\n",
      "Training Loss: 0.17984596561497013\n",
      "Validation Loss: 0.2930121098511028\n",
      "Epoch 8/100\n",
      "Training Loss: 0.22590232295830803\n",
      "Validation Loss: 0.14269119672023697\n",
      "Epoch 9/100\n",
      "Training Loss: 0.2056219447547197\n",
      "Validation Loss: 0.2340748287890249\n",
      "Epoch 10/100\n",
      "Training Loss: 0.23316407943089915\n",
      "Validation Loss: 0.2952461676725007\n",
      "Epoch 11/100\n",
      "Training Loss: 0.2227586063101634\n",
      "Validation Loss: 0.16142767735996405\n",
      "Epoch 12/100\n",
      "Training Loss: 0.27030528973680024\n",
      "Validation Loss: 0.29586854716578415\n",
      "Epoch 13/100\n",
      "Training Loss: 0.19316713013040296\n",
      "Validation Loss: 0.33750993865489404\n",
      "Epoch 14/100\n",
      "Training Loss: 0.2321423178883377\n",
      "Validation Loss: 0.13432400760539903\n",
      "Epoch 15/100\n",
      "Training Loss: 0.2383697256094538\n",
      "Validation Loss: 0.1539281979592152\n",
      "Epoch 16/100\n",
      "Training Loss: 0.16454078120288151\n",
      "Validation Loss: 0.1374349792054425\n",
      "Epoch 17/100\n",
      "Training Loss: 0.20650602124783074\n",
      "Validation Loss: 0.23538526910162433\n",
      "Epoch 18/100\n",
      "Training Loss: 0.20978330700859738\n",
      "Validation Loss: 0.2524244123981327\n",
      "Epoch 19/100\n",
      "Training Loss: 0.20941695017448855\n",
      "Validation Loss: 0.18304028097019392\n",
      "Epoch 20/100\n",
      "Training Loss: 0.1974888978059568\n",
      "Validation Loss: 0.23033274103577606\n",
      "Epoch 21/100\n",
      "Training Loss: 0.15624952166619127\n",
      "Validation Loss: 0.27803302804732194\n",
      "Epoch 22/100\n",
      "Training Loss: 0.22302085646315392\n",
      "Validation Loss: 0.19083340967245419\n",
      "Epoch 23/100\n",
      "Training Loss: 0.1913664882301544\n",
      "Validation Loss: 0.16497796222745237\n",
      "Epoch 24/100\n",
      "Training Loss: 0.1927514805992602\n",
      "Validation Loss: 0.43634245236325836\n",
      "Epoch 25/100\n",
      "Training Loss: 0.21279355249059517\n",
      "Validation Loss: 0.09897194129352926\n",
      "Epoch 26/100\n",
      "Training Loss: 0.1684523336604007\n",
      "Validation Loss: 0.14213290285906308\n",
      "Epoch 27/100\n",
      "Training Loss: 0.16501567822324217\n",
      "Validation Loss: 0.1455157071307662\n",
      "Epoch 28/100\n",
      "Training Loss: 0.21086655100229157\n",
      "Validation Loss: 0.1413391752881204\n",
      "Epoch 29/100\n",
      "Training Loss: 0.16830760693081137\n",
      "Validation Loss: 0.13570398471304118\n",
      "Epoch 30/100\n",
      "Training Loss: 0.17122527503561216\n",
      "Validation Loss: 0.1909823156587541\n",
      "Epoch 31/100\n",
      "Training Loss: 0.1581251234513571\n",
      "Validation Loss: 0.1544965316863361\n",
      "Epoch 32/100\n",
      "Training Loss: 0.15321383304201353\n",
      "Validation Loss: 0.07940457193662051\n",
      "Epoch 33/100\n",
      "Training Loss: 0.17064803633329295\n",
      "Validation Loss: 0.1283393713536766\n",
      "Epoch 34/100\n",
      "Training Loss: 0.1412535166659467\n",
      "Validation Loss: 0.15252119567826328\n",
      "Epoch 35/100\n",
      "Training Loss: 0.19130081474591595\n",
      "Validation Loss: 0.10353667742775316\n",
      "Epoch 36/100\n",
      "Training Loss: 0.15743586513147845\n",
      "Validation Loss: 0.1424985589661139\n",
      "Epoch 37/100\n",
      "Training Loss: 0.15121751379983345\n",
      "Validation Loss: 0.08807268104300324\n",
      "Epoch 38/100\n",
      "Training Loss: 0.1598031585492357\n",
      "Validation Loss: 0.12845287526220067\n",
      "Epoch 39/100\n",
      "Training Loss: 0.1572586765706616\n",
      "Validation Loss: 0.08909783818578706\n",
      "Epoch 40/100\n",
      "Training Loss: 0.14734100739593484\n",
      "Validation Loss: 0.20386772757114727\n",
      "Epoch 41/100\n",
      "Training Loss: 0.1290947766976379\n",
      "Validation Loss: 0.10804358545763493\n",
      "Epoch 42/100\n",
      "Training Loss: 0.17960014276453387\n",
      "Validation Loss: 0.2075971117150539\n",
      "Epoch 43/100\n",
      "Training Loss: 0.15085602235018544\n",
      "Validation Loss: 0.11711483827338073\n",
      "Epoch 44/100\n",
      "Training Loss: 0.1461363731756478\n",
      "Validation Loss: 0.15856316990315444\n",
      "Epoch 45/100\n",
      "Training Loss: 0.13137311769022383\n",
      "Validation Loss: 0.1322519371076723\n",
      "Epoch 46/100\n",
      "Training Loss: 0.1434605812183788\n",
      "Validation Loss: 0.12711553935053968\n",
      "Epoch 47/100\n",
      "Training Loss: 0.13771924712882158\n",
      "Validation Loss: 0.12470155606509219\n",
      "Epoch 48/100\n",
      "Training Loss: 0.1488061773071686\n",
      "Validation Loss: 0.1748307439916789\n",
      "Epoch 49/100\n",
      "Training Loss: 0.15982772281939234\n",
      "Validation Loss: 0.09004172007819446\n",
      "Epoch 50/100\n",
      "Training Loss: 0.12437993089766573\n",
      "Validation Loss: 0.10426013681370531\n",
      "Epoch 51/100\n",
      "Training Loss: 0.1488293624839359\n",
      "Validation Loss: 0.12431393061645708\n",
      "Epoch 52/100\n",
      "Training Loss: 0.12230956691217652\n",
      "Validation Loss: 0.11413346704044658\n",
      "Epoch 53/100\n",
      "Training Loss: 0.12290679811926855\n",
      "Validation Loss: 0.0671913778201351\n",
      "Epoch 54/100\n",
      "Training Loss: 0.14295593558881542\n",
      "Validation Loss: 0.13926046588933377\n",
      "Epoch 55/100\n",
      "Training Loss: 0.1553869371734594\n",
      "Validation Loss: 0.13878002690661742\n",
      "Epoch 56/100\n",
      "Training Loss: 0.14590384881583982\n",
      "Validation Loss: 0.13172459621135515\n",
      "Epoch 57/100\n",
      "Training Loss: 0.1226494190897644\n",
      "Validation Loss: 0.11520934703130925\n",
      "Epoch 58/100\n",
      "Training Loss: 0.13307887356133558\n",
      "Validation Loss: 0.09038189188003347\n",
      "Epoch 59/100\n",
      "Training Loss: 0.13152502041252964\n",
      "Validation Loss: 0.10647584689912278\n",
      "Epoch 60/100\n",
      "Training Loss: 0.11919113986112634\n",
      "Validation Loss: 0.08704229219081414\n",
      "Epoch 61/100\n",
      "Training Loss: 0.12419169682755574\n",
      "Validation Loss: 0.12813978896931816\n",
      "Epoch 62/100\n",
      "Training Loss: 0.1136520833096571\n",
      "Validation Loss: 0.0995092216049716\n",
      "Epoch 63/100\n",
      "Training Loss: 0.13399941528049417\n",
      "Validation Loss: 0.13990489394701458\n",
      "Epoch 64/100\n",
      "Training Loss: 0.11827086438608832\n",
      "Validation Loss: 0.08088948407972965\n",
      "Epoch 65/100\n",
      "Training Loss: 0.13074856765772097\n",
      "Validation Loss: 0.13616162187119435\n",
      "Epoch 66/100\n",
      "Training Loss: 0.1085591246303947\n",
      "Validation Loss: 0.16194819442206795\n",
      "Epoch 67/100\n",
      "Training Loss: 0.13915666356143072\n",
      "Validation Loss: 0.09316776347818302\n",
      "Epoch 68/100\n",
      "Training Loss: 0.1150380747309839\n",
      "Validation Loss: 0.11102056089238851\n",
      "Epoch 69/100\n",
      "Training Loss: 0.1211898063729574\n",
      "Validation Loss: 0.08301416905806197\n",
      "Epoch 70/100\n",
      "Training Loss: 0.12027888107395375\n",
      "Validation Loss: 0.0738650982186452\n",
      "Epoch 71/100\n",
      "Training Loss: 0.11952731290832481\n",
      "Validation Loss: 0.09246218186759819\n",
      "Epoch 72/100\n",
      "Training Loss: 0.11732862443113606\n",
      "Validation Loss: 0.07830806360907172\n",
      "Epoch 73/100\n",
      "Training Loss: 0.12296051643047005\n",
      "Validation Loss: 0.08592314717562917\n",
      "Epoch 74/100\n",
      "Training Loss: 0.13662550512095997\n",
      "Validation Loss: 0.1379460130992305\n",
      "Epoch 75/100\n",
      "Training Loss: 0.12374335329287474\n",
      "Validation Loss: 0.08080463429909114\n",
      "Epoch 76/100\n",
      "Training Loss: 0.12149501492648862\n",
      "Validation Loss: 0.05847519718880748\n",
      "Epoch 77/100\n",
      "Training Loss: 0.11593710303872981\n",
      "Validation Loss: 0.08560572407413372\n",
      "Epoch 78/100\n",
      "Training Loss: 0.10479358371127191\n",
      "Validation Loss: 0.08837781634931059\n",
      "Epoch 79/100\n",
      "Training Loss: 0.10674462434073854\n",
      "Validation Loss: 0.07982595334383225\n",
      "Epoch 80/100\n",
      "Training Loss: 0.11011395548005151\n",
      "Validation Loss: 0.07407167299991999\n",
      "Epoch 81/100\n",
      "Training Loss: 0.10240830525859529\n",
      "Validation Loss: 0.11985131272122478\n",
      "Epoch 82/100\n",
      "Training Loss: 0.10854977360853557\n",
      "Validation Loss: 0.06190771335481916\n",
      "Epoch 83/100\n",
      "Training Loss: 0.10730998327370724\n",
      "Validation Loss: 0.07120744365152835\n",
      "Epoch 84/100\n",
      "Training Loss: 0.12186066689746991\n",
      "Validation Loss: 0.06187191931945145\n",
      "Epoch 85/100\n",
      "Training Loss: 0.12108950340481486\n",
      "Validation Loss: 0.07115603887809396\n",
      "Epoch 86/100\n",
      "Training Loss: 0.115783199034534\n",
      "Validation Loss: 0.06263183917811092\n",
      "Epoch 87/100\n",
      "Training Loss: 0.11824361535213411\n",
      "Validation Loss: 0.06902465425301132\n",
      "Epoch 88/100\n",
      "Training Loss: 0.1088345529445899\n",
      "Validation Loss: 0.06355297379130766\n",
      "Epoch 89/100\n",
      "Training Loss: 0.11116909730086844\n",
      "Validation Loss: 0.09947609084649671\n",
      "Epoch 90/100\n",
      "Training Loss: 0.11157632682457104\n",
      "Validation Loss: 0.08895564893724628\n",
      "Epoch 91/100\n",
      "Training Loss: 0.10506941662775593\n",
      "Validation Loss: 0.09169543803834887\n",
      "Epoch 92/100\n",
      "Training Loss: 0.10495653266046319\n",
      "Validation Loss: 0.08105406989331604\n",
      "Epoch 93/100\n",
      "Training Loss: 0.09642362560539101\n",
      "Validation Loss: 0.0830177210161937\n",
      "Epoch 94/100\n",
      "Training Loss: 0.10601381389337128\n",
      "Validation Loss: 0.09826710916934432\n",
      "Epoch 95/100\n",
      "Training Loss: 0.09836684884037653\n",
      "Validation Loss: 0.05920730422158808\n",
      "Epoch 96/100\n",
      "Training Loss: 0.10008211079493216\n",
      "Validation Loss: 0.06248543023435286\n",
      "Epoch 97/100\n",
      "Training Loss: 0.1230525770103127\n",
      "Validation Loss: 0.049098459895825265\n",
      "Epoch 98/100\n",
      "Training Loss: 0.11041595151269903\n",
      "Validation Loss: 0.06162557885339513\n",
      "Epoch 99/100\n",
      "Training Loss: 0.09797609859539273\n",
      "Validation Loss: 0.08387553635170628\n",
      "Epoch 100/100\n",
      "Training Loss: 0.10571778454352175\n",
      "Validation Loss: 0.06941780835662112\n",
      "    Trial 2/2 for combination 14/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.511183649398447\n",
      "Validation Loss: 0.5646575089284245\n",
      "Epoch 2/100\n",
      "Training Loss: 0.38659144262031137\n",
      "Validation Loss: 0.1979973000081276\n",
      "Epoch 3/100\n",
      "Training Loss: 0.3807567625382928\n",
      "Validation Loss: 0.4458151880865877\n",
      "Epoch 4/100\n",
      "Training Loss: 0.3624987759239844\n",
      "Validation Loss: 0.23507744695753224\n",
      "Epoch 5/100\n",
      "Training Loss: 0.2907659157255536\n",
      "Validation Loss: 0.32959227065548174\n",
      "Epoch 6/100\n",
      "Training Loss: 0.3044057351878121\n",
      "Validation Loss: 0.1381114552668675\n",
      "Epoch 7/100\n",
      "Training Loss: 0.2534858150472674\n",
      "Validation Loss: 0.2226010239664539\n",
      "Epoch 8/100\n",
      "Training Loss: 0.2868409422188322\n",
      "Validation Loss: 0.334153134979288\n",
      "Epoch 9/100\n",
      "Training Loss: 0.2632971097886653\n",
      "Validation Loss: 0.2290958987915502\n",
      "Epoch 10/100\n",
      "Training Loss: 0.22316832107838114\n",
      "Validation Loss: 0.24460472070243466\n",
      "Epoch 11/100\n",
      "Training Loss: 0.19424933594756946\n",
      "Validation Loss: 0.11240778879200226\n",
      "Epoch 12/100\n",
      "Training Loss: 0.23695203010430765\n",
      "Validation Loss: 0.20757588038952016\n",
      "Epoch 13/100\n",
      "Training Loss: 0.20104554415699982\n",
      "Validation Loss: 0.13701307196043438\n",
      "Epoch 14/100\n",
      "Training Loss: 0.24816543866214041\n",
      "Validation Loss: 0.10106056285606897\n",
      "Epoch 15/100\n",
      "Training Loss: 0.2008815867009527\n",
      "Validation Loss: 0.2832749120491857\n",
      "Epoch 16/100\n",
      "Training Loss: 0.2339376026599346\n",
      "Validation Loss: 0.14975263205144856\n",
      "Epoch 17/100\n",
      "Training Loss: 0.18276026898448666\n",
      "Validation Loss: 0.19810116790209314\n",
      "Epoch 18/100\n",
      "Training Loss: 0.20440228520510972\n",
      "Validation Loss: 0.15089112617124772\n",
      "Epoch 19/100\n",
      "Training Loss: 0.1753512410898691\n",
      "Validation Loss: 0.20143442554750499\n",
      "Epoch 20/100\n",
      "Training Loss: 0.21635860684133235\n",
      "Validation Loss: 0.16659109760991933\n",
      "Epoch 21/100\n",
      "Training Loss: 0.21003582189618847\n",
      "Validation Loss: 0.14640719029897536\n",
      "Epoch 22/100\n",
      "Training Loss: 0.18420599203137691\n",
      "Validation Loss: 0.23795761535768895\n",
      "Epoch 23/100\n",
      "Training Loss: 0.2233048987898738\n",
      "Validation Loss: 0.3073493106183625\n",
      "Epoch 24/100\n",
      "Training Loss: 0.20638438230209008\n",
      "Validation Loss: 0.20335421248397498\n",
      "Epoch 25/100\n",
      "Training Loss: 0.1740673849226165\n",
      "Validation Loss: 0.12423125718927144\n",
      "Epoch 26/100\n",
      "Training Loss: 0.16514577646281686\n",
      "Validation Loss: 0.17896613181456933\n",
      "Epoch 27/100\n",
      "Training Loss: 0.16739242199973448\n",
      "Validation Loss: 0.1701424203818341\n",
      "Epoch 28/100\n",
      "Training Loss: 0.19451659080391961\n",
      "Validation Loss: 0.18147896520278173\n",
      "Epoch 29/100\n",
      "Training Loss: 0.18868163656715906\n",
      "Validation Loss: 0.1177349301454381\n",
      "Epoch 30/100\n",
      "Training Loss: 0.19730131275736604\n",
      "Validation Loss: 0.07517024054649052\n",
      "Epoch 31/100\n",
      "Training Loss: 0.17814615875702824\n",
      "Validation Loss: 0.09838632365660975\n",
      "Epoch 32/100\n",
      "Training Loss: 0.16150408622990234\n",
      "Validation Loss: 0.21288841661502525\n",
      "Epoch 33/100\n",
      "Training Loss: 0.1573773279804782\n",
      "Validation Loss: 0.12964025215148217\n",
      "Epoch 34/100\n",
      "Training Loss: 0.15377310158676996\n",
      "Validation Loss: 0.21535621619190484\n",
      "Epoch 35/100\n",
      "Training Loss: 0.146755628607024\n",
      "Validation Loss: 0.12217284371415851\n",
      "Epoch 36/100\n",
      "Training Loss: 0.17328060126594175\n",
      "Validation Loss: 0.2047946106221039\n",
      "Epoch 37/100\n",
      "Training Loss: 0.16780557587873546\n",
      "Validation Loss: 0.16585877087243425\n",
      "Epoch 38/100\n",
      "Training Loss: 0.16038824260880094\n",
      "Validation Loss: 0.1976329376739659\n",
      "Epoch 39/100\n",
      "Training Loss: 0.13940125385167684\n",
      "Validation Loss: 0.17874711758277337\n",
      "Epoch 40/100\n",
      "Training Loss: 0.14237631951576582\n",
      "Validation Loss: 0.1718996151361315\n",
      "Epoch 41/100\n",
      "Training Loss: 0.14409777536834328\n",
      "Validation Loss: 0.11833025633761071\n",
      "Epoch 42/100\n",
      "Training Loss: 0.14456506083430953\n",
      "Validation Loss: 0.14867706170920814\n",
      "Epoch 43/100\n",
      "Training Loss: 0.16646426706364184\n",
      "Validation Loss: 0.08128819484022096\n",
      "Epoch 44/100\n",
      "Training Loss: 0.16825200401541884\n",
      "Validation Loss: 0.1363102551660281\n",
      "Epoch 45/100\n",
      "Training Loss: 0.14216797554380345\n",
      "Validation Loss: 0.15427305467188307\n",
      "Epoch 46/100\n",
      "Training Loss: 0.1348378130445136\n",
      "Validation Loss: 0.15035699810357578\n",
      "Epoch 47/100\n",
      "Training Loss: 0.14512595721166097\n",
      "Validation Loss: 0.11724486573482906\n",
      "Epoch 48/100\n",
      "Training Loss: 0.15443455913454118\n",
      "Validation Loss: 0.10008536226140667\n",
      "Epoch 49/100\n",
      "Training Loss: 0.1418418619350783\n",
      "Validation Loss: 0.10627154670759409\n",
      "Epoch 50/100\n",
      "Training Loss: 0.13991452128275483\n",
      "Validation Loss: 0.10726733664142322\n",
      "Epoch 51/100\n",
      "Training Loss: 0.14219292204119377\n",
      "Validation Loss: 0.09645288107489831\n",
      "Epoch 52/100\n",
      "Training Loss: 0.12518685304989519\n",
      "Validation Loss: 0.08444119446912443\n",
      "Epoch 53/100\n",
      "Training Loss: 0.14348978637613247\n",
      "Validation Loss: 0.1284056416847724\n",
      "Epoch 54/100\n",
      "Training Loss: 0.1551435490094643\n",
      "Validation Loss: 0.10836182492562181\n",
      "Epoch 55/100\n",
      "Training Loss: 0.1413822801441576\n",
      "Validation Loss: 0.10883474631812931\n",
      "Epoch 56/100\n",
      "Training Loss: 0.1446114529486989\n",
      "Validation Loss: 0.08715234075690721\n",
      "Epoch 57/100\n",
      "Training Loss: 0.13375692958617658\n",
      "Validation Loss: 0.09858609466854996\n",
      "Epoch 58/100\n",
      "Training Loss: 0.13597332660510258\n",
      "Validation Loss: 0.1079412844810304\n",
      "Epoch 59/100\n",
      "Training Loss: 0.13095778350324888\n",
      "Validation Loss: 0.08937927888516539\n",
      "Epoch 60/100\n",
      "Training Loss: 0.13317441319638362\n",
      "Validation Loss: 0.11092839849365946\n",
      "Epoch 61/100\n",
      "Training Loss: 0.15151257955647404\n",
      "Validation Loss: 0.10435055128110349\n",
      "Epoch 62/100\n",
      "Training Loss: 0.12836837715823485\n",
      "Validation Loss: 0.06806475872785599\n",
      "Epoch 63/100\n",
      "Training Loss: 0.12485568329980602\n",
      "Validation Loss: 0.09032204163501958\n",
      "Epoch 64/100\n",
      "Training Loss: 0.10571475527328668\n",
      "Validation Loss: 0.10702588656021154\n",
      "Epoch 65/100\n",
      "Training Loss: 0.11963180041698718\n",
      "Validation Loss: 0.11065441280196567\n",
      "Epoch 66/100\n",
      "Training Loss: 0.1389468232618046\n",
      "Validation Loss: 0.11724771971537537\n",
      "Epoch 67/100\n",
      "Training Loss: 0.13165222396046797\n",
      "Validation Loss: 0.07555005398801058\n",
      "Epoch 68/100\n",
      "Training Loss: 0.13281860059682873\n",
      "Validation Loss: 0.12498342067943766\n",
      "Epoch 69/100\n",
      "Training Loss: 0.1151664358295472\n",
      "Validation Loss: 0.06365284314878819\n",
      "Epoch 70/100\n",
      "Training Loss: 0.13694011714573656\n",
      "Validation Loss: 0.11407826144130997\n",
      "Epoch 71/100\n",
      "Training Loss: 0.1193381518232842\n",
      "Validation Loss: 0.08728930788897299\n",
      "Epoch 72/100\n",
      "Training Loss: 0.14462320216809563\n",
      "Validation Loss: 0.1074137586157539\n",
      "Epoch 73/100\n",
      "Training Loss: 0.1220502872192009\n",
      "Validation Loss: 0.06777190971329962\n",
      "Epoch 74/100\n",
      "Training Loss: 0.12515429166306352\n",
      "Validation Loss: 0.07043482008833699\n",
      "Epoch 75/100\n",
      "Training Loss: 0.13101203399578623\n",
      "Validation Loss: 0.11483107913209792\n",
      "Epoch 76/100\n",
      "Training Loss: 0.1125530708040207\n",
      "Validation Loss: 0.08039389427681318\n",
      "Epoch 77/100\n",
      "Training Loss: 0.11794771740891909\n",
      "Validation Loss: 0.08338494117039265\n",
      "Epoch 78/100\n",
      "Training Loss: 0.12448589644925186\n",
      "Validation Loss: 0.06696075219775545\n",
      "Epoch 79/100\n",
      "Training Loss: 0.11009474168800663\n",
      "Validation Loss: 0.06885690906928606\n",
      "Epoch 80/100\n",
      "Training Loss: 0.12244498422017808\n",
      "Validation Loss: 0.07544643260106013\n",
      "Epoch 81/100\n",
      "Training Loss: 0.13126848976109443\n",
      "Validation Loss: 0.07095413296345486\n",
      "Epoch 82/100\n",
      "Training Loss: 0.11531644153886815\n",
      "Validation Loss: 0.07594955418470542\n",
      "Epoch 83/100\n",
      "Training Loss: 0.12187587137196237\n",
      "Validation Loss: 0.08182107764168174\n",
      "Epoch 84/100\n",
      "Training Loss: 0.11655339631774699\n",
      "Validation Loss: 0.11023168200251784\n",
      "Epoch 85/100\n",
      "Training Loss: 0.11738301392223745\n",
      "Validation Loss: 0.11134518105506967\n",
      "Epoch 86/100\n",
      "Training Loss: 0.13194702149369755\n",
      "Validation Loss: 0.10046320404614435\n",
      "Epoch 87/100\n",
      "Training Loss: 0.1259943046176979\n",
      "Validation Loss: 0.08494395147018459\n",
      "Epoch 88/100\n",
      "Training Loss: 0.11310918504877555\n",
      "Validation Loss: 0.0734465885784187\n",
      "Epoch 89/100\n",
      "Training Loss: 0.11130862495320927\n",
      "Validation Loss: 0.07158342236666576\n",
      "Epoch 90/100\n",
      "Training Loss: 0.12252808023362208\n",
      "Validation Loss: 0.08536992308183466\n",
      "Epoch 91/100\n",
      "Training Loss: 0.10775527120449148\n",
      "Validation Loss: 0.08685557155108643\n",
      "Epoch 92/100\n",
      "Training Loss: 0.11060044738910253\n",
      "Validation Loss: 0.05040121029299168\n",
      "Epoch 93/100\n",
      "Training Loss: 0.11167666193311902\n",
      "Validation Loss: 0.10925538215666888\n",
      "Epoch 94/100\n",
      "Training Loss: 0.11094140825866439\n",
      "Validation Loss: 0.07875671568082272\n",
      "Epoch 95/100\n",
      "Training Loss: 0.09733990727935257\n",
      "Validation Loss: 0.08171788720263026\n",
      "Epoch 96/100\n",
      "Training Loss: 0.11501161279251151\n",
      "Validation Loss: 0.06627219485682809\n",
      "Epoch 97/100\n",
      "Training Loss: 0.11454264972256871\n",
      "Validation Loss: 0.0607414232149047\n",
      "Epoch 98/100\n",
      "Training Loss: 0.10201008976914655\n",
      "Validation Loss: 0.07355091935378247\n",
      "Epoch 99/100\n",
      "Training Loss: 0.11557622076200288\n",
      "Validation Loss: 0.0585102503749881\n",
      "Epoch 100/100\n",
      "Training Loss: 0.11320322852128796\n",
      "Validation Loss: 0.06882123160479427\n",
      "Combination 14: Avg Training Loss = 0.16276195978562558, Avg Validation Loss = 0.13936861479932644\n",
      "Testing combination 15/48: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0001, 'sequence_length': 20, 'dropout_rate': 0.1}\n",
      "Truncated Train Data Shape: (70, 20, 1)\n",
      "Truncated Validation Data Shape: (10, 20, 1)\n",
      "    Trial 1/2 for combination 15/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.37613597048230185\n",
      "Validation Loss: 0.35284656876164683\n",
      "Epoch 2/100\n",
      "Training Loss: 0.37031698434840105\n",
      "Validation Loss: 0.6037873885099548\n",
      "Epoch 3/100\n",
      "Training Loss: 0.4055524534510533\n",
      "Validation Loss: 0.2449614725682076\n",
      "Epoch 4/100\n",
      "Training Loss: 0.3309128421835467\n",
      "Validation Loss: 0.35122709207791586\n",
      "Epoch 5/100\n",
      "Training Loss: 0.3028738288441235\n",
      "Validation Loss: 0.14830514897057098\n",
      "Epoch 6/100\n",
      "Training Loss: 0.21388812406205507\n",
      "Validation Loss: 0.12450039896093872\n",
      "Epoch 7/100\n",
      "Training Loss: 0.23415117417180292\n",
      "Validation Loss: 0.1254283042065242\n",
      "Epoch 8/100\n",
      "Training Loss: 0.22385030372250828\n",
      "Validation Loss: 0.09450508280502076\n",
      "Epoch 9/100\n",
      "Training Loss: 0.1719458673038082\n",
      "Validation Loss: 0.14255550110864781\n",
      "Epoch 10/100\n",
      "Training Loss: 0.2521327339237368\n",
      "Validation Loss: 0.217664574693221\n",
      "Epoch 11/100\n",
      "Training Loss: 0.21113363955096326\n",
      "Validation Loss: 0.1204728827527342\n",
      "Epoch 12/100\n",
      "Training Loss: 0.22424739809954608\n",
      "Validation Loss: 0.11646895865166543\n",
      "Epoch 13/100\n",
      "Training Loss: 0.1750865816688349\n",
      "Validation Loss: 0.12567892320145768\n",
      "Epoch 14/100\n",
      "Training Loss: 0.1633722648857279\n",
      "Validation Loss: 0.10170769437846336\n",
      "Epoch 15/100\n",
      "Training Loss: 0.17905816843681416\n",
      "Validation Loss: 0.17894901823990456\n",
      "Epoch 16/100\n",
      "Training Loss: 0.14281456489754515\n",
      "Validation Loss: 0.09974733854928475\n",
      "Epoch 17/100\n",
      "Training Loss: 0.16151004648495518\n",
      "Validation Loss: 0.0987346296778062\n",
      "Epoch 18/100\n",
      "Training Loss: 0.1508046104109017\n",
      "Validation Loss: 0.0737057591389593\n",
      "Epoch 19/100\n",
      "Training Loss: 0.13674758825586822\n",
      "Validation Loss: 0.15487245834342273\n",
      "Epoch 20/100\n",
      "Training Loss: 0.17182399382041208\n",
      "Validation Loss: 0.16388022251773213\n",
      "Epoch 21/100\n",
      "Training Loss: 0.15549513016979694\n",
      "Validation Loss: 0.14536228468502432\n",
      "Epoch 22/100\n",
      "Training Loss: 0.1516768468714874\n",
      "Validation Loss: 0.06817169594660998\n",
      "Epoch 23/100\n",
      "Training Loss: 0.14890277519387096\n",
      "Validation Loss: 0.05176090742123337\n",
      "Epoch 24/100\n",
      "Training Loss: 0.1553300151176215\n",
      "Validation Loss: 0.12962130876006075\n",
      "Epoch 25/100\n",
      "Training Loss: 0.12985727565280167\n",
      "Validation Loss: 0.15680832315396917\n",
      "Epoch 26/100\n",
      "Training Loss: 0.12954094267709068\n",
      "Validation Loss: 0.09872930470050931\n",
      "Epoch 27/100\n",
      "Training Loss: 0.14161761100030223\n",
      "Validation Loss: 0.12601739212245947\n",
      "Epoch 28/100\n",
      "Training Loss: 0.12161630329794382\n",
      "Validation Loss: 0.09787916542004857\n",
      "Epoch 29/100\n",
      "Training Loss: 0.15311144955909162\n",
      "Validation Loss: 0.05613147118062347\n",
      "Epoch 30/100\n",
      "Training Loss: 0.13264173512886096\n",
      "Validation Loss: 0.05450980809015327\n",
      "Epoch 31/100\n",
      "Training Loss: 0.13950920507823458\n",
      "Validation Loss: 0.07471993481305719\n",
      "Epoch 32/100\n",
      "Training Loss: 0.12729648328918686\n",
      "Validation Loss: 0.05636368509198934\n",
      "Epoch 33/100\n",
      "Training Loss: 0.1622755382086393\n",
      "Validation Loss: 0.06328941323723594\n",
      "Epoch 34/100\n",
      "Training Loss: 0.14460541523341805\n",
      "Validation Loss: 0.06366768801216574\n",
      "Epoch 35/100\n",
      "Training Loss: 0.13358944528951844\n",
      "Validation Loss: 0.050211196657082555\n",
      "Epoch 36/100\n",
      "Training Loss: 0.10475441159122438\n",
      "Validation Loss: 0.12655707843529745\n",
      "Epoch 37/100\n",
      "Training Loss: 0.11845217448895633\n",
      "Validation Loss: 0.09623176740484854\n",
      "Epoch 38/100\n",
      "Training Loss: 0.15699988470044846\n",
      "Validation Loss: 0.05437592598125333\n",
      "Epoch 39/100\n",
      "Training Loss: 0.10617307657152726\n",
      "Validation Loss: 0.0661124772619416\n",
      "Epoch 40/100\n",
      "Training Loss: 0.11327838022850564\n",
      "Validation Loss: 0.0837265845775658\n",
      "Epoch 41/100\n",
      "Training Loss: 0.13033462603662832\n",
      "Validation Loss: 0.09780455109401424\n",
      "Epoch 42/100\n",
      "Training Loss: 0.10609757657631444\n",
      "Validation Loss: 0.06645968460553739\n",
      "Epoch 43/100\n",
      "Training Loss: 0.1219921318110353\n",
      "Validation Loss: 0.04749130107415531\n",
      "Epoch 44/100\n",
      "Training Loss: 0.12399378890102157\n",
      "Validation Loss: 0.042715960771998934\n",
      "Epoch 45/100\n",
      "Training Loss: 0.12720765706912673\n",
      "Validation Loss: 0.07582386941896459\n",
      "Epoch 46/100\n",
      "Training Loss: 0.11486862895655768\n",
      "Validation Loss: 0.07644354590121746\n",
      "Epoch 47/100\n",
      "Training Loss: 0.13263660968574603\n",
      "Validation Loss: 0.0792830926432689\n",
      "Epoch 48/100\n",
      "Training Loss: 0.1349404717920153\n",
      "Validation Loss: 0.07922916226895896\n",
      "Epoch 49/100\n",
      "Training Loss: 0.12288350582028164\n",
      "Validation Loss: 0.08778573003902941\n",
      "Epoch 50/100\n",
      "Training Loss: 0.12450305687882256\n",
      "Validation Loss: 0.0633416661174381\n",
      "Epoch 51/100\n",
      "Training Loss: 0.138975682030152\n",
      "Validation Loss: 0.051436163665357935\n",
      "Epoch 52/100\n",
      "Training Loss: 0.10976917032498411\n",
      "Validation Loss: 0.05846257778834337\n",
      "Epoch 53/100\n",
      "Training Loss: 0.12399968089981465\n",
      "Validation Loss: 0.06040567280513196\n",
      "Epoch 54/100\n",
      "Training Loss: 0.11043226310435637\n",
      "Validation Loss: 0.08695515445817974\n",
      "Epoch 55/100\n",
      "Training Loss: 0.10594022627667406\n",
      "Validation Loss: 0.04770929945876725\n",
      "Epoch 56/100\n",
      "Training Loss: 0.13480096741465553\n",
      "Validation Loss: 0.10214637154160748\n",
      "Epoch 57/100\n",
      "Training Loss: 0.1141547615122222\n",
      "Validation Loss: 0.06263243925403396\n",
      "Epoch 58/100\n",
      "Training Loss: 0.0987629105721925\n",
      "Validation Loss: 0.07639116685353675\n",
      "Epoch 59/100\n",
      "Training Loss: 0.10623611640619707\n",
      "Validation Loss: 0.0782997885477273\n",
      "Epoch 60/100\n",
      "Training Loss: 0.10653975901885245\n",
      "Validation Loss: 0.07935745953120164\n",
      "Epoch 61/100\n",
      "Training Loss: 0.10500067282501563\n",
      "Validation Loss: 0.10115976910617816\n",
      "Epoch 62/100\n",
      "Training Loss: 0.1280628543884182\n",
      "Validation Loss: 0.062419805524680363\n",
      "Epoch 63/100\n",
      "Training Loss: 0.10631295127738764\n",
      "Validation Loss: 0.06957459801356979\n",
      "Epoch 64/100\n",
      "Training Loss: 0.09859718668035744\n",
      "Validation Loss: 0.10855796331266303\n",
      "Epoch 65/100\n",
      "Training Loss: 0.10329273171757655\n",
      "Validation Loss: 0.05012693122804246\n",
      "Epoch 66/100\n",
      "Training Loss: 0.1112308843421941\n",
      "Validation Loss: 0.07063117004770779\n",
      "Epoch 67/100\n",
      "Training Loss: 0.10674107987010419\n",
      "Validation Loss: 0.046892777664323086\n",
      "Epoch 68/100\n",
      "Training Loss: 0.12150401202001955\n",
      "Validation Loss: 0.051367886483364336\n",
      "Epoch 69/100\n",
      "Training Loss: 0.11037959311758964\n",
      "Validation Loss: 0.049992531016559275\n",
      "Epoch 70/100\n",
      "Training Loss: 0.09647351405243018\n",
      "Validation Loss: 0.056473734654556317\n",
      "Epoch 71/100\n",
      "Training Loss: 0.10977070400640332\n",
      "Validation Loss: 0.08023217479477873\n",
      "Epoch 72/100\n",
      "Training Loss: 0.08660177190018717\n",
      "Validation Loss: 0.06105574482104046\n",
      "Epoch 73/100\n",
      "Training Loss: 0.09309448493815216\n",
      "Validation Loss: 0.0652224729225634\n",
      "Epoch 74/100\n",
      "Training Loss: 0.09758371117357502\n",
      "Validation Loss: 0.06605620295851763\n",
      "Epoch 75/100\n",
      "Training Loss: 0.11091681117675563\n",
      "Validation Loss: 0.04616108530148696\n",
      "Epoch 76/100\n",
      "Training Loss: 0.09897893008786125\n",
      "Validation Loss: 0.12447331387087143\n",
      "Epoch 77/100\n",
      "Training Loss: 0.10219465499753437\n",
      "Validation Loss: 0.06221057670628051\n",
      "Epoch 78/100\n",
      "Training Loss: 0.08993433935237115\n",
      "Validation Loss: 0.04829258837771673\n",
      "Epoch 79/100\n",
      "Training Loss: 0.09664597260207426\n",
      "Validation Loss: 0.06874999787693432\n",
      "Epoch 80/100\n",
      "Training Loss: 0.10190713107287214\n",
      "Validation Loss: 0.05948792312247318\n",
      "Epoch 81/100\n",
      "Training Loss: 0.08690292865873682\n",
      "Validation Loss: 0.05521807671829328\n",
      "Epoch 82/100\n",
      "Training Loss: 0.09819840212267879\n",
      "Validation Loss: 0.04579092350358786\n",
      "Epoch 83/100\n",
      "Training Loss: 0.10377815104510556\n",
      "Validation Loss: 0.07914722534857475\n",
      "Epoch 84/100\n",
      "Training Loss: 0.09603440882878773\n",
      "Validation Loss: 0.045867574110974516\n",
      "Epoch 85/100\n",
      "Training Loss: 0.0874293670412625\n",
      "Validation Loss: 0.0665897865374156\n",
      "Epoch 86/100\n",
      "Training Loss: 0.0889313325296782\n",
      "Validation Loss: 0.052617619978514774\n",
      "Epoch 87/100\n",
      "Training Loss: 0.09178570971954036\n",
      "Validation Loss: 0.07823396503786068\n",
      "Epoch 88/100\n",
      "Training Loss: 0.10987489437865781\n",
      "Validation Loss: 0.05489759118718303\n",
      "Epoch 89/100\n",
      "Training Loss: 0.09605734378590797\n",
      "Validation Loss: 0.06845347540088158\n",
      "Epoch 90/100\n",
      "Training Loss: 0.09640164739920572\n",
      "Validation Loss: 0.058747686103393884\n",
      "Epoch 91/100\n",
      "Training Loss: 0.08676807804486303\n",
      "Validation Loss: 0.06191449489714003\n",
      "Epoch 92/100\n",
      "Training Loss: 0.09025579389581437\n",
      "Validation Loss: 0.0621924208423509\n",
      "Epoch 93/100\n",
      "Training Loss: 0.09178987384021434\n",
      "Validation Loss: 0.06640321125783667\n",
      "Epoch 94/100\n",
      "Training Loss: 0.08903139764710329\n",
      "Validation Loss: 0.030466941062102286\n",
      "Epoch 95/100\n",
      "Training Loss: 0.09285294313158036\n",
      "Validation Loss: 0.04613667230458926\n",
      "Epoch 96/100\n",
      "Training Loss: 0.10470720508519188\n",
      "Validation Loss: 0.05044011679198136\n",
      "Epoch 97/100\n",
      "Training Loss: 0.09491900504795382\n",
      "Validation Loss: 0.051191582559806895\n",
      "Epoch 98/100\n",
      "Training Loss: 0.08487105254287897\n",
      "Validation Loss: 0.06278677520382851\n",
      "Epoch 99/100\n",
      "Training Loss: 0.08998700710333002\n",
      "Validation Loss: 0.06796917096078749\n",
      "Epoch 100/100\n",
      "Training Loss: 0.09112412978539451\n",
      "Validation Loss: 0.052350063604572374\n",
      "    Trial 2/2 for combination 15/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.4887895967789167\n",
      "Validation Loss: 0.45900792905576077\n",
      "Epoch 2/100\n",
      "Training Loss: 0.47764494875207547\n",
      "Validation Loss: 0.2505325907056998\n",
      "Epoch 3/100\n",
      "Training Loss: 0.38093916433240654\n",
      "Validation Loss: 0.36686102948928206\n",
      "Epoch 4/100\n",
      "Training Loss: 0.33924683349660817\n",
      "Validation Loss: 0.10325131328606132\n",
      "Epoch 5/100\n",
      "Training Loss: 0.3412010177301355\n",
      "Validation Loss: 0.23652322681479135\n",
      "Epoch 6/100\n",
      "Training Loss: 0.26801568053120156\n",
      "Validation Loss: 0.2918964247530734\n",
      "Epoch 7/100\n",
      "Training Loss: 0.22541474992285676\n",
      "Validation Loss: 0.13811414040851522\n",
      "Epoch 8/100\n",
      "Training Loss: 0.27430536776799364\n",
      "Validation Loss: 0.12568426011470707\n",
      "Epoch 9/100\n",
      "Training Loss: 0.22215343820113698\n",
      "Validation Loss: 0.11679740237613945\n",
      "Epoch 10/100\n",
      "Training Loss: 0.22737940238384427\n",
      "Validation Loss: 0.15406900191795558\n",
      "Epoch 11/100\n",
      "Training Loss: 0.25653830537052263\n",
      "Validation Loss: 0.2033365103657922\n",
      "Epoch 12/100\n",
      "Training Loss: 0.20319978906088976\n",
      "Validation Loss: 0.1263133715809137\n",
      "Epoch 13/100\n",
      "Training Loss: 0.19529075673270968\n",
      "Validation Loss: 0.09888164519790767\n",
      "Epoch 14/100\n",
      "Training Loss: 0.22758849485017807\n",
      "Validation Loss: 0.10309661617027074\n",
      "Epoch 15/100\n",
      "Training Loss: 0.22271467919950805\n",
      "Validation Loss: 0.1297198884551176\n",
      "Epoch 16/100\n",
      "Training Loss: 0.2025029419640279\n",
      "Validation Loss: 0.11307723434737674\n",
      "Epoch 17/100\n",
      "Training Loss: 0.1883499650470616\n",
      "Validation Loss: 0.09632802462241605\n",
      "Epoch 18/100\n",
      "Training Loss: 0.1604819356428013\n",
      "Validation Loss: 0.1502982463113121\n",
      "Epoch 19/100\n",
      "Training Loss: 0.17719248936162968\n",
      "Validation Loss: 0.15121212296611736\n",
      "Epoch 20/100\n",
      "Training Loss: 0.1697473299391972\n",
      "Validation Loss: 0.12718585624153392\n",
      "Epoch 21/100\n",
      "Training Loss: 0.21405564689148673\n",
      "Validation Loss: 0.08860340693863157\n",
      "Epoch 22/100\n",
      "Training Loss: 0.1563538525364355\n",
      "Validation Loss: 0.0981146191584893\n",
      "Epoch 23/100\n",
      "Training Loss: 0.15784534998802083\n",
      "Validation Loss: 0.12165324699163563\n",
      "Epoch 24/100\n",
      "Training Loss: 0.1662337219029687\n",
      "Validation Loss: 0.1273261025379425\n",
      "Epoch 25/100\n",
      "Training Loss: 0.1692406006024283\n",
      "Validation Loss: 0.10476385103045023\n",
      "Epoch 26/100\n",
      "Training Loss: 0.16398798220175934\n",
      "Validation Loss: 0.10481072738283023\n",
      "Epoch 27/100\n",
      "Training Loss: 0.14720433688411472\n",
      "Validation Loss: 0.10304200486536826\n",
      "Epoch 28/100\n",
      "Training Loss: 0.15610906033871044\n",
      "Validation Loss: 0.09636569727209994\n",
      "Epoch 29/100\n",
      "Training Loss: 0.16959354535235957\n",
      "Validation Loss: 0.07007339213151782\n",
      "Epoch 30/100\n",
      "Training Loss: 0.15199521021418477\n",
      "Validation Loss: 0.10269540109151747\n",
      "Epoch 31/100\n",
      "Training Loss: 0.16369007162443058\n",
      "Validation Loss: 0.08590918935178946\n",
      "Epoch 32/100\n",
      "Training Loss: 0.15337674404046522\n",
      "Validation Loss: 0.09986169996829086\n",
      "Epoch 33/100\n",
      "Training Loss: 0.18664772448710654\n",
      "Validation Loss: 0.09186092323426746\n",
      "Epoch 34/100\n",
      "Training Loss: 0.15172146137923037\n",
      "Validation Loss: 0.08141744009084967\n",
      "Epoch 35/100\n",
      "Training Loss: 0.15195880274331183\n",
      "Validation Loss: 0.10775273766551749\n",
      "Epoch 36/100\n",
      "Training Loss: 0.14214170634655668\n",
      "Validation Loss: 0.12964800355405265\n",
      "Epoch 37/100\n",
      "Training Loss: 0.12433343289024358\n",
      "Validation Loss: 0.09995764492114709\n",
      "Epoch 38/100\n",
      "Training Loss: 0.13326407711975988\n",
      "Validation Loss: 0.07207722282384349\n",
      "Epoch 39/100\n",
      "Training Loss: 0.13769898339571557\n",
      "Validation Loss: 0.15958303316459968\n",
      "Epoch 40/100\n",
      "Training Loss: 0.13131357756103568\n",
      "Validation Loss: 0.08833789355766575\n",
      "Epoch 41/100\n",
      "Training Loss: 0.1412530112143906\n",
      "Validation Loss: 0.11075376829015995\n",
      "Epoch 42/100\n",
      "Training Loss: 0.13518443309261108\n",
      "Validation Loss: 0.1004334766363042\n",
      "Epoch 43/100\n",
      "Training Loss: 0.13761150069328315\n",
      "Validation Loss: 0.08212691608754229\n",
      "Epoch 44/100\n",
      "Training Loss: 0.13491317970716674\n",
      "Validation Loss: 0.08017890329838509\n",
      "Epoch 45/100\n",
      "Training Loss: 0.1446147421580847\n",
      "Validation Loss: 0.07148844484805346\n",
      "Epoch 46/100\n",
      "Training Loss: 0.1288976377671937\n",
      "Validation Loss: 0.0814653099958219\n",
      "Epoch 47/100\n",
      "Training Loss: 0.14288086013913112\n",
      "Validation Loss: 0.10392797081815089\n",
      "Epoch 48/100\n",
      "Training Loss: 0.12299066129064194\n",
      "Validation Loss: 0.0756223965783522\n",
      "Epoch 49/100\n",
      "Training Loss: 0.13664487039511403\n",
      "Validation Loss: 0.10092111319122146\n",
      "Epoch 50/100\n",
      "Training Loss: 0.12460078197323343\n",
      "Validation Loss: 0.08582074090633687\n",
      "Epoch 51/100\n",
      "Training Loss: 0.1350682583777452\n",
      "Validation Loss: 0.0650036019494152\n",
      "Epoch 52/100\n",
      "Training Loss: 0.12293254528498877\n",
      "Validation Loss: 0.0736849709858436\n",
      "Epoch 53/100\n",
      "Training Loss: 0.12459831959179524\n",
      "Validation Loss: 0.07001701505563564\n",
      "Epoch 54/100\n",
      "Training Loss: 0.1241137040359858\n",
      "Validation Loss: 0.048121671174298206\n",
      "Epoch 55/100\n",
      "Training Loss: 0.12047226396550474\n",
      "Validation Loss: 0.06634402847267672\n",
      "Epoch 56/100\n",
      "Training Loss: 0.12792997878444343\n",
      "Validation Loss: 0.12454323715445274\n",
      "Epoch 57/100\n",
      "Training Loss: 0.11648924387411767\n",
      "Validation Loss: 0.06626537001493712\n",
      "Epoch 58/100\n",
      "Training Loss: 0.11896963836599926\n",
      "Validation Loss: 0.07938382718638856\n",
      "Epoch 59/100\n",
      "Training Loss: 0.11539372131748293\n",
      "Validation Loss: 0.09104911229187568\n",
      "Epoch 60/100\n",
      "Training Loss: 0.10702183361500306\n",
      "Validation Loss: 0.08339697897125149\n",
      "Epoch 61/100\n",
      "Training Loss: 0.10670279389869983\n",
      "Validation Loss: 0.05659204350819895\n",
      "Epoch 62/100\n",
      "Training Loss: 0.12587248854201702\n",
      "Validation Loss: 0.07096197784642067\n",
      "Epoch 63/100\n",
      "Training Loss: 0.1261507187240558\n",
      "Validation Loss: 0.06861932670123337\n",
      "Epoch 64/100\n",
      "Training Loss: 0.12790229635528075\n",
      "Validation Loss: 0.07955458735869227\n",
      "Epoch 65/100\n",
      "Training Loss: 0.12257531472115603\n",
      "Validation Loss: 0.07952236060221483\n",
      "Epoch 66/100\n",
      "Training Loss: 0.12233322275261305\n",
      "Validation Loss: 0.0719134177266244\n",
      "Epoch 67/100\n",
      "Training Loss: 0.12837469685851038\n",
      "Validation Loss: 0.06540080844794566\n",
      "Epoch 68/100\n",
      "Training Loss: 0.11010145915329021\n",
      "Validation Loss: 0.08652265410236415\n",
      "Epoch 69/100\n",
      "Training Loss: 0.118629174228134\n",
      "Validation Loss: 0.056653203431020094\n",
      "Epoch 70/100\n",
      "Training Loss: 0.1070447977370222\n",
      "Validation Loss: 0.05070639634181671\n",
      "Epoch 71/100\n",
      "Training Loss: 0.11511573817167936\n",
      "Validation Loss: 0.11109373009072003\n",
      "Epoch 72/100\n",
      "Training Loss: 0.10982223043417011\n",
      "Validation Loss: 0.06765750183764832\n",
      "Epoch 73/100\n",
      "Training Loss: 0.1075984352271348\n",
      "Validation Loss: 0.04751530353107747\n",
      "Epoch 74/100\n",
      "Training Loss: 0.11215091763612613\n",
      "Validation Loss: 0.05725029509300328\n",
      "Epoch 75/100\n",
      "Training Loss: 0.1078658323766347\n",
      "Validation Loss: 0.05983286584217956\n",
      "Epoch 76/100\n",
      "Training Loss: 0.09699219726593955\n",
      "Validation Loss: 0.05608371915406263\n",
      "Epoch 77/100\n",
      "Training Loss: 0.11241926927090107\n",
      "Validation Loss: 0.06580691160389751\n",
      "Epoch 78/100\n",
      "Training Loss: 0.101252424965699\n",
      "Validation Loss: 0.04325038106084402\n",
      "Epoch 79/100\n",
      "Training Loss: 0.10907667522304551\n",
      "Validation Loss: 0.04005905362215801\n",
      "Epoch 80/100\n",
      "Training Loss: 0.11340351409690638\n",
      "Validation Loss: 0.0957163690543236\n",
      "Epoch 81/100\n",
      "Training Loss: 0.0997590644919954\n",
      "Validation Loss: 0.04821414006102719\n",
      "Epoch 82/100\n",
      "Training Loss: 0.10672753626530433\n",
      "Validation Loss: 0.06455321247620371\n",
      "Epoch 83/100\n",
      "Training Loss: 0.12838055181764263\n",
      "Validation Loss: 0.0672447219404387\n",
      "Epoch 84/100\n",
      "Training Loss: 0.10692225165958522\n",
      "Validation Loss: 0.04853130165499341\n",
      "Epoch 85/100\n",
      "Training Loss: 0.0991816021676248\n",
      "Validation Loss: 0.03949845490801201\n",
      "Epoch 86/100\n",
      "Training Loss: 0.0969626929171543\n",
      "Validation Loss: 0.06552641616825297\n",
      "Epoch 87/100\n",
      "Training Loss: 0.10043359593905184\n",
      "Validation Loss: 0.05873909238797499\n",
      "Epoch 88/100\n",
      "Training Loss: 0.09912530598569835\n",
      "Validation Loss: 0.0628718208935225\n",
      "Epoch 89/100\n",
      "Training Loss: 0.10238511576973895\n",
      "Validation Loss: 0.0558932846511617\n",
      "Epoch 90/100\n",
      "Training Loss: 0.11145357069449774\n",
      "Validation Loss: 0.05649150425477388\n",
      "Epoch 91/100\n",
      "Training Loss: 0.10307912061186512\n",
      "Validation Loss: 0.061030591162899085\n",
      "Epoch 92/100\n",
      "Training Loss: 0.09386938835470254\n",
      "Validation Loss: 0.039632120503570734\n",
      "Epoch 93/100\n",
      "Training Loss: 0.08755356976555606\n",
      "Validation Loss: 0.07086435965148129\n",
      "Epoch 94/100\n",
      "Training Loss: 0.08149672361474428\n",
      "Validation Loss: 0.05437117726793683\n",
      "Epoch 95/100\n",
      "Training Loss: 0.09411633608590558\n",
      "Validation Loss: 0.04480104639045092\n",
      "Epoch 96/100\n",
      "Training Loss: 0.09510206912651249\n",
      "Validation Loss: 0.03638572922966117\n",
      "Epoch 97/100\n",
      "Training Loss: 0.08164998678012902\n",
      "Validation Loss: 0.07030096133290487\n",
      "Epoch 98/100\n",
      "Training Loss: 0.09673163316143205\n",
      "Validation Loss: 0.1214912994402428\n",
      "Epoch 99/100\n",
      "Training Loss: 0.08697988984449831\n",
      "Validation Loss: 0.045227779723923146\n",
      "Epoch 100/100\n",
      "Training Loss: 0.07433211466032683\n",
      "Validation Loss: 0.05118126056609068\n",
      "Combination 15: Avg Training Loss = 0.14414373885618387, Avg Validation Loss = 0.09508564609281994\n",
      "Testing combination 16/48: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0001, 'sequence_length': 20, 'dropout_rate': 0.2}\n",
      "Truncated Train Data Shape: (70, 20, 1)\n",
      "Truncated Validation Data Shape: (10, 20, 1)\n",
      "    Trial 1/2 for combination 16/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.4186053254022341\n",
      "Validation Loss: 0.18019265966585485\n",
      "Epoch 2/100\n",
      "Training Loss: 0.3467526665608477\n",
      "Validation Loss: 0.1698397014405184\n",
      "Epoch 3/100\n",
      "Training Loss: 0.2719944087330362\n",
      "Validation Loss: 0.22197517215474555\n",
      "Epoch 4/100\n",
      "Training Loss: 0.32558396084149765\n",
      "Validation Loss: 0.14819812411678618\n",
      "Epoch 5/100\n",
      "Training Loss: 0.2586260898040278\n",
      "Validation Loss: 0.29150296835416445\n",
      "Epoch 6/100\n",
      "Training Loss: 0.22014242268563972\n",
      "Validation Loss: 0.23202985325105413\n",
      "Epoch 7/100\n",
      "Training Loss: 0.24752231008792777\n",
      "Validation Loss: 0.08675258305639022\n",
      "Epoch 8/100\n",
      "Training Loss: 0.24309542300951797\n",
      "Validation Loss: 0.08152783482431655\n",
      "Epoch 9/100\n",
      "Training Loss: 0.19456131290739162\n",
      "Validation Loss: 0.11319808889607777\n",
      "Epoch 10/100\n",
      "Training Loss: 0.20768626073778174\n",
      "Validation Loss: 0.17861119951646826\n",
      "Epoch 11/100\n",
      "Training Loss: 0.19202960705389804\n",
      "Validation Loss: 0.17779423042836134\n",
      "Epoch 12/100\n",
      "Training Loss: 0.20130872472670341\n",
      "Validation Loss: 0.11826956712521679\n",
      "Epoch 13/100\n",
      "Training Loss: 0.18384707419278418\n",
      "Validation Loss: 0.11161548517648548\n",
      "Epoch 14/100\n",
      "Training Loss: 0.17595025944456624\n",
      "Validation Loss: 0.2159979749199298\n",
      "Epoch 15/100\n",
      "Training Loss: 0.1743542828416444\n",
      "Validation Loss: 0.08952596767956708\n",
      "Epoch 16/100\n",
      "Training Loss: 0.21111142177146336\n",
      "Validation Loss: 0.15484476450990162\n",
      "Epoch 17/100\n",
      "Training Loss: 0.1478873472787255\n",
      "Validation Loss: 0.1330198884640921\n",
      "Epoch 18/100\n",
      "Training Loss: 0.18046439932860928\n",
      "Validation Loss: 0.15328093174063778\n",
      "Epoch 19/100\n",
      "Training Loss: 0.15977310701444672\n",
      "Validation Loss: 0.168952337020918\n",
      "Epoch 20/100\n",
      "Training Loss: 0.16786559041164723\n",
      "Validation Loss: 0.11954550257206593\n",
      "Epoch 21/100\n",
      "Training Loss: 0.1597662724219041\n",
      "Validation Loss: 0.10090542070278759\n",
      "Epoch 22/100\n",
      "Training Loss: 0.16339172127428744\n",
      "Validation Loss: 0.10152100908494474\n",
      "Epoch 23/100\n",
      "Training Loss: 0.16451358111903772\n",
      "Validation Loss: 0.06196529360929298\n",
      "Epoch 24/100\n",
      "Training Loss: 0.15026171196130797\n",
      "Validation Loss: 0.0709135605913594\n",
      "Epoch 25/100\n",
      "Training Loss: 0.13401044802574666\n",
      "Validation Loss: 0.07228415669146344\n",
      "Epoch 26/100\n",
      "Training Loss: 0.16038291884414255\n",
      "Validation Loss: 0.09919648174136778\n",
      "Epoch 27/100\n",
      "Training Loss: 0.14112620246082835\n",
      "Validation Loss: 0.15692760728506022\n",
      "Epoch 28/100\n",
      "Training Loss: 0.14657106801138678\n",
      "Validation Loss: 0.08233768935661566\n",
      "Epoch 29/100\n",
      "Training Loss: 0.14896265186087196\n",
      "Validation Loss: 0.10667249630240976\n",
      "Epoch 30/100\n",
      "Training Loss: 0.1469296315364965\n",
      "Validation Loss: 0.14317097256950642\n",
      "Epoch 31/100\n",
      "Training Loss: 0.1600076002824524\n",
      "Validation Loss: 0.10196137157181835\n",
      "Epoch 32/100\n",
      "Training Loss: 0.14236519149828236\n",
      "Validation Loss: 0.17381330880998097\n",
      "Epoch 33/100\n",
      "Training Loss: 0.13815516140638168\n",
      "Validation Loss: 0.1243426019062199\n",
      "Epoch 34/100\n",
      "Training Loss: 0.14327404759857448\n",
      "Validation Loss: 0.0714872873784428\n",
      "Epoch 35/100\n",
      "Training Loss: 0.1236828985563436\n",
      "Validation Loss: 0.07028081142540643\n",
      "Epoch 36/100\n",
      "Training Loss: 0.1500495726523737\n",
      "Validation Loss: 0.0531646413048145\n",
      "Epoch 37/100\n",
      "Training Loss: 0.14486912213979974\n",
      "Validation Loss: 0.08291900196970828\n",
      "Epoch 38/100\n",
      "Training Loss: 0.13271128123040077\n",
      "Validation Loss: 0.05936521283697147\n",
      "Epoch 39/100\n",
      "Training Loss: 0.12208543661570921\n",
      "Validation Loss: 0.17623681133216496\n",
      "Epoch 40/100\n",
      "Training Loss: 0.12752451583729865\n",
      "Validation Loss: 0.10318374579915324\n",
      "Epoch 41/100\n",
      "Training Loss: 0.1251370964658456\n",
      "Validation Loss: 0.0862280452793942\n",
      "Epoch 42/100\n",
      "Training Loss: 0.1349758091009922\n",
      "Validation Loss: 0.07292322308124977\n",
      "Epoch 43/100\n",
      "Training Loss: 0.13762401201903215\n",
      "Validation Loss: 0.05042222148208769\n",
      "Epoch 44/100\n",
      "Training Loss: 0.1345671565651207\n",
      "Validation Loss: 0.06463854923504055\n",
      "Epoch 45/100\n",
      "Training Loss: 0.12207374706095779\n",
      "Validation Loss: 0.09203635098105582\n",
      "Epoch 46/100\n",
      "Training Loss: 0.1298326332429801\n",
      "Validation Loss: 0.09007152094499035\n",
      "Epoch 47/100\n",
      "Training Loss: 0.13588267320258549\n",
      "Validation Loss: 0.08490854882947299\n",
      "Epoch 48/100\n",
      "Training Loss: 0.1354302522254469\n",
      "Validation Loss: 0.10916235017681675\n",
      "Epoch 49/100\n",
      "Training Loss: 0.11102100129372745\n",
      "Validation Loss: 0.07171332918130345\n",
      "Epoch 50/100\n",
      "Training Loss: 0.11937035485798561\n",
      "Validation Loss: 0.055644986902563365\n",
      "Epoch 51/100\n",
      "Training Loss: 0.13256805255977872\n",
      "Validation Loss: 0.08465241490315745\n",
      "Epoch 52/100\n",
      "Training Loss: 0.11159766774784008\n",
      "Validation Loss: 0.07978639054811457\n",
      "Epoch 53/100\n",
      "Training Loss: 0.10449893199080496\n",
      "Validation Loss: 0.08003453401008191\n",
      "Epoch 54/100\n",
      "Training Loss: 0.132169523797246\n",
      "Validation Loss: 0.08907536186491695\n",
      "Epoch 55/100\n",
      "Training Loss: 0.12102526550785446\n",
      "Validation Loss: 0.054438185256237136\n",
      "Epoch 56/100\n",
      "Training Loss: 0.11007186359059969\n",
      "Validation Loss: 0.07035940902021051\n",
      "Epoch 57/100\n",
      "Training Loss: 0.11026282840768824\n",
      "Validation Loss: 0.06083704432588608\n",
      "Epoch 58/100\n",
      "Training Loss: 0.11862984601685674\n",
      "Validation Loss: 0.04604688210835383\n",
      "Epoch 59/100\n",
      "Training Loss: 0.11362426940600884\n",
      "Validation Loss: 0.0774942749998491\n",
      "Epoch 60/100\n",
      "Training Loss: 0.09184821634098461\n",
      "Validation Loss: 0.06472450126500384\n",
      "Epoch 61/100\n",
      "Training Loss: 0.10945089416495897\n",
      "Validation Loss: 0.07351277182417185\n",
      "Epoch 62/100\n",
      "Training Loss: 0.12204550425791967\n",
      "Validation Loss: 0.09695152153931734\n",
      "Epoch 63/100\n",
      "Training Loss: 0.11291386093147164\n",
      "Validation Loss: 0.08943309012599833\n",
      "Epoch 64/100\n",
      "Training Loss: 0.10218917218321331\n",
      "Validation Loss: 0.10854840059865364\n",
      "Epoch 65/100\n",
      "Training Loss: 0.11485763157983445\n",
      "Validation Loss: 0.07608176838550952\n",
      "Epoch 66/100\n",
      "Training Loss: 0.0983888498091957\n",
      "Validation Loss: 0.058238989327361254\n",
      "Epoch 67/100\n",
      "Training Loss: 0.09597917048507401\n",
      "Validation Loss: 0.07377254630680803\n",
      "Epoch 68/100\n",
      "Training Loss: 0.09583934522067773\n",
      "Validation Loss: 0.06939809521965455\n",
      "Epoch 69/100\n",
      "Training Loss: 0.11135071442004346\n",
      "Validation Loss: 0.06914846341344792\n",
      "Epoch 70/100\n",
      "Training Loss: 0.09980515913409807\n",
      "Validation Loss: 0.03890803809590293\n",
      "Epoch 71/100\n",
      "Training Loss: 0.10319821113439485\n",
      "Validation Loss: 0.061058900882295694\n",
      "Epoch 72/100\n",
      "Training Loss: 0.10767365140281489\n",
      "Validation Loss: 0.0747670203620249\n",
      "Epoch 73/100\n",
      "Training Loss: 0.10845004179744935\n",
      "Validation Loss: 0.0456649558380836\n",
      "Epoch 74/100\n",
      "Training Loss: 0.10420272025556368\n",
      "Validation Loss: 0.08545672626058096\n",
      "Epoch 75/100\n",
      "Training Loss: 0.09669722736436846\n",
      "Validation Loss: 0.10333364316505282\n",
      "Epoch 76/100\n",
      "Training Loss: 0.08814038513744805\n",
      "Validation Loss: 0.0569581870467071\n",
      "Epoch 77/100\n",
      "Training Loss: 0.1052581527260005\n",
      "Validation Loss: 0.07576449232661019\n",
      "Epoch 78/100\n",
      "Training Loss: 0.08556932370100552\n",
      "Validation Loss: 0.05678589093981061\n",
      "Epoch 79/100\n",
      "Training Loss: 0.10305879734602426\n",
      "Validation Loss: 0.055812669214722235\n",
      "Epoch 80/100\n",
      "Training Loss: 0.1000474407337609\n",
      "Validation Loss: 0.11057368223808364\n",
      "Epoch 81/100\n",
      "Training Loss: 0.09994673338868632\n",
      "Validation Loss: 0.05427546026192476\n",
      "Epoch 82/100\n",
      "Training Loss: 0.09127278703214196\n",
      "Validation Loss: 0.08335085442014448\n",
      "Epoch 83/100\n",
      "Training Loss: 0.09896045133594819\n",
      "Validation Loss: 0.06027873375903185\n",
      "Epoch 84/100\n",
      "Training Loss: 0.09347668310486436\n",
      "Validation Loss: 0.053805005833082044\n",
      "Epoch 85/100\n",
      "Training Loss: 0.10643998211400876\n",
      "Validation Loss: 0.09350486215738388\n",
      "Epoch 86/100\n",
      "Training Loss: 0.08385481112918507\n",
      "Validation Loss: 0.04856474307076937\n",
      "Epoch 87/100\n",
      "Training Loss: 0.0874401622405385\n",
      "Validation Loss: 0.044226682706004994\n",
      "Epoch 88/100\n",
      "Training Loss: 0.09079528269595005\n",
      "Validation Loss: 0.05447743424100204\n",
      "Epoch 89/100\n",
      "Training Loss: 0.0994560346060971\n",
      "Validation Loss: 0.059644374047989515\n",
      "Epoch 90/100\n",
      "Training Loss: 0.09855480562651264\n",
      "Validation Loss: 0.06367487983434675\n",
      "Epoch 91/100\n",
      "Training Loss: 0.08586435619246849\n",
      "Validation Loss: 0.050688971133465086\n",
      "Epoch 92/100\n",
      "Training Loss: 0.09761336547192001\n",
      "Validation Loss: 0.04003456474809078\n",
      "Epoch 93/100\n",
      "Training Loss: 0.08928646571185378\n",
      "Validation Loss: 0.10728670107247865\n",
      "Epoch 94/100\n",
      "Training Loss: 0.08399827739491737\n",
      "Validation Loss: 0.07926645460402257\n",
      "Epoch 95/100\n",
      "Training Loss: 0.0963158911462692\n",
      "Validation Loss: 0.06713467216858868\n",
      "Epoch 96/100\n",
      "Training Loss: 0.08193026700058093\n",
      "Validation Loss: 0.046022389549415295\n",
      "Epoch 97/100\n",
      "Training Loss: 0.09839259572312772\n",
      "Validation Loss: 0.04426511249069239\n",
      "Epoch 98/100\n",
      "Training Loss: 0.08638455561314567\n",
      "Validation Loss: 0.05914384799999336\n",
      "Epoch 99/100\n",
      "Training Loss: 0.08413840520686708\n",
      "Validation Loss: 0.06574031673933521\n",
      "Epoch 100/100\n",
      "Training Loss: 0.08318596453190513\n",
      "Validation Loss: 0.05780172317873873\n",
      "    Trial 2/2 for combination 16/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.4829137930475462\n",
      "Validation Loss: 0.4620602803964899\n",
      "Epoch 2/100\n",
      "Training Loss: 0.32477277482755007\n",
      "Validation Loss: 0.29134593369494305\n",
      "Epoch 3/100\n",
      "Training Loss: 0.33542350828747125\n",
      "Validation Loss: 0.22020341964976042\n",
      "Epoch 4/100\n",
      "Training Loss: 0.31029501937657067\n",
      "Validation Loss: 0.09540976739068752\n",
      "Epoch 5/100\n",
      "Training Loss: 0.2805749852005965\n",
      "Validation Loss: 0.22284007607948775\n",
      "Epoch 6/100\n",
      "Training Loss: 0.20299855668859315\n",
      "Validation Loss: 0.15756901955593866\n",
      "Epoch 7/100\n",
      "Training Loss: 0.2228489931770172\n",
      "Validation Loss: 0.1936664868564743\n",
      "Epoch 8/100\n",
      "Training Loss: 0.23572904115454746\n",
      "Validation Loss: 0.2690566544226\n",
      "Epoch 9/100\n",
      "Training Loss: 0.20746756778487296\n",
      "Validation Loss: 0.1892591384915939\n",
      "Epoch 10/100\n",
      "Training Loss: 0.22971801103675707\n",
      "Validation Loss: 0.17155211762529582\n",
      "Epoch 11/100\n",
      "Training Loss: 0.23515720411301447\n",
      "Validation Loss: 0.2287770878023138\n",
      "Epoch 12/100\n",
      "Training Loss: 0.17811262502300268\n",
      "Validation Loss: 0.1533355107342767\n",
      "Epoch 13/100\n",
      "Training Loss: 0.18085194356521647\n",
      "Validation Loss: 0.20878007759035078\n",
      "Epoch 14/100\n",
      "Training Loss: 0.17929588153834\n",
      "Validation Loss: 0.16753191735899592\n",
      "Epoch 15/100\n",
      "Training Loss: 0.19893685775288147\n",
      "Validation Loss: 0.23312769806907627\n",
      "Epoch 16/100\n",
      "Training Loss: 0.18589357971534998\n",
      "Validation Loss: 0.13318611613931583\n",
      "Epoch 17/100\n",
      "Training Loss: 0.20272631454265608\n",
      "Validation Loss: 0.11663207103702503\n",
      "Epoch 18/100\n",
      "Training Loss: 0.1858904818873061\n",
      "Validation Loss: 0.09890077088209051\n",
      "Epoch 19/100\n",
      "Training Loss: 0.1661299766730996\n",
      "Validation Loss: 0.12129319687022193\n",
      "Epoch 20/100\n",
      "Training Loss: 0.19892756711894935\n",
      "Validation Loss: 0.18202809685676827\n",
      "Epoch 21/100\n",
      "Training Loss: 0.1523965837587703\n",
      "Validation Loss: 0.18318581877197668\n",
      "Epoch 22/100\n",
      "Training Loss: 0.19259294824822845\n",
      "Validation Loss: 0.12443593038910106\n",
      "Epoch 23/100\n",
      "Training Loss: 0.1777692718951807\n",
      "Validation Loss: 0.18756267006822208\n",
      "Epoch 24/100\n",
      "Training Loss: 0.16382032955858927\n",
      "Validation Loss: 0.170160155246929\n",
      "Epoch 25/100\n",
      "Training Loss: 0.2017834576209894\n",
      "Validation Loss: 0.14422468145376904\n",
      "Epoch 26/100\n",
      "Training Loss: 0.1696243430967914\n",
      "Validation Loss: 0.11775635479905813\n",
      "Epoch 27/100\n",
      "Training Loss: 0.166850528570138\n",
      "Validation Loss: 0.1247270541376528\n",
      "Epoch 28/100\n",
      "Training Loss: 0.16378591749554902\n",
      "Validation Loss: 0.11007383246435902\n",
      "Epoch 29/100\n",
      "Training Loss: 0.18026040485254632\n",
      "Validation Loss: 0.06228090152309142\n",
      "Epoch 30/100\n",
      "Training Loss: 0.17022627078533598\n",
      "Validation Loss: 0.0855612786568962\n",
      "Epoch 31/100\n",
      "Training Loss: 0.15932618500972887\n",
      "Validation Loss: 0.08858966808215353\n",
      "Epoch 32/100\n",
      "Training Loss: 0.1479043741345854\n",
      "Validation Loss: 0.092462389093093\n",
      "Epoch 33/100\n",
      "Training Loss: 0.1609127955205237\n",
      "Validation Loss: 0.12004985115296092\n",
      "Epoch 34/100\n",
      "Training Loss: 0.1558366404260018\n",
      "Validation Loss: 0.12376864301682629\n",
      "Epoch 35/100\n",
      "Training Loss: 0.13728469188334685\n",
      "Validation Loss: 0.09252198352136806\n",
      "Epoch 36/100\n",
      "Training Loss: 0.1551057738189242\n",
      "Validation Loss: 0.08280547263839841\n",
      "Epoch 37/100\n",
      "Training Loss: 0.14866089881531863\n",
      "Validation Loss: 0.07572848636581907\n",
      "Epoch 38/100\n",
      "Training Loss: 0.1606846246798513\n",
      "Validation Loss: 0.13005572663518464\n",
      "Epoch 39/100\n",
      "Training Loss: 0.14337484118832103\n",
      "Validation Loss: 0.14711549847088332\n",
      "Epoch 40/100\n",
      "Training Loss: 0.1509614621480115\n",
      "Validation Loss: 0.11684210776180623\n",
      "Epoch 41/100\n",
      "Training Loss: 0.13701751982801122\n",
      "Validation Loss: 0.09562144291982991\n",
      "Epoch 42/100\n",
      "Training Loss: 0.12955230801021214\n",
      "Validation Loss: 0.14500032323802547\n",
      "Epoch 43/100\n",
      "Training Loss: 0.16010892307887167\n",
      "Validation Loss: 0.08185469152158123\n",
      "Epoch 44/100\n",
      "Training Loss: 0.15141990933726993\n",
      "Validation Loss: 0.06732794113769772\n",
      "Epoch 45/100\n",
      "Training Loss: 0.1390022697552666\n",
      "Validation Loss: 0.1000847983988474\n",
      "Epoch 46/100\n",
      "Training Loss: 0.1397318945714002\n",
      "Validation Loss: 0.06563743035966582\n",
      "Epoch 47/100\n",
      "Training Loss: 0.1299706094665725\n",
      "Validation Loss: 0.11468729772077843\n",
      "Epoch 48/100\n",
      "Training Loss: 0.14527500442803354\n",
      "Validation Loss: 0.08816283565444602\n",
      "Epoch 49/100\n",
      "Training Loss: 0.1333892912009452\n",
      "Validation Loss: 0.08916541757997412\n",
      "Epoch 50/100\n",
      "Training Loss: 0.136225588872785\n",
      "Validation Loss: 0.10042942018091512\n",
      "Epoch 51/100\n",
      "Training Loss: 0.13863367825123782\n",
      "Validation Loss: 0.12945745662404243\n",
      "Epoch 52/100\n",
      "Training Loss: 0.12525419923822928\n",
      "Validation Loss: 0.08644058635996651\n",
      "Epoch 53/100\n",
      "Training Loss: 0.1174230671883928\n",
      "Validation Loss: 0.10302378750469905\n",
      "Epoch 54/100\n",
      "Training Loss: 0.13607481860245021\n",
      "Validation Loss: 0.09853742955383667\n",
      "Epoch 55/100\n",
      "Training Loss: 0.1394622514723743\n",
      "Validation Loss: 0.0707462809725177\n",
      "Epoch 56/100\n",
      "Training Loss: 0.12103895247364327\n",
      "Validation Loss: 0.10640491890661416\n",
      "Epoch 57/100\n",
      "Training Loss: 0.1407242467529121\n",
      "Validation Loss: 0.17749245410608344\n",
      "Epoch 58/100\n",
      "Training Loss: 0.13116588358045073\n",
      "Validation Loss: 0.08618992270619784\n",
      "Epoch 59/100\n",
      "Training Loss: 0.13028837441802918\n",
      "Validation Loss: 0.10715091692282115\n",
      "Epoch 60/100\n",
      "Training Loss: 0.1361669367094481\n",
      "Validation Loss: 0.06477057285513771\n",
      "Epoch 61/100\n",
      "Training Loss: 0.11992853401431001\n",
      "Validation Loss: 0.09335459912197233\n",
      "Epoch 62/100\n",
      "Training Loss: 0.12182270454424941\n",
      "Validation Loss: 0.08284981541314666\n",
      "Epoch 63/100\n",
      "Training Loss: 0.13258969411916377\n",
      "Validation Loss: 0.11179068263386019\n",
      "Epoch 64/100\n",
      "Training Loss: 0.13438199829567768\n",
      "Validation Loss: 0.10576251344728475\n",
      "Epoch 65/100\n",
      "Training Loss: 0.12591223318656056\n",
      "Validation Loss: 0.08260661664285092\n",
      "Epoch 66/100\n",
      "Training Loss: 0.12373211822768226\n",
      "Validation Loss: 0.10413713649701554\n",
      "Epoch 67/100\n",
      "Training Loss: 0.12851296089494396\n",
      "Validation Loss: 0.09221575444096246\n",
      "Epoch 68/100\n",
      "Training Loss: 0.11134195757363326\n",
      "Validation Loss: 0.08842776568067515\n",
      "Epoch 69/100\n",
      "Training Loss: 0.11765231076503692\n",
      "Validation Loss: 0.0545967356189471\n",
      "Epoch 70/100\n",
      "Training Loss: 0.10685566796476116\n",
      "Validation Loss: 0.07389664761949935\n",
      "Epoch 71/100\n",
      "Training Loss: 0.1272365153094662\n",
      "Validation Loss: 0.09376104565856405\n",
      "Epoch 72/100\n",
      "Training Loss: 0.11053340543419858\n",
      "Validation Loss: 0.08388205051937622\n",
      "Epoch 73/100\n",
      "Training Loss: 0.11598228447348598\n",
      "Validation Loss: 0.05984810210654862\n",
      "Epoch 74/100\n",
      "Training Loss: 0.11982997022747868\n",
      "Validation Loss: 0.07378924844447203\n",
      "Epoch 75/100\n",
      "Training Loss: 0.12329997462835984\n",
      "Validation Loss: 0.08242240261137485\n",
      "Epoch 76/100\n",
      "Training Loss: 0.12112402516133482\n",
      "Validation Loss: 0.092798914780446\n",
      "Epoch 77/100\n",
      "Training Loss: 0.12326707169840448\n",
      "Validation Loss: 0.09349575080127631\n",
      "Epoch 78/100\n",
      "Training Loss: 0.0993052952451033\n",
      "Validation Loss: 0.08715901231457093\n",
      "Epoch 79/100\n",
      "Training Loss: 0.10019483884176691\n",
      "Validation Loss: 0.062874819076519\n",
      "Epoch 80/100\n",
      "Training Loss: 0.10875589672046139\n",
      "Validation Loss: 0.11228937771852414\n",
      "Epoch 81/100\n",
      "Training Loss: 0.10647348363817562\n",
      "Validation Loss: 0.06577334170037359\n",
      "Epoch 82/100\n",
      "Training Loss: 0.11143238317486083\n",
      "Validation Loss: 0.0822110551492657\n",
      "Epoch 83/100\n",
      "Training Loss: 0.1108343191647506\n",
      "Validation Loss: 0.052671459689020164\n",
      "Epoch 84/100\n",
      "Training Loss: 0.11004621392947563\n",
      "Validation Loss: 0.07674422554954419\n",
      "Epoch 85/100\n",
      "Training Loss: 0.11687875443407506\n",
      "Validation Loss: 0.09711659692970692\n",
      "Epoch 86/100\n",
      "Training Loss: 0.10847473017597403\n",
      "Validation Loss: 0.06002362000189376\n",
      "Epoch 87/100\n",
      "Training Loss: 0.10878706780044622\n",
      "Validation Loss: 0.07621352204318935\n",
      "Epoch 88/100\n",
      "Training Loss: 0.09899419734021256\n",
      "Validation Loss: 0.052855643424548085\n",
      "Epoch 89/100\n",
      "Training Loss: 0.10824482429881224\n",
      "Validation Loss: 0.07652992204507698\n",
      "Epoch 90/100\n",
      "Training Loss: 0.10277360319378313\n",
      "Validation Loss: 0.0795843619874315\n",
      "Epoch 91/100\n",
      "Training Loss: 0.10861443938770557\n",
      "Validation Loss: 0.06443861778467067\n",
      "Epoch 92/100\n",
      "Training Loss: 0.10392780990890699\n",
      "Validation Loss: 0.07168294683614743\n",
      "Epoch 93/100\n",
      "Training Loss: 0.10446430721075248\n",
      "Validation Loss: 0.06059953131191094\n",
      "Epoch 94/100\n",
      "Training Loss: 0.10592929896988834\n",
      "Validation Loss: 0.061561508180558513\n",
      "Epoch 95/100\n",
      "Training Loss: 0.1120320221822499\n",
      "Validation Loss: 0.05670078971396862\n",
      "Epoch 96/100\n",
      "Training Loss: 0.09320096167546184\n",
      "Validation Loss: 0.059641613141631876\n",
      "Epoch 97/100\n",
      "Training Loss: 0.10143462311842324\n",
      "Validation Loss: 0.04926983780359784\n",
      "Epoch 98/100\n",
      "Training Loss: 0.08597815289257738\n",
      "Validation Loss: 0.07609761900411506\n",
      "Epoch 99/100\n",
      "Training Loss: 0.09554424114180664\n",
      "Validation Loss: 0.05468119970452044\n",
      "Epoch 100/100\n",
      "Training Loss: 0.09948099430538136\n",
      "Validation Loss: 0.0618956289011298\n",
      "Combination 16: Avg Training Loss = 0.14475998500572534, Avg Validation Loss = 0.10350390988866623\n",
      "Testing combination 17/48: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0001, 'sequence_length': 50, 'dropout_rate': 0.1}\n",
      "Truncated Train Data Shape: (70, 50, 1)\n",
      "Truncated Validation Data Shape: (10, 50, 1)\n",
      "    Trial 1/2 for combination 17/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.4934808074909239\n",
      "Validation Loss: 0.3374485382537495\n",
      "Epoch 2/100\n",
      "Training Loss: 0.35860000593563307\n",
      "Validation Loss: 0.2751531348487529\n",
      "Epoch 3/100\n",
      "Training Loss: 0.25484559327963624\n",
      "Validation Loss: 0.05654116630618779\n",
      "Epoch 4/100\n",
      "Training Loss: 0.23551498318877415\n",
      "Validation Loss: 0.14905853705718555\n",
      "Epoch 5/100\n",
      "Training Loss: 0.201204377149556\n",
      "Validation Loss: 0.07601205868551439\n",
      "Epoch 6/100\n",
      "Training Loss: 0.2340760024312337\n",
      "Validation Loss: 0.13169481098687416\n",
      "Epoch 7/100\n",
      "Training Loss: 0.20556945536791638\n",
      "Validation Loss: 0.11887798085390242\n",
      "Epoch 8/100\n",
      "Training Loss: 0.19477639307304845\n",
      "Validation Loss: 0.17669619595313538\n",
      "Epoch 9/100\n",
      "Training Loss: 0.1979048854571638\n",
      "Validation Loss: 0.08669799120811418\n",
      "Epoch 10/100\n",
      "Training Loss: 0.14869381149460062\n",
      "Validation Loss: 0.08613738010791627\n",
      "Epoch 11/100\n",
      "Training Loss: 0.20595467175618212\n",
      "Validation Loss: 0.08934792455047541\n",
      "Epoch 12/100\n",
      "Training Loss: 0.1784854190373969\n",
      "Validation Loss: 0.12425337581741147\n",
      "Epoch 13/100\n",
      "Training Loss: 0.15803990393571463\n",
      "Validation Loss: 0.07266412590286209\n",
      "Epoch 14/100\n",
      "Training Loss: 0.1850426504187015\n",
      "Validation Loss: 0.09379427249783716\n",
      "Epoch 15/100\n",
      "Training Loss: 0.18841590884476064\n",
      "Validation Loss: 0.07897107444508113\n",
      "Epoch 16/100\n",
      "Training Loss: 0.14912739475437314\n",
      "Validation Loss: 0.07128174657335508\n",
      "Epoch 17/100\n",
      "Training Loss: 0.22829241755852492\n",
      "Validation Loss: 0.07757834609702942\n",
      "Epoch 18/100\n",
      "Training Loss: 0.1792880093512931\n",
      "Validation Loss: 0.10453030425493078\n",
      "Epoch 19/100\n",
      "Training Loss: 0.16115468238174907\n",
      "Validation Loss: 0.09710702743755331\n",
      "Epoch 20/100\n",
      "Training Loss: 0.19019827162933733\n",
      "Validation Loss: 0.08189019466410065\n",
      "Epoch 21/100\n",
      "Training Loss: 0.1567943168948507\n",
      "Validation Loss: 0.08155366000658965\n",
      "Epoch 22/100\n",
      "Training Loss: 0.15390614911573602\n",
      "Validation Loss: 0.06281877196202158\n",
      "Epoch 23/100\n",
      "Training Loss: 0.1485537592965189\n",
      "Validation Loss: 0.058142313535079725\n",
      "Epoch 24/100\n",
      "Training Loss: 0.15140572862935583\n",
      "Validation Loss: 0.06933767984756992\n",
      "Epoch 25/100\n",
      "Training Loss: 0.18118738963820524\n",
      "Validation Loss: 0.10302351912326121\n",
      "Epoch 26/100\n",
      "Training Loss: 0.16203963214452954\n",
      "Validation Loss: 0.07517519559755231\n",
      "Epoch 27/100\n",
      "Training Loss: 0.1519943652372495\n",
      "Validation Loss: 0.0979053799867504\n",
      "Epoch 28/100\n",
      "Training Loss: 0.15048807260333702\n",
      "Validation Loss: 0.09955112062680983\n",
      "Epoch 29/100\n",
      "Training Loss: 0.1798296231776671\n",
      "Validation Loss: 0.0899007137433294\n",
      "Epoch 30/100\n",
      "Training Loss: 0.15376958197318005\n",
      "Validation Loss: 0.04740811612968405\n",
      "Epoch 31/100\n",
      "Training Loss: 0.15679791437387441\n",
      "Validation Loss: 0.09547182541148982\n",
      "Epoch 32/100\n",
      "Training Loss: 0.1645621492398975\n",
      "Validation Loss: 0.05349772514098918\n",
      "Epoch 33/100\n",
      "Training Loss: 0.12929201624669798\n",
      "Validation Loss: 0.04355132931194972\n",
      "Epoch 34/100\n",
      "Training Loss: 0.14260986589608263\n",
      "Validation Loss: 0.04848046639689244\n",
      "Epoch 35/100\n",
      "Training Loss: 0.14267767935781767\n",
      "Validation Loss: 0.0775470451543281\n",
      "Epoch 36/100\n",
      "Training Loss: 0.157168572360812\n",
      "Validation Loss: 0.11330893839110548\n",
      "Epoch 37/100\n",
      "Training Loss: 0.15890679729686444\n",
      "Validation Loss: 0.10928565972254371\n",
      "Epoch 38/100\n",
      "Training Loss: 0.1334907408965644\n",
      "Validation Loss: 0.0868487524068151\n",
      "Epoch 39/100\n",
      "Training Loss: 0.14371212408594738\n",
      "Validation Loss: 0.052330090340311\n",
      "Epoch 40/100\n",
      "Training Loss: 0.14441091104056447\n",
      "Validation Loss: 0.07618923497531215\n",
      "Epoch 41/100\n",
      "Training Loss: 0.13421485323868826\n",
      "Validation Loss: 0.07602832642891122\n",
      "Epoch 42/100\n",
      "Training Loss: 0.13896985338346532\n",
      "Validation Loss: 0.07213697778319507\n",
      "Epoch 43/100\n",
      "Training Loss: 0.14146252185593816\n",
      "Validation Loss: 0.08493611942444187\n",
      "Epoch 44/100\n",
      "Training Loss: 0.1422341187666626\n",
      "Validation Loss: 0.0557369208147401\n",
      "Epoch 45/100\n",
      "Training Loss: 0.14110478653237502\n",
      "Validation Loss: 0.09720574137620205\n",
      "Epoch 46/100\n",
      "Training Loss: 0.13105172613547533\n",
      "Validation Loss: 0.06943260059278608\n",
      "Epoch 47/100\n",
      "Training Loss: 0.1318186472952731\n",
      "Validation Loss: 0.06265674381236598\n",
      "Epoch 48/100\n",
      "Training Loss: 0.1369792885579454\n",
      "Validation Loss: 0.04889038459907426\n",
      "Epoch 49/100\n",
      "Training Loss: 0.127433888773792\n",
      "Validation Loss: 0.06431734653520679\n",
      "Epoch 50/100\n",
      "Training Loss: 0.12582040791936058\n",
      "Validation Loss: 0.0501972661583574\n",
      "Epoch 51/100\n",
      "Training Loss: 0.1432544731763381\n",
      "Validation Loss: 0.07656028245501542\n",
      "Epoch 52/100\n",
      "Training Loss: 0.12398854207858072\n",
      "Validation Loss: 0.08799943370158818\n",
      "Epoch 53/100\n",
      "Training Loss: 0.11703369005120105\n",
      "Validation Loss: 0.08344905421508483\n",
      "Epoch 54/100\n",
      "Training Loss: 0.12935132119523104\n",
      "Validation Loss: 0.05966341189743827\n",
      "Epoch 55/100\n",
      "Training Loss: 0.11770289894799783\n",
      "Validation Loss: 0.05730812884015226\n",
      "Epoch 56/100\n",
      "Training Loss: 0.11814259337639975\n",
      "Validation Loss: 0.0646717101778905\n",
      "Epoch 57/100\n",
      "Training Loss: 0.12254979481258142\n",
      "Validation Loss: 0.06962977914011052\n",
      "Epoch 58/100\n",
      "Training Loss: 0.13913194367575804\n",
      "Validation Loss: 0.04495865264696773\n",
      "Epoch 59/100\n",
      "Training Loss: 0.11952863016976126\n",
      "Validation Loss: 0.06782677076664072\n",
      "Epoch 60/100\n",
      "Training Loss: 0.12115713938365781\n",
      "Validation Loss: 0.04107879852296288\n",
      "Epoch 61/100\n",
      "Training Loss: 0.12182822936208985\n",
      "Validation Loss: 0.0750471068256354\n",
      "Epoch 62/100\n",
      "Training Loss: 0.12328009890894442\n",
      "Validation Loss: 0.044718641079091695\n",
      "Epoch 63/100\n",
      "Training Loss: 0.11295757558088207\n",
      "Validation Loss: 0.050355884895275795\n",
      "Epoch 64/100\n",
      "Training Loss: 0.11475556906715735\n",
      "Validation Loss: 0.07195771640756393\n",
      "Epoch 65/100\n",
      "Training Loss: 0.11970743244723386\n",
      "Validation Loss: 0.06367930748497526\n",
      "Epoch 66/100\n",
      "Training Loss: 0.12890964642993236\n",
      "Validation Loss: 0.05077548495164319\n",
      "Epoch 67/100\n",
      "Training Loss: 0.11730322024911745\n",
      "Validation Loss: 0.09200697798467192\n",
      "Epoch 68/100\n",
      "Training Loss: 0.11676970490688261\n",
      "Validation Loss: 0.08049082004677152\n",
      "Epoch 69/100\n",
      "Training Loss: 0.1144038392332292\n",
      "Validation Loss: 0.1049539944064551\n",
      "Epoch 70/100\n",
      "Training Loss: 0.10328651753462341\n",
      "Validation Loss: 0.07599400253691814\n",
      "Epoch 71/100\n",
      "Training Loss: 0.11706354705332467\n",
      "Validation Loss: 0.05948071955456948\n",
      "Epoch 72/100\n",
      "Training Loss: 0.11591303054859464\n",
      "Validation Loss: 0.05113215869742164\n",
      "Epoch 73/100\n",
      "Training Loss: 0.11128798600360919\n",
      "Validation Loss: 0.04725891766191428\n",
      "Epoch 74/100\n",
      "Training Loss: 0.11375348823632032\n",
      "Validation Loss: 0.029307023455399372\n",
      "Epoch 75/100\n",
      "Training Loss: 0.10662570175045875\n",
      "Validation Loss: 0.05378768720930368\n",
      "Epoch 76/100\n",
      "Training Loss: 0.11153093540536115\n",
      "Validation Loss: 0.04260517642152043\n",
      "Epoch 77/100\n",
      "Training Loss: 0.10856475660836716\n",
      "Validation Loss: 0.050626618075777105\n",
      "Epoch 78/100\n",
      "Training Loss: 0.11773899545908117\n",
      "Validation Loss: 0.03994226698788831\n",
      "Epoch 79/100\n",
      "Training Loss: 0.10700070024397092\n",
      "Validation Loss: 0.0656341296890177\n",
      "Epoch 80/100\n",
      "Training Loss: 0.10748844011382713\n",
      "Validation Loss: 0.05162521580619348\n",
      "Epoch 81/100\n",
      "Training Loss: 0.10496599088648077\n",
      "Validation Loss: 0.060463103200664725\n",
      "Epoch 82/100\n",
      "Training Loss: 0.10762224554709604\n",
      "Validation Loss: 0.05652158144749109\n",
      "Epoch 83/100\n",
      "Training Loss: 0.11492493264775575\n",
      "Validation Loss: 0.03860545714396756\n",
      "Epoch 84/100\n",
      "Training Loss: 0.09338514463328274\n",
      "Validation Loss: 0.04911090018479294\n",
      "Epoch 85/100\n",
      "Training Loss: 0.09642546451615491\n",
      "Validation Loss: 0.0534507975688169\n",
      "Epoch 86/100\n",
      "Training Loss: 0.10269207451695632\n",
      "Validation Loss: 0.04674653651924785\n",
      "Epoch 87/100\n",
      "Training Loss: 0.11545423629075595\n",
      "Validation Loss: 0.049891777768968956\n",
      "Epoch 88/100\n",
      "Training Loss: 0.10903713522836414\n",
      "Validation Loss: 0.0463937438463667\n",
      "Epoch 89/100\n",
      "Training Loss: 0.10804320001718155\n",
      "Validation Loss: 0.06796851611810775\n",
      "Epoch 90/100\n",
      "Training Loss: 0.11545595074163535\n",
      "Validation Loss: 0.044514768964827985\n",
      "Epoch 91/100\n",
      "Training Loss: 0.09516358666201304\n",
      "Validation Loss: 0.0585084598113846\n",
      "Epoch 92/100\n",
      "Training Loss: 0.1071703834675443\n",
      "Validation Loss: 0.039525756641506184\n",
      "Epoch 93/100\n",
      "Training Loss: 0.09032217809266098\n",
      "Validation Loss: 0.07601290811599175\n",
      "Epoch 94/100\n",
      "Training Loss: 0.1007672055720924\n",
      "Validation Loss: 0.061533295947542886\n",
      "Epoch 95/100\n",
      "Training Loss: 0.10354269746772703\n",
      "Validation Loss: 0.06089987385592545\n",
      "Epoch 96/100\n",
      "Training Loss: 0.10145055142175374\n",
      "Validation Loss: 0.06380321495784132\n",
      "Epoch 97/100\n",
      "Training Loss: 0.10295553731080835\n",
      "Validation Loss: 0.04778967356714287\n",
      "Epoch 98/100\n",
      "Training Loss: 0.09319426622694912\n",
      "Validation Loss: 0.060869910952090954\n",
      "Epoch 99/100\n",
      "Training Loss: 0.09469209767407516\n",
      "Validation Loss: 0.08334727192370513\n",
      "Epoch 100/100\n",
      "Training Loss: 0.09358586688031281\n",
      "Validation Loss: 0.055800969320954595\n",
      "    Trial 2/2 for combination 17/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.4211860249645211\n",
      "Validation Loss: 0.1962402062835587\n",
      "Epoch 2/100\n",
      "Training Loss: 0.3187160963089483\n",
      "Validation Loss: 0.1334683662289218\n",
      "Epoch 3/100\n",
      "Training Loss: 0.2898578747388378\n",
      "Validation Loss: 0.155288745951136\n",
      "Epoch 4/100\n",
      "Training Loss: 0.24244164900761994\n",
      "Validation Loss: 0.15015139355002063\n",
      "Epoch 5/100\n",
      "Training Loss: 0.23881394651243285\n",
      "Validation Loss: 0.11555237528376561\n",
      "Epoch 6/100\n",
      "Training Loss: 0.23233191859105168\n",
      "Validation Loss: 0.08738616136443438\n",
      "Epoch 7/100\n",
      "Training Loss: 0.21508529559598927\n",
      "Validation Loss: 0.21395084506721673\n",
      "Epoch 8/100\n",
      "Training Loss: 0.19855498638371685\n",
      "Validation Loss: 0.12786687609407937\n",
      "Epoch 9/100\n",
      "Training Loss: 0.2144961835028166\n",
      "Validation Loss: 0.10943387568461564\n",
      "Epoch 10/100\n",
      "Training Loss: 0.20799090887063962\n",
      "Validation Loss: 0.07783165140340904\n",
      "Epoch 11/100\n",
      "Training Loss: 0.19070368364061893\n",
      "Validation Loss: 0.12150543541701878\n",
      "Epoch 12/100\n",
      "Training Loss: 0.18128011165377822\n",
      "Validation Loss: 0.11359976693848758\n",
      "Epoch 13/100\n",
      "Training Loss: 0.1892780402620574\n",
      "Validation Loss: 0.10928986222199752\n",
      "Epoch 14/100\n",
      "Training Loss: 0.18346978769779274\n",
      "Validation Loss: 0.0676841842962264\n",
      "Epoch 15/100\n",
      "Training Loss: 0.16789511591749023\n",
      "Validation Loss: 0.07570275387132623\n",
      "Epoch 16/100\n",
      "Training Loss: 0.16933738069039353\n",
      "Validation Loss: 0.076024619625319\n",
      "Epoch 17/100\n",
      "Training Loss: 0.20401761476119848\n",
      "Validation Loss: 0.11419937610404671\n",
      "Epoch 18/100\n",
      "Training Loss: 0.187413408679923\n",
      "Validation Loss: 0.08772077599430114\n",
      "Epoch 19/100\n",
      "Training Loss: 0.1770704488054681\n",
      "Validation Loss: 0.13211680957336427\n",
      "Epoch 20/100\n",
      "Training Loss: 0.18202250327550426\n",
      "Validation Loss: 0.10765727726472471\n",
      "Epoch 21/100\n",
      "Training Loss: 0.16779967475089455\n",
      "Validation Loss: 0.25590310627443175\n",
      "Epoch 22/100\n",
      "Training Loss: 0.16156788870078523\n",
      "Validation Loss: 0.10020359676730158\n",
      "Epoch 23/100\n",
      "Training Loss: 0.15272876377557273\n",
      "Validation Loss: 0.07397623287724604\n",
      "Epoch 24/100\n",
      "Training Loss: 0.1583597000155917\n",
      "Validation Loss: 0.10472921897622753\n",
      "Epoch 25/100\n",
      "Training Loss: 0.16415837764980007\n",
      "Validation Loss: 0.08810720754471112\n",
      "Epoch 26/100\n",
      "Training Loss: 0.1619205446229284\n",
      "Validation Loss: 0.045446616589138344\n",
      "Epoch 27/100\n",
      "Training Loss: 0.15867993972623068\n",
      "Validation Loss: 0.1426850879113421\n",
      "Epoch 28/100\n",
      "Training Loss: 0.16459412284136024\n",
      "Validation Loss: 0.05871398264599691\n",
      "Epoch 29/100\n",
      "Training Loss: 0.16763615642611504\n",
      "Validation Loss: 0.09275211758194243\n",
      "Epoch 30/100\n",
      "Training Loss: 0.1628251423664006\n",
      "Validation Loss: 0.1398321321247006\n",
      "Epoch 31/100\n",
      "Training Loss: 0.15669426246019075\n",
      "Validation Loss: 0.06562327010570473\n",
      "Epoch 32/100\n",
      "Training Loss: 0.16758707149428415\n",
      "Validation Loss: 0.07610236957795782\n",
      "Epoch 33/100\n",
      "Training Loss: 0.15158853544051068\n",
      "Validation Loss: 0.07530885746841626\n",
      "Epoch 34/100\n",
      "Training Loss: 0.1453414046188399\n",
      "Validation Loss: 0.03809335499802048\n",
      "Epoch 35/100\n",
      "Training Loss: 0.15826712025831663\n",
      "Validation Loss: 0.08121680354074262\n",
      "Epoch 36/100\n",
      "Training Loss: 0.15285247191069964\n",
      "Validation Loss: 0.05984214517702421\n",
      "Epoch 37/100\n",
      "Training Loss: 0.15850535404107752\n",
      "Validation Loss: 0.07016147126254367\n",
      "Epoch 38/100\n",
      "Training Loss: 0.15290829474790008\n",
      "Validation Loss: 0.06765229708964637\n",
      "Epoch 39/100\n",
      "Training Loss: 0.15674673121186655\n",
      "Validation Loss: 0.06569823805519058\n",
      "Epoch 40/100\n",
      "Training Loss: 0.14220217967832136\n",
      "Validation Loss: 0.053487416371215735\n",
      "Epoch 41/100\n",
      "Training Loss: 0.14800025342072778\n",
      "Validation Loss: 0.08225561433499631\n",
      "Epoch 42/100\n",
      "Training Loss: 0.15421928746688685\n",
      "Validation Loss: 0.11427561949731455\n",
      "Epoch 43/100\n",
      "Training Loss: 0.14086534858565702\n",
      "Validation Loss: 0.06796232190315907\n",
      "Epoch 44/100\n",
      "Training Loss: 0.1363985969054613\n",
      "Validation Loss: 0.07163032840135311\n",
      "Epoch 45/100\n",
      "Training Loss: 0.155607398004396\n",
      "Validation Loss: 0.08189838254833937\n",
      "Epoch 46/100\n",
      "Training Loss: 0.1267769619640107\n",
      "Validation Loss: 0.07760012951968136\n",
      "Epoch 47/100\n",
      "Training Loss: 0.1300491733715342\n",
      "Validation Loss: 0.04379339464499927\n",
      "Epoch 48/100\n",
      "Training Loss: 0.14561778732392075\n",
      "Validation Loss: 0.11476306892698598\n",
      "Epoch 49/100\n",
      "Training Loss: 0.13313489515011281\n",
      "Validation Loss: 0.04415142286142746\n",
      "Epoch 50/100\n",
      "Training Loss: 0.13969642748313663\n",
      "Validation Loss: 0.0806974082520413\n",
      "Epoch 51/100\n",
      "Training Loss: 0.12097309666933574\n",
      "Validation Loss: 0.06652675557976004\n",
      "Epoch 52/100\n",
      "Training Loss: 0.13095337657184639\n",
      "Validation Loss: 0.10328244899012577\n",
      "Epoch 53/100\n",
      "Training Loss: 0.14932948968434195\n",
      "Validation Loss: 0.0775550955619858\n",
      "Epoch 54/100\n",
      "Training Loss: 0.1339449193667345\n",
      "Validation Loss: 0.05814127059223876\n",
      "Epoch 55/100\n",
      "Training Loss: 0.13251537768409813\n",
      "Validation Loss: 0.07886728439849042\n",
      "Epoch 56/100\n",
      "Training Loss: 0.12204680973030577\n",
      "Validation Loss: 0.03381241154344836\n",
      "Epoch 57/100\n",
      "Training Loss: 0.13496171094312792\n",
      "Validation Loss: 0.06087881069886841\n",
      "Epoch 58/100\n",
      "Training Loss: 0.1167867826847277\n",
      "Validation Loss: 0.09935807912878246\n",
      "Epoch 59/100\n",
      "Training Loss: 0.11075789898477321\n",
      "Validation Loss: 0.07063513939811028\n",
      "Epoch 60/100\n",
      "Training Loss: 0.13498688344517373\n",
      "Validation Loss: 0.06866539098951831\n",
      "Epoch 61/100\n",
      "Training Loss: 0.11980315514232909\n",
      "Validation Loss: 0.09195008458136096\n",
      "Epoch 62/100\n",
      "Training Loss: 0.11787647863834681\n",
      "Validation Loss: 0.0771473732981177\n",
      "Epoch 63/100\n",
      "Training Loss: 0.12402017658763767\n",
      "Validation Loss: 0.06589482125820026\n",
      "Epoch 64/100\n",
      "Training Loss: 0.12181565752250632\n",
      "Validation Loss: 0.056560577335917926\n",
      "Epoch 65/100\n",
      "Training Loss: 0.11393473949201124\n",
      "Validation Loss: 0.04945966936554771\n",
      "Epoch 66/100\n",
      "Training Loss: 0.11125109118956987\n",
      "Validation Loss: 0.055603408410107316\n",
      "Epoch 67/100\n",
      "Training Loss: 0.11611601073276634\n",
      "Validation Loss: 0.06590603634570459\n",
      "Epoch 68/100\n",
      "Training Loss: 0.10678123022029218\n",
      "Validation Loss: 0.04912499860798166\n",
      "Epoch 69/100\n",
      "Training Loss: 0.11834413105912253\n",
      "Validation Loss: 0.05560074559300356\n",
      "Epoch 70/100\n",
      "Training Loss: 0.11508661823958966\n",
      "Validation Loss: 0.07197484935139391\n",
      "Epoch 71/100\n",
      "Training Loss: 0.11174932364182695\n",
      "Validation Loss: 0.0579758020149276\n",
      "Epoch 72/100\n",
      "Training Loss: 0.10045494369637657\n",
      "Validation Loss: 0.055567259571360425\n",
      "Epoch 73/100\n",
      "Training Loss: 0.11827861901157186\n",
      "Validation Loss: 0.0499839926143728\n",
      "Epoch 74/100\n",
      "Training Loss: 0.10204680281238994\n",
      "Validation Loss: 0.09309283760920681\n",
      "Epoch 75/100\n",
      "Training Loss: 0.10747134648956602\n",
      "Validation Loss: 0.03408606189988659\n",
      "Epoch 76/100\n",
      "Training Loss: 0.09860170865975933\n",
      "Validation Loss: 0.06165533434932542\n",
      "Epoch 77/100\n",
      "Training Loss: 0.10063847723129912\n",
      "Validation Loss: 0.05442097167312134\n",
      "Epoch 78/100\n",
      "Training Loss: 0.10923014570477761\n",
      "Validation Loss: 0.06535787122016902\n",
      "Epoch 79/100\n",
      "Training Loss: 0.10336282420991139\n",
      "Validation Loss: 0.09642632262793148\n",
      "Epoch 80/100\n",
      "Training Loss: 0.09968634799741305\n",
      "Validation Loss: 0.046473791727778754\n",
      "Epoch 81/100\n",
      "Training Loss: 0.09179232808422667\n",
      "Validation Loss: 0.059526165567468446\n",
      "Epoch 82/100\n",
      "Training Loss: 0.10556730538090056\n",
      "Validation Loss: 0.05484886881576987\n",
      "Epoch 83/100\n",
      "Training Loss: 0.12228201303294163\n",
      "Validation Loss: 0.06715769522118428\n",
      "Epoch 84/100\n",
      "Training Loss: 0.09477609481181341\n",
      "Validation Loss: 0.03729643407571724\n",
      "Epoch 85/100\n",
      "Training Loss: 0.11138007747530053\n",
      "Validation Loss: 0.05869659869249909\n",
      "Epoch 86/100\n",
      "Training Loss: 0.10853576992759696\n",
      "Validation Loss: 0.05195559180056046\n",
      "Epoch 87/100\n",
      "Training Loss: 0.10502498164846881\n",
      "Validation Loss: 0.06682217263887304\n",
      "Epoch 88/100\n",
      "Training Loss: 0.10384467657053512\n",
      "Validation Loss: 0.053608483385464714\n",
      "Epoch 89/100\n",
      "Training Loss: 0.114317181655605\n",
      "Validation Loss: 0.03974321945229203\n",
      "Epoch 90/100\n",
      "Training Loss: 0.10500115031494069\n",
      "Validation Loss: 0.04960554658885368\n",
      "Epoch 91/100\n",
      "Training Loss: 0.09979708865613464\n",
      "Validation Loss: 0.04451928233785003\n",
      "Epoch 92/100\n",
      "Training Loss: 0.10030277755511739\n",
      "Validation Loss: 0.04239788202063411\n",
      "Epoch 93/100\n",
      "Training Loss: 0.10081723106058016\n",
      "Validation Loss: 0.04713460613953627\n",
      "Epoch 94/100\n",
      "Training Loss: 0.10712235050357348\n",
      "Validation Loss: 0.03611764987826596\n",
      "Epoch 95/100\n",
      "Training Loss: 0.09367581532960798\n",
      "Validation Loss: 0.06876637956914536\n",
      "Epoch 96/100\n",
      "Training Loss: 0.09914071229208722\n",
      "Validation Loss: 0.04443038957258538\n",
      "Epoch 97/100\n",
      "Training Loss: 0.09766062452934784\n",
      "Validation Loss: 0.03913278878120346\n",
      "Epoch 98/100\n",
      "Training Loss: 0.08695237423972234\n",
      "Validation Loss: 0.041142875934085665\n",
      "Epoch 99/100\n",
      "Training Loss: 0.09830979445457337\n",
      "Validation Loss: 0.04894677920120098\n",
      "Epoch 100/100\n",
      "Training Loss: 0.10471610147237409\n",
      "Validation Loss: 0.04728715469585044\n",
      "Combination 17: Avg Training Loss = 0.14448164891813334, Avg Validation Loss = 0.07731621587495192\n",
      "Testing combination 18/48: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0001, 'sequence_length': 50, 'dropout_rate': 0.2}\n",
      "Truncated Train Data Shape: (70, 50, 1)\n",
      "Truncated Validation Data Shape: (10, 50, 1)\n",
      "    Trial 1/2 for combination 18/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.4205367450500269\n",
      "Validation Loss: 0.25760715345333157\n",
      "Epoch 2/100\n",
      "Training Loss: 0.30752518617172175\n",
      "Validation Loss: 0.1421273564273631\n",
      "Epoch 3/100\n",
      "Training Loss: 0.2711669913052949\n",
      "Validation Loss: 0.31219889521825306\n",
      "Epoch 4/100\n",
      "Training Loss: 0.23183094714203364\n",
      "Validation Loss: 0.20909299858783506\n",
      "Epoch 5/100\n",
      "Training Loss: 0.24387883289042867\n",
      "Validation Loss: 0.09543167392076682\n",
      "Epoch 6/100\n",
      "Training Loss: 0.19719970753537766\n",
      "Validation Loss: 0.1978422770911626\n",
      "Epoch 7/100\n",
      "Training Loss: 0.1645109750024571\n",
      "Validation Loss: 0.12310399527109099\n",
      "Epoch 8/100\n",
      "Training Loss: 0.17840840692750312\n",
      "Validation Loss: 0.12754149794181152\n",
      "Epoch 9/100\n",
      "Training Loss: 0.1918459838766812\n",
      "Validation Loss: 0.12797312357265783\n",
      "Epoch 10/100\n",
      "Training Loss: 0.18577774325571533\n",
      "Validation Loss: 0.10138133478148523\n",
      "Epoch 11/100\n",
      "Training Loss: 0.1751471667083236\n",
      "Validation Loss: 0.18061605657252106\n",
      "Epoch 12/100\n",
      "Training Loss: 0.16271767643276128\n",
      "Validation Loss: 0.07781901346252709\n",
      "Epoch 13/100\n",
      "Training Loss: 0.15907282482332827\n",
      "Validation Loss: 0.1659899978046509\n",
      "Epoch 14/100\n",
      "Training Loss: 0.16582503172243301\n",
      "Validation Loss: 0.08188572942388399\n",
      "Epoch 15/100\n",
      "Training Loss: 0.19332713459089257\n",
      "Validation Loss: 0.10045206220527603\n",
      "Epoch 16/100\n",
      "Training Loss: 0.17114693654953875\n",
      "Validation Loss: 0.09429643012137483\n",
      "Epoch 17/100\n",
      "Training Loss: 0.15740872981543455\n",
      "Validation Loss: 0.09109339160502379\n",
      "Epoch 18/100\n",
      "Training Loss: 0.14391971000864337\n",
      "Validation Loss: 0.1560037733973458\n",
      "Epoch 19/100\n",
      "Training Loss: 0.17126056333214637\n",
      "Validation Loss: 0.20669441223186355\n",
      "Epoch 20/100\n",
      "Training Loss: 0.15817035957471282\n",
      "Validation Loss: 0.09021170151382012\n",
      "Epoch 21/100\n",
      "Training Loss: 0.13767840612632667\n",
      "Validation Loss: 0.0856803781161619\n",
      "Epoch 22/100\n",
      "Training Loss: 0.13418267446804225\n",
      "Validation Loss: 0.09572690217025064\n",
      "Epoch 23/100\n",
      "Training Loss: 0.14615554193158442\n",
      "Validation Loss: 0.08119269159438357\n",
      "Epoch 24/100\n",
      "Training Loss: 0.15382139542464496\n",
      "Validation Loss: 0.17776323539475192\n",
      "Epoch 25/100\n",
      "Training Loss: 0.17792289283493049\n",
      "Validation Loss: 0.10941128772465633\n",
      "Epoch 26/100\n",
      "Training Loss: 0.16013424725974626\n",
      "Validation Loss: 0.10536735438008651\n",
      "Epoch 27/100\n",
      "Training Loss: 0.12886063276924106\n",
      "Validation Loss: 0.08104828476257489\n",
      "Epoch 28/100\n",
      "Training Loss: 0.15399946762161001\n",
      "Validation Loss: 0.09116156407564643\n",
      "Epoch 29/100\n",
      "Training Loss: 0.1435342363628029\n",
      "Validation Loss: 0.10677610491765863\n",
      "Epoch 30/100\n",
      "Training Loss: 0.151088146019865\n",
      "Validation Loss: 0.1069939472006439\n",
      "Epoch 31/100\n",
      "Training Loss: 0.14248313540728552\n",
      "Validation Loss: 0.06974201203960893\n",
      "Epoch 32/100\n",
      "Training Loss: 0.1333543196562358\n",
      "Validation Loss: 0.12234698611983501\n",
      "Epoch 33/100\n",
      "Training Loss: 0.14286625759922755\n",
      "Validation Loss: 0.10321430015711082\n",
      "Epoch 34/100\n",
      "Training Loss: 0.1425477969497884\n",
      "Validation Loss: 0.0861587259029344\n",
      "Epoch 35/100\n",
      "Training Loss: 0.13214405488390626\n",
      "Validation Loss: 0.06988917404369685\n",
      "Epoch 36/100\n",
      "Training Loss: 0.12927961254272946\n",
      "Validation Loss: 0.10232396181529246\n",
      "Epoch 37/100\n",
      "Training Loss: 0.12686869772590637\n",
      "Validation Loss: 0.07856219418399848\n",
      "Epoch 38/100\n",
      "Training Loss: 0.13591618530629154\n",
      "Validation Loss: 0.07110456060169217\n",
      "Epoch 39/100\n",
      "Training Loss: 0.12338546580408156\n",
      "Validation Loss: 0.06439024958322472\n",
      "Epoch 40/100\n",
      "Training Loss: 0.12307287325083091\n",
      "Validation Loss: 0.0742930397192602\n",
      "Epoch 41/100\n",
      "Training Loss: 0.1265522844497974\n",
      "Validation Loss: 0.07603062378945627\n",
      "Epoch 42/100\n",
      "Training Loss: 0.1326108795113274\n",
      "Validation Loss: 0.08651117485307357\n",
      "Epoch 43/100\n",
      "Training Loss: 0.12295520077125133\n",
      "Validation Loss: 0.04752461297802776\n",
      "Epoch 44/100\n",
      "Training Loss: 0.13864833667597692\n",
      "Validation Loss: 0.13430914441426228\n",
      "Epoch 45/100\n",
      "Training Loss: 0.14078331219956253\n",
      "Validation Loss: 0.06601725245773898\n",
      "Epoch 46/100\n",
      "Training Loss: 0.12135330653113882\n",
      "Validation Loss: 0.07253960516218734\n",
      "Epoch 47/100\n",
      "Training Loss: 0.10892748192717379\n",
      "Validation Loss: 0.050144009725857294\n",
      "Epoch 48/100\n",
      "Training Loss: 0.12308297195128823\n",
      "Validation Loss: 0.07547780745897038\n",
      "Epoch 49/100\n",
      "Training Loss: 0.10966715558899297\n",
      "Validation Loss: 0.05860388873512\n",
      "Epoch 50/100\n",
      "Training Loss: 0.11799204219132982\n",
      "Validation Loss: 0.03838490982068908\n",
      "Epoch 51/100\n",
      "Training Loss: 0.11768002705626418\n",
      "Validation Loss: 0.06927137872405353\n",
      "Epoch 52/100\n",
      "Training Loss: 0.1194436181670271\n",
      "Validation Loss: 0.04719480110433134\n",
      "Epoch 53/100\n",
      "Training Loss: 0.12640656779059722\n",
      "Validation Loss: 0.03974768346542283\n",
      "Epoch 54/100\n",
      "Training Loss: 0.11983491468835139\n",
      "Validation Loss: 0.04518418468734762\n",
      "Epoch 55/100\n",
      "Training Loss: 0.11760779738095738\n",
      "Validation Loss: 0.055356339403597875\n",
      "Epoch 56/100\n",
      "Training Loss: 0.11775144376125779\n",
      "Validation Loss: 0.040620873592010394\n",
      "Epoch 57/100\n",
      "Training Loss: 0.11385537484017828\n",
      "Validation Loss: 0.10441607534512795\n",
      "Epoch 58/100\n",
      "Training Loss: 0.1049347568700902\n",
      "Validation Loss: 0.11701944515779317\n",
      "Epoch 59/100\n",
      "Training Loss: 0.107568281120413\n",
      "Validation Loss: 0.07211906073348333\n",
      "Epoch 60/100\n",
      "Training Loss: 0.10236765181652593\n",
      "Validation Loss: 0.06679904659389158\n",
      "Epoch 61/100\n",
      "Training Loss: 0.09493787830453548\n",
      "Validation Loss: 0.055128080466128285\n",
      "Epoch 62/100\n",
      "Training Loss: 0.11310561164510798\n",
      "Validation Loss: 0.03914039088457258\n",
      "Epoch 63/100\n",
      "Training Loss: 0.09941077654265054\n",
      "Validation Loss: 0.06190989171343988\n",
      "Epoch 64/100\n",
      "Training Loss: 0.10469493135791909\n",
      "Validation Loss: 0.050026744761278254\n",
      "Epoch 65/100\n",
      "Training Loss: 0.09890450427835297\n",
      "Validation Loss: 0.03936272984936525\n",
      "Epoch 66/100\n",
      "Training Loss: 0.10174320120149995\n",
      "Validation Loss: 0.06149215748510419\n",
      "Epoch 67/100\n",
      "Training Loss: 0.10204973743584288\n",
      "Validation Loss: 0.05768398970570193\n",
      "Epoch 68/100\n",
      "Training Loss: 0.09396465621789576\n",
      "Validation Loss: 0.03836000407959724\n",
      "Epoch 69/100\n",
      "Training Loss: 0.10584247885861597\n",
      "Validation Loss: 0.05328393083844739\n",
      "Epoch 70/100\n",
      "Training Loss: 0.0950415516159559\n",
      "Validation Loss: 0.053306573776016594\n",
      "Epoch 71/100\n",
      "Training Loss: 0.10074238564076865\n",
      "Validation Loss: 0.06447719613181237\n",
      "Epoch 72/100\n",
      "Training Loss: 0.09848566369160767\n",
      "Validation Loss: 0.0640575242314464\n",
      "Epoch 73/100\n",
      "Training Loss: 0.0897605915621953\n",
      "Validation Loss: 0.047642116380275705\n",
      "Epoch 74/100\n",
      "Training Loss: 0.08984666134619676\n",
      "Validation Loss: 0.04053774386867408\n",
      "Epoch 75/100\n",
      "Training Loss: 0.09298843628010424\n",
      "Validation Loss: 0.0685536527412471\n",
      "Epoch 76/100\n",
      "Training Loss: 0.08355386224909145\n",
      "Validation Loss: 0.054326805100703944\n",
      "Epoch 77/100\n",
      "Training Loss: 0.09717130264057511\n",
      "Validation Loss: 0.03980917266329177\n",
      "Epoch 78/100\n",
      "Training Loss: 0.08501132283079225\n",
      "Validation Loss: 0.058969424885823164\n",
      "Epoch 79/100\n",
      "Training Loss: 0.08818086945220771\n",
      "Validation Loss: 0.055732618080975335\n",
      "Epoch 80/100\n",
      "Training Loss: 0.08198479030469909\n",
      "Validation Loss: 0.03598269500280841\n",
      "Epoch 81/100\n",
      "Training Loss: 0.09721634019154624\n",
      "Validation Loss: 0.05165496617371177\n",
      "Epoch 82/100\n",
      "Training Loss: 0.09213857192601434\n",
      "Validation Loss: 0.049171508727697294\n",
      "Epoch 83/100\n",
      "Training Loss: 0.09298881247058209\n",
      "Validation Loss: 0.06390168635137053\n",
      "Epoch 84/100\n",
      "Training Loss: 0.09979488670744338\n",
      "Validation Loss: 0.05744921005536909\n",
      "Epoch 85/100\n",
      "Training Loss: 0.08410218924403397\n",
      "Validation Loss: 0.05043559892220991\n",
      "Epoch 86/100\n",
      "Training Loss: 0.07872688677590681\n",
      "Validation Loss: 0.04219966688214142\n",
      "Epoch 87/100\n",
      "Training Loss: 0.09381052382593802\n",
      "Validation Loss: 0.05305651352006091\n",
      "Epoch 88/100\n",
      "Training Loss: 0.08800514409862359\n",
      "Validation Loss: 0.04518541657519946\n",
      "Epoch 89/100\n",
      "Training Loss: 0.08562661302779422\n",
      "Validation Loss: 0.028342838466774656\n",
      "Epoch 90/100\n",
      "Training Loss: 0.08748312562272366\n",
      "Validation Loss: 0.05534816847418013\n",
      "Epoch 91/100\n",
      "Training Loss: 0.08165743133473459\n",
      "Validation Loss: 0.04199903095662293\n",
      "Epoch 92/100\n",
      "Training Loss: 0.09090429254955755\n",
      "Validation Loss: 0.050474018747776406\n",
      "Epoch 93/100\n",
      "Training Loss: 0.08528588493088093\n",
      "Validation Loss: 0.05084674667215253\n",
      "Epoch 94/100\n",
      "Training Loss: 0.08919734561810691\n",
      "Validation Loss: 0.06870647556600086\n",
      "Epoch 95/100\n",
      "Training Loss: 0.0845482750024407\n",
      "Validation Loss: 0.027779280626412033\n",
      "Epoch 96/100\n",
      "Training Loss: 0.08380994263721661\n",
      "Validation Loss: 0.034239441833371305\n",
      "Epoch 97/100\n",
      "Training Loss: 0.07274331789797575\n",
      "Validation Loss: 0.029867667883330236\n",
      "Epoch 98/100\n",
      "Training Loss: 0.08559912087727121\n",
      "Validation Loss: 0.035076387202883356\n",
      "Epoch 99/100\n",
      "Training Loss: 0.07853615732127132\n",
      "Validation Loss: 0.0566345274660904\n",
      "Epoch 100/100\n",
      "Training Loss: 0.08287254360619582\n",
      "Validation Loss: 0.052155243295829004\n",
      "    Trial 2/2 for combination 18/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.32969900275583847\n",
      "Validation Loss: 0.3053952698602197\n",
      "Epoch 2/100\n",
      "Training Loss: 0.25953782254965035\n",
      "Validation Loss: 0.15279779069379051\n",
      "Epoch 3/100\n",
      "Training Loss: 0.23264807038613675\n",
      "Validation Loss: 0.2023467113182047\n",
      "Epoch 4/100\n",
      "Training Loss: 0.20487737614235504\n",
      "Validation Loss: 0.10972915352888524\n",
      "Epoch 5/100\n",
      "Training Loss: 0.2036646076469261\n",
      "Validation Loss: 0.3660008011706323\n",
      "Epoch 6/100\n",
      "Training Loss: 0.19274471787444453\n",
      "Validation Loss: 0.22126472248787254\n",
      "Epoch 7/100\n",
      "Training Loss: 0.21020913137996855\n",
      "Validation Loss: 0.18577666784229402\n",
      "Epoch 8/100\n",
      "Training Loss: 0.20567695624713653\n",
      "Validation Loss: 0.18753553692786468\n",
      "Epoch 9/100\n",
      "Training Loss: 0.19050327676049666\n",
      "Validation Loss: 0.12280554817059411\n",
      "Epoch 10/100\n",
      "Training Loss: 0.18482918594844433\n",
      "Validation Loss: 0.08392125330528184\n",
      "Epoch 11/100\n",
      "Training Loss: 0.17245136504100272\n",
      "Validation Loss: 0.08209012410803396\n",
      "Epoch 12/100\n",
      "Training Loss: 0.16171668901604075\n",
      "Validation Loss: 0.12981212009357856\n",
      "Epoch 13/100\n",
      "Training Loss: 0.17069097018602716\n",
      "Validation Loss: 0.06266890653830917\n",
      "Epoch 14/100\n",
      "Training Loss: 0.1520341884122053\n",
      "Validation Loss: 0.09244716043072411\n",
      "Epoch 15/100\n",
      "Training Loss: 0.16191884577129037\n",
      "Validation Loss: 0.0867210360839885\n",
      "Epoch 16/100\n",
      "Training Loss: 0.1608395772344744\n",
      "Validation Loss: 0.08296371361971686\n",
      "Epoch 17/100\n",
      "Training Loss: 0.13422466922019974\n",
      "Validation Loss: 0.09901952697620386\n",
      "Epoch 18/100\n",
      "Training Loss: 0.14096478574670973\n",
      "Validation Loss: 0.09127612657530597\n",
      "Epoch 19/100\n",
      "Training Loss: 0.16334505677192948\n",
      "Validation Loss: 0.07157046470886716\n",
      "Epoch 20/100\n",
      "Training Loss: 0.14795033648858477\n",
      "Validation Loss: 0.07460644777981681\n",
      "Epoch 21/100\n",
      "Training Loss: 0.15040587588583584\n",
      "Validation Loss: 0.11872576633886418\n",
      "Epoch 22/100\n",
      "Training Loss: 0.1363054466076734\n",
      "Validation Loss: 0.13401570062946283\n",
      "Epoch 23/100\n",
      "Training Loss: 0.14920860651813841\n",
      "Validation Loss: 0.0889666193487805\n",
      "Epoch 24/100\n",
      "Training Loss: 0.1408532236708913\n",
      "Validation Loss: 0.12173933504837671\n",
      "Epoch 25/100\n",
      "Training Loss: 0.12752810092656797\n",
      "Validation Loss: 0.0900980218600873\n",
      "Epoch 26/100\n",
      "Training Loss: 0.13182918211958036\n",
      "Validation Loss: 0.09117575634145167\n",
      "Epoch 27/100\n",
      "Training Loss: 0.13308437576673807\n",
      "Validation Loss: 0.07734745917116241\n",
      "Epoch 28/100\n",
      "Training Loss: 0.12557370175401825\n",
      "Validation Loss: 0.09432995230077243\n",
      "Epoch 29/100\n",
      "Training Loss: 0.12161572524495363\n",
      "Validation Loss: 0.10735101782564464\n",
      "Epoch 30/100\n",
      "Training Loss: 0.1364135202033014\n",
      "Validation Loss: 0.08882807958963833\n",
      "Epoch 31/100\n",
      "Training Loss: 0.139478342041087\n",
      "Validation Loss: 0.08011765920953356\n",
      "Epoch 32/100\n",
      "Training Loss: 0.13481200755075473\n",
      "Validation Loss: 0.09979132171103096\n",
      "Epoch 33/100\n",
      "Training Loss: 0.12604449725506667\n",
      "Validation Loss: 0.07446384641736994\n",
      "Epoch 34/100\n",
      "Training Loss: 0.12989499843576283\n",
      "Validation Loss: 0.07523662922118035\n",
      "Epoch 35/100\n",
      "Training Loss: 0.11638619224990873\n",
      "Validation Loss: 0.08178710196060017\n",
      "Epoch 36/100\n",
      "Training Loss: 0.13694266620975107\n",
      "Validation Loss: 0.06072139579337382\n",
      "Epoch 37/100\n",
      "Training Loss: 0.1243515392226821\n",
      "Validation Loss: 0.09291182242596188\n",
      "Epoch 38/100\n",
      "Training Loss: 0.11567154610555419\n",
      "Validation Loss: 0.09400716102267928\n",
      "Epoch 39/100\n",
      "Training Loss: 0.14725497957239791\n",
      "Validation Loss: 0.06296046925927648\n",
      "Epoch 40/100\n",
      "Training Loss: 0.11662806631744689\n",
      "Validation Loss: 0.08912398953568194\n",
      "Epoch 41/100\n",
      "Training Loss: 0.11424767428452351\n",
      "Validation Loss: 0.07024735005218113\n",
      "Epoch 42/100\n",
      "Training Loss: 0.12223569853272138\n",
      "Validation Loss: 0.06813249761660427\n",
      "Epoch 43/100\n",
      "Training Loss: 0.11201931219793791\n",
      "Validation Loss: 0.06560330477547915\n",
      "Epoch 44/100\n",
      "Training Loss: 0.13843275584814957\n",
      "Validation Loss: 0.06852633049978513\n",
      "Epoch 45/100\n",
      "Training Loss: 0.09974025081787026\n",
      "Validation Loss: 0.09872299276643257\n",
      "Epoch 46/100\n",
      "Training Loss: 0.10754592479804807\n",
      "Validation Loss: 0.08411397288035785\n",
      "Epoch 47/100\n",
      "Training Loss: 0.11728134473031805\n",
      "Validation Loss: 0.07711621452475462\n",
      "Epoch 48/100\n",
      "Training Loss: 0.12155352579897022\n",
      "Validation Loss: 0.06670640092085318\n",
      "Epoch 49/100\n",
      "Training Loss: 0.12181662545018757\n",
      "Validation Loss: 0.10231563649793958\n",
      "Epoch 50/100\n",
      "Training Loss: 0.11409065267620232\n",
      "Validation Loss: 0.0824720799370517\n",
      "Epoch 51/100\n",
      "Training Loss: 0.10541046514483272\n",
      "Validation Loss: 0.07164024499590554\n",
      "Epoch 52/100\n",
      "Training Loss: 0.12152211040032167\n",
      "Validation Loss: 0.09030691002332224\n",
      "Epoch 53/100\n",
      "Training Loss: 0.10680864068854111\n",
      "Validation Loss: 0.07484285459164988\n",
      "Epoch 54/100\n",
      "Training Loss: 0.10078009240936539\n",
      "Validation Loss: 0.05089581195273194\n",
      "Epoch 55/100\n",
      "Training Loss: 0.10756636988040782\n",
      "Validation Loss: 0.06470973456184387\n",
      "Epoch 56/100\n",
      "Training Loss: 0.09275843222461631\n",
      "Validation Loss: 0.08155906795633086\n",
      "Epoch 57/100\n",
      "Training Loss: 0.10801841845339362\n",
      "Validation Loss: 0.06597181475955691\n",
      "Epoch 58/100\n",
      "Training Loss: 0.10880060181971303\n",
      "Validation Loss: 0.06761334424351441\n",
      "Epoch 59/100\n",
      "Training Loss: 0.10553009083923776\n",
      "Validation Loss: 0.05861270293247134\n",
      "Epoch 60/100\n",
      "Training Loss: 0.10259445915533665\n",
      "Validation Loss: 0.07692987728753449\n",
      "Epoch 61/100\n",
      "Training Loss: 0.09934402610455877\n",
      "Validation Loss: 0.048576216382940754\n",
      "Epoch 62/100\n",
      "Training Loss: 0.09945572926971152\n",
      "Validation Loss: 0.06672423280165002\n",
      "Epoch 63/100\n",
      "Training Loss: 0.09596110426769683\n",
      "Validation Loss: 0.04285410995501544\n",
      "Epoch 64/100\n",
      "Training Loss: 0.09226022595455885\n",
      "Validation Loss: 0.07907089726927659\n",
      "Epoch 65/100\n",
      "Training Loss: 0.09728504285961899\n",
      "Validation Loss: 0.029165553162163495\n",
      "Epoch 66/100\n",
      "Training Loss: 0.09608163693531248\n",
      "Validation Loss: 0.07576566393460767\n",
      "Epoch 67/100\n",
      "Training Loss: 0.0948479440249186\n",
      "Validation Loss: 0.08994985289695306\n",
      "Epoch 68/100\n",
      "Training Loss: 0.09758433041749898\n",
      "Validation Loss: 0.020295426307925725\n",
      "Epoch 69/100\n",
      "Training Loss: 0.09562881232303307\n",
      "Validation Loss: 0.09783214492438876\n",
      "Epoch 70/100\n",
      "Training Loss: 0.0980540071712166\n",
      "Validation Loss: 0.04025958856255142\n",
      "Epoch 71/100\n",
      "Training Loss: 0.09515174019205108\n",
      "Validation Loss: 0.04510205927743051\n",
      "Epoch 72/100\n",
      "Training Loss: 0.09671565866138238\n",
      "Validation Loss: 0.08521871745824619\n",
      "Epoch 73/100\n",
      "Training Loss: 0.08556253181999281\n",
      "Validation Loss: 0.04810313744385012\n",
      "Epoch 74/100\n",
      "Training Loss: 0.09513907698596148\n",
      "Validation Loss: 0.08049404434033089\n",
      "Epoch 75/100\n",
      "Training Loss: 0.08319252484665197\n",
      "Validation Loss: 0.07427346115338891\n",
      "Epoch 76/100\n",
      "Training Loss: 0.09652020135951515\n",
      "Validation Loss: 0.11927455708731946\n",
      "Epoch 77/100\n",
      "Training Loss: 0.09202117706300313\n",
      "Validation Loss: 0.0978521908126598\n",
      "Epoch 78/100\n",
      "Training Loss: 0.08578201050373471\n",
      "Validation Loss: 0.04065671250562786\n",
      "Epoch 79/100\n",
      "Training Loss: 0.09914730277680987\n",
      "Validation Loss: 0.08512043533589277\n",
      "Epoch 80/100\n",
      "Training Loss: 0.08839764679006326\n",
      "Validation Loss: 0.04396841036346712\n",
      "Epoch 81/100\n",
      "Training Loss: 0.09705303821076292\n",
      "Validation Loss: 0.08514838634304335\n",
      "Epoch 82/100\n",
      "Training Loss: 0.08809572747599813\n",
      "Validation Loss: 0.07718047339062492\n",
      "Epoch 83/100\n",
      "Training Loss: 0.08086886400138922\n",
      "Validation Loss: 0.08059331325245081\n",
      "Epoch 84/100\n",
      "Training Loss: 0.08169615541394239\n",
      "Validation Loss: 0.05924223561174004\n",
      "Epoch 85/100\n",
      "Training Loss: 0.0761638066787943\n",
      "Validation Loss: 0.08558395327980046\n",
      "Epoch 86/100\n",
      "Training Loss: 0.09112432283197476\n",
      "Validation Loss: 0.15287653769182416\n",
      "Epoch 87/100\n",
      "Training Loss: 0.080399954830298\n",
      "Validation Loss: 0.05950593310651202\n",
      "Epoch 88/100\n",
      "Training Loss: 0.07552438155355538\n",
      "Validation Loss: 0.12436151379026797\n",
      "Epoch 89/100\n",
      "Training Loss: 0.07635985683504187\n",
      "Validation Loss: 0.086352589104654\n",
      "Epoch 90/100\n",
      "Training Loss: 0.07645603590564332\n",
      "Validation Loss: 0.05915401512774445\n",
      "Epoch 91/100\n",
      "Training Loss: 0.0751102535100849\n",
      "Validation Loss: 0.06943325926171837\n",
      "Epoch 92/100\n",
      "Training Loss: 0.08094441453878412\n",
      "Validation Loss: 0.10696073277630488\n",
      "Epoch 93/100\n",
      "Training Loss: 0.07741398858592898\n",
      "Validation Loss: 0.07207681513907663\n",
      "Epoch 94/100\n",
      "Training Loss: 0.07738499916203159\n",
      "Validation Loss: 0.08254330198806012\n",
      "Epoch 95/100\n",
      "Training Loss: 0.07883453195338713\n",
      "Validation Loss: 0.06938757656344338\n",
      "Epoch 96/100\n",
      "Training Loss: 0.07243010910439804\n",
      "Validation Loss: 0.1051484498488086\n",
      "Epoch 97/100\n",
      "Training Loss: 0.07705186323061637\n",
      "Validation Loss: 0.06629503968846857\n",
      "Epoch 98/100\n",
      "Training Loss: 0.07881575049790007\n",
      "Validation Loss: 0.09473205550814214\n",
      "Epoch 99/100\n",
      "Training Loss: 0.07926608125162568\n",
      "Validation Loss: 0.06518058679275386\n",
      "Epoch 100/100\n",
      "Training Loss: 0.07588679190804044\n",
      "Validation Loss: 0.1344970307138696\n",
      "Combination 18: Avg Training Loss = 0.12521521023134533, Avg Validation Loss = 0.0866919406315913\n",
      "Testing combination 19/48: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.001, 'sequence_length': 6, 'dropout_rate': 0.1}\n",
      "Truncated Train Data Shape: (70, 6, 1)\n",
      "Truncated Validation Data Shape: (10, 6, 1)\n",
      "    Trial 1/2 for combination 19/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.3992873774877379\n",
      "Validation Loss: 0.14031110438965522\n",
      "Epoch 2/100\n",
      "Training Loss: 0.2423849835719996\n",
      "Validation Loss: 0.15185178570370725\n",
      "Epoch 3/100\n",
      "Training Loss: 0.2427004281511634\n",
      "Validation Loss: 0.08075856041170856\n",
      "Epoch 4/100\n",
      "Training Loss: 0.19238413040981017\n",
      "Validation Loss: 0.15509787651695817\n",
      "Epoch 5/100\n",
      "Training Loss: 0.22676929019086986\n",
      "Validation Loss: 0.11920201686400593\n",
      "Epoch 6/100\n",
      "Training Loss: 0.1930459142355439\n",
      "Validation Loss: 0.11813995439154865\n",
      "Epoch 7/100\n",
      "Training Loss: 0.16076358835178367\n",
      "Validation Loss: 0.11435714603593787\n",
      "Epoch 8/100\n",
      "Training Loss: 0.17081789346055426\n",
      "Validation Loss: 0.08047035300925329\n",
      "Epoch 9/100\n",
      "Training Loss: 0.15258536190832483\n",
      "Validation Loss: 0.1361978307304216\n",
      "Epoch 10/100\n",
      "Training Loss: 0.12085042715357387\n",
      "Validation Loss: 0.07003182138618039\n",
      "Epoch 11/100\n",
      "Training Loss: 0.137102330462636\n",
      "Validation Loss: 0.118488654290465\n",
      "Epoch 12/100\n",
      "Training Loss: 0.12701325479303968\n",
      "Validation Loss: 0.05348008309576987\n",
      "Epoch 13/100\n",
      "Training Loss: 0.10797399125424152\n",
      "Validation Loss: 0.06818915222265862\n",
      "Epoch 14/100\n",
      "Training Loss: 0.11564465425457175\n",
      "Validation Loss: 0.10985686002930972\n",
      "Epoch 15/100\n",
      "Training Loss: 0.11096009619772729\n",
      "Validation Loss: 0.056456805344655414\n",
      "Epoch 16/100\n",
      "Training Loss: 0.10960600629542598\n",
      "Validation Loss: 0.05837705804789086\n",
      "Epoch 17/100\n",
      "Training Loss: 0.10460076965904917\n",
      "Validation Loss: 0.044305214675812485\n",
      "Epoch 18/100\n",
      "Training Loss: 0.10611576821978339\n",
      "Validation Loss: 0.04344329893427717\n",
      "Epoch 19/100\n",
      "Training Loss: 0.09715309529661874\n",
      "Validation Loss: 0.05281530301182508\n",
      "Epoch 20/100\n",
      "Training Loss: 0.10409054071636907\n",
      "Validation Loss: 0.06385572256658165\n",
      "Epoch 21/100\n",
      "Training Loss: 0.08926708561943152\n",
      "Validation Loss: 0.05213860845772091\n",
      "Epoch 22/100\n",
      "Training Loss: 0.09093611055110518\n",
      "Validation Loss: 0.05347859359169099\n",
      "Epoch 23/100\n",
      "Training Loss: 0.0747767142869676\n",
      "Validation Loss: 0.07010990844047764\n",
      "Epoch 24/100\n",
      "Training Loss: 0.09579124113306156\n",
      "Validation Loss: 0.040891467789116176\n",
      "Epoch 25/100\n",
      "Training Loss: 0.08616972298897498\n",
      "Validation Loss: 0.05287170643721315\n",
      "Epoch 26/100\n",
      "Training Loss: 0.09299253440434042\n",
      "Validation Loss: 0.061109679596135204\n",
      "Epoch 27/100\n",
      "Training Loss: 0.08115063112220366\n",
      "Validation Loss: 0.06370675451808157\n",
      "Epoch 28/100\n",
      "Training Loss: 0.08358531037705202\n",
      "Validation Loss: 0.037886928531182376\n",
      "Epoch 29/100\n",
      "Training Loss: 0.0746717816742575\n",
      "Validation Loss: 0.051163382466471764\n",
      "Epoch 30/100\n",
      "Training Loss: 0.08371488177153623\n",
      "Validation Loss: 0.049571351867818435\n",
      "Epoch 31/100\n",
      "Training Loss: 0.07473631442615769\n",
      "Validation Loss: 0.046013701709751775\n",
      "Epoch 32/100\n",
      "Training Loss: 0.07746555700844533\n",
      "Validation Loss: 0.04523691536889527\n",
      "Epoch 33/100\n",
      "Training Loss: 0.08498023603195182\n",
      "Validation Loss: 0.04756499510543942\n",
      "Epoch 34/100\n",
      "Training Loss: 0.07850980252100435\n",
      "Validation Loss: 0.04189030206105042\n",
      "Epoch 35/100\n",
      "Training Loss: 0.06751863589672705\n",
      "Validation Loss: 0.0505061798339768\n",
      "Epoch 36/100\n",
      "Training Loss: 0.08194581921767916\n",
      "Validation Loss: 0.05745003162290363\n",
      "Epoch 37/100\n",
      "Training Loss: 0.08293978061661021\n",
      "Validation Loss: 0.04531084995731137\n",
      "Epoch 38/100\n",
      "Training Loss: 0.07900397515128055\n",
      "Validation Loss: 0.05652371092662084\n",
      "Epoch 39/100\n",
      "Training Loss: 0.071399210863453\n",
      "Validation Loss: 0.04006283674918539\n",
      "Epoch 40/100\n",
      "Training Loss: 0.08329571412938042\n",
      "Validation Loss: 0.04481809438040014\n",
      "Epoch 41/100\n",
      "Training Loss: 0.0736408446370731\n",
      "Validation Loss: 0.04886456969734184\n",
      "Epoch 42/100\n",
      "Training Loss: 0.06934632389870339\n",
      "Validation Loss: 0.05260319262726595\n",
      "Epoch 43/100\n",
      "Training Loss: 0.07068473071129239\n",
      "Validation Loss: 0.038861594188936185\n",
      "Epoch 44/100\n",
      "Training Loss: 0.06708134491133343\n",
      "Validation Loss: 0.03257819709108503\n",
      "Epoch 45/100\n",
      "Training Loss: 0.07451939217481912\n",
      "Validation Loss: 0.034583641536553446\n",
      "Epoch 46/100\n",
      "Training Loss: 0.07447718309305466\n",
      "Validation Loss: 0.03947054363201099\n",
      "Epoch 47/100\n",
      "Training Loss: 0.06296543624127549\n",
      "Validation Loss: 0.0329975340803733\n",
      "Epoch 48/100\n",
      "Training Loss: 0.0787445788312202\n",
      "Validation Loss: 0.04082448396578734\n",
      "Epoch 49/100\n",
      "Training Loss: 0.07387952018093868\n",
      "Validation Loss: 0.044757237450731666\n",
      "Epoch 50/100\n",
      "Training Loss: 0.07845058007176056\n",
      "Validation Loss: 0.04830195923732124\n",
      "Epoch 51/100\n",
      "Training Loss: 0.06709562166659758\n",
      "Validation Loss: 0.05301747593367941\n",
      "Epoch 52/100\n",
      "Training Loss: 0.07935311987470475\n",
      "Validation Loss: 0.036444678595729744\n",
      "Epoch 53/100\n",
      "Training Loss: 0.07819144404061447\n",
      "Validation Loss: 0.036755073197434894\n",
      "Epoch 54/100\n",
      "Training Loss: 0.06505733409125318\n",
      "Validation Loss: 0.0545246340439559\n",
      "Epoch 55/100\n",
      "Training Loss: 0.07950200225606528\n",
      "Validation Loss: 0.0331258409255995\n",
      "Epoch 56/100\n",
      "Training Loss: 0.07902942540237151\n",
      "Validation Loss: 0.05005384867006024\n",
      "Epoch 57/100\n",
      "Training Loss: 0.07005625463841203\n",
      "Validation Loss: 0.0391293403938804\n",
      "Epoch 58/100\n",
      "Training Loss: 0.07367854732299013\n",
      "Validation Loss: 0.059049443217208994\n",
      "Epoch 59/100\n",
      "Training Loss: 0.07325349247772935\n",
      "Validation Loss: 0.036581867093578226\n",
      "Epoch 60/100\n",
      "Training Loss: 0.0768045796232691\n",
      "Validation Loss: 0.040236357538763465\n",
      "Epoch 61/100\n",
      "Training Loss: 0.0747474182142214\n",
      "Validation Loss: 0.050730776041163037\n",
      "Epoch 62/100\n",
      "Training Loss: 0.06849164107776727\n",
      "Validation Loss: 0.043423141215198274\n",
      "Epoch 63/100\n",
      "Training Loss: 0.07253963885380676\n",
      "Validation Loss: 0.06180117627749615\n",
      "Epoch 64/100\n",
      "Training Loss: 0.06279711738103107\n",
      "Validation Loss: 0.043186356576594406\n",
      "Epoch 65/100\n",
      "Training Loss: 0.08480889852980583\n",
      "Validation Loss: 0.046013893572607364\n",
      "Epoch 66/100\n",
      "Training Loss: 0.06940632183467448\n",
      "Validation Loss: 0.07746304516629748\n",
      "Epoch 67/100\n",
      "Training Loss: 0.0665633417442905\n",
      "Validation Loss: 0.05258284089500017\n",
      "Epoch 68/100\n",
      "Training Loss: 0.08108943683824693\n",
      "Validation Loss: 0.049548972293071425\n",
      "Epoch 69/100\n",
      "Training Loss: 0.07410894925202786\n",
      "Validation Loss: 0.049598432084068216\n",
      "Epoch 70/100\n",
      "Training Loss: 0.07420196229790667\n",
      "Validation Loss: 0.07443998196032489\n",
      "Epoch 71/100\n",
      "Training Loss: 0.07548849589505562\n",
      "Validation Loss: 0.07764720503899727\n",
      "Epoch 72/100\n",
      "Training Loss: 0.07829320661791894\n",
      "Validation Loss: 0.03525583069285225\n",
      "Epoch 73/100\n",
      "Training Loss: 0.07762507088636822\n",
      "Validation Loss: 0.04663244714410315\n",
      "Epoch 74/100\n",
      "Training Loss: 0.0810168374349655\n",
      "Validation Loss: 0.044010557893802565\n",
      "Epoch 75/100\n",
      "Training Loss: 0.07691965966129995\n",
      "Validation Loss: 0.05830477381176685\n",
      "Epoch 76/100\n",
      "Training Loss: 0.07060654030044981\n",
      "Validation Loss: 0.0400846045234707\n",
      "Epoch 77/100\n",
      "Training Loss: 0.06653651641007174\n",
      "Validation Loss: 0.06394464277298532\n",
      "Epoch 78/100\n",
      "Training Loss: 0.07222897270131845\n",
      "Validation Loss: 0.05709373691505648\n",
      "Epoch 79/100\n",
      "Training Loss: 0.056950528651525806\n",
      "Validation Loss: 0.033945323434656324\n",
      "Epoch 80/100\n",
      "Training Loss: 0.06657038769718228\n",
      "Validation Loss: 0.0542082251721966\n",
      "Epoch 81/100\n",
      "Training Loss: 0.07063926095361137\n",
      "Validation Loss: 0.06544698889839058\n",
      "Epoch 82/100\n",
      "Training Loss: 0.07361278806379346\n",
      "Validation Loss: 0.041792252720595766\n",
      "Epoch 83/100\n",
      "Training Loss: 0.07346790325378957\n",
      "Validation Loss: 0.029119628668631447\n",
      "Epoch 84/100\n",
      "Training Loss: 0.0763051446861512\n",
      "Validation Loss: 0.03457486387897204\n",
      "Epoch 85/100\n",
      "Training Loss: 0.08566524633914381\n",
      "Validation Loss: 0.08775408150806119\n",
      "Epoch 86/100\n",
      "Training Loss: 0.07421518389043591\n",
      "Validation Loss: 0.09445699597857168\n",
      "Epoch 87/100\n",
      "Training Loss: 0.08238751095444348\n",
      "Validation Loss: 0.05788176162223644\n",
      "Epoch 88/100\n",
      "Training Loss: 0.07102047711744709\n",
      "Validation Loss: 0.048296341098142945\n",
      "Epoch 89/100\n",
      "Training Loss: 0.073109544000429\n",
      "Validation Loss: 0.044728857488122796\n",
      "Epoch 90/100\n",
      "Training Loss: 0.06960163354923184\n",
      "Validation Loss: 0.07418820370631085\n",
      "Epoch 91/100\n",
      "Training Loss: 0.06043483782541565\n",
      "Validation Loss: 0.07292814040667603\n",
      "Epoch 92/100\n",
      "Training Loss: 0.0747184584463103\n",
      "Validation Loss: 0.08595530031040934\n",
      "Epoch 93/100\n",
      "Training Loss: 0.08565725401739281\n",
      "Validation Loss: 0.05006225589939334\n",
      "Epoch 94/100\n",
      "Training Loss: 0.08181749699362716\n",
      "Validation Loss: 0.07179204463181525\n",
      "Epoch 95/100\n",
      "Training Loss: 0.06454890689033618\n",
      "Validation Loss: 0.06433747402602077\n",
      "Epoch 96/100\n",
      "Training Loss: 0.06836528506351104\n",
      "Validation Loss: 0.05925852154471421\n",
      "Epoch 97/100\n",
      "Training Loss: 0.07551557434950433\n",
      "Validation Loss: 0.039347004005760106\n",
      "Epoch 98/100\n",
      "Training Loss: 0.07150596103388768\n",
      "Validation Loss: 0.03464532034358343\n",
      "Epoch 99/100\n",
      "Training Loss: 0.0691172455418058\n",
      "Validation Loss: 0.06497972533204396\n",
      "Epoch 100/100\n",
      "Training Loss: 0.0639268860809671\n",
      "Validation Loss: 0.05899413992168716\n",
      "    Trial 2/2 for combination 19/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.3890326765735539\n",
      "Validation Loss: 0.3286715793419539\n",
      "Epoch 2/100\n",
      "Training Loss: 0.284411484563986\n",
      "Validation Loss: 0.20484180293729914\n",
      "Epoch 3/100\n",
      "Training Loss: 0.1832855293181101\n",
      "Validation Loss: 0.17370901700344699\n",
      "Epoch 4/100\n",
      "Training Loss: 0.20265720127987988\n",
      "Validation Loss: 0.24010669763559367\n",
      "Epoch 5/100\n",
      "Training Loss: 0.19517935177488838\n",
      "Validation Loss: 0.11198151649458596\n",
      "Epoch 6/100\n",
      "Training Loss: 0.17485259278811752\n",
      "Validation Loss: 0.16464542833646975\n",
      "Epoch 7/100\n",
      "Training Loss: 0.15689157219540759\n",
      "Validation Loss: 0.07556469120484546\n",
      "Epoch 8/100\n",
      "Training Loss: 0.13843976003203953\n",
      "Validation Loss: 0.092012459538354\n",
      "Epoch 9/100\n",
      "Training Loss: 0.11369512399651167\n",
      "Validation Loss: 0.0592695490314746\n",
      "Epoch 10/100\n",
      "Training Loss: 0.13756470878058277\n",
      "Validation Loss: 0.12813779928142555\n",
      "Epoch 11/100\n",
      "Training Loss: 0.13328486607858248\n",
      "Validation Loss: 0.12495577266151518\n",
      "Epoch 12/100\n",
      "Training Loss: 0.09922473384915052\n",
      "Validation Loss: 0.09176563372533184\n",
      "Epoch 13/100\n",
      "Training Loss: 0.11997291781618473\n",
      "Validation Loss: 0.12298424898158236\n",
      "Epoch 14/100\n",
      "Training Loss: 0.10134351727192618\n",
      "Validation Loss: 0.06561166004463528\n",
      "Epoch 15/100\n",
      "Training Loss: 0.09534625304561835\n",
      "Validation Loss: 0.05409893173698728\n",
      "Epoch 16/100\n",
      "Training Loss: 0.10350456902655798\n",
      "Validation Loss: 0.08711270386783009\n",
      "Epoch 17/100\n",
      "Training Loss: 0.10300720140513668\n",
      "Validation Loss: 0.07057053594023918\n",
      "Epoch 18/100\n",
      "Training Loss: 0.0906036701794893\n",
      "Validation Loss: 0.08125198545284226\n",
      "Epoch 19/100\n",
      "Training Loss: 0.09878998375960334\n",
      "Validation Loss: 0.06912325495142536\n",
      "Epoch 20/100\n",
      "Training Loss: 0.08879956482324301\n",
      "Validation Loss: 0.06322984864197208\n",
      "Epoch 21/100\n",
      "Training Loss: 0.10073150095457978\n",
      "Validation Loss: 0.07603135984925452\n",
      "Epoch 22/100\n",
      "Training Loss: 0.08403960130377741\n",
      "Validation Loss: 0.09278778754788822\n",
      "Epoch 23/100\n",
      "Training Loss: 0.08571208652504586\n",
      "Validation Loss: 0.0337302104764894\n",
      "Epoch 24/100\n",
      "Training Loss: 0.0888036264151348\n",
      "Validation Loss: 0.06978596252257942\n",
      "Epoch 25/100\n",
      "Training Loss: 0.07937592613509542\n",
      "Validation Loss: 0.059331202911974926\n",
      "Epoch 26/100\n",
      "Training Loss: 0.08431367022256445\n",
      "Validation Loss: 0.03649681462757745\n",
      "Epoch 27/100\n",
      "Training Loss: 0.08362361327039611\n",
      "Validation Loss: 0.05335938336364672\n",
      "Epoch 28/100\n",
      "Training Loss: 0.07440463312700696\n",
      "Validation Loss: 0.06335544040965665\n",
      "Epoch 29/100\n",
      "Training Loss: 0.07872076490862083\n",
      "Validation Loss: 0.0360696462337451\n",
      "Epoch 30/100\n",
      "Training Loss: 0.08952515427046585\n",
      "Validation Loss: 0.06750924138230334\n",
      "Epoch 31/100\n",
      "Training Loss: 0.08150478602777703\n",
      "Validation Loss: 0.05552146179563119\n",
      "Epoch 32/100\n",
      "Training Loss: 0.07831244659198261\n",
      "Validation Loss: 0.06099484496465416\n",
      "Epoch 33/100\n",
      "Training Loss: 0.08303955432930772\n",
      "Validation Loss: 0.06560539917642992\n",
      "Epoch 34/100\n",
      "Training Loss: 0.08328564593137659\n",
      "Validation Loss: 0.046813495837199716\n",
      "Epoch 35/100\n",
      "Training Loss: 0.06738769441652455\n",
      "Validation Loss: 0.050380422515830525\n",
      "Epoch 36/100\n",
      "Training Loss: 0.09233591257281902\n",
      "Validation Loss: 0.029390866887880934\n",
      "Epoch 37/100\n",
      "Training Loss: 0.08902703278176127\n",
      "Validation Loss: 0.05940358184773763\n",
      "Epoch 38/100\n",
      "Training Loss: 0.07921299713523378\n",
      "Validation Loss: 0.0521957087389602\n",
      "Epoch 39/100\n",
      "Training Loss: 0.07002392950374534\n",
      "Validation Loss: 0.06440223521651772\n",
      "Epoch 40/100\n",
      "Training Loss: 0.06622293553253636\n",
      "Validation Loss: 0.073346854429366\n",
      "Epoch 41/100\n",
      "Training Loss: 0.07509232803546409\n",
      "Validation Loss: 0.04494348726841609\n",
      "Epoch 42/100\n",
      "Training Loss: 0.07011873212626879\n",
      "Validation Loss: 0.05085654315950515\n",
      "Epoch 43/100\n",
      "Training Loss: 0.07690071984134177\n",
      "Validation Loss: 0.08077157201876517\n",
      "Epoch 44/100\n",
      "Training Loss: 0.07219173501409294\n",
      "Validation Loss: 0.055921859633471925\n",
      "Epoch 45/100\n",
      "Training Loss: 0.07430461483810374\n",
      "Validation Loss: 0.04726944711617469\n",
      "Epoch 46/100\n",
      "Training Loss: 0.07172004901282235\n",
      "Validation Loss: 0.05909875362395441\n",
      "Epoch 47/100\n",
      "Training Loss: 0.07740327964094762\n",
      "Validation Loss: 0.05710499877927041\n",
      "Epoch 48/100\n",
      "Training Loss: 0.07205540915009144\n",
      "Validation Loss: 0.04013666413799163\n",
      "Epoch 49/100\n",
      "Training Loss: 0.08445025462868384\n",
      "Validation Loss: 0.055772824949761615\n",
      "Epoch 50/100\n",
      "Training Loss: 0.07482695829180554\n",
      "Validation Loss: 0.02504976474419735\n",
      "Epoch 51/100\n",
      "Training Loss: 0.0803643718419563\n",
      "Validation Loss: 0.04795079450948278\n",
      "Epoch 52/100\n",
      "Training Loss: 0.06700394043302892\n",
      "Validation Loss: 0.04184681524609072\n",
      "Epoch 53/100\n",
      "Training Loss: 0.07594167729806321\n",
      "Validation Loss: 0.06382779720638003\n",
      "Epoch 54/100\n",
      "Training Loss: 0.0786420530195854\n",
      "Validation Loss: 0.044511438499937786\n",
      "Epoch 55/100\n",
      "Training Loss: 0.06884261664443397\n",
      "Validation Loss: 0.02784126147927935\n",
      "Epoch 56/100\n",
      "Training Loss: 0.06498119040190334\n",
      "Validation Loss: 0.025363045404007183\n",
      "Epoch 57/100\n",
      "Training Loss: 0.06941311391548675\n",
      "Validation Loss: 0.06038524807194896\n",
      "Epoch 58/100\n",
      "Training Loss: 0.0771557241968566\n",
      "Validation Loss: 0.05975385408837201\n",
      "Epoch 59/100\n",
      "Training Loss: 0.07172172338250317\n",
      "Validation Loss: 0.055433411845979885\n",
      "Epoch 60/100\n",
      "Training Loss: 0.06263475267476001\n",
      "Validation Loss: 0.03586977876143539\n",
      "Epoch 61/100\n",
      "Training Loss: 0.07495149567415921\n",
      "Validation Loss: 0.034318251027635284\n",
      "Epoch 62/100\n",
      "Training Loss: 0.07176366575534973\n",
      "Validation Loss: 0.03196752090952891\n",
      "Epoch 63/100\n",
      "Training Loss: 0.0735982864564506\n",
      "Validation Loss: 0.0631830134287982\n",
      "Epoch 64/100\n",
      "Training Loss: 0.07506997476922814\n",
      "Validation Loss: 0.058948243054940085\n",
      "Epoch 65/100\n",
      "Training Loss: 0.07183512848974877\n",
      "Validation Loss: 0.04821786776874633\n",
      "Epoch 66/100\n",
      "Training Loss: 0.06863919578457237\n",
      "Validation Loss: 0.03510587132851897\n",
      "Epoch 67/100\n",
      "Training Loss: 0.07723752003173925\n",
      "Validation Loss: 0.04014279815150896\n",
      "Epoch 68/100\n",
      "Training Loss: 0.06200359673108572\n",
      "Validation Loss: 0.06043923808034342\n",
      "Epoch 69/100\n",
      "Training Loss: 0.0682620466515172\n",
      "Validation Loss: 0.048847023539703424\n",
      "Epoch 70/100\n",
      "Training Loss: 0.05991787397296158\n",
      "Validation Loss: 0.06956333209026765\n",
      "Epoch 71/100\n",
      "Training Loss: 0.07728088678088373\n",
      "Validation Loss: 0.026220584460887236\n",
      "Epoch 72/100\n",
      "Training Loss: 0.06377554954498003\n",
      "Validation Loss: 0.07130331090394837\n",
      "Epoch 73/100\n",
      "Training Loss: 0.06830117360015257\n",
      "Validation Loss: 0.044672512990725555\n",
      "Epoch 74/100\n",
      "Training Loss: 0.08258028026950061\n",
      "Validation Loss: 0.060701419118781894\n",
      "Epoch 75/100\n",
      "Training Loss: 0.08738132861215932\n",
      "Validation Loss: 0.059661685420314435\n",
      "Epoch 76/100\n",
      "Training Loss: 0.07806858233263933\n",
      "Validation Loss: 0.034531721388915776\n",
      "Epoch 77/100\n",
      "Training Loss: 0.08306868701960124\n",
      "Validation Loss: 0.034240742605848126\n",
      "Epoch 78/100\n",
      "Training Loss: 0.08673321369591695\n",
      "Validation Loss: 0.05000764698616609\n",
      "Epoch 79/100\n",
      "Training Loss: 0.070619608494367\n",
      "Validation Loss: 0.03221339898047563\n",
      "Epoch 80/100\n",
      "Training Loss: 0.06537506283771259\n",
      "Validation Loss: 0.036728190692671095\n",
      "Epoch 81/100\n",
      "Training Loss: 0.08733911418448506\n",
      "Validation Loss: 0.05228658222528852\n",
      "Epoch 82/100\n",
      "Training Loss: 0.07207465467577093\n",
      "Validation Loss: 0.05555015585284741\n",
      "Epoch 83/100\n",
      "Training Loss: 0.06816765675237874\n",
      "Validation Loss: 0.053147855149145905\n",
      "Epoch 84/100\n",
      "Training Loss: 0.06831345717846732\n",
      "Validation Loss: 0.07728890194422976\n",
      "Epoch 85/100\n",
      "Training Loss: 0.08132758990930052\n",
      "Validation Loss: 0.08181813375347527\n",
      "Epoch 86/100\n",
      "Training Loss: 0.07004796332659766\n",
      "Validation Loss: 0.06246371777360114\n",
      "Epoch 87/100\n",
      "Training Loss: 0.06790645212553287\n",
      "Validation Loss: 0.09618616605004335\n",
      "Epoch 88/100\n",
      "Training Loss: 0.058582007315716174\n",
      "Validation Loss: 0.04993494143425587\n",
      "Epoch 89/100\n",
      "Training Loss: 0.08029106005811547\n",
      "Validation Loss: 0.08138906488204481\n",
      "Epoch 90/100\n",
      "Training Loss: 0.07973876557844607\n",
      "Validation Loss: 0.048716459454715304\n",
      "Epoch 91/100\n",
      "Training Loss: 0.07335609180003473\n",
      "Validation Loss: 0.07625697600085721\n",
      "Epoch 92/100\n",
      "Training Loss: 0.09011800574922096\n",
      "Validation Loss: 0.056480753018074446\n",
      "Epoch 93/100\n",
      "Training Loss: 0.0811700122899264\n",
      "Validation Loss: 0.022916350888477748\n",
      "Epoch 94/100\n",
      "Training Loss: 0.08751711368440002\n",
      "Validation Loss: 0.04126588754375872\n",
      "Epoch 95/100\n",
      "Training Loss: 0.07263395244859362\n",
      "Validation Loss: 0.06946846034820593\n",
      "Epoch 96/100\n",
      "Training Loss: 0.07781414604439911\n",
      "Validation Loss: 0.04585024533327422\n",
      "Epoch 97/100\n",
      "Training Loss: 0.06651774388781395\n",
      "Validation Loss: 0.046127622170374805\n",
      "Epoch 98/100\n",
      "Training Loss: 0.09546224009514495\n",
      "Validation Loss: 0.04668618047252846\n",
      "Epoch 99/100\n",
      "Training Loss: 0.07784503684887796\n",
      "Validation Loss: 0.032582140928136924\n",
      "Epoch 100/100\n",
      "Training Loss: 0.10222715960743763\n",
      "Validation Loss: 0.030619426960449538\n",
      "Combination 19: Avg Training Loss = 0.091896528376535, Avg Validation Loss = 0.0627056338643064\n",
      "Testing combination 20/48: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.001, 'sequence_length': 6, 'dropout_rate': 0.2}\n",
      "Truncated Train Data Shape: (70, 6, 1)\n",
      "Truncated Validation Data Shape: (10, 6, 1)\n",
      "    Trial 1/2 for combination 20/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.34068863848231573\n",
      "Validation Loss: 0.6261227861722178\n",
      "Epoch 2/100\n",
      "Training Loss: 0.28953030516799266\n",
      "Validation Loss: 0.22704174951021078\n",
      "Epoch 3/100\n",
      "Training Loss: 0.2227904922710841\n",
      "Validation Loss: 0.24243624810862033\n",
      "Epoch 4/100\n",
      "Training Loss: 0.20686108164486502\n",
      "Validation Loss: 0.1691389697003837\n",
      "Epoch 5/100\n",
      "Training Loss: 0.15208114401834744\n",
      "Validation Loss: 0.14010948618452523\n",
      "Epoch 6/100\n",
      "Training Loss: 0.1529233704482695\n",
      "Validation Loss: 0.09328452783689209\n",
      "Epoch 7/100\n",
      "Training Loss: 0.14742950387836368\n",
      "Validation Loss: 0.09693486071543864\n",
      "Epoch 8/100\n",
      "Training Loss: 0.15314847469235823\n",
      "Validation Loss: 0.11559903584730542\n",
      "Epoch 9/100\n",
      "Training Loss: 0.13522630928845872\n",
      "Validation Loss: 0.05618735652212995\n",
      "Epoch 10/100\n",
      "Training Loss: 0.11851951926200255\n",
      "Validation Loss: 0.0895087993769009\n",
      "Epoch 11/100\n",
      "Training Loss: 0.13207713550792197\n",
      "Validation Loss: 0.08679539364818128\n",
      "Epoch 12/100\n",
      "Training Loss: 0.1175788412989847\n",
      "Validation Loss: 0.07659092396685843\n",
      "Epoch 13/100\n",
      "Training Loss: 0.0925650917032565\n",
      "Validation Loss: 0.08255638532832903\n",
      "Epoch 14/100\n",
      "Training Loss: 0.10338374431953924\n",
      "Validation Loss: 0.08652289452460069\n",
      "Epoch 15/100\n",
      "Training Loss: 0.11032282142141331\n",
      "Validation Loss: 0.04246786394302997\n",
      "Epoch 16/100\n",
      "Training Loss: 0.0996192907386636\n",
      "Validation Loss: 0.062217130597556826\n",
      "Epoch 17/100\n",
      "Training Loss: 0.10169417271829767\n",
      "Validation Loss: 0.06978903103071313\n",
      "Epoch 18/100\n",
      "Training Loss: 0.08852113316921595\n",
      "Validation Loss: 0.06175304328973802\n",
      "Epoch 19/100\n",
      "Training Loss: 0.08951044156918207\n",
      "Validation Loss: 0.04529579160093737\n",
      "Epoch 20/100\n",
      "Training Loss: 0.08397400770495637\n",
      "Validation Loss: 0.04248471735132924\n",
      "Epoch 21/100\n",
      "Training Loss: 0.09141161659764686\n",
      "Validation Loss: 0.11371905899653705\n",
      "Epoch 22/100\n",
      "Training Loss: 0.08855602661192112\n",
      "Validation Loss: 0.053707179578740494\n",
      "Epoch 23/100\n",
      "Training Loss: 0.07673562323371823\n",
      "Validation Loss: 0.045604161222383706\n",
      "Epoch 24/100\n",
      "Training Loss: 0.08420081367962451\n",
      "Validation Loss: 0.05758402558663648\n",
      "Epoch 25/100\n",
      "Training Loss: 0.07043875913685105\n",
      "Validation Loss: 0.042827973637968975\n",
      "Epoch 26/100\n",
      "Training Loss: 0.08354654445881839\n",
      "Validation Loss: 0.0707919117818171\n",
      "Epoch 27/100\n",
      "Training Loss: 0.07107741775410384\n",
      "Validation Loss: 0.05686647960114551\n",
      "Epoch 28/100\n",
      "Training Loss: 0.08085466411934036\n",
      "Validation Loss: 0.03712053444342267\n",
      "Epoch 29/100\n",
      "Training Loss: 0.07869233700307698\n",
      "Validation Loss: 0.06466588322449783\n",
      "Epoch 30/100\n",
      "Training Loss: 0.08008733382404769\n",
      "Validation Loss: 0.0723416415296663\n",
      "Epoch 31/100\n",
      "Training Loss: 0.06903170511033989\n",
      "Validation Loss: 0.03544436791222779\n",
      "Epoch 32/100\n",
      "Training Loss: 0.07019638677184452\n",
      "Validation Loss: 0.038750974406510284\n",
      "Epoch 33/100\n",
      "Training Loss: 0.08116682311050648\n",
      "Validation Loss: 0.06452701070777447\n",
      "Epoch 34/100\n",
      "Training Loss: 0.07697389028898309\n",
      "Validation Loss: 0.05377783734324021\n",
      "Epoch 35/100\n",
      "Training Loss: 0.075884750865396\n",
      "Validation Loss: 0.04488179173154596\n",
      "Epoch 36/100\n",
      "Training Loss: 0.07965468685284595\n",
      "Validation Loss: 0.03201157561510061\n",
      "Epoch 37/100\n",
      "Training Loss: 0.07635817938116404\n",
      "Validation Loss: 0.05852707027548977\n",
      "Epoch 38/100\n",
      "Training Loss: 0.07091327801101971\n",
      "Validation Loss: 0.056465623788076694\n",
      "Epoch 39/100\n",
      "Training Loss: 0.07261660687649447\n",
      "Validation Loss: 0.049413661173203435\n",
      "Epoch 40/100\n",
      "Training Loss: 0.07501302250121573\n",
      "Validation Loss: 0.04151721182630843\n",
      "Epoch 41/100\n",
      "Training Loss: 0.06904440765051746\n",
      "Validation Loss: 0.048730850013428195\n",
      "Epoch 42/100\n",
      "Training Loss: 0.07572102840776834\n",
      "Validation Loss: 0.06018451030985491\n",
      "Epoch 43/100\n",
      "Training Loss: 0.07343090665784548\n",
      "Validation Loss: 0.051115489902374946\n",
      "Epoch 44/100\n",
      "Training Loss: 0.07018203332694176\n",
      "Validation Loss: 0.05906032974431863\n",
      "Epoch 45/100\n",
      "Training Loss: 0.07507792177655313\n",
      "Validation Loss: 0.05641432239586299\n",
      "Epoch 46/100\n",
      "Training Loss: 0.06904284517596442\n",
      "Validation Loss: 0.04278426945006366\n",
      "Epoch 47/100\n",
      "Training Loss: 0.06554596643655253\n",
      "Validation Loss: 0.037855778499022226\n",
      "Epoch 48/100\n",
      "Training Loss: 0.07650199097806658\n",
      "Validation Loss: 0.048663603242782016\n",
      "Epoch 49/100\n",
      "Training Loss: 0.0679886869949249\n",
      "Validation Loss: 0.029517265776571648\n",
      "Epoch 50/100\n",
      "Training Loss: 0.07008813424228383\n",
      "Validation Loss: 0.07086427619914085\n",
      "Epoch 51/100\n",
      "Training Loss: 0.07083486290278042\n",
      "Validation Loss: 0.06267179159658717\n",
      "Epoch 52/100\n",
      "Training Loss: 0.07538000955832416\n",
      "Validation Loss: 0.06854656892774268\n",
      "Epoch 53/100\n",
      "Training Loss: 0.08260523858527598\n",
      "Validation Loss: 0.06770687496100816\n",
      "Epoch 54/100\n",
      "Training Loss: 0.08595527979309585\n",
      "Validation Loss: 0.03493063781033887\n",
      "Epoch 55/100\n",
      "Training Loss: 0.07062099818140932\n",
      "Validation Loss: 0.05638433650967325\n",
      "Epoch 56/100\n",
      "Training Loss: 0.07586290010717613\n",
      "Validation Loss: 0.0646863656634929\n",
      "Epoch 57/100\n",
      "Training Loss: 0.06151369981191111\n",
      "Validation Loss: 0.0650003239127489\n",
      "Epoch 58/100\n",
      "Training Loss: 0.07663100571139272\n",
      "Validation Loss: 0.06740103952676471\n",
      "Epoch 59/100\n",
      "Training Loss: 0.07109515641938831\n",
      "Validation Loss: 0.07488692840048586\n",
      "Epoch 60/100\n",
      "Training Loss: 0.0707241012022653\n",
      "Validation Loss: 0.03955542157277482\n",
      "Epoch 61/100\n",
      "Training Loss: 0.07443404827807298\n",
      "Validation Loss: 0.06653384329052218\n",
      "Epoch 62/100\n",
      "Training Loss: 0.07190726026834486\n",
      "Validation Loss: 0.029735632662576318\n",
      "Epoch 63/100\n",
      "Training Loss: 0.08058894944748562\n",
      "Validation Loss: 0.05120622222839928\n",
      "Epoch 64/100\n",
      "Training Loss: 0.08285944090911158\n",
      "Validation Loss: 0.05081748472530562\n",
      "Epoch 65/100\n",
      "Training Loss: 0.07530746059388327\n",
      "Validation Loss: 0.05357896985167003\n",
      "Epoch 66/100\n",
      "Training Loss: 0.07244065643731057\n",
      "Validation Loss: 0.03832114108463926\n",
      "Epoch 67/100\n",
      "Training Loss: 0.07973755987867713\n",
      "Validation Loss: 0.06919909827336278\n",
      "Epoch 68/100\n",
      "Training Loss: 0.06834066155047896\n",
      "Validation Loss: 0.023890075259865827\n",
      "Epoch 69/100\n",
      "Training Loss: 0.07523999859667639\n",
      "Validation Loss: 0.0407822842005363\n",
      "Epoch 70/100\n",
      "Training Loss: 0.07511157440861559\n",
      "Validation Loss: 0.022784833675886948\n",
      "Epoch 71/100\n",
      "Training Loss: 0.07796061200968474\n",
      "Validation Loss: 0.02940016254795569\n",
      "Epoch 72/100\n",
      "Training Loss: 0.07644451551114423\n",
      "Validation Loss: 0.05353652362995407\n",
      "Epoch 73/100\n",
      "Training Loss: 0.08592658634382909\n",
      "Validation Loss: 0.05964833621175827\n",
      "Epoch 74/100\n",
      "Training Loss: 0.06341711417975258\n",
      "Validation Loss: 0.04995423105257643\n",
      "Epoch 75/100\n",
      "Training Loss: 0.08035302140729599\n",
      "Validation Loss: 0.03875332813093904\n",
      "Epoch 76/100\n",
      "Training Loss: 0.0783089718720598\n",
      "Validation Loss: 0.08528656519570697\n",
      "Epoch 77/100\n",
      "Training Loss: 0.06530033519107659\n",
      "Validation Loss: 0.07445009888899673\n",
      "Epoch 78/100\n",
      "Training Loss: 0.0779934373399136\n",
      "Validation Loss: 0.050709380943627355\n",
      "Epoch 79/100\n",
      "Training Loss: 0.08142748996559739\n",
      "Validation Loss: 0.11732527349325776\n",
      "Epoch 80/100\n",
      "Training Loss: 0.07173002854715615\n",
      "Validation Loss: 0.0885605679404466\n",
      "Epoch 81/100\n",
      "Training Loss: 0.07724248454582477\n",
      "Validation Loss: 0.08105032923930455\n",
      "Epoch 82/100\n",
      "Training Loss: 0.060106433921700134\n",
      "Validation Loss: 0.05092016772250333\n",
      "Epoch 83/100\n",
      "Training Loss: 0.07471301086172925\n",
      "Validation Loss: 0.09784082038312608\n",
      "Epoch 84/100\n",
      "Training Loss: 0.07527490667897418\n",
      "Validation Loss: 0.0612057746280989\n",
      "Epoch 85/100\n",
      "Training Loss: 0.07579184344633215\n",
      "Validation Loss: 0.045826514402918336\n",
      "Epoch 86/100\n",
      "Training Loss: 0.06766986820673035\n",
      "Validation Loss: 0.08772539422968381\n",
      "Epoch 87/100\n",
      "Training Loss: 0.07501660352434657\n",
      "Validation Loss: 0.09213133773380539\n",
      "Epoch 88/100\n",
      "Training Loss: 0.07053118532834098\n",
      "Validation Loss: 0.10436297218575837\n",
      "Epoch 89/100\n",
      "Training Loss: 0.07396835508446083\n",
      "Validation Loss: 0.05862450450576358\n",
      "Epoch 90/100\n",
      "Training Loss: 0.07824566707124495\n",
      "Validation Loss: 0.07834671697325599\n",
      "Epoch 91/100\n",
      "Training Loss: 0.06603890555549305\n",
      "Validation Loss: 0.05377765262344083\n",
      "Epoch 92/100\n",
      "Training Loss: 0.0859442445900982\n",
      "Validation Loss: 0.043521043053086655\n",
      "Epoch 93/100\n",
      "Training Loss: 0.0821415266847438\n",
      "Validation Loss: 0.05543410184044928\n",
      "Epoch 94/100\n",
      "Training Loss: 0.08735541063529641\n",
      "Validation Loss: 0.06701915706661679\n",
      "Epoch 95/100\n",
      "Training Loss: 0.07283393417106265\n",
      "Validation Loss: 0.10801441839936846\n",
      "Epoch 96/100\n",
      "Training Loss: 0.06571078580662758\n",
      "Validation Loss: 0.05894615740858257\n",
      "Epoch 97/100\n",
      "Training Loss: 0.07574719402584293\n",
      "Validation Loss: 0.08259121455954839\n",
      "Epoch 98/100\n",
      "Training Loss: 0.07409867520649296\n",
      "Validation Loss: 0.05406403892106722\n",
      "Epoch 99/100\n",
      "Training Loss: 0.06436186093891846\n",
      "Validation Loss: 0.1015497393909958\n",
      "Epoch 100/100\n",
      "Training Loss: 0.08618778080606072\n",
      "Validation Loss: 0.08966875318889503\n",
      "    Trial 2/2 for combination 20/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.29860738223004174\n",
      "Validation Loss: 0.38149222910340874\n",
      "Epoch 2/100\n",
      "Training Loss: 0.20377151380921182\n",
      "Validation Loss: 0.17167063205866112\n",
      "Epoch 3/100\n",
      "Training Loss: 0.17838795687777523\n",
      "Validation Loss: 0.10577277366417867\n",
      "Epoch 4/100\n",
      "Training Loss: 0.1673298284192458\n",
      "Validation Loss: 0.17843280422845123\n",
      "Epoch 5/100\n",
      "Training Loss: 0.1660708838989759\n",
      "Validation Loss: 0.11701790571956867\n",
      "Epoch 6/100\n",
      "Training Loss: 0.15627496062478272\n",
      "Validation Loss: 0.14983028830042103\n",
      "Epoch 7/100\n",
      "Training Loss: 0.13378157106680164\n",
      "Validation Loss: 0.12244068086309776\n",
      "Epoch 8/100\n",
      "Training Loss: 0.11286641454570534\n",
      "Validation Loss: 0.08461646171144356\n",
      "Epoch 9/100\n",
      "Training Loss: 0.12292847801334815\n",
      "Validation Loss: 0.07659037249605544\n",
      "Epoch 10/100\n",
      "Training Loss: 0.13188823546453154\n",
      "Validation Loss: 0.06648318487494\n",
      "Epoch 11/100\n",
      "Training Loss: 0.10241267273965698\n",
      "Validation Loss: 0.07929314986477212\n",
      "Epoch 12/100\n",
      "Training Loss: 0.1033952588721383\n",
      "Validation Loss: 0.10535841267142784\n",
      "Epoch 13/100\n",
      "Training Loss: 0.1090809666612114\n",
      "Validation Loss: 0.060288684901028636\n",
      "Epoch 14/100\n",
      "Training Loss: 0.1167880356229654\n",
      "Validation Loss: 0.05134286160639808\n",
      "Epoch 15/100\n",
      "Training Loss: 0.09969880660957307\n",
      "Validation Loss: 0.04892986194274524\n",
      "Epoch 16/100\n",
      "Training Loss: 0.09207712750613015\n",
      "Validation Loss: 0.07118896730843637\n",
      "Epoch 17/100\n",
      "Training Loss: 0.10307098848880233\n",
      "Validation Loss: 0.048284776291154255\n",
      "Epoch 18/100\n",
      "Training Loss: 0.09578015244145735\n",
      "Validation Loss: 0.056151558995304184\n",
      "Epoch 19/100\n",
      "Training Loss: 0.09570353963311405\n",
      "Validation Loss: 0.06500699780908661\n",
      "Epoch 20/100\n",
      "Training Loss: 0.09150098410395581\n",
      "Validation Loss: 0.04836609417582664\n",
      "Epoch 21/100\n",
      "Training Loss: 0.0807001228195329\n",
      "Validation Loss: 0.04976056000278375\n",
      "Epoch 22/100\n",
      "Training Loss: 0.08201973544394393\n",
      "Validation Loss: 0.03728372445181437\n",
      "Epoch 23/100\n",
      "Training Loss: 0.07662159417134594\n",
      "Validation Loss: 0.06312537905668117\n",
      "Epoch 24/100\n",
      "Training Loss: 0.08558163498098399\n",
      "Validation Loss: 0.05192131614563812\n",
      "Epoch 25/100\n",
      "Training Loss: 0.08223704636732676\n",
      "Validation Loss: 0.04212267038701056\n",
      "Epoch 26/100\n",
      "Training Loss: 0.07832679077042982\n",
      "Validation Loss: 0.06187736803935486\n",
      "Epoch 27/100\n",
      "Training Loss: 0.07962937502150108\n",
      "Validation Loss: 0.04808050285069406\n",
      "Epoch 28/100\n",
      "Training Loss: 0.07629140415871727\n",
      "Validation Loss: 0.06707974204770664\n",
      "Epoch 29/100\n",
      "Training Loss: 0.07074718822120188\n",
      "Validation Loss: 0.060658626559812565\n",
      "Epoch 30/100\n",
      "Training Loss: 0.0777314053730326\n",
      "Validation Loss: 0.03592809741491049\n",
      "Epoch 31/100\n",
      "Training Loss: 0.0796291921054216\n",
      "Validation Loss: 0.04816255919026226\n",
      "Epoch 32/100\n",
      "Training Loss: 0.07856909109832935\n",
      "Validation Loss: 0.03154138271040536\n",
      "Epoch 33/100\n",
      "Training Loss: 0.08643158744225328\n",
      "Validation Loss: 0.03307656550958485\n",
      "Epoch 34/100\n",
      "Training Loss: 0.07686538407580472\n",
      "Validation Loss: 0.06160394618897861\n",
      "Epoch 35/100\n",
      "Training Loss: 0.07727645527684532\n",
      "Validation Loss: 0.05064766939147445\n",
      "Epoch 36/100\n",
      "Training Loss: 0.06795836019783838\n",
      "Validation Loss: 0.038172198630714714\n",
      "Epoch 37/100\n",
      "Training Loss: 0.07392680127120617\n",
      "Validation Loss: 0.051450752667474786\n",
      "Epoch 38/100\n",
      "Training Loss: 0.06850944095922354\n",
      "Validation Loss: 0.05798482584665884\n",
      "Epoch 39/100\n",
      "Training Loss: 0.07170331125242124\n",
      "Validation Loss: 0.06204050563539607\n",
      "Epoch 40/100\n",
      "Training Loss: 0.06798204657105972\n",
      "Validation Loss: 0.03998581045285372\n",
      "Epoch 41/100\n",
      "Training Loss: 0.07513948621772049\n",
      "Validation Loss: 0.05133187977419525\n",
      "Epoch 42/100\n",
      "Training Loss: 0.07773795421081166\n",
      "Validation Loss: 0.05774363924654483\n",
      "Epoch 43/100\n",
      "Training Loss: 0.07009052317860064\n",
      "Validation Loss: 0.03540722701789879\n",
      "Epoch 44/100\n",
      "Training Loss: 0.06998188326981183\n",
      "Validation Loss: 0.06871162614653738\n",
      "Epoch 45/100\n",
      "Training Loss: 0.06627420631723717\n",
      "Validation Loss: 0.0716264966821159\n",
      "Epoch 46/100\n",
      "Training Loss: 0.07719655616269534\n",
      "Validation Loss: 0.053559054096773105\n",
      "Epoch 47/100\n",
      "Training Loss: 0.07144351040811067\n",
      "Validation Loss: 0.04031268523478786\n",
      "Epoch 48/100\n",
      "Training Loss: 0.07281705149278518\n",
      "Validation Loss: 0.059032522815797774\n",
      "Epoch 49/100\n",
      "Training Loss: 0.07493739885288668\n",
      "Validation Loss: 0.055272459907899085\n",
      "Epoch 50/100\n",
      "Training Loss: 0.07553274917157901\n",
      "Validation Loss: 0.04583847145970811\n",
      "Epoch 51/100\n",
      "Training Loss: 0.07634070846656657\n",
      "Validation Loss: 0.06915970533861675\n",
      "Epoch 52/100\n",
      "Training Loss: 0.07775289204945329\n",
      "Validation Loss: 0.0519223642186238\n",
      "Epoch 53/100\n",
      "Training Loss: 0.06945160919622088\n",
      "Validation Loss: 0.030187539112230384\n",
      "Epoch 54/100\n",
      "Training Loss: 0.06881415364527359\n",
      "Validation Loss: 0.03862192797469534\n",
      "Epoch 55/100\n",
      "Training Loss: 0.07564162531339115\n",
      "Validation Loss: 0.0294032953469634\n",
      "Epoch 56/100\n",
      "Training Loss: 0.07230873932640554\n",
      "Validation Loss: 0.048647049313335994\n",
      "Epoch 57/100\n",
      "Training Loss: 0.07874356377803818\n",
      "Validation Loss: 0.04998479373040135\n",
      "Epoch 58/100\n",
      "Training Loss: 0.06729481256101896\n",
      "Validation Loss: 0.09259138458444756\n",
      "Epoch 59/100\n",
      "Training Loss: 0.07697340173499494\n",
      "Validation Loss: 0.034433255813009864\n",
      "Epoch 60/100\n",
      "Training Loss: 0.06946851668378771\n",
      "Validation Loss: 0.08944384073437425\n",
      "Epoch 61/100\n",
      "Training Loss: 0.07984025568448501\n",
      "Validation Loss: 0.0723223635199728\n",
      "Epoch 62/100\n",
      "Training Loss: 0.07438097609665488\n",
      "Validation Loss: 0.027171610899974136\n",
      "Epoch 63/100\n",
      "Training Loss: 0.06523615339783335\n",
      "Validation Loss: 0.05426606963662067\n",
      "Epoch 64/100\n",
      "Training Loss: 0.07297069325192829\n",
      "Validation Loss: 0.04374413508126612\n",
      "Epoch 65/100\n",
      "Training Loss: 0.07094237393246586\n",
      "Validation Loss: 0.08748726363295439\n",
      "Epoch 66/100\n",
      "Training Loss: 0.06795118127698534\n",
      "Validation Loss: 0.07077346902983031\n",
      "Epoch 67/100\n",
      "Training Loss: 0.07146285336635656\n",
      "Validation Loss: 0.04883578629191586\n",
      "Epoch 68/100\n",
      "Training Loss: 0.06970840013953786\n",
      "Validation Loss: 0.10946434985706802\n",
      "Epoch 69/100\n",
      "Training Loss: 0.0852818511014118\n",
      "Validation Loss: 0.040771868028004296\n",
      "Epoch 70/100\n",
      "Training Loss: 0.07914316218076194\n",
      "Validation Loss: 0.03345374993214655\n",
      "Epoch 71/100\n",
      "Training Loss: 0.08119958988280836\n",
      "Validation Loss: 0.04523572283049438\n",
      "Epoch 72/100\n",
      "Training Loss: 0.055713054648656016\n",
      "Validation Loss: 0.05672141272185514\n",
      "Epoch 73/100\n",
      "Training Loss: 0.08634265696689387\n",
      "Validation Loss: 0.05090445874043392\n",
      "Epoch 74/100\n",
      "Training Loss: 0.07804587630960051\n",
      "Validation Loss: 0.05034290759245767\n",
      "Epoch 75/100\n",
      "Training Loss: 0.07973282202352736\n",
      "Validation Loss: 0.06982440808469532\n",
      "Epoch 76/100\n",
      "Training Loss: 0.07040804889090826\n",
      "Validation Loss: 0.04187251258983756\n",
      "Epoch 77/100\n",
      "Training Loss: 0.08065913167310836\n",
      "Validation Loss: 0.062165892558487534\n",
      "Epoch 78/100\n",
      "Training Loss: 0.07371375587338093\n",
      "Validation Loss: 0.048050362753597586\n",
      "Epoch 79/100\n",
      "Training Loss: 0.07012821785862412\n",
      "Validation Loss: 0.04561691767342219\n",
      "Epoch 80/100\n",
      "Training Loss: 0.07749998918794758\n",
      "Validation Loss: 0.10673353068910926\n",
      "Epoch 81/100\n",
      "Training Loss: 0.06384857280945573\n",
      "Validation Loss: 0.04685234221800735\n",
      "Epoch 82/100\n",
      "Training Loss: 0.07360565049494262\n",
      "Validation Loss: 0.16436838173397478\n",
      "Epoch 83/100\n",
      "Training Loss: 0.07490095163544604\n",
      "Validation Loss: 0.0751473675665352\n",
      "Epoch 84/100\n",
      "Training Loss: 0.08533429052443776\n",
      "Validation Loss: 0.03802710029046156\n",
      "Epoch 85/100\n",
      "Training Loss: 0.08070389895534495\n",
      "Validation Loss: 0.0633451899473317\n",
      "Epoch 86/100\n",
      "Training Loss: 0.0679362796580905\n",
      "Validation Loss: 0.05650512599224017\n",
      "Epoch 87/100\n",
      "Training Loss: 0.0753848593736212\n",
      "Validation Loss: 0.08655641923621908\n",
      "Epoch 88/100\n",
      "Training Loss: 0.0839573156963384\n",
      "Validation Loss: 0.05193445059772842\n",
      "Epoch 89/100\n",
      "Training Loss: 0.085146938069582\n",
      "Validation Loss: 0.07662973142210325\n",
      "Epoch 90/100\n",
      "Training Loss: 0.08088890255076331\n",
      "Validation Loss: 0.06177375544240754\n",
      "Epoch 91/100\n",
      "Training Loss: 0.0711394485868683\n",
      "Validation Loss: 0.03776681449281969\n",
      "Epoch 92/100\n",
      "Training Loss: 0.08413915356798883\n",
      "Validation Loss: 0.05915331123152647\n",
      "Epoch 93/100\n",
      "Training Loss: 0.07398313500331723\n",
      "Validation Loss: 0.0409997131080237\n",
      "Epoch 94/100\n",
      "Training Loss: 0.0749078736006325\n",
      "Validation Loss: 0.034181774249088706\n",
      "Epoch 95/100\n",
      "Training Loss: 0.08534080883108786\n",
      "Validation Loss: 0.10865285501232995\n",
      "Epoch 96/100\n",
      "Training Loss: 0.06287458483948354\n",
      "Validation Loss: 0.0661431433498302\n",
      "Epoch 97/100\n",
      "Training Loss: 0.07292098787037048\n",
      "Validation Loss: 0.11119316465416862\n",
      "Epoch 98/100\n",
      "Training Loss: 0.08318390831589362\n",
      "Validation Loss: 0.0431548434730369\n",
      "Epoch 99/100\n",
      "Training Loss: 0.08490782319837475\n",
      "Validation Loss: 0.05389893412265749\n",
      "Epoch 100/100\n",
      "Training Loss: 0.07228800564446976\n",
      "Validation Loss: 0.059828315799684596\n",
      "Combination 20: Avg Training Loss = 0.08830681529728496, Avg Validation Loss = 0.06966802231098689\n",
      "Testing combination 21/48: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.001, 'sequence_length': 20, 'dropout_rate': 0.1}\n",
      "Truncated Train Data Shape: (70, 20, 1)\n",
      "Truncated Validation Data Shape: (10, 20, 1)\n",
      "    Trial 1/2 for combination 21/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.32456337443733557\n",
      "Validation Loss: 0.1670032489657851\n",
      "Epoch 2/100\n",
      "Training Loss: 0.22806227476940094\n",
      "Validation Loss: 0.08618593784767958\n",
      "Epoch 3/100\n",
      "Training Loss: 0.15462406402094517\n",
      "Validation Loss: 0.09022735079111595\n",
      "Epoch 4/100\n",
      "Training Loss: 0.16352568883150323\n",
      "Validation Loss: 0.08071375854368146\n",
      "Epoch 5/100\n",
      "Training Loss: 0.14086383645060668\n",
      "Validation Loss: 0.07225904538928467\n",
      "Epoch 6/100\n",
      "Training Loss: 0.12965298111405932\n",
      "Validation Loss: 0.07561512398545792\n",
      "Epoch 7/100\n",
      "Training Loss: 0.14239186785520208\n",
      "Validation Loss: 0.08084745991990994\n",
      "Epoch 8/100\n",
      "Training Loss: 0.11623338880193282\n",
      "Validation Loss: 0.04229157256641474\n",
      "Epoch 9/100\n",
      "Training Loss: 0.10262963130310797\n",
      "Validation Loss: 0.07085470557987109\n",
      "Epoch 10/100\n",
      "Training Loss: 0.10662604253233454\n",
      "Validation Loss: 0.06856895342033668\n",
      "Epoch 11/100\n",
      "Training Loss: 0.10540390669091834\n",
      "Validation Loss: 0.05338561078310518\n",
      "Epoch 12/100\n",
      "Training Loss: 0.09509656951830672\n",
      "Validation Loss: 0.05899892905547118\n",
      "Epoch 13/100\n",
      "Training Loss: 0.09420471946242574\n",
      "Validation Loss: 0.05858708249266157\n",
      "Epoch 14/100\n",
      "Training Loss: 0.08066325200555025\n",
      "Validation Loss: 0.03633884685958037\n",
      "Epoch 15/100\n",
      "Training Loss: 0.08696560634566579\n",
      "Validation Loss: 0.06920965581384356\n",
      "Epoch 16/100\n",
      "Training Loss: 0.08355096403215716\n",
      "Validation Loss: 0.04282123145618344\n",
      "Epoch 17/100\n",
      "Training Loss: 0.06709982151586263\n",
      "Validation Loss: 0.04049700014288895\n",
      "Epoch 18/100\n",
      "Training Loss: 0.07170135561795772\n",
      "Validation Loss: 0.051088345938281085\n",
      "Epoch 19/100\n",
      "Training Loss: 0.06763484242098848\n",
      "Validation Loss: 0.03507886317882135\n",
      "Epoch 20/100\n",
      "Training Loss: 0.07576258105007398\n",
      "Validation Loss: 0.03723706283478313\n",
      "Epoch 21/100\n",
      "Training Loss: 0.0710023463280079\n",
      "Validation Loss: 0.05523845361566252\n",
      "Epoch 22/100\n",
      "Training Loss: 0.06666303285303332\n",
      "Validation Loss: 0.035993623878408786\n",
      "Epoch 23/100\n",
      "Training Loss: 0.053347932280185854\n",
      "Validation Loss: 0.04961588778536456\n",
      "Epoch 24/100\n",
      "Training Loss: 0.061633079506490586\n",
      "Validation Loss: 0.04003425728229606\n",
      "Epoch 25/100\n",
      "Training Loss: 0.058582538203860224\n",
      "Validation Loss: 0.031072993657535797\n",
      "Epoch 26/100\n",
      "Training Loss: 0.05490211347701599\n",
      "Validation Loss: 0.03907552009144345\n",
      "Epoch 27/100\n",
      "Training Loss: 0.05537967938528872\n",
      "Validation Loss: 0.055518197483260825\n",
      "Epoch 28/100\n",
      "Training Loss: 0.05633280059059505\n",
      "Validation Loss: 0.056511738838131945\n",
      "Epoch 29/100\n",
      "Training Loss: 0.05010744088133872\n",
      "Validation Loss: 0.04220341450611909\n",
      "Epoch 30/100\n",
      "Training Loss: 0.05875522471127799\n",
      "Validation Loss: 0.02043888650916131\n",
      "Epoch 31/100\n",
      "Training Loss: 0.054586184174355304\n",
      "Validation Loss: 0.03155005985427966\n",
      "Epoch 32/100\n",
      "Training Loss: 0.0503101537804287\n",
      "Validation Loss: 0.05179886371893264\n",
      "Epoch 33/100\n",
      "Training Loss: 0.05532871684903046\n",
      "Validation Loss: 0.05237034655923052\n",
      "Epoch 34/100\n",
      "Training Loss: 0.05677711988045744\n",
      "Validation Loss: 0.02597440116815445\n",
      "Epoch 35/100\n",
      "Training Loss: 0.058261317579182986\n",
      "Validation Loss: 0.0620114131617641\n",
      "Epoch 36/100\n",
      "Training Loss: 0.050915116842061334\n",
      "Validation Loss: 0.05089305690394356\n",
      "Epoch 37/100\n",
      "Training Loss: 0.06363718634199786\n",
      "Validation Loss: 0.07313225652349234\n",
      "Epoch 38/100\n",
      "Training Loss: 0.047592394782653924\n",
      "Validation Loss: 0.027548320609314687\n",
      "Epoch 39/100\n",
      "Training Loss: 0.049247672021431835\n",
      "Validation Loss: 0.029341631627883676\n",
      "Epoch 40/100\n",
      "Training Loss: 0.04501220531285502\n",
      "Validation Loss: 0.059326375033495415\n",
      "Epoch 41/100\n",
      "Training Loss: 0.055171010076603404\n",
      "Validation Loss: 0.1003600552441724\n",
      "Epoch 42/100\n",
      "Training Loss: 0.05973372391731313\n",
      "Validation Loss: 0.07160557565938136\n",
      "Epoch 43/100\n",
      "Training Loss: 0.05483660575908831\n",
      "Validation Loss: 0.017511573769465293\n",
      "Epoch 44/100\n",
      "Training Loss: 0.048147566583350884\n",
      "Validation Loss: 0.07139706524238025\n",
      "Epoch 45/100\n",
      "Training Loss: 0.05252953390230595\n",
      "Validation Loss: 0.04007184342249691\n",
      "Epoch 46/100\n",
      "Training Loss: 0.0619550862902308\n",
      "Validation Loss: 0.05875725691350521\n",
      "Epoch 47/100\n",
      "Training Loss: 0.049031412564372434\n",
      "Validation Loss: 0.022948572501169735\n",
      "Epoch 48/100\n",
      "Training Loss: 0.05811302143162217\n",
      "Validation Loss: 0.0706142226307502\n",
      "Epoch 49/100\n",
      "Training Loss: 0.06012614561663095\n",
      "Validation Loss: 0.034220531151897035\n",
      "Epoch 50/100\n",
      "Training Loss: 0.05822187452746825\n",
      "Validation Loss: 0.03552692548125784\n",
      "Epoch 51/100\n",
      "Training Loss: 0.04845185255996803\n",
      "Validation Loss: 0.02924788669474332\n",
      "Epoch 52/100\n",
      "Training Loss: 0.056180196699424076\n",
      "Validation Loss: 0.06564853651004877\n",
      "Epoch 53/100\n",
      "Training Loss: 0.059104080233229404\n",
      "Validation Loss: 0.04296193496751068\n",
      "Epoch 54/100\n",
      "Training Loss: 0.05239073453827627\n",
      "Validation Loss: 0.04863850476886714\n",
      "Epoch 55/100\n",
      "Training Loss: 0.04634556584564067\n",
      "Validation Loss: 0.07256815849788176\n",
      "Epoch 56/100\n",
      "Training Loss: 0.051858512952295946\n",
      "Validation Loss: 0.05979702326466039\n",
      "Epoch 57/100\n",
      "Training Loss: 0.05169549353293162\n",
      "Validation Loss: 0.017092437588560834\n",
      "Epoch 58/100\n",
      "Training Loss: 0.04675174724700385\n",
      "Validation Loss: 0.05094541741173427\n",
      "Epoch 59/100\n",
      "Training Loss: 0.05570430648263662\n",
      "Validation Loss: 0.03425008190459863\n",
      "Epoch 60/100\n",
      "Training Loss: 0.060355015063474356\n",
      "Validation Loss: 0.04332134440950576\n",
      "Epoch 61/100\n",
      "Training Loss: 0.07090732442131709\n",
      "Validation Loss: 0.05191837300881576\n",
      "Epoch 62/100\n",
      "Training Loss: 0.05842224112042099\n",
      "Validation Loss: 0.0263328505606933\n",
      "Epoch 63/100\n",
      "Training Loss: 0.054873079773514424\n",
      "Validation Loss: 0.06160843423269487\n",
      "Epoch 64/100\n",
      "Training Loss: 0.05957964910641628\n",
      "Validation Loss: 0.03613532015925834\n",
      "Epoch 65/100\n",
      "Training Loss: 0.06117384052506946\n",
      "Validation Loss: 0.028063048962307928\n",
      "Epoch 66/100\n",
      "Training Loss: 0.04564785047291488\n",
      "Validation Loss: 0.036162516354095474\n",
      "Epoch 67/100\n",
      "Training Loss: 0.05683997791124094\n",
      "Validation Loss: 0.03123083604042045\n",
      "Epoch 68/100\n",
      "Training Loss: 0.055641921930441915\n",
      "Validation Loss: 0.06397156447861427\n",
      "Epoch 69/100\n",
      "Training Loss: 0.06718488435968321\n",
      "Validation Loss: 0.06884651270423099\n",
      "Epoch 70/100\n",
      "Training Loss: 0.05638943137457216\n",
      "Validation Loss: 0.029543616322217657\n",
      "Epoch 71/100\n",
      "Training Loss: 0.04464694187971785\n",
      "Validation Loss: 0.026977731120700503\n",
      "Epoch 72/100\n",
      "Training Loss: 0.058916367435785065\n",
      "Validation Loss: 0.04428263502772392\n",
      "Epoch 73/100\n",
      "Training Loss: 0.061105968080793535\n",
      "Validation Loss: 0.023951481463299708\n",
      "Epoch 74/100\n",
      "Training Loss: 0.06378081335203684\n",
      "Validation Loss: 0.072705527358722\n",
      "Epoch 75/100\n",
      "Training Loss: 0.0456955667113238\n",
      "Validation Loss: 0.04503454222678286\n",
      "Epoch 76/100\n",
      "Training Loss: 0.05923131155617627\n",
      "Validation Loss: 0.019843988131996778\n",
      "Epoch 77/100\n",
      "Training Loss: 0.059021223128401155\n",
      "Validation Loss: 0.0577223180066877\n",
      "Epoch 78/100\n",
      "Training Loss: 0.048919551871795766\n",
      "Validation Loss: 0.044697213087900464\n",
      "Epoch 79/100\n",
      "Training Loss: 0.062324206259839775\n",
      "Validation Loss: 0.022447037639227545\n",
      "Epoch 80/100\n",
      "Training Loss: 0.05592418064265574\n",
      "Validation Loss: 0.08993182102321662\n",
      "Epoch 81/100\n",
      "Training Loss: 0.04942711584456469\n",
      "Validation Loss: 0.0647399303912036\n",
      "Epoch 82/100\n",
      "Training Loss: 0.04901697138429163\n",
      "Validation Loss: 0.0437776118136494\n",
      "Epoch 83/100\n",
      "Training Loss: 0.06287552031061705\n",
      "Validation Loss: 0.03500682046411669\n",
      "Epoch 84/100\n",
      "Training Loss: 0.0694389475747622\n",
      "Validation Loss: 0.08620643792759787\n",
      "Epoch 85/100\n",
      "Training Loss: 0.060170628650359936\n",
      "Validation Loss: 0.0762475135901007\n",
      "Epoch 86/100\n",
      "Training Loss: 0.05864523803426094\n",
      "Validation Loss: 0.049867305980957954\n",
      "Epoch 87/100\n",
      "Training Loss: 0.06230577909551322\n",
      "Validation Loss: 0.05590991616064519\n",
      "Epoch 88/100\n",
      "Training Loss: 0.05628512800491137\n",
      "Validation Loss: 0.07252967206674996\n",
      "Epoch 89/100\n",
      "Training Loss: 0.06311393184441937\n",
      "Validation Loss: 0.03725797594827635\n",
      "Epoch 90/100\n",
      "Training Loss: 0.052389686692964677\n",
      "Validation Loss: 0.04165958930756451\n",
      "Epoch 91/100\n",
      "Training Loss: 0.062020062421864516\n",
      "Validation Loss: 0.04279485271262507\n",
      "Epoch 92/100\n",
      "Training Loss: 0.07142319524357635\n",
      "Validation Loss: 0.03725518945470056\n",
      "Epoch 93/100\n",
      "Training Loss: 0.052219717592191496\n",
      "Validation Loss: 0.05641208461895172\n",
      "Epoch 94/100\n",
      "Training Loss: 0.05054470119101083\n",
      "Validation Loss: 0.04936172767544658\n",
      "Epoch 95/100\n",
      "Training Loss: 0.06780255044350943\n",
      "Validation Loss: 0.09461944035226921\n",
      "Epoch 96/100\n",
      "Training Loss: 0.069076386440562\n",
      "Validation Loss: 0.036137791886001185\n",
      "Epoch 97/100\n",
      "Training Loss: 0.06972103030701886\n",
      "Validation Loss: 0.04623025712006174\n",
      "Epoch 98/100\n",
      "Training Loss: 0.059001575201901996\n",
      "Validation Loss: 0.044330361204281216\n",
      "Epoch 99/100\n",
      "Training Loss: 0.06914774160689278\n",
      "Validation Loss: 0.050411707641420875\n",
      "Epoch 100/100\n",
      "Training Loss: 0.051278542611174524\n",
      "Validation Loss: 0.04700687119506934\n",
      "    Trial 2/2 for combination 21/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.29722563604797214\n",
      "Validation Loss: 0.10205171555623452\n",
      "Epoch 2/100\n",
      "Training Loss: 0.24367287322439765\n",
      "Validation Loss: 0.09677657701631893\n",
      "Epoch 3/100\n",
      "Training Loss: 0.18771287603894274\n",
      "Validation Loss: 0.10290334254074114\n",
      "Epoch 4/100\n",
      "Training Loss: 0.15139632372879874\n",
      "Validation Loss: 0.14930775379344183\n",
      "Epoch 5/100\n",
      "Training Loss: 0.15024115206047436\n",
      "Validation Loss: 0.07368891818070804\n",
      "Epoch 6/100\n",
      "Training Loss: 0.11498000540208098\n",
      "Validation Loss: 0.07889045277376713\n",
      "Epoch 7/100\n",
      "Training Loss: 0.1136937108734969\n",
      "Validation Loss: 0.06525323622806278\n",
      "Epoch 8/100\n",
      "Training Loss: 0.10868785790075673\n",
      "Validation Loss: 0.07150460307062753\n",
      "Epoch 9/100\n",
      "Training Loss: 0.11896271229735496\n",
      "Validation Loss: 0.06514009067109787\n",
      "Epoch 10/100\n",
      "Training Loss: 0.11873536383683869\n",
      "Validation Loss: 0.06959015884067478\n",
      "Epoch 11/100\n",
      "Training Loss: 0.10932976002493698\n",
      "Validation Loss: 0.06425526115113836\n",
      "Epoch 12/100\n",
      "Training Loss: 0.08799707098639004\n",
      "Validation Loss: 0.06628712417568948\n",
      "Epoch 13/100\n",
      "Training Loss: 0.09400755468544163\n",
      "Validation Loss: 0.052967904142401656\n",
      "Epoch 14/100\n",
      "Training Loss: 0.08853059752368674\n",
      "Validation Loss: 0.04493751537243516\n",
      "Epoch 15/100\n",
      "Training Loss: 0.08197267901873521\n",
      "Validation Loss: 0.03899549123807865\n",
      "Epoch 16/100\n",
      "Training Loss: 0.08919850506412194\n",
      "Validation Loss: 0.05498780292790586\n",
      "Epoch 17/100\n",
      "Training Loss: 0.07455858663057952\n",
      "Validation Loss: 0.03815432318980031\n",
      "Epoch 18/100\n",
      "Training Loss: 0.06875837326541902\n",
      "Validation Loss: 0.03964969486224443\n",
      "Epoch 19/100\n",
      "Training Loss: 0.0702709306615673\n",
      "Validation Loss: 0.05585133754305847\n",
      "Epoch 20/100\n",
      "Training Loss: 0.06786544695155135\n",
      "Validation Loss: 0.05111969438450788\n",
      "Epoch 21/100\n",
      "Training Loss: 0.07007656813134167\n",
      "Validation Loss: 0.05546278630901602\n",
      "Epoch 22/100\n",
      "Training Loss: 0.06858528845760642\n",
      "Validation Loss: 0.06895143744228954\n",
      "Epoch 23/100\n",
      "Training Loss: 0.06409001143426177\n",
      "Validation Loss: 0.08627821209411381\n",
      "Epoch 24/100\n",
      "Training Loss: 0.06639699551444501\n",
      "Validation Loss: 0.05249916619215057\n",
      "Epoch 25/100\n",
      "Training Loss: 0.0640135027797838\n",
      "Validation Loss: 0.027306343531428155\n",
      "Epoch 26/100\n",
      "Training Loss: 0.053353257580065844\n",
      "Validation Loss: 0.03021655946429805\n",
      "Epoch 27/100\n",
      "Training Loss: 0.05759699281106391\n",
      "Validation Loss: 0.05154820549038412\n",
      "Epoch 28/100\n",
      "Training Loss: 0.05934202243796758\n",
      "Validation Loss: 0.045998684066194395\n",
      "Epoch 29/100\n",
      "Training Loss: 0.05981853583266308\n",
      "Validation Loss: 0.04774272275032845\n",
      "Epoch 30/100\n",
      "Training Loss: 0.05737967835555235\n",
      "Validation Loss: 0.05662698167745921\n",
      "Epoch 31/100\n",
      "Training Loss: 0.051308578537474205\n",
      "Validation Loss: 0.031265523019306726\n",
      "Epoch 32/100\n",
      "Training Loss: 0.0534202306567022\n",
      "Validation Loss: 0.051512902112221516\n",
      "Epoch 33/100\n",
      "Training Loss: 0.06279964251220632\n",
      "Validation Loss: 0.08414564686921557\n",
      "Epoch 34/100\n",
      "Training Loss: 0.051587929473049764\n",
      "Validation Loss: 0.0406187735713216\n",
      "Epoch 35/100\n",
      "Training Loss: 0.05342150763738096\n",
      "Validation Loss: 0.06146552074699909\n",
      "Epoch 36/100\n",
      "Training Loss: 0.05114787559246766\n",
      "Validation Loss: 0.022263535764741167\n",
      "Epoch 37/100\n",
      "Training Loss: 0.05857837851747094\n",
      "Validation Loss: 0.05471045619904894\n",
      "Epoch 38/100\n",
      "Training Loss: 0.046783019302938336\n",
      "Validation Loss: 0.03646326343084839\n",
      "Epoch 39/100\n",
      "Training Loss: 0.0514560653276715\n",
      "Validation Loss: 0.04858752491895131\n",
      "Epoch 40/100\n",
      "Training Loss: 0.07069138841752166\n",
      "Validation Loss: 0.051373160222244516\n",
      "Epoch 41/100\n",
      "Training Loss: 0.06205704654908301\n",
      "Validation Loss: 0.04741525811575821\n",
      "Epoch 42/100\n",
      "Training Loss: 0.04243029934546991\n",
      "Validation Loss: 0.028322796315008746\n",
      "Epoch 43/100\n",
      "Training Loss: 0.05988186422140248\n",
      "Validation Loss: 0.03654097523935755\n",
      "Epoch 44/100\n",
      "Training Loss: 0.05846765993499558\n",
      "Validation Loss: 0.055892782780276554\n",
      "Epoch 45/100\n",
      "Training Loss: 0.05288191725822752\n",
      "Validation Loss: 0.03997257969166874\n",
      "Epoch 46/100\n",
      "Training Loss: 0.04919227828216034\n",
      "Validation Loss: 0.027475559387693933\n",
      "Epoch 47/100\n",
      "Training Loss: 0.04607972943485615\n",
      "Validation Loss: 0.03843341323002543\n",
      "Epoch 48/100\n",
      "Training Loss: 0.0477704683466686\n",
      "Validation Loss: 0.03682986682450344\n",
      "Epoch 49/100\n",
      "Training Loss: 0.056564400263876956\n",
      "Validation Loss: 0.02403739663169887\n",
      "Epoch 50/100\n",
      "Training Loss: 0.06332147606303222\n",
      "Validation Loss: 0.0174599433236677\n",
      "Epoch 51/100\n",
      "Training Loss: 0.05915900498203583\n",
      "Validation Loss: 0.025498188806126892\n",
      "Epoch 52/100\n",
      "Training Loss: 0.06554787388788537\n",
      "Validation Loss: 0.03780347677691282\n",
      "Epoch 53/100\n",
      "Training Loss: 0.04924038734561438\n",
      "Validation Loss: 0.027053094152972957\n",
      "Epoch 54/100\n",
      "Training Loss: 0.050257201800808234\n",
      "Validation Loss: 0.04792113044466133\n",
      "Epoch 55/100\n",
      "Training Loss: 0.05255561902064753\n",
      "Validation Loss: 0.048876701151508016\n",
      "Epoch 56/100\n",
      "Training Loss: 0.04990661218662919\n",
      "Validation Loss: 0.022531737260885087\n",
      "Epoch 57/100\n",
      "Training Loss: 0.07409171471513108\n",
      "Validation Loss: 0.03237560908951531\n",
      "Epoch 58/100\n",
      "Training Loss: 0.05166548978295423\n",
      "Validation Loss: 0.0824897727461861\n",
      "Epoch 59/100\n",
      "Training Loss: 0.047911250868320245\n",
      "Validation Loss: 0.0673961953320072\n",
      "Epoch 60/100\n",
      "Training Loss: 0.06038594595503286\n",
      "Validation Loss: 0.08642541762547165\n",
      "Epoch 61/100\n",
      "Training Loss: 0.06291104287945173\n",
      "Validation Loss: 0.025316937313487425\n",
      "Epoch 62/100\n",
      "Training Loss: 0.05240852920033746\n",
      "Validation Loss: 0.043794321562910674\n",
      "Epoch 63/100\n",
      "Training Loss: 0.05281837145768201\n",
      "Validation Loss: 0.06152828489856812\n",
      "Epoch 64/100\n",
      "Training Loss: 0.06097082357827973\n",
      "Validation Loss: 0.05817200246622396\n",
      "Epoch 65/100\n",
      "Training Loss: 0.05353056759037042\n",
      "Validation Loss: 0.04394102878313459\n",
      "Epoch 66/100\n",
      "Training Loss: 0.048689642625317005\n",
      "Validation Loss: 0.02762180637470929\n",
      "Epoch 67/100\n",
      "Training Loss: 0.06165934443348398\n",
      "Validation Loss: 0.05225834382102643\n",
      "Epoch 68/100\n",
      "Training Loss: 0.0540262672342384\n",
      "Validation Loss: 0.0563688762862677\n",
      "Epoch 69/100\n",
      "Training Loss: 0.0550145855693567\n",
      "Validation Loss: 0.03157745194321626\n",
      "Epoch 70/100\n",
      "Training Loss: 0.056459960029988626\n",
      "Validation Loss: 0.04675601240355585\n",
      "Epoch 71/100\n",
      "Training Loss: 0.0573697833978888\n",
      "Validation Loss: 0.034467162440939156\n",
      "Epoch 72/100\n",
      "Training Loss: 0.06582099222334577\n",
      "Validation Loss: 0.05853919715018657\n",
      "Epoch 73/100\n",
      "Training Loss: 0.06433294963598486\n",
      "Validation Loss: 0.07749973353105802\n",
      "Epoch 74/100\n",
      "Training Loss: 0.05930601618261134\n",
      "Validation Loss: 0.02057191460342394\n",
      "Epoch 75/100\n",
      "Training Loss: 0.05178852711230342\n",
      "Validation Loss: 0.028306631978002532\n",
      "Epoch 76/100\n",
      "Training Loss: 0.05278639835423812\n",
      "Validation Loss: 0.08245064798700712\n",
      "Epoch 77/100\n",
      "Training Loss: 0.06961336761358251\n",
      "Validation Loss: 0.045261592350724786\n",
      "Epoch 78/100\n",
      "Training Loss: 0.06009727252287267\n",
      "Validation Loss: 0.06210465391903778\n",
      "Epoch 79/100\n",
      "Training Loss: 0.06249927560835029\n",
      "Validation Loss: 0.026490848646168887\n",
      "Epoch 80/100\n",
      "Training Loss: 0.06530984278218244\n",
      "Validation Loss: 0.0843027247876512\n",
      "Epoch 81/100\n",
      "Training Loss: 0.06704227673726235\n",
      "Validation Loss: 0.06190939258016376\n",
      "Epoch 82/100\n",
      "Training Loss: 0.0559215523769413\n",
      "Validation Loss: 0.03400927268954705\n",
      "Epoch 83/100\n",
      "Training Loss: 0.04959365410970974\n",
      "Validation Loss: 0.0814764289758574\n",
      "Epoch 84/100\n",
      "Training Loss: 0.06089528378592909\n",
      "Validation Loss: 0.04835597397893487\n",
      "Epoch 85/100\n",
      "Training Loss: 0.062174601666080104\n",
      "Validation Loss: 0.01867320098524383\n",
      "Epoch 86/100\n",
      "Training Loss: 0.05905373280762014\n",
      "Validation Loss: 0.057633145051948484\n",
      "Epoch 87/100\n",
      "Training Loss: 0.06777777603818722\n",
      "Validation Loss: 0.07091226715961109\n",
      "Epoch 88/100\n",
      "Training Loss: 0.059201973673634625\n",
      "Validation Loss: 0.026662609961936555\n",
      "Epoch 89/100\n",
      "Training Loss: 0.05749924261954785\n",
      "Validation Loss: 0.04059363565312629\n",
      "Epoch 90/100\n",
      "Training Loss: 0.05515131179022947\n",
      "Validation Loss: 0.029902575125063058\n",
      "Epoch 91/100\n",
      "Training Loss: 0.05846308335024836\n",
      "Validation Loss: 0.04396160941248739\n",
      "Epoch 92/100\n",
      "Training Loss: 0.043153438736781354\n",
      "Validation Loss: 0.049373731682374584\n",
      "Epoch 93/100\n",
      "Training Loss: 0.06724442974279254\n",
      "Validation Loss: 0.040735596068184596\n",
      "Epoch 94/100\n",
      "Training Loss: 0.058378322925027946\n",
      "Validation Loss: 0.03759761065274188\n",
      "Epoch 95/100\n",
      "Training Loss: 0.05602657537432781\n",
      "Validation Loss: 0.03596651173636734\n",
      "Epoch 96/100\n",
      "Training Loss: 0.05829620856220493\n",
      "Validation Loss: 0.025031196478375727\n",
      "Epoch 97/100\n",
      "Training Loss: 0.06142896160574861\n",
      "Validation Loss: 0.05694715032421467\n",
      "Epoch 98/100\n",
      "Training Loss: 0.05329513756425466\n",
      "Validation Loss: 0.031481166640496897\n",
      "Epoch 99/100\n",
      "Training Loss: 0.06127989667209319\n",
      "Validation Loss: 0.03759982531530156\n",
      "Epoch 100/100\n",
      "Training Loss: 0.056778322905583496\n",
      "Validation Loss: 0.04972419981008674\n",
      "Combination 21: Avg Training Loss = 0.07019774180823159, Avg Validation Loss = 0.05104047214907841\n",
      "Testing combination 22/48: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.001, 'sequence_length': 20, 'dropout_rate': 0.2}\n",
      "Truncated Train Data Shape: (70, 20, 1)\n",
      "Truncated Validation Data Shape: (10, 20, 1)\n",
      "    Trial 1/2 for combination 22/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.3385721665172825\n",
      "Validation Loss: 0.19200016627335414\n",
      "Epoch 2/100\n",
      "Training Loss: 0.19727577208448066\n",
      "Validation Loss: 0.18176766332974686\n",
      "Epoch 3/100\n",
      "Training Loss: 0.1679945634447279\n",
      "Validation Loss: 0.129215855790756\n",
      "Epoch 4/100\n",
      "Training Loss: 0.16925141501074129\n",
      "Validation Loss: 0.14336822877300756\n",
      "Epoch 5/100\n",
      "Training Loss: 0.12244271419746762\n",
      "Validation Loss: 0.11512819404689463\n",
      "Epoch 6/100\n",
      "Training Loss: 0.1498511896267405\n",
      "Validation Loss: 0.0740221969040718\n",
      "Epoch 7/100\n",
      "Training Loss: 0.1240880219109312\n",
      "Validation Loss: 0.06672605217683411\n",
      "Epoch 8/100\n",
      "Training Loss: 0.09969042050716888\n",
      "Validation Loss: 0.05294946995694033\n",
      "Epoch 9/100\n",
      "Training Loss: 0.10816618117133023\n",
      "Validation Loss: 0.06306676297456418\n",
      "Epoch 10/100\n",
      "Training Loss: 0.09593446071511127\n",
      "Validation Loss: 0.044513961986571\n",
      "Epoch 11/100\n",
      "Training Loss: 0.08668034435375943\n",
      "Validation Loss: 0.049841859193897116\n",
      "Epoch 12/100\n",
      "Training Loss: 0.08447748166662244\n",
      "Validation Loss: 0.035271701436993944\n",
      "Epoch 13/100\n",
      "Training Loss: 0.09022721802970021\n",
      "Validation Loss: 0.08140425036099888\n",
      "Epoch 14/100\n",
      "Training Loss: 0.08023122234276103\n",
      "Validation Loss: 0.0431270610791875\n",
      "Epoch 15/100\n",
      "Training Loss: 0.07569425630535002\n",
      "Validation Loss: 0.04529376554648193\n",
      "Epoch 16/100\n",
      "Training Loss: 0.07424783952595465\n",
      "Validation Loss: 0.046471490108848226\n",
      "Epoch 17/100\n",
      "Training Loss: 0.07049397475668247\n",
      "Validation Loss: 0.0649019415670427\n",
      "Epoch 18/100\n",
      "Training Loss: 0.0717005733724696\n",
      "Validation Loss: 0.06298148128856382\n",
      "Epoch 19/100\n",
      "Training Loss: 0.06921132740470232\n",
      "Validation Loss: 0.043284485089602434\n",
      "Epoch 20/100\n",
      "Training Loss: 0.06748134460800224\n",
      "Validation Loss: 0.035773251057418194\n",
      "Epoch 21/100\n",
      "Training Loss: 0.06315399835547253\n",
      "Validation Loss: 0.048687806882035015\n",
      "Epoch 22/100\n",
      "Training Loss: 0.0664598711104827\n",
      "Validation Loss: 0.048245309002600545\n",
      "Epoch 23/100\n",
      "Training Loss: 0.06157483910931625\n",
      "Validation Loss: 0.033275892251815674\n",
      "Epoch 24/100\n",
      "Training Loss: 0.06107719380965302\n",
      "Validation Loss: 0.023391318629956713\n",
      "Epoch 25/100\n",
      "Training Loss: 0.06346971380271908\n",
      "Validation Loss: 0.05534144777670709\n",
      "Epoch 26/100\n",
      "Training Loss: 0.06098948788207703\n",
      "Validation Loss: 0.0841848752710044\n",
      "Epoch 27/100\n",
      "Training Loss: 0.054847256085284235\n",
      "Validation Loss: 0.059920616000787096\n",
      "Epoch 28/100\n",
      "Training Loss: 0.06493582105953061\n",
      "Validation Loss: 0.08065709665829693\n",
      "Epoch 29/100\n",
      "Training Loss: 0.05424206335329783\n",
      "Validation Loss: 0.06272867047906802\n",
      "Epoch 30/100\n",
      "Training Loss: 0.052571652247660784\n",
      "Validation Loss: 0.049494403627449626\n",
      "Epoch 31/100\n",
      "Training Loss: 0.061871720133905746\n",
      "Validation Loss: 0.0383880560404823\n",
      "Epoch 32/100\n",
      "Training Loss: 0.05797690863104536\n",
      "Validation Loss: 0.052655753663539494\n",
      "Epoch 33/100\n",
      "Training Loss: 0.06443539898590903\n",
      "Validation Loss: 0.03628612012117009\n",
      "Epoch 34/100\n",
      "Training Loss: 0.058409284651195915\n",
      "Validation Loss: 0.07121343717394626\n",
      "Epoch 35/100\n",
      "Training Loss: 0.061637278040099414\n",
      "Validation Loss: 0.03855049436829555\n",
      "Epoch 36/100\n",
      "Training Loss: 0.053908767228880244\n",
      "Validation Loss: 0.10703608818288339\n",
      "Epoch 37/100\n",
      "Training Loss: 0.06029862150692866\n",
      "Validation Loss: 0.02422271990969426\n",
      "Epoch 38/100\n",
      "Training Loss: 0.05150607857498296\n",
      "Validation Loss: 0.037355409500073926\n",
      "Epoch 39/100\n",
      "Training Loss: 0.05215843138981422\n",
      "Validation Loss: 0.06616594446252186\n",
      "Epoch 40/100\n",
      "Training Loss: 0.048668731195257185\n",
      "Validation Loss: 0.09561719943425499\n",
      "Epoch 41/100\n",
      "Training Loss: 0.054003499285065495\n",
      "Validation Loss: 0.06756378432349064\n",
      "Epoch 42/100\n",
      "Training Loss: 0.054435874906294236\n",
      "Validation Loss: 0.04330173409296351\n",
      "Epoch 43/100\n",
      "Training Loss: 0.05157927062299294\n",
      "Validation Loss: 0.045271531623187256\n",
      "Epoch 44/100\n",
      "Training Loss: 0.05785258302207348\n",
      "Validation Loss: 0.047031283173461044\n",
      "Epoch 45/100\n",
      "Training Loss: 0.06894086839076613\n",
      "Validation Loss: 0.06383267877590865\n",
      "Epoch 46/100\n",
      "Training Loss: 0.061391381652727325\n",
      "Validation Loss: 0.04332679475829043\n",
      "Epoch 47/100\n",
      "Training Loss: 0.05066401434559207\n",
      "Validation Loss: 0.059705990598561895\n",
      "Epoch 48/100\n",
      "Training Loss: 0.062115062348083644\n",
      "Validation Loss: 0.031497253264548594\n",
      "Epoch 49/100\n",
      "Training Loss: 0.05679481190252387\n",
      "Validation Loss: 0.03898660641510862\n",
      "Epoch 50/100\n",
      "Training Loss: 0.06338389338882784\n",
      "Validation Loss: 0.06616373601281195\n",
      "Epoch 51/100\n",
      "Training Loss: 0.06316216179813555\n",
      "Validation Loss: 0.08310815289469956\n",
      "Epoch 52/100\n",
      "Training Loss: 0.05491695823054553\n",
      "Validation Loss: 0.12771756844142665\n",
      "Epoch 53/100\n",
      "Training Loss: 0.05632343151617806\n",
      "Validation Loss: 0.06040823649889436\n",
      "Epoch 54/100\n",
      "Training Loss: 0.05530683240817709\n",
      "Validation Loss: 0.08513372538777882\n",
      "Epoch 55/100\n",
      "Training Loss: 0.05834942588106903\n",
      "Validation Loss: 0.046569622311813566\n",
      "Epoch 56/100\n",
      "Training Loss: 0.06507576471333992\n",
      "Validation Loss: 0.061868110748807235\n",
      "Epoch 57/100\n",
      "Training Loss: 0.05890171651983508\n",
      "Validation Loss: 0.04366419852436086\n",
      "Epoch 58/100\n",
      "Training Loss: 0.06244621180877866\n",
      "Validation Loss: 0.06611790399666541\n",
      "Epoch 59/100\n",
      "Training Loss: 0.0602596637921592\n",
      "Validation Loss: 0.037237791173641795\n",
      "Epoch 60/100\n",
      "Training Loss: 0.048721416438859994\n",
      "Validation Loss: 0.12375096706460761\n",
      "Epoch 61/100\n",
      "Training Loss: 0.05127015566807248\n",
      "Validation Loss: 0.04658846644057853\n",
      "Epoch 62/100\n",
      "Training Loss: 0.052979033835362954\n",
      "Validation Loss: 0.08257044770556984\n",
      "Epoch 63/100\n",
      "Training Loss: 0.06096185723696577\n",
      "Validation Loss: 0.031153677965270448\n",
      "Epoch 64/100\n",
      "Training Loss: 0.05745349966578732\n",
      "Validation Loss: 0.05242577031737301\n",
      "Epoch 65/100\n",
      "Training Loss: 0.04618891161241623\n",
      "Validation Loss: 0.10093215456270335\n",
      "Epoch 66/100\n",
      "Training Loss: 0.058706253487593264\n",
      "Validation Loss: 0.07086911997177245\n",
      "Epoch 67/100\n",
      "Training Loss: 0.05146779966044259\n",
      "Validation Loss: 0.055120820919503766\n",
      "Epoch 68/100\n",
      "Training Loss: 0.05500736648431043\n",
      "Validation Loss: 0.06677220301750923\n",
      "Epoch 69/100\n",
      "Training Loss: 0.05661268499714571\n",
      "Validation Loss: 0.026532113108973143\n",
      "Epoch 70/100\n",
      "Training Loss: 0.05549607277481525\n",
      "Validation Loss: 0.06715491279556492\n",
      "Epoch 71/100\n",
      "Training Loss: 0.05813398650526417\n",
      "Validation Loss: 0.0487404901101138\n",
      "Epoch 72/100\n",
      "Training Loss: 0.057730663166263456\n",
      "Validation Loss: 0.12057767962852195\n",
      "Epoch 73/100\n",
      "Training Loss: 0.052332321477677766\n",
      "Validation Loss: 0.06517466669732352\n",
      "Epoch 74/100\n",
      "Training Loss: 0.06124366069147372\n",
      "Validation Loss: 0.04942324094743016\n",
      "Epoch 75/100\n",
      "Training Loss: 0.05513843163354854\n",
      "Validation Loss: 0.13955946054052182\n",
      "Epoch 76/100\n",
      "Training Loss: 0.05899447498058657\n",
      "Validation Loss: 0.051884321983581075\n",
      "Epoch 77/100\n",
      "Training Loss: 0.05127054128456446\n",
      "Validation Loss: 0.10081734927937522\n",
      "Epoch 78/100\n",
      "Training Loss: 0.053500838269276874\n",
      "Validation Loss: 0.060106020852301024\n",
      "Epoch 79/100\n",
      "Training Loss: 0.05528125944221488\n",
      "Validation Loss: 0.04767043525119842\n",
      "Epoch 80/100\n",
      "Training Loss: 0.04492733761651972\n",
      "Validation Loss: 0.10378362421954782\n",
      "Epoch 81/100\n",
      "Training Loss: 0.04845351271353413\n",
      "Validation Loss: 0.04402142376627083\n",
      "Epoch 82/100\n",
      "Training Loss: 0.0578256033526409\n",
      "Validation Loss: 0.11893891911202933\n",
      "Epoch 83/100\n",
      "Training Loss: 0.054141374063991665\n",
      "Validation Loss: 0.05096351167822814\n",
      "Epoch 84/100\n",
      "Training Loss: 0.05746137856574421\n",
      "Validation Loss: 0.0965024651339571\n",
      "Epoch 85/100\n",
      "Training Loss: 0.05727183039635877\n",
      "Validation Loss: 0.0396348285470669\n",
      "Epoch 86/100\n",
      "Training Loss: 0.055268722987078266\n",
      "Validation Loss: 0.1293012269178962\n",
      "Epoch 87/100\n",
      "Training Loss: 0.0577017486249343\n",
      "Validation Loss: 0.0638831602134417\n",
      "Epoch 88/100\n",
      "Training Loss: 0.05484047856982897\n",
      "Validation Loss: 0.0918896941384407\n",
      "Epoch 89/100\n",
      "Training Loss: 0.057611555827245794\n",
      "Validation Loss: 0.048455054900589656\n",
      "Epoch 90/100\n",
      "Training Loss: 0.0571754567940973\n",
      "Validation Loss: 0.08166532857142605\n",
      "Epoch 91/100\n",
      "Training Loss: 0.05661654836068902\n",
      "Validation Loss: 0.03938505299352677\n",
      "Epoch 92/100\n",
      "Training Loss: 0.05957929603154166\n",
      "Validation Loss: 0.057275933532835034\n",
      "Epoch 93/100\n",
      "Training Loss: 0.06441012731214485\n",
      "Validation Loss: 0.06219082510677802\n",
      "Epoch 94/100\n",
      "Training Loss: 0.05133182332187162\n",
      "Validation Loss: 0.10349612447440051\n",
      "Epoch 95/100\n",
      "Training Loss: 0.06365506529687331\n",
      "Validation Loss: 0.05717603798320529\n",
      "Epoch 96/100\n",
      "Training Loss: 0.058187395819115736\n",
      "Validation Loss: 0.04136120906559936\n",
      "Epoch 97/100\n",
      "Training Loss: 0.058642385264803476\n",
      "Validation Loss: 0.03786775381958704\n",
      "Epoch 98/100\n",
      "Training Loss: 0.051925936662045036\n",
      "Validation Loss: 0.041925099362450834\n",
      "Epoch 99/100\n",
      "Training Loss: 0.04764918040654404\n",
      "Validation Loss: 0.0372831346885585\n",
      "Epoch 100/100\n",
      "Training Loss: 0.05595262340419631\n",
      "Validation Loss: 0.05961776550871658\n",
      "    Trial 2/2 for combination 22/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.3047280358327489\n",
      "Validation Loss: 0.12798349674541326\n",
      "Epoch 2/100\n",
      "Training Loss: 0.20046714201461294\n",
      "Validation Loss: 0.09437046517097394\n",
      "Epoch 3/100\n",
      "Training Loss: 0.19071339805554471\n",
      "Validation Loss: 0.1908919811028063\n",
      "Epoch 4/100\n",
      "Training Loss: 0.17775334241105067\n",
      "Validation Loss: 0.08083075273984894\n",
      "Epoch 5/100\n",
      "Training Loss: 0.14578447598381755\n",
      "Validation Loss: 0.08850521931511204\n",
      "Epoch 6/100\n",
      "Training Loss: 0.11661441101306172\n",
      "Validation Loss: 0.07640130207665305\n",
      "Epoch 7/100\n",
      "Training Loss: 0.11723201707610313\n",
      "Validation Loss: 0.06664837031484563\n",
      "Epoch 8/100\n",
      "Training Loss: 0.10383966636787015\n",
      "Validation Loss: 0.07000155589030346\n",
      "Epoch 9/100\n",
      "Training Loss: 0.11205920887231229\n",
      "Validation Loss: 0.07375092009029502\n",
      "Epoch 10/100\n",
      "Training Loss: 0.10564708847316345\n",
      "Validation Loss: 0.06808449249537926\n",
      "Epoch 11/100\n",
      "Training Loss: 0.10250562677635584\n",
      "Validation Loss: 0.06113854024003333\n",
      "Epoch 12/100\n",
      "Training Loss: 0.08627689545163567\n",
      "Validation Loss: 0.03323123713888361\n",
      "Epoch 13/100\n",
      "Training Loss: 0.08628845211712204\n",
      "Validation Loss: 0.05057123238219139\n",
      "Epoch 14/100\n",
      "Training Loss: 0.0764970991737354\n",
      "Validation Loss: 0.07313175544048021\n",
      "Epoch 15/100\n",
      "Training Loss: 0.07074813649828289\n",
      "Validation Loss: 0.044542464869371697\n",
      "Epoch 16/100\n",
      "Training Loss: 0.07592939980557627\n",
      "Validation Loss: 0.05912072880874768\n",
      "Epoch 17/100\n",
      "Training Loss: 0.06590233217856817\n",
      "Validation Loss: 0.025772683576544532\n",
      "Epoch 18/100\n",
      "Training Loss: 0.07130595453943145\n",
      "Validation Loss: 0.07604293317664994\n",
      "Epoch 19/100\n",
      "Training Loss: 0.06717547516235035\n",
      "Validation Loss: 0.039466533582849496\n",
      "Epoch 20/100\n",
      "Training Loss: 0.06880182345112353\n",
      "Validation Loss: 0.04742422435780263\n",
      "Epoch 21/100\n",
      "Training Loss: 0.06891617263683159\n",
      "Validation Loss: 0.05911429329476742\n",
      "Epoch 22/100\n",
      "Training Loss: 0.06428191619476932\n",
      "Validation Loss: 0.08606569196243946\n",
      "Epoch 23/100\n",
      "Training Loss: 0.06489517893621116\n",
      "Validation Loss: 0.08681236114649366\n",
      "Epoch 24/100\n",
      "Training Loss: 0.053113210222443565\n",
      "Validation Loss: 0.041982935742259885\n",
      "Epoch 25/100\n",
      "Training Loss: 0.056981524658231454\n",
      "Validation Loss: 0.06702415591475305\n",
      "Epoch 26/100\n",
      "Training Loss: 0.059095641876354806\n",
      "Validation Loss: 0.05058553655858281\n",
      "Epoch 27/100\n",
      "Training Loss: 0.06104505796764434\n",
      "Validation Loss: 0.04977346540303836\n",
      "Epoch 28/100\n",
      "Training Loss: 0.05239313348796396\n",
      "Validation Loss: 0.058381710643962505\n",
      "Epoch 29/100\n",
      "Training Loss: 0.068997341235924\n",
      "Validation Loss: 0.05014296549529743\n",
      "Epoch 30/100\n",
      "Training Loss: 0.05951200454533833\n",
      "Validation Loss: 0.02460845783787201\n",
      "Epoch 31/100\n",
      "Training Loss: 0.059720095945478134\n",
      "Validation Loss: 0.07939737514956274\n",
      "Epoch 32/100\n",
      "Training Loss: 0.054646261944497745\n",
      "Validation Loss: 0.07148079406739656\n",
      "Epoch 33/100\n",
      "Training Loss: 0.05783425010338702\n",
      "Validation Loss: 0.03152839303131384\n",
      "Epoch 34/100\n",
      "Training Loss: 0.05546357815876766\n",
      "Validation Loss: 0.03059014348595896\n",
      "Epoch 35/100\n",
      "Training Loss: 0.059450463128677265\n",
      "Validation Loss: 0.03556489797417327\n",
      "Epoch 36/100\n",
      "Training Loss: 0.05392299848180392\n",
      "Validation Loss: 0.07491263371763482\n",
      "Epoch 37/100\n",
      "Training Loss: 0.05518157412447571\n",
      "Validation Loss: 0.0935710857884642\n",
      "Epoch 38/100\n",
      "Training Loss: 0.0561054442318029\n",
      "Validation Loss: 0.03735171333630323\n",
      "Epoch 39/100\n",
      "Training Loss: 0.06114785429397581\n",
      "Validation Loss: 0.07861886050879459\n",
      "Epoch 40/100\n",
      "Training Loss: 0.07067947762855388\n",
      "Validation Loss: 0.039526036859988734\n",
      "Epoch 41/100\n",
      "Training Loss: 0.05419972141529782\n",
      "Validation Loss: 0.0710467544336617\n",
      "Epoch 42/100\n",
      "Training Loss: 0.05518780294914289\n",
      "Validation Loss: 0.07447738325348684\n",
      "Epoch 43/100\n",
      "Training Loss: 0.05944216196476306\n",
      "Validation Loss: 0.08598794954912446\n",
      "Epoch 44/100\n",
      "Training Loss: 0.05814498431937505\n",
      "Validation Loss: 0.06310965927573901\n",
      "Epoch 45/100\n",
      "Training Loss: 0.051511191242341714\n",
      "Validation Loss: 0.04922417726079157\n",
      "Epoch 46/100\n",
      "Training Loss: 0.05396962517445284\n",
      "Validation Loss: 0.09935304141968564\n",
      "Epoch 47/100\n",
      "Training Loss: 0.05109609273266083\n",
      "Validation Loss: 0.04859457997020213\n",
      "Epoch 48/100\n",
      "Training Loss: 0.05558770630670429\n",
      "Validation Loss: 0.09921555627352953\n",
      "Epoch 49/100\n",
      "Training Loss: 0.0693687608460661\n",
      "Validation Loss: 0.08486781714535749\n",
      "Epoch 50/100\n",
      "Training Loss: 0.051033961936527636\n",
      "Validation Loss: 0.07768511387325336\n",
      "Epoch 51/100\n",
      "Training Loss: 0.06469344833930561\n",
      "Validation Loss: 0.026539309246729342\n",
      "Epoch 52/100\n",
      "Training Loss: 0.0510563974568553\n",
      "Validation Loss: 0.09046746825670417\n",
      "Epoch 53/100\n",
      "Training Loss: 0.05152851750304581\n",
      "Validation Loss: 0.08822332668013515\n",
      "Epoch 54/100\n",
      "Training Loss: 0.060882883865878056\n",
      "Validation Loss: 0.050635436495296594\n",
      "Epoch 55/100\n",
      "Training Loss: 0.0561542509497299\n",
      "Validation Loss: 0.07211390866288774\n",
      "Epoch 56/100\n",
      "Training Loss: 0.06280757616908587\n",
      "Validation Loss: 0.07436276511075748\n",
      "Epoch 57/100\n",
      "Training Loss: 0.054441214014981386\n",
      "Validation Loss: 0.032467893496774355\n",
      "Epoch 58/100\n",
      "Training Loss: 0.06891096788200805\n",
      "Validation Loss: 0.060427635836758184\n",
      "Epoch 59/100\n",
      "Training Loss: 0.0565264758250699\n",
      "Validation Loss: 0.0418005914520241\n",
      "Epoch 60/100\n",
      "Training Loss: 0.06159505958753383\n",
      "Validation Loss: 0.10360908360449583\n",
      "Epoch 61/100\n",
      "Training Loss: 0.0588518049193116\n",
      "Validation Loss: 0.030446378534088374\n",
      "Epoch 62/100\n",
      "Training Loss: 0.05558026524334681\n",
      "Validation Loss: 0.05277699032138938\n",
      "Epoch 63/100\n",
      "Training Loss: 0.06401323934583585\n",
      "Validation Loss: 0.06497160113672237\n",
      "Epoch 64/100\n",
      "Training Loss: 0.05788216852136841\n",
      "Validation Loss: 0.09893491387748873\n",
      "Epoch 65/100\n",
      "Training Loss: 0.05792281412945178\n",
      "Validation Loss: 0.05625702998119012\n",
      "Epoch 66/100\n",
      "Training Loss: 0.06078307237399072\n",
      "Validation Loss: 0.06146275736364944\n",
      "Epoch 67/100\n",
      "Training Loss: 0.05744166473669853\n",
      "Validation Loss: 0.06284548831178924\n",
      "Epoch 68/100\n",
      "Training Loss: 0.05770871665822243\n",
      "Validation Loss: 0.05685865965597775\n",
      "Epoch 69/100\n",
      "Training Loss: 0.0543534529638288\n",
      "Validation Loss: 0.10176612480640128\n",
      "Epoch 70/100\n",
      "Training Loss: 0.0565673120093384\n",
      "Validation Loss: 0.09737260744616819\n",
      "Epoch 71/100\n",
      "Training Loss: 0.06367795605961961\n",
      "Validation Loss: 0.1155715142304012\n",
      "Epoch 72/100\n",
      "Training Loss: 0.052748970233128675\n",
      "Validation Loss: 0.04832530394781578\n",
      "Epoch 73/100\n",
      "Training Loss: 0.05793275531655308\n",
      "Validation Loss: 0.10820843477048525\n",
      "Epoch 74/100\n",
      "Training Loss: 0.051502612211954145\n",
      "Validation Loss: 0.11677860781085128\n",
      "Epoch 75/100\n",
      "Training Loss: 0.0595534985288918\n",
      "Validation Loss: 0.08661118646553496\n",
      "Epoch 76/100\n",
      "Training Loss: 0.0543048379514112\n",
      "Validation Loss: 0.06962586813354035\n",
      "Epoch 77/100\n",
      "Training Loss: 0.053879123701064284\n",
      "Validation Loss: 0.12361655287902554\n",
      "Epoch 78/100\n",
      "Training Loss: 0.05928941955767442\n",
      "Validation Loss: 0.10182907939782682\n",
      "Epoch 79/100\n",
      "Training Loss: 0.0599345544815286\n",
      "Validation Loss: 0.051167519607905054\n",
      "Epoch 80/100\n",
      "Training Loss: 0.05004713298697951\n",
      "Validation Loss: 0.037807118254189656\n",
      "Epoch 81/100\n",
      "Training Loss: 0.06260225352282882\n",
      "Validation Loss: 0.11889402558169737\n",
      "Epoch 82/100\n",
      "Training Loss: 0.052457046345811414\n",
      "Validation Loss: 0.08583674869192277\n",
      "Epoch 83/100\n",
      "Training Loss: 0.04994642986125336\n",
      "Validation Loss: 0.0493163290977183\n",
      "Epoch 84/100\n",
      "Training Loss: 0.05646831890964876\n",
      "Validation Loss: 0.05943045201996276\n",
      "Epoch 85/100\n",
      "Training Loss: 0.07069727546705061\n",
      "Validation Loss: 0.052063235924598626\n",
      "Epoch 86/100\n",
      "Training Loss: 0.06318811202313628\n",
      "Validation Loss: 0.060817765807347615\n",
      "Epoch 87/100\n",
      "Training Loss: 0.05881425763003859\n",
      "Validation Loss: 0.050607714155395125\n",
      "Epoch 88/100\n",
      "Training Loss: 0.05156935188345739\n",
      "Validation Loss: 0.041273546263516515\n",
      "Epoch 89/100\n",
      "Training Loss: 0.06352491823727237\n",
      "Validation Loss: 0.05429307273228888\n",
      "Epoch 90/100\n",
      "Training Loss: 0.053943958169973646\n",
      "Validation Loss: 0.12706253689969774\n",
      "Epoch 91/100\n",
      "Training Loss: 0.06258987517288057\n",
      "Validation Loss: 0.09250396855502141\n",
      "Epoch 92/100\n",
      "Training Loss: 0.0636565555725782\n",
      "Validation Loss: 0.030517591319663882\n",
      "Epoch 93/100\n",
      "Training Loss: 0.06449384486041068\n",
      "Validation Loss: 0.0949126225886969\n",
      "Epoch 94/100\n",
      "Training Loss: 0.0616972315201749\n",
      "Validation Loss: 0.10686631463722344\n",
      "Epoch 95/100\n",
      "Training Loss: 0.06229163178550277\n",
      "Validation Loss: 0.10109341806484314\n",
      "Epoch 96/100\n",
      "Training Loss: 0.05534306523472176\n",
      "Validation Loss: 0.09333389228971427\n",
      "Epoch 97/100\n",
      "Training Loss: 0.05819278197413763\n",
      "Validation Loss: 0.08420903811866201\n",
      "Epoch 98/100\n",
      "Training Loss: 0.06085601893833008\n",
      "Validation Loss: 0.06854168991489154\n",
      "Epoch 99/100\n",
      "Training Loss: 0.062480150980951624\n",
      "Validation Loss: 0.07711872027395464\n",
      "Epoch 100/100\n",
      "Training Loss: 0.06422726948948033\n",
      "Validation Loss: 0.07003903065539466\n",
      "Combination 22: Avg Training Loss = 0.06966868661495684, Avg Validation Loss = 0.06834298496772707\n",
      "Testing combination 23/48: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.001, 'sequence_length': 50, 'dropout_rate': 0.1}\n",
      "Truncated Train Data Shape: (70, 50, 1)\n",
      "Truncated Validation Data Shape: (10, 50, 1)\n",
      "    Trial 1/2 for combination 23/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.2817427094287016\n",
      "Validation Loss: 0.13923313704310128\n",
      "Epoch 2/100\n",
      "Training Loss: 0.16177805995851033\n",
      "Validation Loss: 0.11881991420584885\n",
      "Epoch 3/100\n",
      "Training Loss: 0.1539649645049787\n",
      "Validation Loss: 0.11881512692952048\n",
      "Epoch 4/100\n",
      "Training Loss: 0.14705295168960156\n",
      "Validation Loss: 0.09952478573502335\n",
      "Epoch 5/100\n",
      "Training Loss: 0.12437285408989202\n",
      "Validation Loss: 0.04365428138496813\n",
      "Epoch 6/100\n",
      "Training Loss: 0.11186440694956329\n",
      "Validation Loss: 0.047051084286877924\n",
      "Epoch 7/100\n",
      "Training Loss: 0.10419949615587441\n",
      "Validation Loss: 0.04404460380685277\n",
      "Epoch 8/100\n",
      "Training Loss: 0.11020435326693827\n",
      "Validation Loss: 0.05277908974027645\n",
      "Epoch 9/100\n",
      "Training Loss: 0.08997468886073139\n",
      "Validation Loss: 0.08469447164315541\n",
      "Epoch 10/100\n",
      "Training Loss: 0.09041845622604369\n",
      "Validation Loss: 0.086877126306136\n",
      "Epoch 11/100\n",
      "Training Loss: 0.0817700825623802\n",
      "Validation Loss: 0.04547788642148502\n",
      "Epoch 12/100\n",
      "Training Loss: 0.06943242302328742\n",
      "Validation Loss: 0.045151607488430494\n",
      "Epoch 13/100\n",
      "Training Loss: 0.05954683619777866\n",
      "Validation Loss: 0.06481187847443008\n",
      "Epoch 14/100\n",
      "Training Loss: 0.06767606737334642\n",
      "Validation Loss: 0.045127286727855556\n",
      "Epoch 15/100\n",
      "Training Loss: 0.06668657137656012\n",
      "Validation Loss: 0.039616975917009405\n",
      "Epoch 16/100\n",
      "Training Loss: 0.060762985030575326\n",
      "Validation Loss: 0.06235261294908938\n",
      "Epoch 17/100\n",
      "Training Loss: 0.06001329955136038\n",
      "Validation Loss: 0.04295277784641653\n",
      "Epoch 18/100\n",
      "Training Loss: 0.05422518987990706\n",
      "Validation Loss: 0.060809204735937386\n",
      "Epoch 19/100\n",
      "Training Loss: 0.06022117592511215\n",
      "Validation Loss: 0.039858536145019975\n",
      "Epoch 20/100\n",
      "Training Loss: 0.04946225465561173\n",
      "Validation Loss: 0.0876460293830187\n",
      "Epoch 21/100\n",
      "Training Loss: 0.05536151920633631\n",
      "Validation Loss: 0.06769490920000262\n",
      "Epoch 22/100\n",
      "Training Loss: 0.05164013170677032\n",
      "Validation Loss: 0.04667129130138555\n",
      "Epoch 23/100\n",
      "Training Loss: 0.03832144868084449\n",
      "Validation Loss: 0.09527736685741553\n",
      "Epoch 24/100\n",
      "Training Loss: 0.05180820693899076\n",
      "Validation Loss: 0.04309199485644208\n",
      "Epoch 25/100\n",
      "Training Loss: 0.052263236735343485\n",
      "Validation Loss: 0.04098578242049065\n",
      "Epoch 26/100\n",
      "Training Loss: 0.04879204425759822\n",
      "Validation Loss: 0.050733318832823934\n",
      "Epoch 27/100\n",
      "Training Loss: 0.04653970471255521\n",
      "Validation Loss: 0.06139321432726162\n",
      "Epoch 28/100\n",
      "Training Loss: 0.04892048108794383\n",
      "Validation Loss: 0.05636957672244737\n",
      "Epoch 29/100\n",
      "Training Loss: 0.045675952840702215\n",
      "Validation Loss: 0.08525313677415272\n",
      "Epoch 30/100\n",
      "Training Loss: 0.0462289924609164\n",
      "Validation Loss: 0.027717913439489074\n",
      "Epoch 31/100\n",
      "Training Loss: 0.042820739940654905\n",
      "Validation Loss: 0.030543739211924093\n",
      "Epoch 32/100\n",
      "Training Loss: 0.049727401791803724\n",
      "Validation Loss: 0.03616325566616229\n",
      "Epoch 33/100\n",
      "Training Loss: 0.04968198594338728\n",
      "Validation Loss: 0.04320786223348398\n",
      "Epoch 34/100\n",
      "Training Loss: 0.04481348743135617\n",
      "Validation Loss: 0.06383084563683397\n",
      "Epoch 35/100\n",
      "Training Loss: 0.05615283626687154\n",
      "Validation Loss: 0.05147510990673319\n",
      "Epoch 36/100\n",
      "Training Loss: 0.04267335773584604\n",
      "Validation Loss: 0.0294736446889367\n",
      "Epoch 37/100\n",
      "Training Loss: 0.04829306783211912\n",
      "Validation Loss: 0.028649082469612796\n",
      "Epoch 38/100\n",
      "Training Loss: 0.04733899448210915\n",
      "Validation Loss: 0.030909425394310193\n",
      "Epoch 39/100\n",
      "Training Loss: 0.047285506406682574\n",
      "Validation Loss: 0.03242675181895156\n",
      "Epoch 40/100\n",
      "Training Loss: 0.044599060318193134\n",
      "Validation Loss: 0.052555849034360905\n",
      "Epoch 41/100\n",
      "Training Loss: 0.054367751005460596\n",
      "Validation Loss: 0.03060695907536673\n",
      "Epoch 42/100\n",
      "Training Loss: 0.056556651907767805\n",
      "Validation Loss: 0.027283466095755087\n",
      "Epoch 43/100\n",
      "Training Loss: 0.048152603298109324\n",
      "Validation Loss: 0.03691226042478121\n",
      "Epoch 44/100\n",
      "Training Loss: 0.053632442327325204\n",
      "Validation Loss: 0.0651639745809256\n",
      "Epoch 45/100\n",
      "Training Loss: 0.05226801026549762\n",
      "Validation Loss: 0.06635187557524994\n",
      "Epoch 46/100\n",
      "Training Loss: 0.05910212778513217\n",
      "Validation Loss: 0.06753745509803992\n",
      "Epoch 47/100\n",
      "Training Loss: 0.06036248110918292\n",
      "Validation Loss: 0.03354510015169658\n",
      "Epoch 48/100\n",
      "Training Loss: 0.05606191457102777\n",
      "Validation Loss: 0.038001510017187115\n",
      "Epoch 49/100\n",
      "Training Loss: 0.0518738441077111\n",
      "Validation Loss: 0.08243169530866767\n",
      "Epoch 50/100\n",
      "Training Loss: 0.054119614827826415\n",
      "Validation Loss: 0.0800802446114495\n",
      "Epoch 51/100\n",
      "Training Loss: 0.06228171569481385\n",
      "Validation Loss: 0.06360757508911993\n",
      "Epoch 52/100\n",
      "Training Loss: 0.06669252628618952\n",
      "Validation Loss: 0.056890217830189514\n",
      "Epoch 53/100\n",
      "Training Loss: 0.06382787732711583\n",
      "Validation Loss: 0.06289045140160573\n",
      "Epoch 54/100\n",
      "Training Loss: 0.06145124810114899\n",
      "Validation Loss: 0.028622828469265265\n",
      "Epoch 55/100\n",
      "Training Loss: 0.06058282767328296\n",
      "Validation Loss: 0.06792355758748352\n",
      "Epoch 56/100\n",
      "Training Loss: 0.06720272353608521\n",
      "Validation Loss: 0.03332745443052507\n",
      "Epoch 57/100\n",
      "Training Loss: 0.060219746770538776\n",
      "Validation Loss: 0.04186126978404188\n",
      "Epoch 58/100\n",
      "Training Loss: 0.059001364200779206\n",
      "Validation Loss: 0.05397998418973934\n",
      "Epoch 59/100\n",
      "Training Loss: 0.05598815262360469\n",
      "Validation Loss: 0.046067488028517053\n",
      "Epoch 60/100\n",
      "Training Loss: 0.05615776089857997\n",
      "Validation Loss: 0.04643008149044649\n",
      "Epoch 61/100\n",
      "Training Loss: 0.05914991465037207\n",
      "Validation Loss: 0.03994053545573671\n",
      "Epoch 62/100\n",
      "Training Loss: 0.05633181161979014\n",
      "Validation Loss: 0.02877982322894603\n",
      "Epoch 63/100\n",
      "Training Loss: 0.05795580502832422\n",
      "Validation Loss: 0.06174377101845814\n",
      "Epoch 64/100\n",
      "Training Loss: 0.056643064338504086\n",
      "Validation Loss: 0.04040589589139864\n",
      "Epoch 65/100\n",
      "Training Loss: 0.0636506660892052\n",
      "Validation Loss: 0.07250099576213001\n",
      "Epoch 66/100\n",
      "Training Loss: 0.061758621564787966\n",
      "Validation Loss: 0.0477979290685929\n",
      "Epoch 67/100\n",
      "Training Loss: 0.05754753241516373\n",
      "Validation Loss: 0.06860320111914228\n",
      "Epoch 68/100\n",
      "Training Loss: 0.048993031868159634\n",
      "Validation Loss: 0.03519997814628517\n",
      "Epoch 69/100\n",
      "Training Loss: 0.05539478011410203\n",
      "Validation Loss: 0.06602154050672418\n",
      "Epoch 70/100\n",
      "Training Loss: 0.06038745254589896\n",
      "Validation Loss: 0.0373280026778187\n",
      "Epoch 71/100\n",
      "Training Loss: 0.053681943693248635\n",
      "Validation Loss: 0.04133383701869556\n",
      "Epoch 72/100\n",
      "Training Loss: 0.057843307638439666\n",
      "Validation Loss: 0.038724939720537324\n",
      "Epoch 73/100\n",
      "Training Loss: 0.060113625409191206\n",
      "Validation Loss: 0.03311140943910779\n",
      "Epoch 74/100\n",
      "Training Loss: 0.04904625954827318\n",
      "Validation Loss: 0.03450264881978026\n",
      "Epoch 75/100\n",
      "Training Loss: 0.05524533619316504\n",
      "Validation Loss: 0.027166540251501847\n",
      "Epoch 76/100\n",
      "Training Loss: 0.05863587506592159\n",
      "Validation Loss: 0.05531615830938347\n",
      "Epoch 77/100\n",
      "Training Loss: 0.0513391573616143\n",
      "Validation Loss: 0.0429872881236086\n",
      "Epoch 78/100\n",
      "Training Loss: 0.051443433733989685\n",
      "Validation Loss: 0.045541129477636715\n",
      "Epoch 79/100\n",
      "Training Loss: 0.051456459569976604\n",
      "Validation Loss: 0.033006533504990124\n",
      "Epoch 80/100\n",
      "Training Loss: 0.04973118179948864\n",
      "Validation Loss: 0.04839499754803374\n",
      "Epoch 81/100\n",
      "Training Loss: 0.06005710751556644\n",
      "Validation Loss: 0.04695932000198882\n",
      "Epoch 82/100\n",
      "Training Loss: 0.056130703116239355\n",
      "Validation Loss: 0.05300290720195675\n",
      "Epoch 83/100\n",
      "Training Loss: 0.052091921262881626\n",
      "Validation Loss: 0.04614708841194553\n",
      "Epoch 84/100\n",
      "Training Loss: 0.05291753948855709\n",
      "Validation Loss: 0.036330131369728326\n",
      "Epoch 85/100\n",
      "Training Loss: 0.054357122541940125\n",
      "Validation Loss: 0.08606556811092944\n",
      "Epoch 86/100\n",
      "Training Loss: 0.0589777069266726\n",
      "Validation Loss: 0.04894384480661854\n",
      "Epoch 87/100\n",
      "Training Loss: 0.054286711166306095\n",
      "Validation Loss: 0.03053028796779924\n",
      "Epoch 88/100\n",
      "Training Loss: 0.04558606854838375\n",
      "Validation Loss: 0.024178375932755132\n",
      "Epoch 89/100\n",
      "Training Loss: 0.05395921503305038\n",
      "Validation Loss: 0.040536465118613405\n",
      "Epoch 90/100\n",
      "Training Loss: 0.060078689742341075\n",
      "Validation Loss: 0.04439714852060518\n",
      "Epoch 91/100\n",
      "Training Loss: 0.05223696551002392\n",
      "Validation Loss: 0.05354336036578825\n",
      "Epoch 92/100\n",
      "Training Loss: 0.0486947346639129\n",
      "Validation Loss: 0.042997901907628616\n",
      "Epoch 93/100\n",
      "Training Loss: 0.048982205599231\n",
      "Validation Loss: 0.041720823172796385\n",
      "Epoch 94/100\n",
      "Training Loss: 0.05870262276466884\n",
      "Validation Loss: 0.03414857389575401\n",
      "Epoch 95/100\n",
      "Training Loss: 0.05140155311674952\n",
      "Validation Loss: 0.03451359594185245\n",
      "Epoch 96/100\n",
      "Training Loss: 0.05987618733599755\n",
      "Validation Loss: 0.05197863870292304\n",
      "Epoch 97/100\n",
      "Training Loss: 0.055031260734902505\n",
      "Validation Loss: 0.038959009868656705\n",
      "Epoch 98/100\n",
      "Training Loss: 0.05522332690511328\n",
      "Validation Loss: 0.02257144549091392\n",
      "Epoch 99/100\n",
      "Training Loss: 0.049470000309350635\n",
      "Validation Loss: 0.03926534978932736\n",
      "Epoch 100/100\n",
      "Training Loss: 0.04846154015064554\n",
      "Validation Loss: 0.04873548011310327\n",
      "    Trial 2/2 for combination 23/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.2891538239157058\n",
      "Validation Loss: 0.0723520438565746\n",
      "Epoch 2/100\n",
      "Training Loss: 0.17126833122758928\n",
      "Validation Loss: 0.14259629735654883\n",
      "Epoch 3/100\n",
      "Training Loss: 0.15553563971633458\n",
      "Validation Loss: 0.04604997968677756\n",
      "Epoch 4/100\n",
      "Training Loss: 0.1183545242927594\n",
      "Validation Loss: 0.07902505947737014\n",
      "Epoch 5/100\n",
      "Training Loss: 0.13252037198485286\n",
      "Validation Loss: 0.05559851155744313\n",
      "Epoch 6/100\n",
      "Training Loss: 0.10765246181528353\n",
      "Validation Loss: 0.08070382004460672\n",
      "Epoch 7/100\n",
      "Training Loss: 0.09903324469094571\n",
      "Validation Loss: 0.04155832345896498\n",
      "Epoch 8/100\n",
      "Training Loss: 0.08820196146830966\n",
      "Validation Loss: 0.02518895690609465\n",
      "Epoch 9/100\n",
      "Training Loss: 0.07855096361027065\n",
      "Validation Loss: 0.06634148883603512\n",
      "Epoch 10/100\n",
      "Training Loss: 0.07059761783841727\n",
      "Validation Loss: 0.049085577924640744\n",
      "Epoch 11/100\n",
      "Training Loss: 0.07525927481602238\n",
      "Validation Loss: 0.03953264635727903\n",
      "Epoch 12/100\n",
      "Training Loss: 0.0680528402038644\n",
      "Validation Loss: 0.06403213185632597\n",
      "Epoch 13/100\n",
      "Training Loss: 0.05344209707718504\n",
      "Validation Loss: 0.04734857256279113\n",
      "Epoch 14/100\n",
      "Training Loss: 0.05751258742587053\n",
      "Validation Loss: 0.04584391267786918\n",
      "Epoch 15/100\n",
      "Training Loss: 0.05674158545570871\n",
      "Validation Loss: 0.037632948807409666\n",
      "Epoch 16/100\n",
      "Training Loss: 0.05703844207983233\n",
      "Validation Loss: 0.06305585049836926\n",
      "Epoch 17/100\n",
      "Training Loss: 0.06102924905682045\n",
      "Validation Loss: 0.08368152978635364\n",
      "Epoch 18/100\n",
      "Training Loss: 0.053019221928650946\n",
      "Validation Loss: 0.06683903167071757\n",
      "Epoch 19/100\n",
      "Training Loss: 0.05763834826901197\n",
      "Validation Loss: 0.03509577915994146\n",
      "Epoch 20/100\n",
      "Training Loss: 0.04496039687712339\n",
      "Validation Loss: 0.08217682274549251\n",
      "Epoch 21/100\n",
      "Training Loss: 0.04399455415300849\n",
      "Validation Loss: 0.04190156546219972\n",
      "Epoch 22/100\n",
      "Training Loss: 0.0481736306375378\n",
      "Validation Loss: 0.0685590713146654\n",
      "Epoch 23/100\n",
      "Training Loss: 0.03965320955405699\n",
      "Validation Loss: 0.05312035563633222\n",
      "Epoch 24/100\n",
      "Training Loss: 0.04462109074580582\n",
      "Validation Loss: 0.04640204814657616\n",
      "Epoch 25/100\n",
      "Training Loss: 0.050804252151498155\n",
      "Validation Loss: 0.04972999016320717\n",
      "Epoch 26/100\n",
      "Training Loss: 0.061053562210423885\n",
      "Validation Loss: 0.05004726260344983\n",
      "Epoch 27/100\n",
      "Training Loss: 0.056840910488377194\n",
      "Validation Loss: 0.051792575940764654\n",
      "Epoch 28/100\n",
      "Training Loss: 0.055317303369882526\n",
      "Validation Loss: 0.043552021416064325\n",
      "Epoch 29/100\n",
      "Training Loss: 0.04334726019138267\n",
      "Validation Loss: 0.03754462384449418\n",
      "Epoch 30/100\n",
      "Training Loss: 0.04929280366011504\n",
      "Validation Loss: 0.06469140721206987\n",
      "Epoch 31/100\n",
      "Training Loss: 0.04894811311355476\n",
      "Validation Loss: 0.08640940938614829\n",
      "Epoch 32/100\n",
      "Training Loss: 0.04464430556666539\n",
      "Validation Loss: 0.07758908424353363\n",
      "Epoch 33/100\n",
      "Training Loss: 0.045358736276714294\n",
      "Validation Loss: 0.04163625301775916\n",
      "Epoch 34/100\n",
      "Training Loss: 0.041616629650143365\n",
      "Validation Loss: 0.08686242932266196\n",
      "Epoch 35/100\n",
      "Training Loss: 0.04865725219873442\n",
      "Validation Loss: 0.03544602152774158\n",
      "Epoch 36/100\n",
      "Training Loss: 0.0576791641181639\n",
      "Validation Loss: 0.043544624847148515\n",
      "Epoch 37/100\n",
      "Training Loss: 0.05394693428230397\n",
      "Validation Loss: 0.02450495033193434\n",
      "Epoch 38/100\n",
      "Training Loss: 0.04510471514823348\n",
      "Validation Loss: 0.049668832743287906\n",
      "Epoch 39/100\n",
      "Training Loss: 0.05803496210157809\n",
      "Validation Loss: 0.06437023837348761\n",
      "Epoch 40/100\n",
      "Training Loss: 0.05521346924307075\n",
      "Validation Loss: 0.07927607888185688\n",
      "Epoch 41/100\n",
      "Training Loss: 0.05191321996831837\n",
      "Validation Loss: 0.06305986569202673\n",
      "Epoch 42/100\n",
      "Training Loss: 0.05958715156296787\n",
      "Validation Loss: 0.05980732018361572\n",
      "Epoch 43/100\n",
      "Training Loss: 0.05022125852863133\n",
      "Validation Loss: 0.031058555454219306\n",
      "Epoch 44/100\n",
      "Training Loss: 0.05504004367229548\n",
      "Validation Loss: 0.04086222724174242\n",
      "Epoch 45/100\n",
      "Training Loss: 0.053804092965723936\n",
      "Validation Loss: 0.029604365661131123\n",
      "Epoch 46/100\n",
      "Training Loss: 0.0580062751363395\n",
      "Validation Loss: 0.09894960999915567\n",
      "Epoch 47/100\n",
      "Training Loss: 0.06092940272287776\n",
      "Validation Loss: 0.04330537119418737\n",
      "Epoch 48/100\n",
      "Training Loss: 0.05722314390414488\n",
      "Validation Loss: 0.029605744523530986\n",
      "Epoch 49/100\n",
      "Training Loss: 0.0526420907081322\n",
      "Validation Loss: 0.10071830239630535\n",
      "Epoch 50/100\n",
      "Training Loss: 0.06225429241771359\n",
      "Validation Loss: 0.043110792444884784\n",
      "Epoch 51/100\n",
      "Training Loss: 0.05838151567382204\n",
      "Validation Loss: 0.03418328636492737\n",
      "Epoch 52/100\n",
      "Training Loss: 0.05145385510604853\n",
      "Validation Loss: 0.035024138506308235\n",
      "Epoch 53/100\n",
      "Training Loss: 0.05415764420417855\n",
      "Validation Loss: 0.05384964375573008\n",
      "Epoch 54/100\n",
      "Training Loss: 0.05624468434821774\n",
      "Validation Loss: 0.05544704003145221\n",
      "Epoch 55/100\n",
      "Training Loss: 0.0553383945917839\n",
      "Validation Loss: 0.051630341484531064\n",
      "Epoch 56/100\n",
      "Training Loss: 0.06192606362963694\n",
      "Validation Loss: 0.03007948599944657\n",
      "Epoch 57/100\n",
      "Training Loss: 0.04945137306745541\n",
      "Validation Loss: 0.05232651171000976\n",
      "Epoch 58/100\n",
      "Training Loss: 0.06527316740635293\n",
      "Validation Loss: 0.05237626773662311\n",
      "Epoch 59/100\n",
      "Training Loss: 0.05855541507725459\n",
      "Validation Loss: 0.08612331367094524\n",
      "Epoch 60/100\n",
      "Training Loss: 0.05786104846780672\n",
      "Validation Loss: 0.0329494490099898\n",
      "Epoch 61/100\n",
      "Training Loss: 0.05584557617137179\n",
      "Validation Loss: 0.05931328385310175\n",
      "Epoch 62/100\n",
      "Training Loss: 0.050047901618788415\n",
      "Validation Loss: 0.05202639257961448\n",
      "Epoch 63/100\n",
      "Training Loss: 0.06131082184674154\n",
      "Validation Loss: 0.07101129848131021\n",
      "Epoch 64/100\n",
      "Training Loss: 0.056174718014436915\n",
      "Validation Loss: 0.04347117107940402\n",
      "Epoch 65/100\n",
      "Training Loss: 0.05225737029924425\n",
      "Validation Loss: 0.030521947991794796\n",
      "Epoch 66/100\n",
      "Training Loss: 0.05799584926665777\n",
      "Validation Loss: 0.032559326624749044\n",
      "Epoch 67/100\n",
      "Training Loss: 0.06063543199757046\n",
      "Validation Loss: 0.03448372795826743\n",
      "Epoch 68/100\n",
      "Training Loss: 0.058079616152361145\n",
      "Validation Loss: 0.0499290976731696\n",
      "Epoch 69/100\n",
      "Training Loss: 0.055079917038167316\n",
      "Validation Loss: 0.03425259643479285\n",
      "Epoch 70/100\n",
      "Training Loss: 0.05053014699612216\n",
      "Validation Loss: 0.025965542118693764\n",
      "Epoch 71/100\n",
      "Training Loss: 0.04813490506699922\n",
      "Validation Loss: 0.036831863254404004\n",
      "Epoch 72/100\n",
      "Training Loss: 0.05072356350194598\n",
      "Validation Loss: 0.046748912579183025\n",
      "Epoch 73/100\n",
      "Training Loss: 0.050309134130337906\n",
      "Validation Loss: 0.026462707488191007\n",
      "Epoch 74/100\n",
      "Training Loss: 0.053757300257284486\n",
      "Validation Loss: 0.05787640752628862\n",
      "Epoch 75/100\n",
      "Training Loss: 0.06229806411816572\n",
      "Validation Loss: 0.035687349326439584\n",
      "Epoch 76/100\n",
      "Training Loss: 0.05569041119230984\n",
      "Validation Loss: 0.0289483664495122\n",
      "Epoch 77/100\n",
      "Training Loss: 0.05599104312868164\n",
      "Validation Loss: 0.06097496711137852\n",
      "Epoch 78/100\n",
      "Training Loss: 0.05870826216925451\n",
      "Validation Loss: 0.04926288925288345\n",
      "Epoch 79/100\n",
      "Training Loss: 0.051688163257856336\n",
      "Validation Loss: 0.05737954490428302\n",
      "Epoch 80/100\n",
      "Training Loss: 0.06471407696028546\n",
      "Validation Loss: 0.035181687785681155\n",
      "Epoch 81/100\n",
      "Training Loss: 0.05680060251859739\n",
      "Validation Loss: 0.04478119186957279\n",
      "Epoch 82/100\n",
      "Training Loss: 0.051343179651727966\n",
      "Validation Loss: 0.03566388642108297\n",
      "Epoch 83/100\n",
      "Training Loss: 0.05938874722640427\n",
      "Validation Loss: 0.04600705735482633\n",
      "Epoch 84/100\n",
      "Training Loss: 0.0551807608038798\n",
      "Validation Loss: 0.026022872603445817\n",
      "Epoch 85/100\n",
      "Training Loss: 0.049467548751505025\n",
      "Validation Loss: 0.051386063278039795\n",
      "Epoch 86/100\n",
      "Training Loss: 0.05297329850092017\n",
      "Validation Loss: 0.04450052521942378\n",
      "Epoch 87/100\n",
      "Training Loss: 0.057529797269288745\n",
      "Validation Loss: 0.03315317061655866\n",
      "Epoch 88/100\n",
      "Training Loss: 0.058948529692420364\n",
      "Validation Loss: 0.0626122765775117\n",
      "Epoch 89/100\n",
      "Training Loss: 0.040861663871601506\n",
      "Validation Loss: 0.03759018106823047\n",
      "Epoch 90/100\n",
      "Training Loss: 0.06405026549718215\n",
      "Validation Loss: 0.03683274861615104\n",
      "Epoch 91/100\n",
      "Training Loss: 0.05302184833298894\n",
      "Validation Loss: 0.06101738986803938\n",
      "Epoch 92/100\n",
      "Training Loss: 0.059080702093955474\n",
      "Validation Loss: 0.04558606213404569\n",
      "Epoch 93/100\n",
      "Training Loss: 0.05192397663928087\n",
      "Validation Loss: 0.044735567086330044\n",
      "Epoch 94/100\n",
      "Training Loss: 0.048537976539750144\n",
      "Validation Loss: 0.02841317209000177\n",
      "Epoch 95/100\n",
      "Training Loss: 0.04753631083329843\n",
      "Validation Loss: 0.04900333078282532\n",
      "Epoch 96/100\n",
      "Training Loss: 0.05977260986432383\n",
      "Validation Loss: 0.04754527020527312\n",
      "Epoch 97/100\n",
      "Training Loss: 0.05249756221057626\n",
      "Validation Loss: 0.03987691220322885\n",
      "Epoch 98/100\n",
      "Training Loss: 0.050856251960498366\n",
      "Validation Loss: 0.04272464891986179\n",
      "Epoch 99/100\n",
      "Training Loss: 0.053681770533227\n",
      "Validation Loss: 0.020647803529568923\n",
      "Epoch 100/100\n",
      "Training Loss: 0.05863437997947501\n",
      "Validation Loss: 0.04059966262781621\n",
      "Combination 23: Avg Training Loss = 0.06263636921104053, Avg Validation Loss = 0.05094609522616325\n",
      "Testing combination 24/48: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.001, 'sequence_length': 50, 'dropout_rate': 0.2}\n",
      "Truncated Train Data Shape: (70, 50, 1)\n",
      "Truncated Validation Data Shape: (10, 50, 1)\n",
      "    Trial 1/2 for combination 24/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.2564129109249557\n",
      "Validation Loss: 0.05934166616487492\n",
      "Epoch 2/100\n",
      "Training Loss: 0.19195121086404282\n",
      "Validation Loss: 0.07866216033487221\n",
      "Epoch 3/100\n",
      "Training Loss: 0.14905898091269487\n",
      "Validation Loss: 0.08113416859419717\n",
      "Epoch 4/100\n",
      "Training Loss: 0.13292146417372788\n",
      "Validation Loss: 0.0934132314123684\n",
      "Epoch 5/100\n",
      "Training Loss: 0.14195454861607318\n",
      "Validation Loss: 0.08920208693931943\n",
      "Epoch 6/100\n",
      "Training Loss: 0.1116877390959646\n",
      "Validation Loss: 0.05214290968258887\n",
      "Epoch 7/100\n",
      "Training Loss: 0.10710653095514928\n",
      "Validation Loss: 0.07888963298507637\n",
      "Epoch 8/100\n",
      "Training Loss: 0.09012272936314643\n",
      "Validation Loss: 0.09219446541899903\n",
      "Epoch 9/100\n",
      "Training Loss: 0.09380155529934416\n",
      "Validation Loss: 0.08702459403047083\n",
      "Epoch 10/100\n",
      "Training Loss: 0.07757327573277789\n",
      "Validation Loss: 0.11067363014022454\n",
      "Epoch 11/100\n",
      "Training Loss: 0.07560262684296723\n",
      "Validation Loss: 0.06282656980671483\n",
      "Epoch 12/100\n",
      "Training Loss: 0.0773136225313757\n",
      "Validation Loss: 0.05394851127085328\n",
      "Epoch 13/100\n",
      "Training Loss: 0.061084343832811284\n",
      "Validation Loss: 0.060052125732116754\n",
      "Epoch 14/100\n",
      "Training Loss: 0.07376132317661459\n",
      "Validation Loss: 0.04645520036335984\n",
      "Epoch 15/100\n",
      "Training Loss: 0.06016252852195599\n",
      "Validation Loss: 0.0842483463584067\n",
      "Epoch 16/100\n",
      "Training Loss: 0.05922051565146347\n",
      "Validation Loss: 0.06907423515146952\n",
      "Epoch 17/100\n",
      "Training Loss: 0.05983264691444848\n",
      "Validation Loss: 0.03650519614799668\n",
      "Epoch 18/100\n",
      "Training Loss: 0.05824845208683411\n",
      "Validation Loss: 0.10660641271106701\n",
      "Epoch 19/100\n",
      "Training Loss: 0.0545266636126832\n",
      "Validation Loss: 0.05328008437817502\n",
      "Epoch 20/100\n",
      "Training Loss: 0.051111660348397266\n",
      "Validation Loss: 0.09760529091466037\n",
      "Epoch 21/100\n",
      "Training Loss: 0.06190034852717027\n",
      "Validation Loss: 0.05477100661038757\n",
      "Epoch 22/100\n",
      "Training Loss: 0.05789028151322243\n",
      "Validation Loss: 0.03217568625396526\n",
      "Epoch 23/100\n",
      "Training Loss: 0.05263371015223616\n",
      "Validation Loss: 0.12017735849005559\n",
      "Epoch 24/100\n",
      "Training Loss: 0.05557914321677051\n",
      "Validation Loss: 0.08946504143111514\n",
      "Epoch 25/100\n",
      "Training Loss: 0.05458983386529004\n",
      "Validation Loss: 0.12269842587668903\n",
      "Epoch 26/100\n",
      "Training Loss: 0.05651201920331973\n",
      "Validation Loss: 0.05137590962173052\n",
      "Epoch 27/100\n",
      "Training Loss: 0.04991306536690317\n",
      "Validation Loss: 0.045321449999716816\n",
      "Epoch 28/100\n",
      "Training Loss: 0.05371828332109037\n",
      "Validation Loss: 0.06812637859250462\n",
      "Epoch 29/100\n",
      "Training Loss: 0.05423161418443868\n",
      "Validation Loss: 0.10218811692285605\n",
      "Epoch 30/100\n",
      "Training Loss: 0.055546347454599125\n",
      "Validation Loss: 0.05007811715849507\n",
      "Epoch 31/100\n",
      "Training Loss: 0.05481526943150305\n",
      "Validation Loss: 0.05916192287686881\n",
      "Epoch 32/100\n",
      "Training Loss: 0.05354699156543582\n",
      "Validation Loss: 0.06736449564019527\n",
      "Epoch 33/100\n",
      "Training Loss: 0.06094091421859314\n",
      "Validation Loss: 0.053313632791844354\n",
      "Epoch 34/100\n",
      "Training Loss: 0.06781308464774481\n",
      "Validation Loss: 0.03620771687224902\n",
      "Epoch 35/100\n",
      "Training Loss: 0.05472762914775581\n",
      "Validation Loss: 0.05916735485621535\n",
      "Epoch 36/100\n",
      "Training Loss: 0.0603797289274404\n",
      "Validation Loss: 0.03352068682387908\n",
      "Epoch 37/100\n",
      "Training Loss: 0.054518090746466914\n",
      "Validation Loss: 0.04158183271116487\n",
      "Epoch 38/100\n",
      "Training Loss: 0.053846805707162285\n",
      "Validation Loss: 0.05136356877650925\n",
      "Epoch 39/100\n",
      "Training Loss: 0.049418159718101186\n",
      "Validation Loss: 0.036702064389028795\n",
      "Epoch 40/100\n",
      "Training Loss: 0.05878004058536406\n",
      "Validation Loss: 0.09306792364195962\n",
      "Epoch 41/100\n",
      "Training Loss: 0.051906155442110115\n",
      "Validation Loss: 0.04305883873390261\n",
      "Epoch 42/100\n",
      "Training Loss: 0.05194960605332443\n",
      "Validation Loss: 0.03027734883583037\n",
      "Epoch 43/100\n",
      "Training Loss: 0.05780572481570583\n",
      "Validation Loss: 0.08644752304551431\n",
      "Epoch 44/100\n",
      "Training Loss: 0.056136254535676146\n",
      "Validation Loss: 0.058017274807919975\n",
      "Epoch 45/100\n",
      "Training Loss: 0.07058442612839166\n",
      "Validation Loss: 0.05705480587142549\n",
      "Epoch 46/100\n",
      "Training Loss: 0.06222595240091555\n",
      "Validation Loss: 0.05511646226423879\n",
      "Epoch 47/100\n",
      "Training Loss: 0.05805544420409812\n",
      "Validation Loss: 0.037193585670344864\n",
      "Epoch 48/100\n",
      "Training Loss: 0.05522231489301971\n",
      "Validation Loss: 0.04435482188200442\n",
      "Epoch 49/100\n",
      "Training Loss: 0.06136656170635639\n",
      "Validation Loss: 0.059245282273418354\n",
      "Epoch 50/100\n",
      "Training Loss: 0.061633449476769665\n",
      "Validation Loss: 0.041145723803124826\n",
      "Epoch 51/100\n",
      "Training Loss: 0.050537462203982884\n",
      "Validation Loss: 0.07315871830537521\n",
      "Epoch 52/100\n",
      "Training Loss: 0.05046074408229094\n",
      "Validation Loss: 0.07485261532438427\n",
      "Epoch 53/100\n",
      "Training Loss: 0.05936077391227558\n",
      "Validation Loss: 0.030983735768492802\n",
      "Epoch 54/100\n",
      "Training Loss: 0.05906269848759954\n",
      "Validation Loss: 0.07177317784736743\n",
      "Epoch 55/100\n",
      "Training Loss: 0.06131225405476002\n",
      "Validation Loss: 0.07484657294445764\n",
      "Epoch 56/100\n",
      "Training Loss: 0.05856500567541652\n",
      "Validation Loss: 0.0789236189535843\n",
      "Epoch 57/100\n",
      "Training Loss: 0.059513812807302925\n",
      "Validation Loss: 0.0992494750141494\n",
      "Epoch 58/100\n",
      "Training Loss: 0.05669430722950458\n",
      "Validation Loss: 0.11056413184674559\n",
      "Epoch 59/100\n",
      "Training Loss: 0.05388436198760863\n",
      "Validation Loss: 0.06756488273629695\n",
      "Epoch 60/100\n",
      "Training Loss: 0.061118610974170005\n",
      "Validation Loss: 0.05332305228670029\n",
      "Epoch 61/100\n",
      "Training Loss: 0.05789815939059707\n",
      "Validation Loss: 0.05429598396935975\n",
      "Epoch 62/100\n",
      "Training Loss: 0.05301249218856977\n",
      "Validation Loss: 0.17622305929293594\n",
      "Epoch 63/100\n",
      "Training Loss: 0.05970817757579496\n",
      "Validation Loss: 0.082303776802607\n",
      "Epoch 64/100\n",
      "Training Loss: 0.058681325968084556\n",
      "Validation Loss: 0.10813328243132711\n",
      "Epoch 65/100\n",
      "Training Loss: 0.0543095176935856\n",
      "Validation Loss: 0.09922128536155192\n",
      "Epoch 66/100\n",
      "Training Loss: 0.05763398356465543\n",
      "Validation Loss: 0.05695867532101141\n",
      "Epoch 67/100\n",
      "Training Loss: 0.05246783990706925\n",
      "Validation Loss: 0.09844386612010636\n",
      "Epoch 68/100\n",
      "Training Loss: 0.05901089157965034\n",
      "Validation Loss: 0.10834579357339236\n",
      "Epoch 69/100\n",
      "Training Loss: 0.057778017407589845\n",
      "Validation Loss: 0.07779751886755278\n",
      "Epoch 70/100\n",
      "Training Loss: 0.05999756315598059\n",
      "Validation Loss: 0.04712542614557034\n",
      "Epoch 71/100\n",
      "Training Loss: 0.04755182453337741\n",
      "Validation Loss: 0.09221579825100035\n",
      "Epoch 72/100\n",
      "Training Loss: 0.05198997436350747\n",
      "Validation Loss: 0.08281441856872442\n",
      "Epoch 73/100\n",
      "Training Loss: 0.05261846590165333\n",
      "Validation Loss: 0.12360118749380886\n",
      "Epoch 74/100\n",
      "Training Loss: 0.05541244181186183\n",
      "Validation Loss: 0.11360466677190557\n",
      "Epoch 75/100\n",
      "Training Loss: 0.04826459030877745\n",
      "Validation Loss: 0.05972183272739164\n",
      "Epoch 76/100\n",
      "Training Loss: 0.055991579165106525\n",
      "Validation Loss: 0.08123507035081821\n",
      "Epoch 77/100\n",
      "Training Loss: 0.05335559473292025\n",
      "Validation Loss: 0.09390828911644863\n",
      "Epoch 78/100\n",
      "Training Loss: 0.05175185244111625\n",
      "Validation Loss: 0.09274597049846664\n",
      "Epoch 79/100\n",
      "Training Loss: 0.05765951844425985\n",
      "Validation Loss: 0.056707928426563206\n",
      "Epoch 80/100\n",
      "Training Loss: 0.05261737318586955\n",
      "Validation Loss: 0.1145991320888117\n",
      "Epoch 81/100\n",
      "Training Loss: 0.05415105788442659\n",
      "Validation Loss: 0.061966780117467625\n",
      "Epoch 82/100\n",
      "Training Loss: 0.05410362713834694\n",
      "Validation Loss: 0.18510380484855207\n",
      "Epoch 83/100\n",
      "Training Loss: 0.04668221693009728\n",
      "Validation Loss: 0.10605334729712204\n",
      "Epoch 84/100\n",
      "Training Loss: 0.051849933098720526\n",
      "Validation Loss: 0.1090655873583662\n",
      "Epoch 85/100\n",
      "Training Loss: 0.04492989933824873\n",
      "Validation Loss: 0.12662147034153434\n",
      "Epoch 86/100\n",
      "Training Loss: 0.04208442200357865\n",
      "Validation Loss: 0.10113779403810155\n",
      "Epoch 87/100\n",
      "Training Loss: 0.056352023231151564\n",
      "Validation Loss: 0.1710505744315408\n",
      "Epoch 88/100\n",
      "Training Loss: 0.04851753484990262\n",
      "Validation Loss: 0.11173210767386946\n",
      "Epoch 89/100\n",
      "Training Loss: 0.05604735489190581\n",
      "Validation Loss: 0.03753184238002264\n",
      "Epoch 90/100\n",
      "Training Loss: 0.05067480167677337\n",
      "Validation Loss: 0.07612203780459914\n",
      "Epoch 91/100\n",
      "Training Loss: 0.046198919277631645\n",
      "Validation Loss: 0.05897918052549165\n",
      "Epoch 92/100\n",
      "Training Loss: 0.05105188514264454\n",
      "Validation Loss: 0.07346578001635044\n",
      "Epoch 93/100\n",
      "Training Loss: 0.0510348730088708\n",
      "Validation Loss: 0.10914981023677509\n",
      "Epoch 94/100\n",
      "Training Loss: 0.047325940484512365\n",
      "Validation Loss: 0.08085641620325397\n",
      "Epoch 95/100\n",
      "Training Loss: 0.04705635697564833\n",
      "Validation Loss: 0.11690100105457905\n",
      "Epoch 96/100\n",
      "Training Loss: 0.046350966386331134\n",
      "Validation Loss: 0.10286824294969214\n",
      "Epoch 97/100\n",
      "Training Loss: 0.047491210397489346\n",
      "Validation Loss: 0.07980370328211164\n",
      "Epoch 98/100\n",
      "Training Loss: 0.04898895748550114\n",
      "Validation Loss: 0.06621103902337874\n",
      "Epoch 99/100\n",
      "Training Loss: 0.04813883861761219\n",
      "Validation Loss: 0.10692659286685897\n",
      "Epoch 100/100\n",
      "Training Loss: 0.05303724649134425\n",
      "Validation Loss: 0.04870045772968217\n",
      "    Trial 2/2 for combination 24/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.26712101289805734\n",
      "Validation Loss: 0.16894378994806009\n",
      "Epoch 2/100\n",
      "Training Loss: 0.1951958435716448\n",
      "Validation Loss: 0.1101907693999578\n",
      "Epoch 3/100\n",
      "Training Loss: 0.15153693401656634\n",
      "Validation Loss: 0.1146810523918758\n",
      "Epoch 4/100\n",
      "Training Loss: 0.14874328049913182\n",
      "Validation Loss: 0.0675482662391692\n",
      "Epoch 5/100\n",
      "Training Loss: 0.12601239334692135\n",
      "Validation Loss: 0.05129394238626833\n",
      "Epoch 6/100\n",
      "Training Loss: 0.11769117463779222\n",
      "Validation Loss: 0.07231296702567969\n",
      "Epoch 7/100\n",
      "Training Loss: 0.11762456624261447\n",
      "Validation Loss: 0.07317396785248112\n",
      "Epoch 8/100\n",
      "Training Loss: 0.0947015865972352\n",
      "Validation Loss: 0.09187968214799037\n",
      "Epoch 9/100\n",
      "Training Loss: 0.09023172513728989\n",
      "Validation Loss: 0.08163974099688245\n",
      "Epoch 10/100\n",
      "Training Loss: 0.08528249097232445\n",
      "Validation Loss: 0.14408824787534819\n",
      "Epoch 11/100\n",
      "Training Loss: 0.08484008190277018\n",
      "Validation Loss: 0.0480744359788205\n",
      "Epoch 12/100\n",
      "Training Loss: 0.07175523366664457\n",
      "Validation Loss: 0.08431866767385246\n",
      "Epoch 13/100\n",
      "Training Loss: 0.07486241065046381\n",
      "Validation Loss: 0.0615511632652755\n",
      "Epoch 14/100\n",
      "Training Loss: 0.06948036063309843\n",
      "Validation Loss: 0.10194797086256793\n",
      "Epoch 15/100\n",
      "Training Loss: 0.06315675039344046\n",
      "Validation Loss: 0.05832019376402381\n",
      "Epoch 16/100\n",
      "Training Loss: 0.06154791075183227\n",
      "Validation Loss: 0.07991374388957373\n",
      "Epoch 17/100\n",
      "Training Loss: 0.06101115995891599\n",
      "Validation Loss: 0.0809668469584709\n",
      "Epoch 18/100\n",
      "Training Loss: 0.06125450366054085\n",
      "Validation Loss: 0.050065053667258816\n",
      "Epoch 19/100\n",
      "Training Loss: 0.06443483424420209\n",
      "Validation Loss: 0.04178958578394815\n",
      "Epoch 20/100\n",
      "Training Loss: 0.05393235875729646\n",
      "Validation Loss: 0.07998062544960859\n",
      "Epoch 21/100\n",
      "Training Loss: 0.05980999992216827\n",
      "Validation Loss: 0.11709964963649018\n",
      "Epoch 22/100\n",
      "Training Loss: 0.04989170166259149\n",
      "Validation Loss: 0.06722676237329281\n",
      "Epoch 23/100\n",
      "Training Loss: 0.0618755093276129\n",
      "Validation Loss: 0.06673953609418962\n",
      "Epoch 24/100\n",
      "Training Loss: 0.05853684687553987\n",
      "Validation Loss: 0.03682369005408705\n",
      "Epoch 25/100\n",
      "Training Loss: 0.05289453611801538\n",
      "Validation Loss: 0.08440068645877988\n",
      "Epoch 26/100\n",
      "Training Loss: 0.055134252202320086\n",
      "Validation Loss: 0.08842422802592595\n",
      "Epoch 27/100\n",
      "Training Loss: 0.05747602779708569\n",
      "Validation Loss: 0.0566870583960656\n",
      "Epoch 28/100\n",
      "Training Loss: 0.053910394102621724\n",
      "Validation Loss: 0.036705973320659904\n",
      "Epoch 29/100\n",
      "Training Loss: 0.053065619994633946\n",
      "Validation Loss: 0.030924241339509724\n",
      "Epoch 30/100\n",
      "Training Loss: 0.05383794213091611\n",
      "Validation Loss: 0.0638513786129921\n",
      "Epoch 31/100\n",
      "Training Loss: 0.05272122518531892\n",
      "Validation Loss: 0.06508564777095369\n",
      "Epoch 32/100\n",
      "Training Loss: 0.05335482700164694\n",
      "Validation Loss: 0.037156534131164196\n",
      "Epoch 33/100\n",
      "Training Loss: 0.052809826080814054\n",
      "Validation Loss: 0.04058553874509285\n",
      "Epoch 34/100\n",
      "Training Loss: 0.06295302400625731\n",
      "Validation Loss: 0.055720039414519175\n",
      "Epoch 35/100\n",
      "Training Loss: 0.05975615850300441\n",
      "Validation Loss: 0.03742534641517674\n",
      "Epoch 36/100\n",
      "Training Loss: 0.052431571854601317\n",
      "Validation Loss: 0.15120083363694953\n",
      "Epoch 37/100\n",
      "Training Loss: 0.050694444563738186\n",
      "Validation Loss: 0.09823560411669066\n",
      "Epoch 38/100\n",
      "Training Loss: 0.06101015789889327\n",
      "Validation Loss: 0.08186559912472022\n",
      "Epoch 39/100\n",
      "Training Loss: 0.05321521568055336\n",
      "Validation Loss: 0.047498756069085574\n",
      "Epoch 40/100\n",
      "Training Loss: 0.050163265723914295\n",
      "Validation Loss: 0.08932471797720497\n",
      "Epoch 41/100\n",
      "Training Loss: 0.05942101387605388\n",
      "Validation Loss: 0.0317278255802094\n",
      "Epoch 42/100\n",
      "Training Loss: 0.05443578268804556\n",
      "Validation Loss: 0.047162615471374854\n",
      "Epoch 43/100\n",
      "Training Loss: 0.052479662536850755\n",
      "Validation Loss: 0.050936094709789104\n",
      "Epoch 44/100\n",
      "Training Loss: 0.053670392731676377\n",
      "Validation Loss: 0.06472394062548534\n",
      "Epoch 45/100\n",
      "Training Loss: 0.05738507404762255\n",
      "Validation Loss: 0.021232930593543177\n",
      "Epoch 46/100\n",
      "Training Loss: 0.054979181601489326\n",
      "Validation Loss: 0.04166407617781818\n",
      "Epoch 47/100\n",
      "Training Loss: 0.05738939751981305\n",
      "Validation Loss: 0.040236383935678396\n",
      "Epoch 48/100\n",
      "Training Loss: 0.060543832568570974\n",
      "Validation Loss: 0.07271815215929842\n",
      "Epoch 49/100\n",
      "Training Loss: 0.05545742817936668\n",
      "Validation Loss: 0.05569649705243788\n",
      "Epoch 50/100\n",
      "Training Loss: 0.050250746013351\n",
      "Validation Loss: 0.0933944655566184\n",
      "Epoch 51/100\n",
      "Training Loss: 0.05677084480155777\n",
      "Validation Loss: 0.020556462709719555\n",
      "Epoch 52/100\n",
      "Training Loss: 0.05897499741030974\n",
      "Validation Loss: 0.02610945940974598\n",
      "Epoch 53/100\n",
      "Training Loss: 0.05300133059232289\n",
      "Validation Loss: 0.07180798227605342\n",
      "Epoch 54/100\n",
      "Training Loss: 0.055171288640454307\n",
      "Validation Loss: 0.06227688563493881\n",
      "Epoch 55/100\n",
      "Training Loss: 0.053576369224674875\n",
      "Validation Loss: 0.050366088295237324\n",
      "Epoch 56/100\n",
      "Training Loss: 0.053749064853701255\n",
      "Validation Loss: 0.050550817551269424\n",
      "Epoch 57/100\n",
      "Training Loss: 0.055144589606783444\n",
      "Validation Loss: 0.12513493531250336\n",
      "Epoch 58/100\n",
      "Training Loss: 0.05882783567569762\n",
      "Validation Loss: 0.012847747156087075\n",
      "Epoch 59/100\n",
      "Training Loss: 0.048025890049604864\n",
      "Validation Loss: 0.036450277627905606\n",
      "Epoch 60/100\n",
      "Training Loss: 0.05315637127449545\n",
      "Validation Loss: 0.06406017522288938\n",
      "Epoch 61/100\n",
      "Training Loss: 0.05891780437265722\n",
      "Validation Loss: 0.054243294147606515\n",
      "Epoch 62/100\n",
      "Training Loss: 0.056104061292960895\n",
      "Validation Loss: 0.11215681271814193\n",
      "Epoch 63/100\n",
      "Training Loss: 0.04669152814882707\n",
      "Validation Loss: 0.05632815551032035\n",
      "Epoch 64/100\n",
      "Training Loss: 0.06011245214122224\n",
      "Validation Loss: 0.04782621988986784\n",
      "Epoch 65/100\n",
      "Training Loss: 0.04639089769194133\n",
      "Validation Loss: 0.029390421488136427\n",
      "Epoch 66/100\n",
      "Training Loss: 0.05741829987900084\n",
      "Validation Loss: 0.07149728682340674\n",
      "Epoch 67/100\n",
      "Training Loss: 0.06315774001939652\n",
      "Validation Loss: 0.04373295867901926\n",
      "Epoch 68/100\n",
      "Training Loss: 0.05151910464929296\n",
      "Validation Loss: 0.04714130060122205\n",
      "Epoch 69/100\n",
      "Training Loss: 0.054688851467417865\n",
      "Validation Loss: 0.061376273995039254\n",
      "Epoch 70/100\n",
      "Training Loss: 0.054146324102873955\n",
      "Validation Loss: 0.07076569709231675\n",
      "Epoch 71/100\n",
      "Training Loss: 0.05735253729615578\n",
      "Validation Loss: 0.07561209450371047\n",
      "Epoch 72/100\n",
      "Training Loss: 0.04576494803650246\n",
      "Validation Loss: 0.038174786681410014\n",
      "Epoch 73/100\n",
      "Training Loss: 0.05773384780592076\n",
      "Validation Loss: 0.0508707561668589\n",
      "Epoch 74/100\n",
      "Training Loss: 0.04875963559618505\n",
      "Validation Loss: 0.026055401549577063\n",
      "Epoch 75/100\n",
      "Training Loss: 0.048976579724300286\n",
      "Validation Loss: 0.06100482690231833\n",
      "Epoch 76/100\n",
      "Training Loss: 0.059932724821667116\n",
      "Validation Loss: 0.06644296059722839\n",
      "Epoch 77/100\n",
      "Training Loss: 0.0592828252182244\n",
      "Validation Loss: 0.06419505031113905\n",
      "Epoch 78/100\n",
      "Training Loss: 0.05433350282963565\n",
      "Validation Loss: 0.05423560850340262\n",
      "Epoch 79/100\n",
      "Training Loss: 0.047247700962942805\n",
      "Validation Loss: 0.13311996720964586\n",
      "Epoch 80/100\n",
      "Training Loss: 0.05479360868400442\n",
      "Validation Loss: 0.06502094156392493\n",
      "Epoch 81/100\n",
      "Training Loss: 0.0517560989281209\n",
      "Validation Loss: 0.0731076559018067\n",
      "Epoch 82/100\n",
      "Training Loss: 0.0446632727328441\n",
      "Validation Loss: 0.08557659319660525\n",
      "Epoch 83/100\n",
      "Training Loss: 0.04960235823898807\n",
      "Validation Loss: 0.09561754954927788\n",
      "Epoch 84/100\n",
      "Training Loss: 0.04848128599607365\n",
      "Validation Loss: 0.045557994582231386\n",
      "Epoch 85/100\n",
      "Training Loss: 0.05518546653243472\n",
      "Validation Loss: 0.07384001149750845\n",
      "Epoch 86/100\n",
      "Training Loss: 0.05509552452626411\n",
      "Validation Loss: 0.11909391220266445\n",
      "Epoch 87/100\n",
      "Training Loss: 0.05522275460128947\n",
      "Validation Loss: 0.08592303331105856\n",
      "Epoch 88/100\n",
      "Training Loss: 0.059772689087903313\n",
      "Validation Loss: 0.058404338938494005\n",
      "Epoch 89/100\n",
      "Training Loss: 0.05739059709579296\n",
      "Validation Loss: 0.07070043132940877\n",
      "Epoch 90/100\n",
      "Training Loss: 0.05841371066389577\n",
      "Validation Loss: 0.10734139470531497\n",
      "Epoch 91/100\n",
      "Training Loss: 0.050748163969381685\n",
      "Validation Loss: 0.04521007709351134\n",
      "Epoch 92/100\n",
      "Training Loss: 0.04338938908638\n",
      "Validation Loss: 0.06363322481680403\n",
      "Epoch 93/100\n",
      "Training Loss: 0.06554747077176966\n",
      "Validation Loss: 0.04285254267425587\n",
      "Epoch 94/100\n",
      "Training Loss: 0.05125506636153166\n",
      "Validation Loss: 0.07630528978150722\n",
      "Epoch 95/100\n",
      "Training Loss: 0.056630980607389435\n",
      "Validation Loss: 0.06743652619944959\n",
      "Epoch 96/100\n",
      "Training Loss: 0.04459275884804303\n",
      "Validation Loss: 0.08730896698429422\n",
      "Epoch 97/100\n",
      "Training Loss: 0.06272390473370971\n",
      "Validation Loss: 0.09384372620694717\n",
      "Epoch 98/100\n",
      "Training Loss: 0.061704395679812445\n",
      "Validation Loss: 0.032321434325048796\n",
      "Epoch 99/100\n",
      "Training Loss: 0.060004140768043245\n",
      "Validation Loss: 0.050084220863852336\n",
      "Epoch 100/100\n",
      "Training Loss: 0.052412138144265735\n",
      "Validation Loss: 0.11257408698949271\n",
      "Combination 24: Avg Training Loss = 0.06394109627919309, Avg Validation Loss = 0.07264875781931159\n",
      "Testing combination 25/48: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'sequence_length': 6, 'dropout_rate': 0.1}\n",
      "Truncated Train Data Shape: (70, 6, 1)\n",
      "Truncated Validation Data Shape: (10, 6, 1)\n",
      "    Trial 1/2 for combination 25/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.3978331826316831\n",
      "Validation Loss: 0.5323336381115249\n",
      "Epoch 2/100\n",
      "Training Loss: 0.4633629142294135\n",
      "Validation Loss: 0.3847922045683574\n",
      "Epoch 3/100\n",
      "Training Loss: 0.4485237773895398\n",
      "Validation Loss: 0.19169910435632292\n",
      "Epoch 4/100\n",
      "Training Loss: 0.359563408921486\n",
      "Validation Loss: 0.29491560025731944\n",
      "Epoch 5/100\n",
      "Training Loss: 0.33262988439463936\n",
      "Validation Loss: 0.32693252490268565\n",
      "Epoch 6/100\n",
      "Training Loss: 0.38910675020711794\n",
      "Validation Loss: 0.2940076445270622\n",
      "Epoch 7/100\n",
      "Training Loss: 0.31172098699181744\n",
      "Validation Loss: 0.2696393131100211\n",
      "Epoch 8/100\n",
      "Training Loss: 0.2723821416825717\n",
      "Validation Loss: 0.1585703870871833\n",
      "Epoch 9/100\n",
      "Training Loss: 0.2999602225913348\n",
      "Validation Loss: 0.1738516995041885\n",
      "Epoch 10/100\n",
      "Training Loss: 0.31340475329154993\n",
      "Validation Loss: 0.25591517651306445\n",
      "Epoch 11/100\n",
      "Training Loss: 0.271726118548373\n",
      "Validation Loss: 0.14572148227315335\n",
      "Epoch 12/100\n",
      "Training Loss: 0.19025033308783001\n",
      "Validation Loss: 0.26149933882753296\n",
      "Epoch 13/100\n",
      "Training Loss: 0.26797493470618483\n",
      "Validation Loss: 0.10282664729987871\n",
      "Epoch 14/100\n",
      "Training Loss: 0.23353720827229865\n",
      "Validation Loss: 0.1400970292670307\n",
      "Epoch 15/100\n",
      "Training Loss: 0.31467085749545176\n",
      "Validation Loss: 0.12543417325827927\n",
      "Epoch 16/100\n",
      "Training Loss: 0.25235223840985416\n",
      "Validation Loss: 0.18230477584931418\n",
      "Epoch 17/100\n",
      "Training Loss: 0.2761641400833146\n",
      "Validation Loss: 0.21677696981956415\n",
      "Epoch 18/100\n",
      "Training Loss: 0.2872943574026617\n",
      "Validation Loss: 0.16268583546108126\n",
      "Epoch 19/100\n",
      "Training Loss: 0.32088754355645027\n",
      "Validation Loss: 0.14818688779656985\n",
      "Epoch 20/100\n",
      "Training Loss: 0.22494228319619636\n",
      "Validation Loss: 0.16972494823401157\n",
      "Epoch 21/100\n",
      "Training Loss: 0.22039020458190534\n",
      "Validation Loss: 0.09606924659485307\n",
      "Epoch 22/100\n",
      "Training Loss: 0.22462689767471408\n",
      "Validation Loss: 0.1516908524795112\n",
      "Epoch 23/100\n",
      "Training Loss: 0.2563413146381353\n",
      "Validation Loss: 0.17121795831215672\n",
      "Epoch 24/100\n",
      "Training Loss: 0.21678806232219844\n",
      "Validation Loss: 0.20239402969468762\n",
      "Epoch 25/100\n",
      "Training Loss: 0.26460301781599266\n",
      "Validation Loss: 0.16379966974707044\n",
      "Epoch 26/100\n",
      "Training Loss: 0.2090950837099814\n",
      "Validation Loss: 0.1507276679729063\n",
      "Epoch 27/100\n",
      "Training Loss: 0.194527605757464\n",
      "Validation Loss: 0.15848100551171496\n",
      "Epoch 28/100\n",
      "Training Loss: 0.27763464313461417\n",
      "Validation Loss: 0.13410093549385832\n",
      "Epoch 29/100\n",
      "Training Loss: 0.25209024919123196\n",
      "Validation Loss: 0.08179675532229783\n",
      "Epoch 30/100\n",
      "Training Loss: 0.2490501066790349\n",
      "Validation Loss: 0.2132787048361234\n",
      "Epoch 31/100\n",
      "Training Loss: 0.22554794697008404\n",
      "Validation Loss: 0.18556600337670326\n",
      "Epoch 32/100\n",
      "Training Loss: 0.22319402633631386\n",
      "Validation Loss: 0.09576749323679724\n",
      "Epoch 33/100\n",
      "Training Loss: 0.25784481540184295\n",
      "Validation Loss: 0.09173491653971705\n",
      "Epoch 34/100\n",
      "Training Loss: 0.20187236300267636\n",
      "Validation Loss: 0.14005810353843445\n",
      "Epoch 35/100\n",
      "Training Loss: 0.19802999285210945\n",
      "Validation Loss: 0.16949500792251943\n",
      "Epoch 36/100\n",
      "Training Loss: 0.19679062811977227\n",
      "Validation Loss: 0.22185596638784863\n",
      "Epoch 37/100\n",
      "Training Loss: 0.20142304913253303\n",
      "Validation Loss: 0.17423460069568747\n",
      "Epoch 38/100\n",
      "Training Loss: 0.1944178012923557\n",
      "Validation Loss: 0.18431210066655193\n",
      "Epoch 39/100\n",
      "Training Loss: 0.24904214565776925\n",
      "Validation Loss: 0.13549789238170462\n",
      "Epoch 40/100\n",
      "Training Loss: 0.2139608130195781\n",
      "Validation Loss: 0.11986928876605814\n",
      "Epoch 41/100\n",
      "Training Loss: 0.1783552386134505\n",
      "Validation Loss: 0.2251512975854963\n",
      "Epoch 42/100\n",
      "Training Loss: 0.15127265908766238\n",
      "Validation Loss: 0.10317914980691027\n",
      "Epoch 43/100\n",
      "Training Loss: 0.18014997374716168\n",
      "Validation Loss: 0.12799104414046156\n",
      "Epoch 44/100\n",
      "Training Loss: 0.17803251414436375\n",
      "Validation Loss: 0.1361105151217501\n",
      "Epoch 45/100\n",
      "Training Loss: 0.24834636872612467\n",
      "Validation Loss: 0.19045293877180686\n",
      "Epoch 46/100\n",
      "Training Loss: 0.16537394691137614\n",
      "Validation Loss: 0.21916383765335637\n",
      "Epoch 47/100\n",
      "Training Loss: 0.19775040791123125\n",
      "Validation Loss: 0.07897049335868192\n",
      "Epoch 48/100\n",
      "Training Loss: 0.2024894923119326\n",
      "Validation Loss: 0.1722957266842814\n",
      "Epoch 49/100\n",
      "Training Loss: 0.1756808210800789\n",
      "Validation Loss: 0.19542018495753333\n",
      "Epoch 50/100\n",
      "Training Loss: 0.1898147909424801\n",
      "Validation Loss: 0.11549124240633983\n",
      "Epoch 51/100\n",
      "Training Loss: 0.18344505515554074\n",
      "Validation Loss: 0.10086692191638731\n",
      "Epoch 52/100\n",
      "Training Loss: 0.2097345966011278\n",
      "Validation Loss: 0.11876383166975824\n",
      "Epoch 53/100\n",
      "Training Loss: 0.20915389032471254\n",
      "Validation Loss: 0.14100033106188473\n",
      "Epoch 54/100\n",
      "Training Loss: 0.15538480080361436\n",
      "Validation Loss: 0.13119381788688125\n",
      "Epoch 55/100\n",
      "Training Loss: 0.1519404976310423\n",
      "Validation Loss: 0.118577107664435\n",
      "Epoch 56/100\n",
      "Training Loss: 0.16461398801313734\n",
      "Validation Loss: 0.11628811417198195\n",
      "Epoch 57/100\n",
      "Training Loss: 0.17226502596959758\n",
      "Validation Loss: 0.17682498871783206\n",
      "Epoch 58/100\n",
      "Training Loss: 0.1393784812545608\n",
      "Validation Loss: 0.1456793788512311\n",
      "Epoch 59/100\n",
      "Training Loss: 0.1945584987564127\n",
      "Validation Loss: 0.07669411890749246\n",
      "Epoch 60/100\n",
      "Training Loss: 0.16517545975984066\n",
      "Validation Loss: 0.1127482284021846\n",
      "Epoch 61/100\n",
      "Training Loss: 0.1629158635225374\n",
      "Validation Loss: 0.06922313189648382\n",
      "Epoch 62/100\n",
      "Training Loss: 0.18954243745829624\n",
      "Validation Loss: 0.09345258369327007\n",
      "Epoch 63/100\n",
      "Training Loss: 0.1641841150895456\n",
      "Validation Loss: 0.1582153224883786\n",
      "Epoch 64/100\n",
      "Training Loss: 0.13730989804415195\n",
      "Validation Loss: 0.1380864866120807\n",
      "Epoch 65/100\n",
      "Training Loss: 0.19876436454279298\n",
      "Validation Loss: 0.09601078473501998\n",
      "Epoch 66/100\n",
      "Training Loss: 0.17772904050576577\n",
      "Validation Loss: 0.09449334073481683\n",
      "Epoch 67/100\n",
      "Training Loss: 0.191663123599013\n",
      "Validation Loss: 0.07339538892666289\n",
      "Epoch 68/100\n",
      "Training Loss: 0.16647322379827856\n",
      "Validation Loss: 0.1788195051943568\n",
      "Epoch 69/100\n",
      "Training Loss: 0.15989629695118315\n",
      "Validation Loss: 0.07992345170185075\n",
      "Epoch 70/100\n",
      "Training Loss: 0.16879199813627213\n",
      "Validation Loss: 0.12468799266273345\n",
      "Epoch 71/100\n",
      "Training Loss: 0.1601476567182999\n",
      "Validation Loss: 0.14543173272196822\n",
      "Epoch 72/100\n",
      "Training Loss: 0.1783585610765738\n",
      "Validation Loss: 0.11273436679706446\n",
      "Epoch 73/100\n",
      "Training Loss: 0.1753486843950396\n",
      "Validation Loss: 0.18736687815216696\n",
      "Epoch 74/100\n",
      "Training Loss: 0.14630063591665307\n",
      "Validation Loss: 0.10609752847764171\n",
      "Epoch 75/100\n",
      "Training Loss: 0.1369099671062407\n",
      "Validation Loss: 0.08700628340774869\n",
      "Epoch 76/100\n",
      "Training Loss: 0.16192638196673656\n",
      "Validation Loss: 0.1200544073383335\n",
      "Epoch 77/100\n",
      "Training Loss: 0.13208881437213324\n",
      "Validation Loss: 0.10030402877833729\n",
      "Epoch 78/100\n",
      "Training Loss: 0.17436589536433114\n",
      "Validation Loss: 0.06650001603594904\n",
      "Epoch 79/100\n",
      "Training Loss: 0.17039193042607642\n",
      "Validation Loss: 0.06767973143822524\n",
      "Epoch 80/100\n",
      "Training Loss: 0.16945351867185787\n",
      "Validation Loss: 0.10568897232908173\n",
      "Epoch 81/100\n",
      "Training Loss: 0.14603925741871823\n",
      "Validation Loss: 0.0882925395524958\n",
      "Epoch 82/100\n",
      "Training Loss: 0.17765499044176666\n",
      "Validation Loss: 0.13034376847257892\n",
      "Epoch 83/100\n",
      "Training Loss: 0.14672029910410167\n",
      "Validation Loss: 0.0621456902123936\n",
      "Epoch 84/100\n",
      "Training Loss: 0.14854133326278726\n",
      "Validation Loss: 0.08665901774790506\n",
      "Epoch 85/100\n",
      "Training Loss: 0.119256568406616\n",
      "Validation Loss: 0.10355348651981333\n",
      "Epoch 86/100\n",
      "Training Loss: 0.1746435460593668\n",
      "Validation Loss: 0.12201872388183496\n",
      "Epoch 87/100\n",
      "Training Loss: 0.11654036701477001\n",
      "Validation Loss: 0.12237242711500743\n",
      "Epoch 88/100\n",
      "Training Loss: 0.12593290104111007\n",
      "Validation Loss: 0.10108250511803293\n",
      "Epoch 89/100\n",
      "Training Loss: 0.13844207678524187\n",
      "Validation Loss: 0.1856449948798032\n",
      "Epoch 90/100\n",
      "Training Loss: 0.19180249053882295\n",
      "Validation Loss: 0.07297247325031844\n",
      "Epoch 91/100\n",
      "Training Loss: 0.11804939970933834\n",
      "Validation Loss: 0.06782614454485765\n",
      "Epoch 92/100\n",
      "Training Loss: 0.14447649959766073\n",
      "Validation Loss: 0.08175679407654747\n",
      "Epoch 93/100\n",
      "Training Loss: 0.1584287200820881\n",
      "Validation Loss: 0.06651607549267582\n",
      "Epoch 94/100\n",
      "Training Loss: 0.18137607651804943\n",
      "Validation Loss: 0.1150634027633187\n",
      "Epoch 95/100\n",
      "Training Loss: 0.12573458252852796\n",
      "Validation Loss: 0.12969597883757705\n",
      "Epoch 96/100\n",
      "Training Loss: 0.17989715785857396\n",
      "Validation Loss: 0.10530750120132844\n",
      "Epoch 97/100\n",
      "Training Loss: 0.16157001467611584\n",
      "Validation Loss: 0.06409352985183969\n",
      "Epoch 98/100\n",
      "Training Loss: 0.12752739077965372\n",
      "Validation Loss: 0.0811383171053728\n",
      "Epoch 99/100\n",
      "Training Loss: 0.13238327207154343\n",
      "Validation Loss: 0.17859514898362114\n",
      "Epoch 100/100\n",
      "Training Loss: 0.14934875283681498\n",
      "Validation Loss: 0.0915289799518909\n",
      "    Trial 2/2 for combination 25/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.524762856432492\n",
      "Validation Loss: 0.5036447401019947\n",
      "Epoch 2/100\n",
      "Training Loss: 0.49585990745027186\n",
      "Validation Loss: 0.2298279711360683\n",
      "Epoch 3/100\n",
      "Training Loss: 0.42852969104656735\n",
      "Validation Loss: 0.4250078781798817\n",
      "Epoch 4/100\n",
      "Training Loss: 0.4621767858514741\n",
      "Validation Loss: 0.42571118079812403\n",
      "Epoch 5/100\n",
      "Training Loss: 0.3629806933152275\n",
      "Validation Loss: 0.31790683293098526\n",
      "Epoch 6/100\n",
      "Training Loss: 0.37109370574713146\n",
      "Validation Loss: 0.2225083167219029\n",
      "Epoch 7/100\n",
      "Training Loss: 0.32620855656043424\n",
      "Validation Loss: 0.19927673956012715\n",
      "Epoch 8/100\n",
      "Training Loss: 0.2546534663820255\n",
      "Validation Loss: 0.26484051711758816\n",
      "Epoch 9/100\n",
      "Training Loss: 0.3033444642595539\n",
      "Validation Loss: 0.16387341461850252\n",
      "Epoch 10/100\n",
      "Training Loss: 0.329654947310589\n",
      "Validation Loss: 0.2599134673286987\n",
      "Epoch 11/100\n",
      "Training Loss: 0.34783334220731943\n",
      "Validation Loss: 0.41209292349043525\n",
      "Epoch 12/100\n",
      "Training Loss: 0.340661883766802\n",
      "Validation Loss: 0.3001355984724524\n",
      "Epoch 13/100\n",
      "Training Loss: 0.26852715866814414\n",
      "Validation Loss: 0.22379311066322\n",
      "Epoch 14/100\n",
      "Training Loss: 0.32562299228247804\n",
      "Validation Loss: 0.19517390190068934\n",
      "Epoch 15/100\n",
      "Training Loss: 0.2570906490268263\n",
      "Validation Loss: 0.35170265057695305\n",
      "Epoch 16/100\n",
      "Training Loss: 0.2820061798169365\n",
      "Validation Loss: 0.08185836688515949\n",
      "Epoch 17/100\n",
      "Training Loss: 0.2913573430450197\n",
      "Validation Loss: 0.18220542978401927\n",
      "Epoch 18/100\n",
      "Training Loss: 0.2359756816161344\n",
      "Validation Loss: 0.1308144308540225\n",
      "Epoch 19/100\n",
      "Training Loss: 0.2995346476483551\n",
      "Validation Loss: 0.16843273031920541\n",
      "Epoch 20/100\n",
      "Training Loss: 0.2634825055940488\n",
      "Validation Loss: 0.1099667122594153\n",
      "Epoch 21/100\n",
      "Training Loss: 0.231989368601787\n",
      "Validation Loss: 0.14208462908789032\n",
      "Epoch 22/100\n",
      "Training Loss: 0.31419316695959143\n",
      "Validation Loss: 0.301020882492837\n",
      "Epoch 23/100\n",
      "Training Loss: 0.22225344390884932\n",
      "Validation Loss: 0.1176941175998945\n",
      "Epoch 24/100\n",
      "Training Loss: 0.3073941485290947\n",
      "Validation Loss: 0.15770784001637486\n",
      "Epoch 25/100\n",
      "Training Loss: 0.34289358501338346\n",
      "Validation Loss: 0.08957712931708478\n",
      "Epoch 26/100\n",
      "Training Loss: 0.24737487239183342\n",
      "Validation Loss: 0.3482405369794025\n",
      "Epoch 27/100\n",
      "Training Loss: 0.25271867924369384\n",
      "Validation Loss: 0.1893514140547054\n",
      "Epoch 28/100\n",
      "Training Loss: 0.21722830163088352\n",
      "Validation Loss: 0.20261303029556502\n",
      "Epoch 29/100\n",
      "Training Loss: 0.22260314874843892\n",
      "Validation Loss: 0.11126281037324852\n",
      "Epoch 30/100\n",
      "Training Loss: 0.2512030619356971\n",
      "Validation Loss: 0.19486326846101895\n",
      "Epoch 31/100\n",
      "Training Loss: 0.2252616946566137\n",
      "Validation Loss: 0.23275119993694116\n",
      "Epoch 32/100\n",
      "Training Loss: 0.20738166790763232\n",
      "Validation Loss: 0.16929670166887328\n",
      "Epoch 33/100\n",
      "Training Loss: 0.2363513710146391\n",
      "Validation Loss: 0.11940684644198478\n",
      "Epoch 34/100\n",
      "Training Loss: 0.2363929661611977\n",
      "Validation Loss: 0.12495106490859968\n",
      "Epoch 35/100\n",
      "Training Loss: 0.23857281188687912\n",
      "Validation Loss: 0.13036782172056005\n",
      "Epoch 36/100\n",
      "Training Loss: 0.1937394713241271\n",
      "Validation Loss: 0.16932125142438414\n",
      "Epoch 37/100\n",
      "Training Loss: 0.23241976082186444\n",
      "Validation Loss: 0.212051259006742\n",
      "Epoch 38/100\n",
      "Training Loss: 0.22383107805611943\n",
      "Validation Loss: 0.18967066336839\n",
      "Epoch 39/100\n",
      "Training Loss: 0.2154238346617179\n",
      "Validation Loss: 0.13212566848026036\n",
      "Epoch 40/100\n",
      "Training Loss: 0.2764367800897385\n",
      "Validation Loss: 0.1650745079984712\n",
      "Epoch 41/100\n",
      "Training Loss: 0.20246221271920042\n",
      "Validation Loss: 0.14156053218246506\n",
      "Epoch 42/100\n",
      "Training Loss: 0.19556864683495884\n",
      "Validation Loss: 0.11683045768422426\n",
      "Epoch 43/100\n",
      "Training Loss: 0.20392687288626848\n",
      "Validation Loss: 0.24678455629977752\n",
      "Epoch 44/100\n",
      "Training Loss: 0.2582583684807617\n",
      "Validation Loss: 0.09699831354329405\n",
      "Epoch 45/100\n",
      "Training Loss: 0.17817097454257538\n",
      "Validation Loss: 0.31856359730985184\n",
      "Epoch 46/100\n",
      "Training Loss: 0.20155719582296147\n",
      "Validation Loss: 0.14340532376261333\n",
      "Epoch 47/100\n",
      "Training Loss: 0.2284206991472044\n",
      "Validation Loss: 0.10775677416647111\n",
      "Epoch 48/100\n",
      "Training Loss: 0.21543157411541636\n",
      "Validation Loss: 0.15080918718133537\n",
      "Epoch 49/100\n",
      "Training Loss: 0.16909057966028965\n",
      "Validation Loss: 0.1161984322348401\n",
      "Epoch 50/100\n",
      "Training Loss: 0.17243795131810333\n",
      "Validation Loss: 0.14503360769038237\n",
      "Epoch 51/100\n",
      "Training Loss: 0.19142851272178255\n",
      "Validation Loss: 0.1304378702545798\n",
      "Epoch 52/100\n",
      "Training Loss: 0.23161205357223044\n",
      "Validation Loss: 0.14493111049219123\n",
      "Epoch 53/100\n",
      "Training Loss: 0.16889179204614937\n",
      "Validation Loss: 0.10510660606960057\n",
      "Epoch 54/100\n",
      "Training Loss: 0.18873170649729054\n",
      "Validation Loss: 0.14313649940406367\n",
      "Epoch 55/100\n",
      "Training Loss: 0.20323283410908677\n",
      "Validation Loss: 0.14350451034429887\n",
      "Epoch 56/100\n",
      "Training Loss: 0.18548905613901895\n",
      "Validation Loss: 0.180548541326672\n",
      "Epoch 57/100\n",
      "Training Loss: 0.17090654562415677\n",
      "Validation Loss: 0.07627817278095843\n",
      "Epoch 58/100\n",
      "Training Loss: 0.1810847177211624\n",
      "Validation Loss: 0.08336834173637851\n",
      "Epoch 59/100\n",
      "Training Loss: 0.14244552541854294\n",
      "Validation Loss: 0.15035327818982855\n",
      "Epoch 60/100\n",
      "Training Loss: 0.21606935593727247\n",
      "Validation Loss: 0.17265449039411454\n",
      "Epoch 61/100\n",
      "Training Loss: 0.19983228968350936\n",
      "Validation Loss: 0.19646262106616855\n",
      "Epoch 62/100\n",
      "Training Loss: 0.11992540929849996\n",
      "Validation Loss: 0.10778988816597238\n",
      "Epoch 63/100\n",
      "Training Loss: 0.25563568427334343\n",
      "Validation Loss: 0.09335638240897808\n",
      "Epoch 64/100\n",
      "Training Loss: 0.205367624284949\n",
      "Validation Loss: 0.11990436413010479\n",
      "Epoch 65/100\n",
      "Training Loss: 0.18573541114828943\n",
      "Validation Loss: 0.1387769152589827\n",
      "Epoch 66/100\n",
      "Training Loss: 0.18856882938094013\n",
      "Validation Loss: 0.16292987121275565\n",
      "Epoch 67/100\n",
      "Training Loss: 0.1766457294321362\n",
      "Validation Loss: 0.06735467821663169\n",
      "Epoch 68/100\n",
      "Training Loss: 0.18935926151527882\n",
      "Validation Loss: 0.06541666417769566\n",
      "Epoch 69/100\n",
      "Training Loss: 0.19975251733420127\n",
      "Validation Loss: 0.08552067311422844\n",
      "Epoch 70/100\n",
      "Training Loss: 0.2019805789938685\n",
      "Validation Loss: 0.18945022348760954\n",
      "Epoch 71/100\n",
      "Training Loss: 0.18128282137307783\n",
      "Validation Loss: 0.1413872892414691\n",
      "Epoch 72/100\n",
      "Training Loss: 0.13535628219411594\n",
      "Validation Loss: 0.14970517367092737\n",
      "Epoch 73/100\n",
      "Training Loss: 0.19104752823214038\n",
      "Validation Loss: 0.1300990509063004\n",
      "Epoch 74/100\n",
      "Training Loss: 0.21566815555168597\n",
      "Validation Loss: 0.14951035636018953\n",
      "Epoch 75/100\n",
      "Training Loss: 0.12172514047936411\n",
      "Validation Loss: 0.1207872690182376\n",
      "Epoch 76/100\n",
      "Training Loss: 0.166483258409183\n",
      "Validation Loss: 0.22624977000661536\n",
      "Epoch 77/100\n",
      "Training Loss: 0.14147966679083201\n",
      "Validation Loss: 0.07325437843729041\n",
      "Epoch 78/100\n",
      "Training Loss: 0.14793006455527885\n",
      "Validation Loss: 0.20464131813022246\n",
      "Epoch 79/100\n",
      "Training Loss: 0.12954349469871057\n",
      "Validation Loss: 0.14118612106781886\n",
      "Epoch 80/100\n",
      "Training Loss: 0.14301983078049943\n",
      "Validation Loss: 0.06117098013953089\n",
      "Epoch 81/100\n",
      "Training Loss: 0.18454183276908595\n",
      "Validation Loss: 0.17161983162865077\n",
      "Epoch 82/100\n",
      "Training Loss: 0.16658813135751238\n",
      "Validation Loss: 0.10547544418503538\n",
      "Epoch 83/100\n",
      "Training Loss: 0.13293037249814613\n",
      "Validation Loss: 0.07559526284962734\n",
      "Epoch 84/100\n",
      "Training Loss: 0.1414613987231237\n",
      "Validation Loss: 0.10385193998605542\n",
      "Epoch 85/100\n",
      "Training Loss: 0.17814787460113019\n",
      "Validation Loss: 0.06744127763799787\n",
      "Epoch 86/100\n",
      "Training Loss: 0.18434100045507212\n",
      "Validation Loss: 0.1566917381546329\n",
      "Epoch 87/100\n",
      "Training Loss: 0.15385430898660996\n",
      "Validation Loss: 0.1004047027159507\n",
      "Epoch 88/100\n",
      "Training Loss: 0.14776483890198758\n",
      "Validation Loss: 0.09579870502181956\n",
      "Epoch 89/100\n",
      "Training Loss: 0.17613772577935327\n",
      "Validation Loss: 0.09404888251415236\n",
      "Epoch 90/100\n",
      "Training Loss: 0.1629957905780252\n",
      "Validation Loss: 0.09573724525504426\n",
      "Epoch 91/100\n",
      "Training Loss: 0.17320218866316797\n",
      "Validation Loss: 0.09154355279392097\n",
      "Epoch 92/100\n",
      "Training Loss: 0.15196936082424636\n",
      "Validation Loss: 0.07252647843140891\n",
      "Epoch 93/100\n",
      "Training Loss: 0.1608554159301613\n",
      "Validation Loss: 0.1356663684435288\n",
      "Epoch 94/100\n",
      "Training Loss: 0.12226956754909415\n",
      "Validation Loss: 0.11311209875258314\n",
      "Epoch 95/100\n",
      "Training Loss: 0.14325885590268073\n",
      "Validation Loss: 0.11761584663525967\n",
      "Epoch 96/100\n",
      "Training Loss: 0.1367447639488294\n",
      "Validation Loss: 0.16941486887821905\n",
      "Epoch 97/100\n",
      "Training Loss: 0.15856533196878933\n",
      "Validation Loss: 0.12043672585429063\n",
      "Epoch 98/100\n",
      "Training Loss: 0.12345563675880282\n",
      "Validation Loss: 0.05424402263218196\n",
      "Epoch 99/100\n",
      "Training Loss: 0.17540648749546336\n",
      "Validation Loss: 0.09344631452685345\n",
      "Epoch 100/100\n",
      "Training Loss: 0.1266295137163091\n",
      "Validation Loss: 0.17289611705350602\n",
      "Combination 25: Avg Training Loss = 0.21412550619611256, Avg Validation Loss = 0.15529703728234243\n",
      "Testing combination 26/48: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'sequence_length': 6, 'dropout_rate': 0.2}\n",
      "Truncated Train Data Shape: (70, 6, 1)\n",
      "Truncated Validation Data Shape: (10, 6, 1)\n",
      "    Trial 1/2 for combination 26/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.5499957004580593\n",
      "Validation Loss: 0.21091440398836953\n",
      "Epoch 2/100\n",
      "Training Loss: 0.46711821485604715\n",
      "Validation Loss: 0.3414292459559714\n",
      "Epoch 3/100\n",
      "Training Loss: 0.4280991655069313\n",
      "Validation Loss: 0.26947336194439536\n",
      "Epoch 4/100\n",
      "Training Loss: 0.3881598922818895\n",
      "Validation Loss: 0.32624520284438213\n",
      "Epoch 5/100\n",
      "Training Loss: 0.4263687357521565\n",
      "Validation Loss: 0.5391203936821787\n",
      "Epoch 6/100\n",
      "Training Loss: 0.3521962706042567\n",
      "Validation Loss: 0.2986163473085302\n",
      "Epoch 7/100\n",
      "Training Loss: 0.38617247912191843\n",
      "Validation Loss: 0.47990302992203854\n",
      "Epoch 8/100\n",
      "Training Loss: 0.3394417472045523\n",
      "Validation Loss: 0.21470153155331886\n",
      "Epoch 9/100\n",
      "Training Loss: 0.2995174379553209\n",
      "Validation Loss: 0.3770102736669546\n",
      "Epoch 10/100\n",
      "Training Loss: 0.34298115890851566\n",
      "Validation Loss: 0.24580675197955815\n",
      "Epoch 11/100\n",
      "Training Loss: 0.2840611057277671\n",
      "Validation Loss: 0.21054684636829685\n",
      "Epoch 12/100\n",
      "Training Loss: 0.24933673800235934\n",
      "Validation Loss: 0.29241559201471057\n",
      "Epoch 13/100\n",
      "Training Loss: 0.3084376329872607\n",
      "Validation Loss: 0.17735861005224496\n",
      "Epoch 14/100\n",
      "Training Loss: 0.2665847864767326\n",
      "Validation Loss: 0.2600980671488663\n",
      "Epoch 15/100\n",
      "Training Loss: 0.202414144301503\n",
      "Validation Loss: 0.3480897064394865\n",
      "Epoch 16/100\n",
      "Training Loss: 0.2834779987218181\n",
      "Validation Loss: 0.28076415003586563\n",
      "Epoch 17/100\n",
      "Training Loss: 0.33853677578002433\n",
      "Validation Loss: 0.22149278408121376\n",
      "Epoch 18/100\n",
      "Training Loss: 0.22107180717116254\n",
      "Validation Loss: 0.19566839852333678\n",
      "Epoch 19/100\n",
      "Training Loss: 0.2829315688508311\n",
      "Validation Loss: 0.20635280213133816\n",
      "Epoch 20/100\n",
      "Training Loss: 0.24108781662903656\n",
      "Validation Loss: 0.29037855853912\n",
      "Epoch 21/100\n",
      "Training Loss: 0.2546535996829003\n",
      "Validation Loss: 0.16104702448625302\n",
      "Epoch 22/100\n",
      "Training Loss: 0.22133381002163588\n",
      "Validation Loss: 0.12392401769435453\n",
      "Epoch 23/100\n",
      "Training Loss: 0.2302708932919831\n",
      "Validation Loss: 0.14839258522721507\n",
      "Epoch 24/100\n",
      "Training Loss: 0.24234456281157465\n",
      "Validation Loss: 0.24310626684531336\n",
      "Epoch 25/100\n",
      "Training Loss: 0.2683174843543679\n",
      "Validation Loss: 0.1953806116157683\n",
      "Epoch 26/100\n",
      "Training Loss: 0.20995661742749752\n",
      "Validation Loss: 0.14716346534759037\n",
      "Epoch 27/100\n",
      "Training Loss: 0.2034358424344693\n",
      "Validation Loss: 0.3403678742957589\n",
      "Epoch 28/100\n",
      "Training Loss: 0.22389792370022898\n",
      "Validation Loss: 0.15942497059424318\n",
      "Epoch 29/100\n",
      "Training Loss: 0.2334949374395304\n",
      "Validation Loss: 0.16128188440327212\n",
      "Epoch 30/100\n",
      "Training Loss: 0.20916234780368329\n",
      "Validation Loss: 0.18521165878422244\n",
      "Epoch 31/100\n",
      "Training Loss: 0.1835732704867818\n",
      "Validation Loss: 0.28432590894281407\n",
      "Epoch 32/100\n",
      "Training Loss: 0.1724471862326753\n",
      "Validation Loss: 0.15756281887947587\n",
      "Epoch 33/100\n",
      "Training Loss: 0.2047179079594677\n",
      "Validation Loss: 0.27009106696424734\n",
      "Epoch 34/100\n",
      "Training Loss: 0.24611168907076564\n",
      "Validation Loss: 0.20676355796661866\n",
      "Epoch 35/100\n",
      "Training Loss: 0.1579949421870475\n",
      "Validation Loss: 0.26597838781013156\n",
      "Epoch 36/100\n",
      "Training Loss: 0.19169915562649734\n",
      "Validation Loss: 0.2605927809268527\n",
      "Epoch 37/100\n",
      "Training Loss: 0.17030098103208455\n",
      "Validation Loss: 0.14494542268381683\n",
      "Epoch 38/100\n",
      "Training Loss: 0.2284586137234131\n",
      "Validation Loss: 0.0916270930861693\n",
      "Epoch 39/100\n",
      "Training Loss: 0.1867423685795536\n",
      "Validation Loss: 0.13780702658890864\n",
      "Epoch 40/100\n",
      "Training Loss: 0.2111481437007865\n",
      "Validation Loss: 0.17266554858765262\n",
      "Epoch 41/100\n",
      "Training Loss: 0.20025767426117475\n",
      "Validation Loss: 0.14376112791933818\n",
      "Epoch 42/100\n",
      "Training Loss: 0.1874428632662724\n",
      "Validation Loss: 0.22813985660347239\n",
      "Epoch 43/100\n",
      "Training Loss: 0.16048657397382107\n",
      "Validation Loss: 0.16713362252528824\n",
      "Epoch 44/100\n",
      "Training Loss: 0.18331672605294255\n",
      "Validation Loss: 0.1482881839747765\n",
      "Epoch 45/100\n",
      "Training Loss: 0.1928678773058746\n",
      "Validation Loss: 0.18247693355461794\n",
      "Epoch 46/100\n",
      "Training Loss: 0.16219088994791384\n",
      "Validation Loss: 0.14837148254597682\n",
      "Epoch 47/100\n",
      "Training Loss: 0.20138670975392692\n",
      "Validation Loss: 0.1133869717677114\n",
      "Epoch 48/100\n",
      "Training Loss: 0.1597813633249648\n",
      "Validation Loss: 0.21875939048191304\n",
      "Epoch 49/100\n",
      "Training Loss: 0.18935122274924382\n",
      "Validation Loss: 0.0754445457842622\n",
      "Epoch 50/100\n",
      "Training Loss: 0.23230646358436427\n",
      "Validation Loss: 0.10874280503617777\n",
      "Epoch 51/100\n",
      "Training Loss: 0.2085026376492355\n",
      "Validation Loss: 0.10060038373813751\n",
      "Epoch 52/100\n",
      "Training Loss: 0.19709516629747525\n",
      "Validation Loss: 0.12527776487896763\n",
      "Epoch 53/100\n",
      "Training Loss: 0.15518319845000117\n",
      "Validation Loss: 0.13793528249678214\n",
      "Epoch 54/100\n",
      "Training Loss: 0.17934313691405657\n",
      "Validation Loss: 0.14805627083032874\n",
      "Epoch 55/100\n",
      "Training Loss: 0.20134843468889474\n",
      "Validation Loss: 0.22839998644158652\n",
      "Epoch 56/100\n",
      "Training Loss: 0.17788702793452535\n",
      "Validation Loss: 0.15880400518260793\n",
      "Epoch 57/100\n",
      "Training Loss: 0.23821618662844862\n",
      "Validation Loss: 0.0922543240432644\n",
      "Epoch 58/100\n",
      "Training Loss: 0.16471152543402373\n",
      "Validation Loss: 0.08783553451475708\n",
      "Epoch 59/100\n",
      "Training Loss: 0.1846554487086938\n",
      "Validation Loss: 0.15044049473593157\n",
      "Epoch 60/100\n",
      "Training Loss: 0.19114656611033237\n",
      "Validation Loss: 0.09209835575581027\n",
      "Epoch 61/100\n",
      "Training Loss: 0.21230949117753778\n",
      "Validation Loss: 0.0947985888732787\n",
      "Epoch 62/100\n",
      "Training Loss: 0.17427313121681962\n",
      "Validation Loss: 0.1055761107853104\n",
      "Epoch 63/100\n",
      "Training Loss: 0.15360092472973633\n",
      "Validation Loss: 0.2107290744609512\n",
      "Epoch 64/100\n",
      "Training Loss: 0.1592647605247806\n",
      "Validation Loss: 0.15243275415937058\n",
      "Epoch 65/100\n",
      "Training Loss: 0.19091860713044317\n",
      "Validation Loss: 0.09415047131971463\n",
      "Epoch 66/100\n",
      "Training Loss: 0.22712959964401244\n",
      "Validation Loss: 0.12299564639885281\n",
      "Epoch 67/100\n",
      "Training Loss: 0.1897872990892997\n",
      "Validation Loss: 0.13849926437646076\n",
      "Epoch 68/100\n",
      "Training Loss: 0.1673033846787243\n",
      "Validation Loss: 0.09276282610546085\n",
      "Epoch 69/100\n",
      "Training Loss: 0.15760367752424104\n",
      "Validation Loss: 0.11378196514391399\n",
      "Epoch 70/100\n",
      "Training Loss: 0.20480538507228344\n",
      "Validation Loss: 0.17048361029966072\n",
      "Epoch 71/100\n",
      "Training Loss: 0.12867928065931242\n",
      "Validation Loss: 0.13114702407976475\n",
      "Epoch 72/100\n",
      "Training Loss: 0.1897600121718669\n",
      "Validation Loss: 0.1491767026909986\n",
      "Epoch 73/100\n",
      "Training Loss: 0.1407092284149791\n",
      "Validation Loss: 0.13055220805179168\n",
      "Epoch 74/100\n",
      "Training Loss: 0.1309309437480458\n",
      "Validation Loss: 0.1405148338773053\n",
      "Epoch 75/100\n",
      "Training Loss: 0.20189657421562365\n",
      "Validation Loss: 0.11510754027913825\n",
      "Epoch 76/100\n",
      "Training Loss: 0.1371083813797693\n",
      "Validation Loss: 0.17184846253592\n",
      "Epoch 77/100\n",
      "Training Loss: 0.18449882911231288\n",
      "Validation Loss: 0.08929868048484503\n",
      "Epoch 78/100\n",
      "Training Loss: 0.13334516216356446\n",
      "Validation Loss: 0.10200756158290802\n",
      "Epoch 79/100\n",
      "Training Loss: 0.18748469040832577\n",
      "Validation Loss: 0.10975919264276832\n",
      "Epoch 80/100\n",
      "Training Loss: 0.15203742209521662\n",
      "Validation Loss: 0.18340572118816953\n",
      "Epoch 81/100\n",
      "Training Loss: 0.1599468711388473\n",
      "Validation Loss: 0.06485058935312224\n",
      "Epoch 82/100\n",
      "Training Loss: 0.1407546933329011\n",
      "Validation Loss: 0.13828807830161408\n",
      "Epoch 83/100\n",
      "Training Loss: 0.14715719147293532\n",
      "Validation Loss: 0.25596753517070153\n",
      "Epoch 84/100\n",
      "Training Loss: 0.1736288478358665\n",
      "Validation Loss: 0.09016187013570649\n",
      "Epoch 85/100\n",
      "Training Loss: 0.18237720219853804\n",
      "Validation Loss: 0.17602148081329055\n",
      "Epoch 86/100\n",
      "Training Loss: 0.14987121897548764\n",
      "Validation Loss: 0.11556929055157579\n",
      "Epoch 87/100\n",
      "Training Loss: 0.14276613809280414\n",
      "Validation Loss: 0.09389250744124242\n",
      "Epoch 88/100\n",
      "Training Loss: 0.13816843551941732\n",
      "Validation Loss: 0.05958032993113014\n",
      "Epoch 89/100\n",
      "Training Loss: 0.15548006953572852\n",
      "Validation Loss: 0.1101928053763811\n",
      "Epoch 90/100\n",
      "Training Loss: 0.1444458286146862\n",
      "Validation Loss: 0.0876658993342347\n",
      "Epoch 91/100\n",
      "Training Loss: 0.16333246822405303\n",
      "Validation Loss: 0.09972710173199321\n",
      "Epoch 92/100\n",
      "Training Loss: 0.1734462938840562\n",
      "Validation Loss: 0.06303674879600181\n",
      "Epoch 93/100\n",
      "Training Loss: 0.15133303255105643\n",
      "Validation Loss: 0.12075575476248734\n",
      "Epoch 94/100\n",
      "Training Loss: 0.14736607216940528\n",
      "Validation Loss: 0.14321917525282288\n",
      "Epoch 95/100\n",
      "Training Loss: 0.1487980137548131\n",
      "Validation Loss: 0.17340928340050618\n",
      "Epoch 96/100\n",
      "Training Loss: 0.16405251830015746\n",
      "Validation Loss: 0.1067581111645354\n",
      "Epoch 97/100\n",
      "Training Loss: 0.1270397830647906\n",
      "Validation Loss: 0.10626980652310689\n",
      "Epoch 98/100\n",
      "Training Loss: 0.12073745794145369\n",
      "Validation Loss: 0.14842458451943571\n",
      "Epoch 99/100\n",
      "Training Loss: 0.1387483917704511\n",
      "Validation Loss: 0.11129789586072351\n",
      "Epoch 100/100\n",
      "Training Loss: 0.1541892220455628\n",
      "Validation Loss: 0.16586935801295138\n",
      "    Trial 2/2 for combination 26/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.5464233920640228\n",
      "Validation Loss: 0.38377769662644645\n",
      "Epoch 2/100\n",
      "Training Loss: 0.4879261084224145\n",
      "Validation Loss: 0.2408176106432427\n",
      "Epoch 3/100\n",
      "Training Loss: 0.41608374053818775\n",
      "Validation Loss: 0.47980091407495945\n",
      "Epoch 4/100\n",
      "Training Loss: 0.4304746211599563\n",
      "Validation Loss: 0.10788644534773413\n",
      "Epoch 5/100\n",
      "Training Loss: 0.3847818363402352\n",
      "Validation Loss: 0.13847536470107294\n",
      "Epoch 6/100\n",
      "Training Loss: 0.3797153739446296\n",
      "Validation Loss: 0.4077264213325741\n",
      "Epoch 7/100\n",
      "Training Loss: 0.39833343316962233\n",
      "Validation Loss: 0.3238519802142171\n",
      "Epoch 8/100\n",
      "Training Loss: 0.34596746011554924\n",
      "Validation Loss: 0.24383989491998612\n",
      "Epoch 9/100\n",
      "Training Loss: 0.3557659378864721\n",
      "Validation Loss: 0.2673571104013758\n",
      "Epoch 10/100\n",
      "Training Loss: 0.2842808333520488\n",
      "Validation Loss: 0.38178707201482065\n",
      "Epoch 11/100\n",
      "Training Loss: 0.3157127115533799\n",
      "Validation Loss: 0.16907834045810005\n",
      "Epoch 12/100\n",
      "Training Loss: 0.29733560368256134\n",
      "Validation Loss: 0.1581859190340346\n",
      "Epoch 13/100\n",
      "Training Loss: 0.27824128358973693\n",
      "Validation Loss: 0.3087473186260258\n",
      "Epoch 14/100\n",
      "Training Loss: 0.2973818673715598\n",
      "Validation Loss: 0.14576848565959946\n",
      "Epoch 15/100\n",
      "Training Loss: 0.17098035584982646\n",
      "Validation Loss: 0.2101815359721031\n",
      "Epoch 16/100\n",
      "Training Loss: 0.29662154984993605\n",
      "Validation Loss: 0.22665028841628745\n",
      "Epoch 17/100\n",
      "Training Loss: 0.2868979514330588\n",
      "Validation Loss: 0.08099666959439662\n",
      "Epoch 18/100\n",
      "Training Loss: 0.21033622524082116\n",
      "Validation Loss: 0.27717374840750897\n",
      "Epoch 19/100\n",
      "Training Loss: 0.2846763867879845\n",
      "Validation Loss: 0.11032855299993907\n",
      "Epoch 20/100\n",
      "Training Loss: 0.24190674515559704\n",
      "Validation Loss: 0.08002094746234972\n",
      "Epoch 21/100\n",
      "Training Loss: 0.34092208338326974\n",
      "Validation Loss: 0.4044484183183137\n",
      "Epoch 22/100\n",
      "Training Loss: 0.25778884812166203\n",
      "Validation Loss: 0.12310472369913916\n",
      "Epoch 23/100\n",
      "Training Loss: 0.21369307565743023\n",
      "Validation Loss: 0.08140557404675125\n",
      "Epoch 24/100\n",
      "Training Loss: 0.21297842443424833\n",
      "Validation Loss: 0.25643594373533246\n",
      "Epoch 25/100\n",
      "Training Loss: 0.22429521471732847\n",
      "Validation Loss: 0.1875241242669681\n",
      "Epoch 26/100\n",
      "Training Loss: 0.23881681141053543\n",
      "Validation Loss: 0.3081125226871072\n",
      "Epoch 27/100\n",
      "Training Loss: 0.20394608915502288\n",
      "Validation Loss: 0.140542646707111\n",
      "Epoch 28/100\n",
      "Training Loss: 0.22940124392014144\n",
      "Validation Loss: 0.41069230481791275\n",
      "Epoch 29/100\n",
      "Training Loss: 0.268606951303966\n",
      "Validation Loss: 0.17067798689992017\n",
      "Epoch 30/100\n",
      "Training Loss: 0.20428299947969852\n",
      "Validation Loss: 0.19825475313202334\n",
      "Epoch 31/100\n",
      "Training Loss: 0.22900363486846514\n",
      "Validation Loss: 0.20851492752221829\n",
      "Epoch 32/100\n",
      "Training Loss: 0.16443417921491738\n",
      "Validation Loss: 0.16446519220295341\n",
      "Epoch 33/100\n",
      "Training Loss: 0.17049329035213812\n",
      "Validation Loss: 0.26278927198199475\n",
      "Epoch 34/100\n",
      "Training Loss: 0.20869067505153424\n",
      "Validation Loss: 0.1865005613375897\n",
      "Epoch 35/100\n",
      "Training Loss: 0.17335095749964582\n",
      "Validation Loss: 0.2221186160183331\n",
      "Epoch 36/100\n",
      "Training Loss: 0.2021757148673107\n",
      "Validation Loss: 0.17092895496942034\n",
      "Epoch 37/100\n",
      "Training Loss: 0.2223118096266742\n",
      "Validation Loss: 0.20705846837599898\n",
      "Epoch 38/100\n",
      "Training Loss: 0.18546114129591948\n",
      "Validation Loss: 0.24127139542243808\n",
      "Epoch 39/100\n",
      "Training Loss: 0.20883716671036587\n",
      "Validation Loss: 0.2109939231863839\n",
      "Epoch 40/100\n",
      "Training Loss: 0.1887860360873513\n",
      "Validation Loss: 0.20451415739823336\n",
      "Epoch 41/100\n",
      "Training Loss: 0.18930645310319988\n",
      "Validation Loss: 0.11004738911439212\n",
      "Epoch 42/100\n",
      "Training Loss: 0.1670934584582171\n",
      "Validation Loss: 0.12102882862687266\n",
      "Epoch 43/100\n",
      "Training Loss: 0.15607065231862555\n",
      "Validation Loss: 0.1385970376118574\n",
      "Epoch 44/100\n",
      "Training Loss: 0.16638818989320997\n",
      "Validation Loss: 0.1356672374229404\n",
      "Epoch 45/100\n",
      "Training Loss: 0.18998125765300902\n",
      "Validation Loss: 0.07694229754459324\n",
      "Epoch 46/100\n",
      "Training Loss: 0.155247500413109\n",
      "Validation Loss: 0.06568818700607348\n",
      "Epoch 47/100\n",
      "Training Loss: 0.1967720066436266\n",
      "Validation Loss: 0.1030785592915048\n",
      "Epoch 48/100\n",
      "Training Loss: 0.17092067118319945\n",
      "Validation Loss: 0.12589600826311623\n",
      "Epoch 49/100\n",
      "Training Loss: 0.19935580604413758\n",
      "Validation Loss: 0.20551808786624784\n",
      "Epoch 50/100\n",
      "Training Loss: 0.19252452097860953\n",
      "Validation Loss: 0.11224103421935883\n",
      "Epoch 51/100\n",
      "Training Loss: 0.24445558402966613\n",
      "Validation Loss: 0.16959023922076466\n",
      "Epoch 52/100\n",
      "Training Loss: 0.18749186917016047\n",
      "Validation Loss: 0.10889815276059042\n",
      "Epoch 53/100\n",
      "Training Loss: 0.16478078374779873\n",
      "Validation Loss: 0.15881266619827922\n",
      "Epoch 54/100\n",
      "Training Loss: 0.19147250426904275\n",
      "Validation Loss: 0.17948730739903523\n",
      "Epoch 55/100\n",
      "Training Loss: 0.17417781743159966\n",
      "Validation Loss: 0.13377678713068009\n",
      "Epoch 56/100\n",
      "Training Loss: 0.1886390107077717\n",
      "Validation Loss: 0.1783390541339962\n",
      "Epoch 57/100\n",
      "Training Loss: 0.21128877875579802\n",
      "Validation Loss: 0.27563664202961685\n",
      "Epoch 58/100\n",
      "Training Loss: 0.19514073731443823\n",
      "Validation Loss: 0.22441495660656638\n",
      "Epoch 59/100\n",
      "Training Loss: 0.1952837486049555\n",
      "Validation Loss: 0.14408594214197984\n",
      "Epoch 60/100\n",
      "Training Loss: 0.17927918939522988\n",
      "Validation Loss: 0.12190994453091002\n",
      "Epoch 61/100\n",
      "Training Loss: 0.1891904931788639\n",
      "Validation Loss: 0.2680470401464009\n",
      "Epoch 62/100\n",
      "Training Loss: 0.16392371152774202\n",
      "Validation Loss: 0.20074370540607617\n",
      "Epoch 63/100\n",
      "Training Loss: 0.17919728684606878\n",
      "Validation Loss: 0.10665672429756781\n",
      "Epoch 64/100\n",
      "Training Loss: 0.17103204343898448\n",
      "Validation Loss: 0.08890147213689194\n",
      "Epoch 65/100\n",
      "Training Loss: 0.1422150971540068\n",
      "Validation Loss: 0.15831496765049594\n",
      "Epoch 66/100\n",
      "Training Loss: 0.1825021694280801\n",
      "Validation Loss: 0.1002098107818071\n",
      "Epoch 67/100\n",
      "Training Loss: 0.19405942494003356\n",
      "Validation Loss: 0.20403966326892736\n",
      "Epoch 68/100\n",
      "Training Loss: 0.16892754129872734\n",
      "Validation Loss: 0.15281840478115682\n",
      "Epoch 69/100\n",
      "Training Loss: 0.18899842051817775\n",
      "Validation Loss: 0.14411261743723802\n",
      "Epoch 70/100\n",
      "Training Loss: 0.17032736289568567\n",
      "Validation Loss: 0.13946086152556475\n",
      "Epoch 71/100\n",
      "Training Loss: 0.15478475802311184\n",
      "Validation Loss: 0.16227851436689233\n",
      "Epoch 72/100\n",
      "Training Loss: 0.13227607658874913\n",
      "Validation Loss: 0.16741439450040185\n",
      "Epoch 73/100\n",
      "Training Loss: 0.18608883857663788\n",
      "Validation Loss: 0.14489885527743984\n",
      "Epoch 74/100\n",
      "Training Loss: 0.16873273806430877\n",
      "Validation Loss: 0.11000721010868084\n",
      "Epoch 75/100\n",
      "Training Loss: 0.17278827166296518\n",
      "Validation Loss: 0.12756296728815025\n",
      "Epoch 76/100\n",
      "Training Loss: 0.17135935218325096\n",
      "Validation Loss: 0.12479121348859132\n",
      "Epoch 77/100\n",
      "Training Loss: 0.16548512853186567\n",
      "Validation Loss: 0.10816912281888687\n",
      "Epoch 78/100\n",
      "Training Loss: 0.14607093753698244\n",
      "Validation Loss: 0.19059177956808973\n",
      "Epoch 79/100\n",
      "Training Loss: 0.1504082289939924\n",
      "Validation Loss: 0.16999800861150124\n",
      "Epoch 80/100\n",
      "Training Loss: 0.23029150589233582\n",
      "Validation Loss: 0.09856053370302043\n",
      "Epoch 81/100\n",
      "Training Loss: 0.1258023966235388\n",
      "Validation Loss: 0.18403676669871066\n",
      "Epoch 82/100\n",
      "Training Loss: 0.12221943681410763\n",
      "Validation Loss: 0.10058343234935492\n",
      "Epoch 83/100\n",
      "Training Loss: 0.1383881385790196\n",
      "Validation Loss: 0.08304194165607295\n",
      "Epoch 84/100\n",
      "Training Loss: 0.1266108248514771\n",
      "Validation Loss: 0.15135723850138408\n",
      "Epoch 85/100\n",
      "Training Loss: 0.162939155005133\n",
      "Validation Loss: 0.11531435199572489\n",
      "Epoch 86/100\n",
      "Training Loss: 0.15136938942525213\n",
      "Validation Loss: 0.10918913687693046\n",
      "Epoch 87/100\n",
      "Training Loss: 0.1609590586297832\n",
      "Validation Loss: 0.1888715888010199\n",
      "Epoch 88/100\n",
      "Training Loss: 0.15941195770057126\n",
      "Validation Loss: 0.10077622977845127\n",
      "Epoch 89/100\n",
      "Training Loss: 0.13051653053533954\n",
      "Validation Loss: 0.07691253521301954\n",
      "Epoch 90/100\n",
      "Training Loss: 0.1682671079333069\n",
      "Validation Loss: 0.10696336560013882\n",
      "Epoch 91/100\n",
      "Training Loss: 0.1432919184600398\n",
      "Validation Loss: 0.13713250135795063\n",
      "Epoch 92/100\n",
      "Training Loss: 0.10930738507316663\n",
      "Validation Loss: 0.0869033464531376\n",
      "Epoch 93/100\n",
      "Training Loss: 0.16392310386014727\n",
      "Validation Loss: 0.0937740310391351\n",
      "Epoch 94/100\n",
      "Training Loss: 0.15169054866405476\n",
      "Validation Loss: 0.13815074327522908\n",
      "Epoch 95/100\n",
      "Training Loss: 0.14183329254262486\n",
      "Validation Loss: 0.12126918250675348\n",
      "Epoch 96/100\n",
      "Training Loss: 0.15787686230381565\n",
      "Validation Loss: 0.0802762313704404\n",
      "Epoch 97/100\n",
      "Training Loss: 0.13449101869520977\n",
      "Validation Loss: 0.07747918331659243\n",
      "Epoch 98/100\n",
      "Training Loss: 0.10573078712921107\n",
      "Validation Loss: 0.0877448256837124\n",
      "Epoch 99/100\n",
      "Training Loss: 0.16135193652167784\n",
      "Validation Loss: 0.12080113909387871\n",
      "Epoch 100/100\n",
      "Training Loss: 0.14222753124770787\n",
      "Validation Loss: 0.07809819144710244\n",
      "Combination 26: Avg Training Loss = 0.21091637878068784, Avg Validation Loss = 0.1737447838571176\n",
      "Testing combination 27/48: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'sequence_length': 20, 'dropout_rate': 0.1}\n",
      "Truncated Train Data Shape: (70, 20, 1)\n",
      "Truncated Validation Data Shape: (10, 20, 1)\n",
      "    Trial 1/2 for combination 27/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.4946536756006189\n",
      "Validation Loss: 0.3155817479137506\n",
      "Epoch 2/100\n",
      "Training Loss: 0.41939388725740234\n",
      "Validation Loss: 0.3688491257416947\n",
      "Epoch 3/100\n",
      "Training Loss: 0.3999773063573805\n",
      "Validation Loss: 0.19725022586063226\n",
      "Epoch 4/100\n",
      "Training Loss: 0.36234988245798966\n",
      "Validation Loss: 0.5568311752290265\n",
      "Epoch 5/100\n",
      "Training Loss: 0.26540476891240344\n",
      "Validation Loss: 0.30485533661332803\n",
      "Epoch 6/100\n",
      "Training Loss: 0.2448523518174658\n",
      "Validation Loss: 0.1953347331915239\n",
      "Epoch 7/100\n",
      "Training Loss: 0.27003427309057315\n",
      "Validation Loss: 0.24470888653906891\n",
      "Epoch 8/100\n",
      "Training Loss: 0.25982393071795284\n",
      "Validation Loss: 0.11385946835907053\n",
      "Epoch 9/100\n",
      "Training Loss: 0.2850299685356069\n",
      "Validation Loss: 0.22460220064045583\n",
      "Epoch 10/100\n",
      "Training Loss: 0.2381588893939954\n",
      "Validation Loss: 0.1358240859913267\n",
      "Epoch 11/100\n",
      "Training Loss: 0.286128476272555\n",
      "Validation Loss: 0.09795653873848192\n",
      "Epoch 12/100\n",
      "Training Loss: 0.21285383833124544\n",
      "Validation Loss: 0.15598594071602517\n",
      "Epoch 13/100\n",
      "Training Loss: 0.18518050069320358\n",
      "Validation Loss: 0.10768204508597883\n",
      "Epoch 14/100\n",
      "Training Loss: 0.2444597627042072\n",
      "Validation Loss: 0.0837682870842281\n",
      "Epoch 15/100\n",
      "Training Loss: 0.22147355733224527\n",
      "Validation Loss: 0.099689529206402\n",
      "Epoch 16/100\n",
      "Training Loss: 0.17262114932957595\n",
      "Validation Loss: 0.07468798281167571\n",
      "Epoch 17/100\n",
      "Training Loss: 0.21725238232331293\n",
      "Validation Loss: 0.11505058832047141\n",
      "Epoch 18/100\n",
      "Training Loss: 0.21391606782695227\n",
      "Validation Loss: 0.1084791792606993\n",
      "Epoch 19/100\n",
      "Training Loss: 0.19385673451328445\n",
      "Validation Loss: 0.10211169420401242\n",
      "Epoch 20/100\n",
      "Training Loss: 0.22093993114475458\n",
      "Validation Loss: 0.08133341769249072\n",
      "Epoch 21/100\n",
      "Training Loss: 0.1784206944264718\n",
      "Validation Loss: 0.1506479345288289\n",
      "Epoch 22/100\n",
      "Training Loss: 0.16159753030935187\n",
      "Validation Loss: 0.11935493365673147\n",
      "Epoch 23/100\n",
      "Training Loss: 0.16441379504433812\n",
      "Validation Loss: 0.16054450435809725\n",
      "Epoch 24/100\n",
      "Training Loss: 0.20466486181737945\n",
      "Validation Loss: 0.10170895547551637\n",
      "Epoch 25/100\n",
      "Training Loss: 0.13108689781974703\n",
      "Validation Loss: 0.15987278119720122\n",
      "Epoch 26/100\n",
      "Training Loss: 0.18631990816117405\n",
      "Validation Loss: 0.08886099451104579\n",
      "Epoch 27/100\n",
      "Training Loss: 0.1641867735977829\n",
      "Validation Loss: 0.07647983700805396\n",
      "Epoch 28/100\n",
      "Training Loss: 0.16749347970663728\n",
      "Validation Loss: 0.09132819662429234\n",
      "Epoch 29/100\n",
      "Training Loss: 0.202859961705149\n",
      "Validation Loss: 0.1407500085915951\n",
      "Epoch 30/100\n",
      "Training Loss: 0.168133028724047\n",
      "Validation Loss: 0.19911762933637295\n",
      "Epoch 31/100\n",
      "Training Loss: 0.17752955859427463\n",
      "Validation Loss: 0.11951522085057983\n",
      "Epoch 32/100\n",
      "Training Loss: 0.14674268567319007\n",
      "Validation Loss: 0.08093712612651005\n",
      "Epoch 33/100\n",
      "Training Loss: 0.1469943837970164\n",
      "Validation Loss: 0.07989088217528166\n",
      "Epoch 34/100\n",
      "Training Loss: 0.14851040370916835\n",
      "Validation Loss: 0.09228546755731512\n",
      "Epoch 35/100\n",
      "Training Loss: 0.15858009906030715\n",
      "Validation Loss: 0.14040712939755384\n",
      "Epoch 36/100\n",
      "Training Loss: 0.1812197909846361\n",
      "Validation Loss: 0.0640863932067561\n",
      "Epoch 37/100\n",
      "Training Loss: 0.17148338119618556\n",
      "Validation Loss: 0.1225154182563426\n",
      "Epoch 38/100\n",
      "Training Loss: 0.15565836751384773\n",
      "Validation Loss: 0.11002103970569763\n",
      "Epoch 39/100\n",
      "Training Loss: 0.15110369920341776\n",
      "Validation Loss: 0.05478854604030856\n",
      "Epoch 40/100\n",
      "Training Loss: 0.14916997927822978\n",
      "Validation Loss: 0.09698520260271684\n",
      "Epoch 41/100\n",
      "Training Loss: 0.16386199785975772\n",
      "Validation Loss: 0.09508028795592195\n",
      "Epoch 42/100\n",
      "Training Loss: 0.1600209686900141\n",
      "Validation Loss: 0.09639182665079639\n",
      "Epoch 43/100\n",
      "Training Loss: 0.19647359849582677\n",
      "Validation Loss: 0.15580078343244724\n",
      "Epoch 44/100\n",
      "Training Loss: 0.16608890258328607\n",
      "Validation Loss: 0.09698805828842115\n",
      "Epoch 45/100\n",
      "Training Loss: 0.14374171218003365\n",
      "Validation Loss: 0.06810134608614833\n",
      "Epoch 46/100\n",
      "Training Loss: 0.15079747091695503\n",
      "Validation Loss: 0.1357998477686719\n",
      "Epoch 47/100\n",
      "Training Loss: 0.13284127039769228\n",
      "Validation Loss: 0.15950677476356068\n",
      "Epoch 48/100\n",
      "Training Loss: 0.1617983518152318\n",
      "Validation Loss: 0.08892172250032139\n",
      "Epoch 49/100\n",
      "Training Loss: 0.11806557411661842\n",
      "Validation Loss: 0.07680078482577701\n",
      "Epoch 50/100\n",
      "Training Loss: 0.15882735699204242\n",
      "Validation Loss: 0.1301895634696823\n",
      "Epoch 51/100\n",
      "Training Loss: 0.16486030398846693\n",
      "Validation Loss: 0.08661525488638674\n",
      "Epoch 52/100\n",
      "Training Loss: 0.1775601127037428\n",
      "Validation Loss: 0.11543796002531112\n",
      "Epoch 53/100\n",
      "Training Loss: 0.141261144442975\n",
      "Validation Loss: 0.056245877968220725\n",
      "Epoch 54/100\n",
      "Training Loss: 0.1477142858014417\n",
      "Validation Loss: 0.12020559068006927\n",
      "Epoch 55/100\n",
      "Training Loss: 0.13734904023434008\n",
      "Validation Loss: 0.09086181010548645\n",
      "Epoch 56/100\n",
      "Training Loss: 0.1577976311971195\n",
      "Validation Loss: 0.09697320083805913\n",
      "Epoch 57/100\n",
      "Training Loss: 0.14134099737915246\n",
      "Validation Loss: 0.08671526281443474\n",
      "Epoch 58/100\n",
      "Training Loss: 0.10585357811523394\n",
      "Validation Loss: 0.07480738212497806\n",
      "Epoch 59/100\n",
      "Training Loss: 0.1598761817985053\n",
      "Validation Loss: 0.09263655447344926\n",
      "Epoch 60/100\n",
      "Training Loss: 0.13145781748290353\n",
      "Validation Loss: 0.08019649971009059\n",
      "Epoch 61/100\n",
      "Training Loss: 0.13176793234841985\n",
      "Validation Loss: 0.08914953318051451\n",
      "Epoch 62/100\n",
      "Training Loss: 0.14130332894621223\n",
      "Validation Loss: 0.14324171481707512\n",
      "Epoch 63/100\n",
      "Training Loss: 0.11991809359182952\n",
      "Validation Loss: 0.08044969316520814\n",
      "Epoch 64/100\n",
      "Training Loss: 0.13274464289099652\n",
      "Validation Loss: 0.08430685791061779\n",
      "Epoch 65/100\n",
      "Training Loss: 0.13947505567205262\n",
      "Validation Loss: 0.10103434636963601\n",
      "Epoch 66/100\n",
      "Training Loss: 0.15565405906341961\n",
      "Validation Loss: 0.07627305986341416\n",
      "Epoch 67/100\n",
      "Training Loss: 0.13277846896165987\n",
      "Validation Loss: 0.05830792633573675\n",
      "Epoch 68/100\n",
      "Training Loss: 0.13387060007985083\n",
      "Validation Loss: 0.07188228381722052\n",
      "Epoch 69/100\n",
      "Training Loss: 0.12838155011417576\n",
      "Validation Loss: 0.08912580197437382\n",
      "Epoch 70/100\n",
      "Training Loss: 0.12657503055795616\n",
      "Validation Loss: 0.10936111009061766\n",
      "Epoch 71/100\n",
      "Training Loss: 0.13652247240683218\n",
      "Validation Loss: 0.0793039355606269\n",
      "Epoch 72/100\n",
      "Training Loss: 0.1364476063541387\n",
      "Validation Loss: 0.12513490304759237\n",
      "Epoch 73/100\n",
      "Training Loss: 0.13498432070954505\n",
      "Validation Loss: 0.09238657436662323\n",
      "Epoch 74/100\n",
      "Training Loss: 0.128589670593434\n",
      "Validation Loss: 0.12813097576715554\n",
      "Epoch 75/100\n",
      "Training Loss: 0.1278525314601376\n",
      "Validation Loss: 0.12162552729384372\n",
      "Epoch 76/100\n",
      "Training Loss: 0.1646120562252767\n",
      "Validation Loss: 0.12693737598144839\n",
      "Epoch 77/100\n",
      "Training Loss: 0.14478097410879548\n",
      "Validation Loss: 0.04336966369918804\n",
      "Epoch 78/100\n",
      "Training Loss: 0.11992312286581892\n",
      "Validation Loss: 0.09075428925649723\n",
      "Epoch 79/100\n",
      "Training Loss: 0.15572364152227278\n",
      "Validation Loss: 0.11733047806654855\n",
      "Epoch 80/100\n",
      "Training Loss: 0.13848065530795428\n",
      "Validation Loss: 0.09617073289619282\n",
      "Epoch 81/100\n",
      "Training Loss: 0.12297274932539987\n",
      "Validation Loss: 0.06748212679745316\n",
      "Epoch 82/100\n",
      "Training Loss: 0.13897993454882943\n",
      "Validation Loss: 0.06076054196552979\n",
      "Epoch 83/100\n",
      "Training Loss: 0.11804535812383753\n",
      "Validation Loss: 0.06943143029072939\n",
      "Epoch 84/100\n",
      "Training Loss: 0.1435997264841539\n",
      "Validation Loss: 0.08592212164038827\n",
      "Epoch 85/100\n",
      "Training Loss: 0.11558700591907296\n",
      "Validation Loss: 0.0771822863485698\n",
      "Epoch 86/100\n",
      "Training Loss: 0.10216905501246086\n",
      "Validation Loss: 0.09543181474595278\n",
      "Epoch 87/100\n",
      "Training Loss: 0.13637584056540566\n",
      "Validation Loss: 0.09343083748075301\n",
      "Epoch 88/100\n",
      "Training Loss: 0.14301653558619232\n",
      "Validation Loss: 0.08912841964337508\n",
      "Epoch 89/100\n",
      "Training Loss: 0.10371449661214606\n",
      "Validation Loss: 0.08525055377230116\n",
      "Epoch 90/100\n",
      "Training Loss: 0.11692168998147888\n",
      "Validation Loss: 0.041262527389084196\n",
      "Epoch 91/100\n",
      "Training Loss: 0.11388792252338403\n",
      "Validation Loss: 0.041072731976001906\n",
      "Epoch 92/100\n",
      "Training Loss: 0.12118082766918835\n",
      "Validation Loss: 0.08335770234674235\n",
      "Epoch 93/100\n",
      "Training Loss: 0.11453782860008184\n",
      "Validation Loss: 0.05839619824450228\n",
      "Epoch 94/100\n",
      "Training Loss: 0.11654462878967138\n",
      "Validation Loss: 0.09337978216998366\n",
      "Epoch 95/100\n",
      "Training Loss: 0.10181478617747758\n",
      "Validation Loss: 0.07303012819677139\n",
      "Epoch 96/100\n",
      "Training Loss: 0.1395149220450823\n",
      "Validation Loss: 0.0803565234208965\n",
      "Epoch 97/100\n",
      "Training Loss: 0.1279201054991608\n",
      "Validation Loss: 0.12891175552088427\n",
      "Epoch 98/100\n",
      "Training Loss: 0.1330778675984633\n",
      "Validation Loss: 0.07355565705492068\n",
      "Epoch 99/100\n",
      "Training Loss: 0.10493634392845963\n",
      "Validation Loss: 0.10652275182527622\n",
      "Epoch 100/100\n",
      "Training Loss: 0.11011991852780019\n",
      "Validation Loss: 0.08065741761121101\n",
      "    Trial 2/2 for combination 27/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.5730578833390568\n",
      "Validation Loss: 0.3912670227423457\n",
      "Epoch 2/100\n",
      "Training Loss: 0.4297821765826075\n",
      "Validation Loss: 0.340906834736432\n",
      "Epoch 3/100\n",
      "Training Loss: 0.28017340297493615\n",
      "Validation Loss: 0.1669053524424004\n",
      "Epoch 4/100\n",
      "Training Loss: 0.41821679539439777\n",
      "Validation Loss: 0.16942937148087936\n",
      "Epoch 5/100\n",
      "Training Loss: 0.3814974449958334\n",
      "Validation Loss: 0.21395061652347983\n",
      "Epoch 6/100\n",
      "Training Loss: 0.3210498662200315\n",
      "Validation Loss: 0.2696942692295526\n",
      "Epoch 7/100\n",
      "Training Loss: 0.305980411965526\n",
      "Validation Loss: 0.1842286374564846\n",
      "Epoch 8/100\n",
      "Training Loss: 0.2959549975458884\n",
      "Validation Loss: 0.12959303034576955\n",
      "Epoch 9/100\n",
      "Training Loss: 0.2619218700283127\n",
      "Validation Loss: 0.11010939802098847\n",
      "Epoch 10/100\n",
      "Training Loss: 0.22562028355542646\n",
      "Validation Loss: 0.17222673817709797\n",
      "Epoch 11/100\n",
      "Training Loss: 0.2575443890172476\n",
      "Validation Loss: 0.2451061458831798\n",
      "Epoch 12/100\n",
      "Training Loss: 0.2423216919410203\n",
      "Validation Loss: 0.18983933043806803\n",
      "Epoch 13/100\n",
      "Training Loss: 0.2812554340253547\n",
      "Validation Loss: 0.14217697062062024\n",
      "Epoch 14/100\n",
      "Training Loss: 0.2873605224114493\n",
      "Validation Loss: 0.06765542195592905\n",
      "Epoch 15/100\n",
      "Training Loss: 0.24402240702896166\n",
      "Validation Loss: 0.2574314002303919\n",
      "Epoch 16/100\n",
      "Training Loss: 0.2361222572322509\n",
      "Validation Loss: 0.31060174881802566\n",
      "Epoch 17/100\n",
      "Training Loss: 0.20683986117887784\n",
      "Validation Loss: 0.09655842926220592\n",
      "Epoch 18/100\n",
      "Training Loss: 0.18687781187768265\n",
      "Validation Loss: 0.17950489795402164\n",
      "Epoch 19/100\n",
      "Training Loss: 0.2929887620766061\n",
      "Validation Loss: 0.11216790329889632\n",
      "Epoch 20/100\n",
      "Training Loss: 0.195321290516742\n",
      "Validation Loss: 0.14043363104913978\n",
      "Epoch 21/100\n",
      "Training Loss: 0.2359595960504767\n",
      "Validation Loss: 0.1153371976226621\n",
      "Epoch 22/100\n",
      "Training Loss: 0.1987581288832467\n",
      "Validation Loss: 0.10196511418744081\n",
      "Epoch 23/100\n",
      "Training Loss: 0.19481237838898183\n",
      "Validation Loss: 0.16096256943708379\n",
      "Epoch 24/100\n",
      "Training Loss: 0.20930683242998271\n",
      "Validation Loss: 0.08225194545919357\n",
      "Epoch 25/100\n",
      "Training Loss: 0.19386782350858764\n",
      "Validation Loss: 0.11120771046686798\n",
      "Epoch 26/100\n",
      "Training Loss: 0.1878355052268833\n",
      "Validation Loss: 0.12267141214087958\n",
      "Epoch 27/100\n",
      "Training Loss: 0.1678971916561749\n",
      "Validation Loss: 0.1312777659050252\n",
      "Epoch 28/100\n",
      "Training Loss: 0.17982058245586546\n",
      "Validation Loss: 0.16878648366722007\n",
      "Epoch 29/100\n",
      "Training Loss: 0.20719040119510448\n",
      "Validation Loss: 0.13994302146589388\n",
      "Epoch 30/100\n",
      "Training Loss: 0.20280026124078487\n",
      "Validation Loss: 0.266764997929792\n",
      "Epoch 31/100\n",
      "Training Loss: 0.194658866082965\n",
      "Validation Loss: 0.17045498363300218\n",
      "Epoch 32/100\n",
      "Training Loss: 0.23235311914753248\n",
      "Validation Loss: 0.20010206891040827\n",
      "Epoch 33/100\n",
      "Training Loss: 0.16558612152042043\n",
      "Validation Loss: 0.09911046550230948\n",
      "Epoch 34/100\n",
      "Training Loss: 0.19632326691874047\n",
      "Validation Loss: 0.09014284143457471\n",
      "Epoch 35/100\n",
      "Training Loss: 0.2066519874305615\n",
      "Validation Loss: 0.11926898133940662\n",
      "Epoch 36/100\n",
      "Training Loss: 0.17773429971700655\n",
      "Validation Loss: 0.1615977265632357\n",
      "Epoch 37/100\n",
      "Training Loss: 0.18669559468547328\n",
      "Validation Loss: 0.10701728170427245\n",
      "Epoch 38/100\n",
      "Training Loss: 0.1890893163081137\n",
      "Validation Loss: 0.1434211640280248\n",
      "Epoch 39/100\n",
      "Training Loss: 0.22828606597465687\n",
      "Validation Loss: 0.11995147466281982\n",
      "Epoch 40/100\n",
      "Training Loss: 0.14311867085595273\n",
      "Validation Loss: 0.08571822858561111\n",
      "Epoch 41/100\n",
      "Training Loss: 0.16673010128678248\n",
      "Validation Loss: 0.08507486571657799\n",
      "Epoch 42/100\n",
      "Training Loss: 0.13857224857724187\n",
      "Validation Loss: 0.09332019510862916\n",
      "Epoch 43/100\n",
      "Training Loss: 0.15582885543482242\n",
      "Validation Loss: 0.09785330940904312\n",
      "Epoch 44/100\n",
      "Training Loss: 0.1663047144561943\n",
      "Validation Loss: 0.07340377267784193\n",
      "Epoch 45/100\n",
      "Training Loss: 0.1747467642647854\n",
      "Validation Loss: 0.10016082020232611\n",
      "Epoch 46/100\n",
      "Training Loss: 0.1656320703349789\n",
      "Validation Loss: 0.09785969372408199\n",
      "Epoch 47/100\n",
      "Training Loss: 0.19853704606935288\n",
      "Validation Loss: 0.07298226472076627\n",
      "Epoch 48/100\n",
      "Training Loss: 0.1595980963769803\n",
      "Validation Loss: 0.14310066574743324\n",
      "Epoch 49/100\n",
      "Training Loss: 0.15601743785121067\n",
      "Validation Loss: 0.06093196678533548\n",
      "Epoch 50/100\n",
      "Training Loss: 0.1808256561515693\n",
      "Validation Loss: 0.09419714317832642\n",
      "Epoch 51/100\n",
      "Training Loss: 0.1776851212339161\n",
      "Validation Loss: 0.1348145889991287\n",
      "Epoch 52/100\n",
      "Training Loss: 0.168278988593926\n",
      "Validation Loss: 0.12155364145730423\n",
      "Epoch 53/100\n",
      "Training Loss: 0.1439500251184129\n",
      "Validation Loss: 0.1072567318645121\n",
      "Epoch 54/100\n",
      "Training Loss: 0.2004779270402066\n",
      "Validation Loss: 0.10651629562451737\n",
      "Epoch 55/100\n",
      "Training Loss: 0.17411158151858774\n",
      "Validation Loss: 0.0784676728779458\n",
      "Epoch 56/100\n",
      "Training Loss: 0.14871023735828118\n",
      "Validation Loss: 0.12386988544226116\n",
      "Epoch 57/100\n",
      "Training Loss: 0.17977211554186004\n",
      "Validation Loss: 0.06967618531032681\n",
      "Epoch 58/100\n",
      "Training Loss: 0.1483599572262283\n",
      "Validation Loss: 0.06176265552519785\n",
      "Epoch 59/100\n",
      "Training Loss: 0.19422054258073285\n",
      "Validation Loss: 0.08466167503495135\n",
      "Epoch 60/100\n",
      "Training Loss: 0.1424191582886608\n",
      "Validation Loss: 0.07166264785110064\n",
      "Epoch 61/100\n",
      "Training Loss: 0.16979815655400646\n",
      "Validation Loss: 0.06197181933220147\n",
      "Epoch 62/100\n",
      "Training Loss: 0.16386287574913921\n",
      "Validation Loss: 0.10621589659206272\n",
      "Epoch 63/100\n",
      "Training Loss: 0.17646179332536813\n",
      "Validation Loss: 0.11412106511076092\n",
      "Epoch 64/100\n",
      "Training Loss: 0.16264361522041224\n",
      "Validation Loss: 0.14012153640885255\n",
      "Epoch 65/100\n",
      "Training Loss: 0.15967138723652827\n",
      "Validation Loss: 0.06186911982216409\n",
      "Epoch 66/100\n",
      "Training Loss: 0.15191491232345122\n",
      "Validation Loss: 0.11629954369428819\n",
      "Epoch 67/100\n",
      "Training Loss: 0.15144043561085996\n",
      "Validation Loss: 0.07378483471211575\n",
      "Epoch 68/100\n",
      "Training Loss: 0.13456198539695183\n",
      "Validation Loss: 0.08191436248413654\n",
      "Epoch 69/100\n",
      "Training Loss: 0.15617843504342796\n",
      "Validation Loss: 0.10074802073308828\n",
      "Epoch 70/100\n",
      "Training Loss: 0.14567466916742147\n",
      "Validation Loss: 0.11684920423455691\n",
      "Epoch 71/100\n",
      "Training Loss: 0.13794338534462106\n",
      "Validation Loss: 0.0640269492849613\n",
      "Epoch 72/100\n",
      "Training Loss: 0.12812827198631063\n",
      "Validation Loss: 0.08326705811474375\n",
      "Epoch 73/100\n",
      "Training Loss: 0.147441807410171\n",
      "Validation Loss: 0.12106598484062507\n",
      "Epoch 74/100\n",
      "Training Loss: 0.14270303321245897\n",
      "Validation Loss: 0.07732820218308165\n",
      "Epoch 75/100\n",
      "Training Loss: 0.12033260385104028\n",
      "Validation Loss: 0.06813457455381143\n",
      "Epoch 76/100\n",
      "Training Loss: 0.13702591851482623\n",
      "Validation Loss: 0.07606282791218133\n",
      "Epoch 77/100\n",
      "Training Loss: 0.13220321049668793\n",
      "Validation Loss: 0.08477537356118418\n",
      "Epoch 78/100\n",
      "Training Loss: 0.16517063993750258\n",
      "Validation Loss: 0.06910335922615565\n",
      "Epoch 79/100\n",
      "Training Loss: 0.14283102388585284\n",
      "Validation Loss: 0.05305272602227675\n",
      "Epoch 80/100\n",
      "Training Loss: 0.13200218631177232\n",
      "Validation Loss: 0.0687200378539484\n",
      "Epoch 81/100\n",
      "Training Loss: 0.15308418508897859\n",
      "Validation Loss: 0.07510074320314222\n",
      "Epoch 82/100\n",
      "Training Loss: 0.1818424101422125\n",
      "Validation Loss: 0.05430951022462459\n",
      "Epoch 83/100\n",
      "Training Loss: 0.1586575537494749\n",
      "Validation Loss: 0.07580621983491553\n",
      "Epoch 84/100\n",
      "Training Loss: 0.14821026406150262\n",
      "Validation Loss: 0.10065023370996981\n",
      "Epoch 85/100\n",
      "Training Loss: 0.1408656232753907\n",
      "Validation Loss: 0.12038984441653892\n",
      "Epoch 86/100\n",
      "Training Loss: 0.15268600468745414\n",
      "Validation Loss: 0.0894528132682598\n",
      "Epoch 87/100\n",
      "Training Loss: 0.14558032155656653\n",
      "Validation Loss: 0.05247536737953566\n",
      "Epoch 88/100\n",
      "Training Loss: 0.1528973854068869\n",
      "Validation Loss: 0.05614215082818393\n",
      "Epoch 89/100\n",
      "Training Loss: 0.12944221195115704\n",
      "Validation Loss: 0.08031565585805378\n",
      "Epoch 90/100\n",
      "Training Loss: 0.1422565345336509\n",
      "Validation Loss: 0.08318814615776222\n",
      "Epoch 91/100\n",
      "Training Loss: 0.1314248002487919\n",
      "Validation Loss: 0.055069591583911046\n",
      "Epoch 92/100\n",
      "Training Loss: 0.1338770764678413\n",
      "Validation Loss: 0.08364573241331072\n",
      "Epoch 93/100\n",
      "Training Loss: 0.10663276718560967\n",
      "Validation Loss: 0.06838091704452555\n",
      "Epoch 94/100\n",
      "Training Loss: 0.1448205369674994\n",
      "Validation Loss: 0.044605660184994914\n",
      "Epoch 95/100\n",
      "Training Loss: 0.12956319351943035\n",
      "Validation Loss: 0.06333423954094579\n",
      "Epoch 96/100\n",
      "Training Loss: 0.1338888745073943\n",
      "Validation Loss: 0.07396459651529533\n",
      "Epoch 97/100\n",
      "Training Loss: 0.13052991390463894\n",
      "Validation Loss: 0.14487691773418662\n",
      "Epoch 98/100\n",
      "Training Loss: 0.11823970758776749\n",
      "Validation Loss: 0.1132060424146962\n",
      "Epoch 99/100\n",
      "Training Loss: 0.13064139366629604\n",
      "Validation Loss: 0.0570282136551583\n",
      "Epoch 100/100\n",
      "Training Loss: 0.12077495480268231\n",
      "Validation Loss: 0.09185799178862698\n",
      "Combination 27: Avg Training Loss = 0.17902381272102716, Avg Validation Loss = 0.11618858609339955\n",
      "Testing combination 28/48: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'sequence_length': 20, 'dropout_rate': 0.2}\n",
      "Truncated Train Data Shape: (70, 20, 1)\n",
      "Truncated Validation Data Shape: (10, 20, 1)\n",
      "    Trial 1/2 for combination 28/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.4893894701637659\n",
      "Validation Loss: 0.4082780101950312\n",
      "Epoch 2/100\n",
      "Training Loss: 0.40037582052720394\n",
      "Validation Loss: 0.2929947614914043\n",
      "Epoch 3/100\n",
      "Training Loss: 0.4629315177297417\n",
      "Validation Loss: 0.4066563815548155\n",
      "Epoch 4/100\n",
      "Training Loss: 0.35150979552703754\n",
      "Validation Loss: 0.23768410754654545\n",
      "Epoch 5/100\n",
      "Training Loss: 0.27383221220038534\n",
      "Validation Loss: 0.23096199701180858\n",
      "Epoch 6/100\n",
      "Training Loss: 0.331173876446124\n",
      "Validation Loss: 0.3329022475929253\n",
      "Epoch 7/100\n",
      "Training Loss: 0.2859238127314312\n",
      "Validation Loss: 0.1754613318088474\n",
      "Epoch 8/100\n",
      "Training Loss: 0.17371003956303765\n",
      "Validation Loss: 0.12436003444861868\n",
      "Epoch 9/100\n",
      "Training Loss: 0.20817290914959177\n",
      "Validation Loss: 0.2016221246331468\n",
      "Epoch 10/100\n",
      "Training Loss: 0.2706430940993171\n",
      "Validation Loss: 0.16313823541107578\n",
      "Epoch 11/100\n",
      "Training Loss: 0.22315549665496467\n",
      "Validation Loss: 0.13798363522599363\n",
      "Epoch 12/100\n",
      "Training Loss: 0.23980733145740554\n",
      "Validation Loss: 0.24191823853536318\n",
      "Epoch 13/100\n",
      "Training Loss: 0.20258205573562815\n",
      "Validation Loss: 0.08624788849963269\n",
      "Epoch 14/100\n",
      "Training Loss: 0.2847913197759282\n",
      "Validation Loss: 0.07725426620451865\n",
      "Epoch 15/100\n",
      "Training Loss: 0.20154806315842913\n",
      "Validation Loss: 0.1569357139890924\n",
      "Epoch 16/100\n",
      "Training Loss: 0.19339715323363973\n",
      "Validation Loss: 0.19513136980341944\n",
      "Epoch 17/100\n",
      "Training Loss: 0.16581565271956356\n",
      "Validation Loss: 0.1420657412505993\n",
      "Epoch 18/100\n",
      "Training Loss: 0.18434224861215742\n",
      "Validation Loss: 0.20507126652224877\n",
      "Epoch 19/100\n",
      "Training Loss: 0.21246806879641733\n",
      "Validation Loss: 0.12778578310123798\n",
      "Epoch 20/100\n",
      "Training Loss: 0.19697422720948535\n",
      "Validation Loss: 0.12787729868342487\n",
      "Epoch 21/100\n",
      "Training Loss: 0.21853318815852285\n",
      "Validation Loss: 0.15782462627923438\n",
      "Epoch 22/100\n",
      "Training Loss: 0.1915927662466032\n",
      "Validation Loss: 0.07867262100438979\n",
      "Epoch 23/100\n",
      "Training Loss: 0.17166754370648984\n",
      "Validation Loss: 0.2409745709964782\n",
      "Epoch 24/100\n",
      "Training Loss: 0.19047903500954602\n",
      "Validation Loss: 0.1454334150408061\n",
      "Epoch 25/100\n",
      "Training Loss: 0.18290846766655447\n",
      "Validation Loss: 0.14373339179886557\n",
      "Epoch 26/100\n",
      "Training Loss: 0.20019420664143603\n",
      "Validation Loss: 0.12782895886821874\n",
      "Epoch 27/100\n",
      "Training Loss: 0.1578910863319271\n",
      "Validation Loss: 0.11768289796844227\n",
      "Epoch 28/100\n",
      "Training Loss: 0.1775199274646271\n",
      "Validation Loss: 0.17346866353337206\n",
      "Epoch 29/100\n",
      "Training Loss: 0.18698311043397928\n",
      "Validation Loss: 0.061283870417778954\n",
      "Epoch 30/100\n",
      "Training Loss: 0.14972857386342261\n",
      "Validation Loss: 0.07977269147502246\n",
      "Epoch 31/100\n",
      "Training Loss: 0.18499388756622462\n",
      "Validation Loss: 0.1735469605554182\n",
      "Epoch 32/100\n",
      "Training Loss: 0.1527546508256194\n",
      "Validation Loss: 0.12710615951675241\n",
      "Epoch 33/100\n",
      "Training Loss: 0.1856125362391865\n",
      "Validation Loss: 0.1513962920235266\n",
      "Epoch 34/100\n",
      "Training Loss: 0.15589656577328528\n",
      "Validation Loss: 0.10270231612437922\n",
      "Epoch 35/100\n",
      "Training Loss: 0.17396845267481167\n",
      "Validation Loss: 0.08432414276693126\n",
      "Epoch 36/100\n",
      "Training Loss: 0.1514572990406521\n",
      "Validation Loss: 0.12345345842436084\n",
      "Epoch 37/100\n",
      "Training Loss: 0.19321034262838666\n",
      "Validation Loss: 0.06166039062312743\n",
      "Epoch 38/100\n",
      "Training Loss: 0.1373721538302848\n",
      "Validation Loss: 0.0803186190658221\n",
      "Epoch 39/100\n",
      "Training Loss: 0.1879526474677605\n",
      "Validation Loss: 0.10179219978419829\n",
      "Epoch 40/100\n",
      "Training Loss: 0.14836040679817977\n",
      "Validation Loss: 0.08909639262011558\n",
      "Epoch 41/100\n",
      "Training Loss: 0.16604984947042245\n",
      "Validation Loss: 0.10466991098579954\n",
      "Epoch 42/100\n",
      "Training Loss: 0.15733445332346951\n",
      "Validation Loss: 0.0816830514272825\n",
      "Epoch 43/100\n",
      "Training Loss: 0.1791082055773989\n",
      "Validation Loss: 0.10086544739377165\n",
      "Epoch 44/100\n",
      "Training Loss: 0.16314822947436727\n",
      "Validation Loss: 0.12943930294706513\n",
      "Epoch 45/100\n",
      "Training Loss: 0.18831479875712726\n",
      "Validation Loss: 0.09100413156124623\n",
      "Epoch 46/100\n",
      "Training Loss: 0.14748665376445838\n",
      "Validation Loss: 0.12727168255840304\n",
      "Epoch 47/100\n",
      "Training Loss: 0.19936213895531857\n",
      "Validation Loss: 0.13054307081825445\n",
      "Epoch 48/100\n",
      "Training Loss: 0.1763593830560137\n",
      "Validation Loss: 0.06935149629380946\n",
      "Epoch 49/100\n",
      "Training Loss: 0.14119854714331292\n",
      "Validation Loss: 0.08437743008872431\n",
      "Epoch 50/100\n",
      "Training Loss: 0.14322875366170446\n",
      "Validation Loss: 0.10954909558318711\n",
      "Epoch 51/100\n",
      "Training Loss: 0.15530482591503478\n",
      "Validation Loss: 0.10500399752952294\n",
      "Epoch 52/100\n",
      "Training Loss: 0.1509433060599953\n",
      "Validation Loss: 0.06347987221434774\n",
      "Epoch 53/100\n",
      "Training Loss: 0.12137359495637308\n",
      "Validation Loss: 0.13978655397808276\n",
      "Epoch 54/100\n",
      "Training Loss: 0.13836916903921073\n",
      "Validation Loss: 0.05853147352324197\n",
      "Epoch 55/100\n",
      "Training Loss: 0.15484201670951978\n",
      "Validation Loss: 0.1035011659065884\n",
      "Epoch 56/100\n",
      "Training Loss: 0.12940351922796284\n",
      "Validation Loss: 0.05271979030976\n",
      "Epoch 57/100\n",
      "Training Loss: 0.16515408545974436\n",
      "Validation Loss: 0.1112834313850863\n",
      "Epoch 58/100\n",
      "Training Loss: 0.14370007110198116\n",
      "Validation Loss: 0.050402969528234566\n",
      "Epoch 59/100\n",
      "Training Loss: 0.15906668140799793\n",
      "Validation Loss: 0.07464486260461303\n",
      "Epoch 60/100\n",
      "Training Loss: 0.14774374867595036\n",
      "Validation Loss: 0.0676098181035318\n",
      "Epoch 61/100\n",
      "Training Loss: 0.16665316174509853\n",
      "Validation Loss: 0.08362848255514405\n",
      "Epoch 62/100\n",
      "Training Loss: 0.1378661862136069\n",
      "Validation Loss: 0.11391224627606433\n",
      "Epoch 63/100\n",
      "Training Loss: 0.1362854940877803\n",
      "Validation Loss: 0.09382627269821889\n",
      "Epoch 64/100\n",
      "Training Loss: 0.145131217010435\n",
      "Validation Loss: 0.08029899215829661\n",
      "Epoch 65/100\n",
      "Training Loss: 0.13646851911847824\n",
      "Validation Loss: 0.09951414249169985\n",
      "Epoch 66/100\n",
      "Training Loss: 0.12952266002170912\n",
      "Validation Loss: 0.07071368749777543\n",
      "Epoch 67/100\n",
      "Training Loss: 0.1547871659038337\n",
      "Validation Loss: 0.08447642427203267\n",
      "Epoch 68/100\n",
      "Training Loss: 0.13280081748965739\n",
      "Validation Loss: 0.06850678595379588\n",
      "Epoch 69/100\n",
      "Training Loss: 0.1404403537386849\n",
      "Validation Loss: 0.07951748109084096\n",
      "Epoch 70/100\n",
      "Training Loss: 0.1655883144157586\n",
      "Validation Loss: 0.09321577477767318\n",
      "Epoch 71/100\n",
      "Training Loss: 0.1452797450327715\n",
      "Validation Loss: 0.13339228509224293\n",
      "Epoch 72/100\n",
      "Training Loss: 0.13941164631390962\n",
      "Validation Loss: 0.05097798465179285\n",
      "Epoch 73/100\n",
      "Training Loss: 0.1619622395314456\n",
      "Validation Loss: 0.07043699675813256\n",
      "Epoch 74/100\n",
      "Training Loss: 0.14285558589848935\n",
      "Validation Loss: 0.06712590929321595\n",
      "Epoch 75/100\n",
      "Training Loss: 0.11895455059909213\n",
      "Validation Loss: 0.053324052334000525\n",
      "Epoch 76/100\n",
      "Training Loss: 0.10397432209184972\n",
      "Validation Loss: 0.05028202405227049\n",
      "Epoch 77/100\n",
      "Training Loss: 0.13536820266432903\n",
      "Validation Loss: 0.06150487183278773\n",
      "Epoch 78/100\n",
      "Training Loss: 0.11275868886179395\n",
      "Validation Loss: 0.07425662190813904\n",
      "Epoch 79/100\n",
      "Training Loss: 0.12768741925500318\n",
      "Validation Loss: 0.045382503962004196\n",
      "Epoch 80/100\n",
      "Training Loss: 0.12919801569442724\n",
      "Validation Loss: 0.05225338977406992\n",
      "Epoch 81/100\n",
      "Training Loss: 0.11053403708456333\n",
      "Validation Loss: 0.10281727819410613\n",
      "Epoch 82/100\n",
      "Training Loss: 0.09651711133032607\n",
      "Validation Loss: 0.11567446630752831\n",
      "Epoch 83/100\n",
      "Training Loss: 0.13181623205009627\n",
      "Validation Loss: 0.09609906430265298\n",
      "Epoch 84/100\n",
      "Training Loss: 0.14530531237053249\n",
      "Validation Loss: 0.04131279069954822\n",
      "Epoch 85/100\n",
      "Training Loss: 0.11657155089155309\n",
      "Validation Loss: 0.1483141434221908\n",
      "Epoch 86/100\n",
      "Training Loss: 0.1170907997924319\n",
      "Validation Loss: 0.06938706548168355\n",
      "Epoch 87/100\n",
      "Training Loss: 0.13787419432600947\n",
      "Validation Loss: 0.06762876569988494\n",
      "Epoch 88/100\n",
      "Training Loss: 0.10563481832566562\n",
      "Validation Loss: 0.05471533409049902\n",
      "Epoch 89/100\n",
      "Training Loss: 0.13992069068555174\n",
      "Validation Loss: 0.08825312710819232\n",
      "Epoch 90/100\n",
      "Training Loss: 0.12090416166454594\n",
      "Validation Loss: 0.07512282018308501\n",
      "Epoch 91/100\n",
      "Training Loss: 0.12498171886549839\n",
      "Validation Loss: 0.0676729385913762\n",
      "Epoch 92/100\n",
      "Training Loss: 0.13850536464811705\n",
      "Validation Loss: 0.051025211663119185\n",
      "Epoch 93/100\n",
      "Training Loss: 0.12434105076989993\n",
      "Validation Loss: 0.07105162733317552\n",
      "Epoch 94/100\n",
      "Training Loss: 0.1250490992126046\n",
      "Validation Loss: 0.0847512295377496\n",
      "Epoch 95/100\n",
      "Training Loss: 0.10314818519366521\n",
      "Validation Loss: 0.08459533892154969\n",
      "Epoch 96/100\n",
      "Training Loss: 0.12939312363081437\n",
      "Validation Loss: 0.06474235651400365\n",
      "Epoch 97/100\n",
      "Training Loss: 0.1257491038841722\n",
      "Validation Loss: 0.050932626428465556\n",
      "Epoch 98/100\n",
      "Training Loss: 0.12498423828269882\n",
      "Validation Loss: 0.08588411954077926\n",
      "Epoch 99/100\n",
      "Training Loss: 0.11797914481556401\n",
      "Validation Loss: 0.052211706635048774\n",
      "Epoch 100/100\n",
      "Training Loss: 0.10529438164760839\n",
      "Validation Loss: 0.05340252865776875\n",
      "    Trial 2/2 for combination 28/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.5256145110278599\n",
      "Validation Loss: 0.6981813325041302\n",
      "Epoch 2/100\n",
      "Training Loss: 0.3434140238427504\n",
      "Validation Loss: 0.34523539663688696\n",
      "Epoch 3/100\n",
      "Training Loss: 0.38912543341262745\n",
      "Validation Loss: 0.2111590635514924\n",
      "Epoch 4/100\n",
      "Training Loss: 0.39121769239600623\n",
      "Validation Loss: 0.34614163167661727\n",
      "Epoch 5/100\n",
      "Training Loss: 0.3262176793552024\n",
      "Validation Loss: 0.15993697787884223\n",
      "Epoch 6/100\n",
      "Training Loss: 0.28537462158868754\n",
      "Validation Loss: 0.2397018313117544\n",
      "Epoch 7/100\n",
      "Training Loss: 0.25168588885320936\n",
      "Validation Loss: 0.1309019941265938\n",
      "Epoch 8/100\n",
      "Training Loss: 0.22846151228690337\n",
      "Validation Loss: 0.16487486097567253\n",
      "Epoch 9/100\n",
      "Training Loss: 0.28752181218342304\n",
      "Validation Loss: 0.27232360230809666\n",
      "Epoch 10/100\n",
      "Training Loss: 0.23742919472186255\n",
      "Validation Loss: 0.15762079324660122\n",
      "Epoch 11/100\n",
      "Training Loss: 0.2085874128837553\n",
      "Validation Loss: 0.18404722246736302\n",
      "Epoch 12/100\n",
      "Training Loss: 0.23440101671142827\n",
      "Validation Loss: 0.13502061892978784\n",
      "Epoch 13/100\n",
      "Training Loss: 0.23370873944576798\n",
      "Validation Loss: 0.10635328297755076\n",
      "Epoch 14/100\n",
      "Training Loss: 0.29041050559715875\n",
      "Validation Loss: 0.11775302377800775\n",
      "Epoch 15/100\n",
      "Training Loss: 0.24754839871340004\n",
      "Validation Loss: 0.07771434042674862\n",
      "Epoch 16/100\n",
      "Training Loss: 0.20970532963058863\n",
      "Validation Loss: 0.11582732727418008\n",
      "Epoch 17/100\n",
      "Training Loss: 0.2526226118193181\n",
      "Validation Loss: 0.19655406721892363\n",
      "Epoch 18/100\n",
      "Training Loss: 0.18651514794076343\n",
      "Validation Loss: 0.12024717702786318\n",
      "Epoch 19/100\n",
      "Training Loss: 0.2220534469733162\n",
      "Validation Loss: 0.19374252116267116\n",
      "Epoch 20/100\n",
      "Training Loss: 0.1734779073667142\n",
      "Validation Loss: 0.11498673208189787\n",
      "Epoch 21/100\n",
      "Training Loss: 0.2041223859083268\n",
      "Validation Loss: 0.18796369490004322\n",
      "Epoch 22/100\n",
      "Training Loss: 0.19581573624546153\n",
      "Validation Loss: 0.18737316223325054\n",
      "Epoch 23/100\n",
      "Training Loss: 0.22416160739957247\n",
      "Validation Loss: 0.20772070894818198\n",
      "Epoch 24/100\n",
      "Training Loss: 0.16942576714170052\n",
      "Validation Loss: 0.19694570292814986\n",
      "Epoch 25/100\n",
      "Training Loss: 0.2077078085417494\n",
      "Validation Loss: 0.14704780402169465\n",
      "Epoch 26/100\n",
      "Training Loss: 0.20120149077355282\n",
      "Validation Loss: 0.16545608016785684\n",
      "Epoch 27/100\n",
      "Training Loss: 0.22313191047538836\n",
      "Validation Loss: 0.08339608263404281\n",
      "Epoch 28/100\n",
      "Training Loss: 0.15219966583183728\n",
      "Validation Loss: 0.08722199169113656\n",
      "Epoch 29/100\n",
      "Training Loss: 0.16842602403956616\n",
      "Validation Loss: 0.2042720385859403\n",
      "Epoch 30/100\n",
      "Training Loss: 0.1561779618741349\n",
      "Validation Loss: 0.05909099188034056\n",
      "Epoch 31/100\n",
      "Training Loss: 0.14333026141020444\n",
      "Validation Loss: 0.1198877335078257\n",
      "Epoch 32/100\n",
      "Training Loss: 0.17964486220500195\n",
      "Validation Loss: 0.08619765307610301\n",
      "Epoch 33/100\n",
      "Training Loss: 0.20442058297513185\n",
      "Validation Loss: 0.12034664387575411\n",
      "Epoch 34/100\n",
      "Training Loss: 0.13872658669363833\n",
      "Validation Loss: 0.13103258799444895\n",
      "Epoch 35/100\n",
      "Training Loss: 0.1828323082255276\n",
      "Validation Loss: 0.06576169361658338\n",
      "Epoch 36/100\n",
      "Training Loss: 0.20068824550385958\n",
      "Validation Loss: 0.08320553819307691\n",
      "Epoch 37/100\n",
      "Training Loss: 0.2097215447066282\n",
      "Validation Loss: 0.1492022597024975\n",
      "Epoch 38/100\n",
      "Training Loss: 0.16450763201733817\n",
      "Validation Loss: 0.04321733219844344\n",
      "Epoch 39/100\n",
      "Training Loss: 0.16273559752331185\n",
      "Validation Loss: 0.09326173740916172\n",
      "Epoch 40/100\n",
      "Training Loss: 0.17816994896553132\n",
      "Validation Loss: 0.1418883476810681\n",
      "Epoch 41/100\n",
      "Training Loss: 0.1932274701485022\n",
      "Validation Loss: 0.10001756789938765\n",
      "Epoch 42/100\n",
      "Training Loss: 0.1713914792527915\n",
      "Validation Loss: 0.10361523982172975\n",
      "Epoch 43/100\n",
      "Training Loss: 0.1859937967030072\n",
      "Validation Loss: 0.1734560675521353\n",
      "Epoch 44/100\n",
      "Training Loss: 0.16615756083156843\n",
      "Validation Loss: 0.08770175086052663\n",
      "Epoch 45/100\n",
      "Training Loss: 0.17972247349698775\n",
      "Validation Loss: 0.2158642887686733\n",
      "Epoch 46/100\n",
      "Training Loss: 0.1778785604550804\n",
      "Validation Loss: 0.0864600551535527\n",
      "Epoch 47/100\n",
      "Training Loss: 0.13341594038057047\n",
      "Validation Loss: 0.1025439847036768\n",
      "Epoch 48/100\n",
      "Training Loss: 0.1601093564776736\n",
      "Validation Loss: 0.11579523466106507\n",
      "Epoch 49/100\n",
      "Training Loss: 0.15099886160425438\n",
      "Validation Loss: 0.09101901715797484\n",
      "Epoch 50/100\n",
      "Training Loss: 0.13211619166707694\n",
      "Validation Loss: 0.11130923011744838\n",
      "Epoch 51/100\n",
      "Training Loss: 0.14372392431318826\n",
      "Validation Loss: 0.10565875154297126\n",
      "Epoch 52/100\n",
      "Training Loss: 0.18006405075831072\n",
      "Validation Loss: 0.11164830099814005\n",
      "Epoch 53/100\n",
      "Training Loss: 0.1701054708644651\n",
      "Validation Loss: 0.09572932950782051\n",
      "Epoch 54/100\n",
      "Training Loss: 0.14528583241785917\n",
      "Validation Loss: 0.06950682557137808\n",
      "Epoch 55/100\n",
      "Training Loss: 0.1497280941802044\n",
      "Validation Loss: 0.06501984021744264\n",
      "Epoch 56/100\n",
      "Training Loss: 0.1349165827702308\n",
      "Validation Loss: 0.10547574711412114\n",
      "Epoch 57/100\n",
      "Training Loss: 0.16653307210857576\n",
      "Validation Loss: 0.05727988018702555\n",
      "Epoch 58/100\n",
      "Training Loss: 0.14408821151505258\n",
      "Validation Loss: 0.12988026632310318\n",
      "Epoch 59/100\n",
      "Training Loss: 0.16839776692862155\n",
      "Validation Loss: 0.11622308991405106\n",
      "Epoch 60/100\n",
      "Training Loss: 0.12686562179555874\n",
      "Validation Loss: 0.06332363336235868\n",
      "Epoch 61/100\n",
      "Training Loss: 0.14643620655537548\n",
      "Validation Loss: 0.09622357066262151\n",
      "Epoch 62/100\n",
      "Training Loss: 0.14337674084262564\n",
      "Validation Loss: 0.07503279417822464\n",
      "Epoch 63/100\n",
      "Training Loss: 0.1333514176281454\n",
      "Validation Loss: 0.06572607402140489\n",
      "Epoch 64/100\n",
      "Training Loss: 0.15470118080499415\n",
      "Validation Loss: 0.0757695536788848\n",
      "Epoch 65/100\n",
      "Training Loss: 0.16739764646522912\n",
      "Validation Loss: 0.09138817462289768\n",
      "Epoch 66/100\n",
      "Training Loss: 0.132292926290696\n",
      "Validation Loss: 0.13586740034631156\n",
      "Epoch 67/100\n",
      "Training Loss: 0.13943190422744756\n",
      "Validation Loss: 0.07453888833198591\n",
      "Epoch 68/100\n",
      "Training Loss: 0.12730108342943955\n",
      "Validation Loss: 0.09206586694395293\n",
      "Epoch 69/100\n",
      "Training Loss: 0.17791855039337146\n",
      "Validation Loss: 0.12433049771861429\n",
      "Epoch 70/100\n",
      "Training Loss: 0.13728232927191245\n",
      "Validation Loss: 0.06862379842470315\n",
      "Epoch 71/100\n",
      "Training Loss: 0.14438623537470177\n",
      "Validation Loss: 0.08483684408435958\n",
      "Epoch 72/100\n",
      "Training Loss: 0.131451541135894\n",
      "Validation Loss: 0.06029165607835951\n",
      "Epoch 73/100\n",
      "Training Loss: 0.13368565616333494\n",
      "Validation Loss: 0.06655367624553774\n",
      "Epoch 74/100\n",
      "Training Loss: 0.15815956196200529\n",
      "Validation Loss: 0.06630292759998616\n",
      "Epoch 75/100\n",
      "Training Loss: 0.14596101260696692\n",
      "Validation Loss: 0.08542224349299427\n",
      "Epoch 76/100\n",
      "Training Loss: 0.14638222405411352\n",
      "Validation Loss: 0.11181516071846953\n",
      "Epoch 77/100\n",
      "Training Loss: 0.12766408368330293\n",
      "Validation Loss: 0.06599492960243372\n",
      "Epoch 78/100\n",
      "Training Loss: 0.14073056351410768\n",
      "Validation Loss: 0.06487413734654317\n",
      "Epoch 79/100\n",
      "Training Loss: 0.13662115215769757\n",
      "Validation Loss: 0.06815499819290605\n",
      "Epoch 80/100\n",
      "Training Loss: 0.13476121247995354\n",
      "Validation Loss: 0.08293777399028793\n",
      "Epoch 81/100\n",
      "Training Loss: 0.15023615548954053\n",
      "Validation Loss: 0.07192408255714673\n",
      "Epoch 82/100\n",
      "Training Loss: 0.12247343098090184\n",
      "Validation Loss: 0.06167393943474625\n",
      "Epoch 83/100\n",
      "Training Loss: 0.13488677930881135\n",
      "Validation Loss: 0.07420194860305891\n",
      "Epoch 84/100\n",
      "Training Loss: 0.1579540412906915\n",
      "Validation Loss: 0.10574137362137263\n",
      "Epoch 85/100\n",
      "Training Loss: 0.13501123530316336\n",
      "Validation Loss: 0.0628056622853079\n",
      "Epoch 86/100\n",
      "Training Loss: 0.12618814491459582\n",
      "Validation Loss: 0.06924537606466347\n",
      "Epoch 87/100\n",
      "Training Loss: 0.11629880639162544\n",
      "Validation Loss: 0.06635325604577202\n",
      "Epoch 88/100\n",
      "Training Loss: 0.1333136406972062\n",
      "Validation Loss: 0.09862422770846778\n",
      "Epoch 89/100\n",
      "Training Loss: 0.11769442200594159\n",
      "Validation Loss: 0.08157581301593489\n",
      "Epoch 90/100\n",
      "Training Loss: 0.1416081407375835\n",
      "Validation Loss: 0.05275734012544135\n",
      "Epoch 91/100\n",
      "Training Loss: 0.1263418667538819\n",
      "Validation Loss: 0.06515366430150235\n",
      "Epoch 92/100\n",
      "Training Loss: 0.11290027761293493\n",
      "Validation Loss: 0.11250424430134072\n",
      "Epoch 93/100\n",
      "Training Loss: 0.13229308818917898\n",
      "Validation Loss: 0.0736774728727551\n",
      "Epoch 94/100\n",
      "Training Loss: 0.11996294102285336\n",
      "Validation Loss: 0.10733719046124257\n",
      "Epoch 95/100\n",
      "Training Loss: 0.13267183777239663\n",
      "Validation Loss: 0.10901786189915105\n",
      "Epoch 96/100\n",
      "Training Loss: 0.12405228991813355\n",
      "Validation Loss: 0.08654060629766722\n",
      "Epoch 97/100\n",
      "Training Loss: 0.12244139210404043\n",
      "Validation Loss: 0.09122322168885688\n",
      "Epoch 98/100\n",
      "Training Loss: 0.12780991933882396\n",
      "Validation Loss: 0.059493228596426403\n",
      "Epoch 99/100\n",
      "Training Loss: 0.12264065039717205\n",
      "Validation Loss: 0.07629276757082791\n",
      "Epoch 100/100\n",
      "Training Loss: 0.11355499752128174\n",
      "Validation Loss: 0.06263950250255233\n",
      "Combination 28: Avg Training Loss = 0.17493145045713027, Avg Validation Loss = 0.11702069263178866\n",
      "Testing combination 29/48: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'sequence_length': 50, 'dropout_rate': 0.1}\n",
      "Truncated Train Data Shape: (70, 50, 1)\n",
      "Truncated Validation Data Shape: (10, 50, 1)\n",
      "    Trial 1/2 for combination 29/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.34517752816973407\n",
      "Validation Loss: 0.17619789705947003\n",
      "Epoch 2/100\n",
      "Training Loss: 0.2879760865421053\n",
      "Validation Loss: 0.25097862299818713\n",
      "Epoch 3/100\n",
      "Training Loss: 0.31147229006635513\n",
      "Validation Loss: 0.14306739727266116\n",
      "Epoch 4/100\n",
      "Training Loss: 0.24059019333141382\n",
      "Validation Loss: 0.09952711342088269\n",
      "Epoch 5/100\n",
      "Training Loss: 0.23671042225739672\n",
      "Validation Loss: 0.0969904792752867\n",
      "Epoch 6/100\n",
      "Training Loss: 0.2513921353481424\n",
      "Validation Loss: 0.1244498289809816\n",
      "Epoch 7/100\n",
      "Training Loss: 0.2361792316624492\n",
      "Validation Loss: 0.16477007075347946\n",
      "Epoch 8/100\n",
      "Training Loss: 0.16639095309466012\n",
      "Validation Loss: 0.08413492943751774\n",
      "Epoch 9/100\n",
      "Training Loss: 0.22782358084970517\n",
      "Validation Loss: 0.2070410798618449\n",
      "Epoch 10/100\n",
      "Training Loss: 0.154938447686012\n",
      "Validation Loss: 0.1511207350760741\n",
      "Epoch 11/100\n",
      "Training Loss: 0.20624760330536793\n",
      "Validation Loss: 0.12368652534521034\n",
      "Epoch 12/100\n",
      "Training Loss: 0.15342875853467164\n",
      "Validation Loss: 0.06514209857589562\n",
      "Epoch 13/100\n",
      "Training Loss: 0.20065630028929063\n",
      "Validation Loss: 0.1856090568230444\n",
      "Epoch 14/100\n",
      "Training Loss: 0.17513678005686215\n",
      "Validation Loss: 0.06318077788104996\n",
      "Epoch 15/100\n",
      "Training Loss: 0.1548328688684236\n",
      "Validation Loss: 0.278316929967399\n",
      "Epoch 16/100\n",
      "Training Loss: 0.17854374967060352\n",
      "Validation Loss: 0.09679155022101481\n",
      "Epoch 17/100\n",
      "Training Loss: 0.18406804386073541\n",
      "Validation Loss: 0.06135072721345811\n",
      "Epoch 18/100\n",
      "Training Loss: 0.15493183574106736\n",
      "Validation Loss: 0.11356725987687126\n",
      "Epoch 19/100\n",
      "Training Loss: 0.1872002796702265\n",
      "Validation Loss: 0.11213926188577175\n",
      "Epoch 20/100\n",
      "Training Loss: 0.173423698439193\n",
      "Validation Loss: 0.13191713115763232\n",
      "Epoch 21/100\n",
      "Training Loss: 0.1441931441202171\n",
      "Validation Loss: 0.1359285382464359\n",
      "Epoch 22/100\n",
      "Training Loss: 0.15288647085402354\n",
      "Validation Loss: 0.07258904389309738\n",
      "Epoch 23/100\n",
      "Training Loss: 0.18021314693367912\n",
      "Validation Loss: 0.05574243757209683\n",
      "Epoch 24/100\n",
      "Training Loss: 0.1558294110605145\n",
      "Validation Loss: 0.13555081403174776\n",
      "Epoch 25/100\n",
      "Training Loss: 0.14427115968827478\n",
      "Validation Loss: 0.058001505784329055\n",
      "Epoch 26/100\n",
      "Training Loss: 0.12278620516362382\n",
      "Validation Loss: 0.0706550577806788\n",
      "Epoch 27/100\n",
      "Training Loss: 0.1632648517428831\n",
      "Validation Loss: 0.07287583398463623\n",
      "Epoch 28/100\n",
      "Training Loss: 0.1502354695770201\n",
      "Validation Loss: 0.04926625466292546\n",
      "Epoch 29/100\n",
      "Training Loss: 0.12834242634260598\n",
      "Validation Loss: 0.05141765465058116\n",
      "Epoch 30/100\n",
      "Training Loss: 0.16171905502167602\n",
      "Validation Loss: 0.0923893137389899\n",
      "Epoch 31/100\n",
      "Training Loss: 0.1614999261513733\n",
      "Validation Loss: 0.08574247334179677\n",
      "Epoch 32/100\n",
      "Training Loss: 0.17028533077978697\n",
      "Validation Loss: 0.05955323879868907\n",
      "Epoch 33/100\n",
      "Training Loss: 0.1391919739427725\n",
      "Validation Loss: 0.12310571442749274\n",
      "Epoch 34/100\n",
      "Training Loss: 0.13641064583065957\n",
      "Validation Loss: 0.05962162134619401\n",
      "Epoch 35/100\n",
      "Training Loss: 0.1402149511979056\n",
      "Validation Loss: 0.10725888642460372\n",
      "Epoch 36/100\n",
      "Training Loss: 0.15470920973724842\n",
      "Validation Loss: 0.08550823296969953\n",
      "Epoch 37/100\n",
      "Training Loss: 0.13387603008269058\n",
      "Validation Loss: 0.10400071838079225\n",
      "Epoch 38/100\n",
      "Training Loss: 0.13294860878842787\n",
      "Validation Loss: 0.09678972084803951\n",
      "Epoch 39/100\n",
      "Training Loss: 0.16982166983753616\n",
      "Validation Loss: 0.051574478138751434\n",
      "Epoch 40/100\n",
      "Training Loss: 0.18824004633422398\n",
      "Validation Loss: 0.0613394948346608\n",
      "Epoch 41/100\n",
      "Training Loss: 0.14563172038628144\n",
      "Validation Loss: 0.06815563859372702\n",
      "Epoch 42/100\n",
      "Training Loss: 0.1507300804493678\n",
      "Validation Loss: 0.04525705788023688\n",
      "Epoch 43/100\n",
      "Training Loss: 0.14798268674920825\n",
      "Validation Loss: 0.08257534972005628\n",
      "Epoch 44/100\n",
      "Training Loss: 0.13157851576672383\n",
      "Validation Loss: 0.1113218741716746\n",
      "Epoch 45/100\n",
      "Training Loss: 0.13215404820347287\n",
      "Validation Loss: 0.08084133439550244\n",
      "Epoch 46/100\n",
      "Training Loss: 0.1505387510624461\n",
      "Validation Loss: 0.08781822857213982\n",
      "Epoch 47/100\n",
      "Training Loss: 0.13352484161285166\n",
      "Validation Loss: 0.17685544325232255\n",
      "Epoch 48/100\n",
      "Training Loss: 0.1699701409519657\n",
      "Validation Loss: 0.11922326708937081\n",
      "Epoch 49/100\n",
      "Training Loss: 0.12976919132588055\n",
      "Validation Loss: 0.06644263511870593\n",
      "Epoch 50/100\n",
      "Training Loss: 0.12930499763856518\n",
      "Validation Loss: 0.0570853869559846\n",
      "Epoch 51/100\n",
      "Training Loss: 0.1508528382897852\n",
      "Validation Loss: 0.09593198935934091\n",
      "Epoch 52/100\n",
      "Training Loss: 0.1171474654231866\n",
      "Validation Loss: 0.06540365559127767\n",
      "Epoch 53/100\n",
      "Training Loss: 0.13928676890495426\n",
      "Validation Loss: 0.08972447886099222\n",
      "Epoch 54/100\n",
      "Training Loss: 0.15477941810164428\n",
      "Validation Loss: 0.05653560958337778\n",
      "Epoch 55/100\n",
      "Training Loss: 0.12432007383826245\n",
      "Validation Loss: 0.07547602931694722\n",
      "Epoch 56/100\n",
      "Training Loss: 0.1553066735035983\n",
      "Validation Loss: 0.060740213992202655\n",
      "Epoch 57/100\n",
      "Training Loss: 0.1324693140074046\n",
      "Validation Loss: 0.08036554461767963\n",
      "Epoch 58/100\n",
      "Training Loss: 0.14029958533968165\n",
      "Validation Loss: 0.1317508679534284\n",
      "Epoch 59/100\n",
      "Training Loss: 0.14892747628185027\n",
      "Validation Loss: 0.05932450338689384\n",
      "Epoch 60/100\n",
      "Training Loss: 0.14216433206756812\n",
      "Validation Loss: 0.03765317443003346\n",
      "Epoch 61/100\n",
      "Training Loss: 0.15453071645272942\n",
      "Validation Loss: 0.07800659521147955\n",
      "Epoch 62/100\n",
      "Training Loss: 0.12329006369042492\n",
      "Validation Loss: 0.05578965064300647\n",
      "Epoch 63/100\n",
      "Training Loss: 0.13748426249464357\n",
      "Validation Loss: 0.06592849088036754\n",
      "Epoch 64/100\n",
      "Training Loss: 0.11930629410078024\n",
      "Validation Loss: 0.08416862713584015\n",
      "Epoch 65/100\n",
      "Training Loss: 0.15761554666282324\n",
      "Validation Loss: 0.12113354208963656\n",
      "Epoch 66/100\n",
      "Training Loss: 0.1269200175280871\n",
      "Validation Loss: 0.03141391513384753\n",
      "Epoch 67/100\n",
      "Training Loss: 0.14445591825308046\n",
      "Validation Loss: 0.05501429402156259\n",
      "Epoch 68/100\n",
      "Training Loss: 0.1227745951620948\n",
      "Validation Loss: 0.13781704792318245\n",
      "Epoch 69/100\n",
      "Training Loss: 0.13043917603416544\n",
      "Validation Loss: 0.06829855632820352\n",
      "Epoch 70/100\n",
      "Training Loss: 0.13103243300640624\n",
      "Validation Loss: 0.09393022502485542\n",
      "Epoch 71/100\n",
      "Training Loss: 0.12323426162989176\n",
      "Validation Loss: 0.06992185205291616\n",
      "Epoch 72/100\n",
      "Training Loss: 0.13375489097230797\n",
      "Validation Loss: 0.06391402094905455\n",
      "Epoch 73/100\n",
      "Training Loss: 0.13704894819215038\n",
      "Validation Loss: 0.043580607930688096\n",
      "Epoch 74/100\n",
      "Training Loss: 0.13342847289647952\n",
      "Validation Loss: 0.05688468826929183\n",
      "Epoch 75/100\n",
      "Training Loss: 0.13480839270614423\n",
      "Validation Loss: 0.060941694647245984\n",
      "Epoch 76/100\n",
      "Training Loss: 0.11373628832564599\n",
      "Validation Loss: 0.04808824741189442\n",
      "Epoch 77/100\n",
      "Training Loss: 0.11011635531080227\n",
      "Validation Loss: 0.07931475004376225\n",
      "Epoch 78/100\n",
      "Training Loss: 0.15718006446866858\n",
      "Validation Loss: 0.05163825657690487\n",
      "Epoch 79/100\n",
      "Training Loss: 0.13236435589444828\n",
      "Validation Loss: 0.06546454038103602\n",
      "Epoch 80/100\n",
      "Training Loss: 0.12999639293906967\n",
      "Validation Loss: 0.0671895268728499\n",
      "Epoch 81/100\n",
      "Training Loss: 0.1393495066958176\n",
      "Validation Loss: 0.0686254826642739\n",
      "Epoch 82/100\n",
      "Training Loss: 0.12496612237920386\n",
      "Validation Loss: 0.0786928439855474\n",
      "Epoch 83/100\n",
      "Training Loss: 0.13338456852682995\n",
      "Validation Loss: 0.0655010110418747\n",
      "Epoch 84/100\n",
      "Training Loss: 0.1226439035161396\n",
      "Validation Loss: 0.04203510851914362\n",
      "Epoch 85/100\n",
      "Training Loss: 0.135759295002425\n",
      "Validation Loss: 0.07194308630139969\n",
      "Epoch 86/100\n",
      "Training Loss: 0.09226618861151059\n",
      "Validation Loss: 0.09630385447497103\n",
      "Epoch 87/100\n",
      "Training Loss: 0.12046098970466278\n",
      "Validation Loss: 0.045397860186489794\n",
      "Epoch 88/100\n",
      "Training Loss: 0.10803852875274735\n",
      "Validation Loss: 0.05076358115538082\n",
      "Epoch 89/100\n",
      "Training Loss: 0.11326528544914417\n",
      "Validation Loss: 0.07095535772349101\n",
      "Epoch 90/100\n",
      "Training Loss: 0.11327925325148702\n",
      "Validation Loss: 0.05291312727365653\n",
      "Epoch 91/100\n",
      "Training Loss: 0.12528642566611356\n",
      "Validation Loss: 0.05980534204227613\n",
      "Epoch 92/100\n",
      "Training Loss: 0.12222432291474024\n",
      "Validation Loss: 0.05070079643246027\n",
      "Epoch 93/100\n",
      "Training Loss: 0.13267474656049946\n",
      "Validation Loss: 0.07347380076639291\n",
      "Epoch 94/100\n",
      "Training Loss: 0.12375163042034262\n",
      "Validation Loss: 0.06409307770247835\n",
      "Epoch 95/100\n",
      "Training Loss: 0.12258426127266461\n",
      "Validation Loss: 0.06786010381223757\n",
      "Epoch 96/100\n",
      "Training Loss: 0.10875915740896011\n",
      "Validation Loss: 0.03643462835249227\n",
      "Epoch 97/100\n",
      "Training Loss: 0.12497688792991732\n",
      "Validation Loss: 0.1685383807527026\n",
      "Epoch 98/100\n",
      "Training Loss: 0.12018602340383959\n",
      "Validation Loss: 0.06877221267938574\n",
      "Epoch 99/100\n",
      "Training Loss: 0.13397445658851703\n",
      "Validation Loss: 0.0693981644647348\n",
      "Epoch 100/100\n",
      "Training Loss: 0.12398924048113388\n",
      "Validation Loss: 0.06736863182881944\n",
      "    Trial 2/2 for combination 29/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.40908116519580034\n",
      "Validation Loss: 0.18537343483118374\n",
      "Epoch 2/100\n",
      "Training Loss: 0.3578969969548616\n",
      "Validation Loss: 0.3213070744264825\n",
      "Epoch 3/100\n",
      "Training Loss: 0.36355877825952726\n",
      "Validation Loss: 0.1349628085952305\n",
      "Epoch 4/100\n",
      "Training Loss: 0.26299965510071294\n",
      "Validation Loss: 0.17075328986887778\n",
      "Epoch 5/100\n",
      "Training Loss: 0.23698968826233938\n",
      "Validation Loss: 0.12233365330789572\n",
      "Epoch 6/100\n",
      "Training Loss: 0.28634213334455233\n",
      "Validation Loss: 0.18606460150643161\n",
      "Epoch 7/100\n",
      "Training Loss: 0.19656644761336114\n",
      "Validation Loss: 0.10247198351365713\n",
      "Epoch 8/100\n",
      "Training Loss: 0.3028297307573081\n",
      "Validation Loss: 0.1263581672352127\n",
      "Epoch 9/100\n",
      "Training Loss: 0.17688593967623634\n",
      "Validation Loss: 0.13041265531652396\n",
      "Epoch 10/100\n",
      "Training Loss: 0.19585734663834492\n",
      "Validation Loss: 0.12466809132867399\n",
      "Epoch 11/100\n",
      "Training Loss: 0.2221397492248523\n",
      "Validation Loss: 0.15473834480901033\n",
      "Epoch 12/100\n",
      "Training Loss: 0.1882367072217058\n",
      "Validation Loss: 0.10821160606882892\n",
      "Epoch 13/100\n",
      "Training Loss: 0.17939501729649507\n",
      "Validation Loss: 0.1962517910704615\n",
      "Epoch 14/100\n",
      "Training Loss: 0.19972059507855128\n",
      "Validation Loss: 0.18765908794434197\n",
      "Epoch 15/100\n",
      "Training Loss: 0.21924017203180415\n",
      "Validation Loss: 0.08159974083023139\n",
      "Epoch 16/100\n",
      "Training Loss: 0.16713564742580608\n",
      "Validation Loss: 0.15357091047223234\n",
      "Epoch 17/100\n",
      "Training Loss: 0.1808048820479164\n",
      "Validation Loss: 0.10652342158460633\n",
      "Epoch 18/100\n",
      "Training Loss: 0.2289679666092749\n",
      "Validation Loss: 0.07774863627731951\n",
      "Epoch 19/100\n",
      "Training Loss: 0.1842358369644752\n",
      "Validation Loss: 0.06589589426274366\n",
      "Epoch 20/100\n",
      "Training Loss: 0.2071225029175421\n",
      "Validation Loss: 0.05136590178615673\n",
      "Epoch 21/100\n",
      "Training Loss: 0.16233801740870732\n",
      "Validation Loss: 0.08903439652277093\n",
      "Epoch 22/100\n",
      "Training Loss: 0.22251773152970336\n",
      "Validation Loss: 0.06730610312422515\n",
      "Epoch 23/100\n",
      "Training Loss: 0.19292393125052226\n",
      "Validation Loss: 0.052582996380702705\n",
      "Epoch 24/100\n",
      "Training Loss: 0.1638425187239694\n",
      "Validation Loss: 0.10786220252428631\n",
      "Epoch 25/100\n",
      "Training Loss: 0.2200504012094442\n",
      "Validation Loss: 0.08535599264122412\n",
      "Epoch 26/100\n",
      "Training Loss: 0.16361670356258032\n",
      "Validation Loss: 0.11539224926193525\n",
      "Epoch 27/100\n",
      "Training Loss: 0.15667767647998482\n",
      "Validation Loss: 0.17098639889352063\n",
      "Epoch 28/100\n",
      "Training Loss: 0.1770570179377609\n",
      "Validation Loss: 0.08628078640927725\n",
      "Epoch 29/100\n",
      "Training Loss: 0.1881697972904372\n",
      "Validation Loss: 0.06969454775632866\n",
      "Epoch 30/100\n",
      "Training Loss: 0.15267724073185746\n",
      "Validation Loss: 0.09900974290461516\n",
      "Epoch 31/100\n",
      "Training Loss: 0.19077558639082032\n",
      "Validation Loss: 0.07881802929005645\n",
      "Epoch 32/100\n",
      "Training Loss: 0.16029852001573625\n",
      "Validation Loss: 0.09761430299576679\n",
      "Epoch 33/100\n",
      "Training Loss: 0.16992537584544157\n",
      "Validation Loss: 0.10658734632973417\n",
      "Epoch 34/100\n",
      "Training Loss: 0.16040222165408138\n",
      "Validation Loss: 0.058141896158805696\n",
      "Epoch 35/100\n",
      "Training Loss: 0.17299493644620445\n",
      "Validation Loss: 0.09771961363302356\n",
      "Epoch 36/100\n",
      "Training Loss: 0.16577543155144356\n",
      "Validation Loss: 0.15013036515731987\n",
      "Epoch 37/100\n",
      "Training Loss: 0.1550005415928364\n",
      "Validation Loss: 0.05120513183444081\n",
      "Epoch 38/100\n",
      "Training Loss: 0.18240629621098972\n",
      "Validation Loss: 0.0974905919439161\n",
      "Epoch 39/100\n",
      "Training Loss: 0.1375083259735553\n",
      "Validation Loss: 0.09020277163232389\n",
      "Epoch 40/100\n",
      "Training Loss: 0.14008862584404555\n",
      "Validation Loss: 0.05118853499116736\n",
      "Epoch 41/100\n",
      "Training Loss: 0.1323073936486673\n",
      "Validation Loss: 0.05699063131382807\n",
      "Epoch 42/100\n",
      "Training Loss: 0.15472131134340808\n",
      "Validation Loss: 0.10339914760295192\n",
      "Epoch 43/100\n",
      "Training Loss: 0.16009966525320612\n",
      "Validation Loss: 0.07494884924850996\n",
      "Epoch 44/100\n",
      "Training Loss: 0.1744945938757884\n",
      "Validation Loss: 0.08540375993510917\n",
      "Epoch 45/100\n",
      "Training Loss: 0.17681107726634968\n",
      "Validation Loss: 0.054276090038768855\n",
      "Epoch 46/100\n",
      "Training Loss: 0.16505092497777632\n",
      "Validation Loss: 0.06517161700014934\n",
      "Epoch 47/100\n",
      "Training Loss: 0.14922898707911184\n",
      "Validation Loss: 0.1496192307064131\n",
      "Epoch 48/100\n",
      "Training Loss: 0.17153207028662623\n",
      "Validation Loss: 0.10265976835516841\n",
      "Epoch 49/100\n",
      "Training Loss: 0.1249495357056487\n",
      "Validation Loss: 0.08176226534730006\n",
      "Epoch 50/100\n",
      "Training Loss: 0.1271650156021235\n",
      "Validation Loss: 0.08845499159604574\n",
      "Epoch 51/100\n",
      "Training Loss: 0.15773405316235944\n",
      "Validation Loss: 0.06582401776217409\n",
      "Epoch 52/100\n",
      "Training Loss: 0.156487870587768\n",
      "Validation Loss: 0.13492938170288787\n",
      "Epoch 53/100\n",
      "Training Loss: 0.18546970905554394\n",
      "Validation Loss: 0.08963584584981302\n",
      "Epoch 54/100\n",
      "Training Loss: 0.13123913748133076\n",
      "Validation Loss: 0.04890920044797666\n",
      "Epoch 55/100\n",
      "Training Loss: 0.14430236421292228\n",
      "Validation Loss: 0.06447979582550221\n",
      "Epoch 56/100\n",
      "Training Loss: 0.15095073896953706\n",
      "Validation Loss: 0.06401675809983355\n",
      "Epoch 57/100\n",
      "Training Loss: 0.14699338655034444\n",
      "Validation Loss: 0.156422433015155\n",
      "Epoch 58/100\n",
      "Training Loss: 0.13424495484952073\n",
      "Validation Loss: 0.04786892676877337\n",
      "Epoch 59/100\n",
      "Training Loss: 0.14424857363061538\n",
      "Validation Loss: 0.110570429469358\n",
      "Epoch 60/100\n",
      "Training Loss: 0.1258219393405061\n",
      "Validation Loss: 0.06912679271513195\n",
      "Epoch 61/100\n",
      "Training Loss: 0.1349363869268362\n",
      "Validation Loss: 0.07309528976478233\n",
      "Epoch 62/100\n",
      "Training Loss: 0.13811670518467598\n",
      "Validation Loss: 0.05422555132198611\n",
      "Epoch 63/100\n",
      "Training Loss: 0.16083592096911214\n",
      "Validation Loss: 0.08697112332788462\n",
      "Epoch 64/100\n",
      "Training Loss: 0.14894417754164402\n",
      "Validation Loss: 0.0582800463666381\n",
      "Epoch 65/100\n",
      "Training Loss: 0.14895153405055253\n",
      "Validation Loss: 0.06520919408834465\n",
      "Epoch 66/100\n",
      "Training Loss: 0.16440289752817636\n",
      "Validation Loss: 0.10434284651032491\n",
      "Epoch 67/100\n",
      "Training Loss: 0.14884493783127362\n",
      "Validation Loss: 0.05824134430114355\n",
      "Epoch 68/100\n",
      "Training Loss: 0.15293762542338477\n",
      "Validation Loss: 0.13680578203275245\n",
      "Epoch 69/100\n",
      "Training Loss: 0.16174302079693295\n",
      "Validation Loss: 0.1367114335164017\n",
      "Epoch 70/100\n",
      "Training Loss: 0.12195477619457262\n",
      "Validation Loss: 0.1256534063352286\n",
      "Epoch 71/100\n",
      "Training Loss: 0.1295560902394275\n",
      "Validation Loss: 0.05972935074122308\n",
      "Epoch 72/100\n",
      "Training Loss: 0.11922169801287415\n",
      "Validation Loss: 0.0898285297470347\n",
      "Epoch 73/100\n",
      "Training Loss: 0.15736875130150704\n",
      "Validation Loss: 0.07351909130510441\n",
      "Epoch 74/100\n",
      "Training Loss: 0.13710590795066446\n",
      "Validation Loss: 0.0729882149724548\n",
      "Epoch 75/100\n",
      "Training Loss: 0.13758675765983533\n",
      "Validation Loss: 0.08557460077225368\n",
      "Epoch 76/100\n",
      "Training Loss: 0.11877680802005983\n",
      "Validation Loss: 0.06490211678141805\n",
      "Epoch 77/100\n",
      "Training Loss: 0.14101736576397159\n",
      "Validation Loss: 0.14218488531869722\n",
      "Epoch 78/100\n",
      "Training Loss: 0.1379092787880904\n",
      "Validation Loss: 0.052092765165643894\n",
      "Epoch 79/100\n",
      "Training Loss: 0.12415310883180573\n",
      "Validation Loss: 0.04501646099680352\n",
      "Epoch 80/100\n",
      "Training Loss: 0.11830945375390975\n",
      "Validation Loss: 0.08152218640577533\n",
      "Epoch 81/100\n",
      "Training Loss: 0.12876910131542077\n",
      "Validation Loss: 0.06994273896456993\n",
      "Epoch 82/100\n",
      "Training Loss: 0.14068958903759513\n",
      "Validation Loss: 0.057902450781374036\n",
      "Epoch 83/100\n",
      "Training Loss: 0.12163279980791472\n",
      "Validation Loss: 0.05711051544755287\n",
      "Epoch 84/100\n",
      "Training Loss: 0.13461305825771006\n",
      "Validation Loss: 0.04362973772137134\n",
      "Epoch 85/100\n",
      "Training Loss: 0.12226567733023799\n",
      "Validation Loss: 0.09624620498231817\n",
      "Epoch 86/100\n",
      "Training Loss: 0.14298754541931552\n",
      "Validation Loss: 0.0880642591935418\n",
      "Epoch 87/100\n",
      "Training Loss: 0.13370770457273612\n",
      "Validation Loss: 0.05836757941515448\n",
      "Epoch 88/100\n",
      "Training Loss: 0.11712421758004274\n",
      "Validation Loss: 0.08974042075220355\n",
      "Epoch 89/100\n",
      "Training Loss: 0.16613939648113768\n",
      "Validation Loss: 0.0826372906911008\n",
      "Epoch 90/100\n",
      "Training Loss: 0.12041323850903181\n",
      "Validation Loss: 0.09612145543862467\n",
      "Epoch 91/100\n",
      "Training Loss: 0.13244085727716484\n",
      "Validation Loss: 0.06568803520283481\n",
      "Epoch 92/100\n",
      "Training Loss: 0.13133418061000043\n",
      "Validation Loss: 0.06465746180460351\n",
      "Epoch 93/100\n",
      "Training Loss: 0.13136766330653404\n",
      "Validation Loss: 0.0533216661047566\n",
      "Epoch 94/100\n",
      "Training Loss: 0.1422036410151305\n",
      "Validation Loss: 0.15589769755801428\n",
      "Epoch 95/100\n",
      "Training Loss: 0.14008703993693\n",
      "Validation Loss: 0.08083180367377776\n",
      "Epoch 96/100\n",
      "Training Loss: 0.10151765399315511\n",
      "Validation Loss: 0.10403508229843572\n",
      "Epoch 97/100\n",
      "Training Loss: 0.10804057866053596\n",
      "Validation Loss: 0.05417417706991531\n",
      "Epoch 98/100\n",
      "Training Loss: 0.13492788289011334\n",
      "Validation Loss: 0.06854391433059148\n",
      "Epoch 99/100\n",
      "Training Loss: 0.11555221864454726\n",
      "Validation Loss: 0.07538644219615391\n",
      "Epoch 100/100\n",
      "Training Loss: 0.12416616615688252\n",
      "Validation Loss: 0.06589946905473124\n",
      "Combination 29: Avg Training Loss = 0.15893849127257992, Avg Validation Loss = 0.09147913011072806\n",
      "Testing combination 30/48: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'sequence_length': 50, 'dropout_rate': 0.2}\n",
      "Truncated Train Data Shape: (70, 50, 1)\n",
      "Truncated Validation Data Shape: (10, 50, 1)\n",
      "    Trial 1/2 for combination 30/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.3002107209165466\n",
      "Validation Loss: 0.11381171768789602\n",
      "Epoch 2/100\n",
      "Training Loss: 0.3156783881332086\n",
      "Validation Loss: 0.18624554717368066\n",
      "Epoch 3/100\n",
      "Training Loss: 0.25233285578087206\n",
      "Validation Loss: 0.15847950310657502\n",
      "Epoch 4/100\n",
      "Training Loss: 0.167734195300158\n",
      "Validation Loss: 0.24497979389562535\n",
      "Epoch 5/100\n",
      "Training Loss: 0.2526477415551264\n",
      "Validation Loss: 0.21912268195178833\n",
      "Epoch 6/100\n",
      "Training Loss: 0.19968110732592473\n",
      "Validation Loss: 0.21742347500109224\n",
      "Epoch 7/100\n",
      "Training Loss: 0.22477513725874662\n",
      "Validation Loss: 0.09614544127780475\n",
      "Epoch 8/100\n",
      "Training Loss: 0.18268586465320266\n",
      "Validation Loss: 0.10979675578567953\n",
      "Epoch 9/100\n",
      "Training Loss: 0.2576215607228784\n",
      "Validation Loss: 0.08912207371055791\n",
      "Epoch 10/100\n",
      "Training Loss: 0.23540508603014862\n",
      "Validation Loss: 0.16834873032608996\n",
      "Epoch 11/100\n",
      "Training Loss: 0.17914518675363914\n",
      "Validation Loss: 0.0797372072330563\n",
      "Epoch 12/100\n",
      "Training Loss: 0.21559195473367654\n",
      "Validation Loss: 0.0882079197581326\n",
      "Epoch 13/100\n",
      "Training Loss: 0.16477522910462594\n",
      "Validation Loss: 0.11272208429493258\n",
      "Epoch 14/100\n",
      "Training Loss: 0.17304311373156936\n",
      "Validation Loss: 0.11348281990101823\n",
      "Epoch 15/100\n",
      "Training Loss: 0.1675590900206424\n",
      "Validation Loss: 0.0847398927666062\n",
      "Epoch 16/100\n",
      "Training Loss: 0.14729696964131622\n",
      "Validation Loss: 0.06561484557533498\n",
      "Epoch 17/100\n",
      "Training Loss: 0.19920373457176205\n",
      "Validation Loss: 0.09674397524726557\n",
      "Epoch 18/100\n",
      "Training Loss: 0.18963620772833698\n",
      "Validation Loss: 0.10018219702322455\n",
      "Epoch 19/100\n",
      "Training Loss: 0.18532406339151744\n",
      "Validation Loss: 0.10100097036255531\n",
      "Epoch 20/100\n",
      "Training Loss: 0.17503654347577668\n",
      "Validation Loss: 0.11989898356924944\n",
      "Epoch 21/100\n",
      "Training Loss: 0.19331916474014973\n",
      "Validation Loss: 0.08545692923320646\n",
      "Epoch 22/100\n",
      "Training Loss: 0.20974956195663214\n",
      "Validation Loss: 0.14263494246228575\n",
      "Epoch 23/100\n",
      "Training Loss: 0.12830138958135698\n",
      "Validation Loss: 0.043930316870055774\n",
      "Epoch 24/100\n",
      "Training Loss: 0.17878309907985546\n",
      "Validation Loss: 0.13042737617281924\n",
      "Epoch 25/100\n",
      "Training Loss: 0.15196739313817312\n",
      "Validation Loss: 0.09796238629120449\n",
      "Epoch 26/100\n",
      "Training Loss: 0.16559339156381192\n",
      "Validation Loss: 0.10797142553586096\n",
      "Epoch 27/100\n",
      "Training Loss: 0.1409357812318502\n",
      "Validation Loss: 0.05400770299143948\n",
      "Epoch 28/100\n",
      "Training Loss: 0.16491060645378472\n",
      "Validation Loss: 0.0810803203685577\n",
      "Epoch 29/100\n",
      "Training Loss: 0.18004404074701938\n",
      "Validation Loss: 0.060368377700999856\n",
      "Epoch 30/100\n",
      "Training Loss: 0.1578404343960411\n",
      "Validation Loss: 0.08120803787909985\n",
      "Epoch 31/100\n",
      "Training Loss: 0.1395019158070415\n",
      "Validation Loss: 0.056320456733011316\n",
      "Epoch 32/100\n",
      "Training Loss: 0.12853331366434034\n",
      "Validation Loss: 0.11082264891121914\n",
      "Epoch 33/100\n",
      "Training Loss: 0.16226963457790644\n",
      "Validation Loss: 0.06189423637132776\n",
      "Epoch 34/100\n",
      "Training Loss: 0.1549485329363079\n",
      "Validation Loss: 0.19402932600287032\n",
      "Epoch 35/100\n",
      "Training Loss: 0.15755922083671017\n",
      "Validation Loss: 0.08905998200105131\n",
      "Epoch 36/100\n",
      "Training Loss: 0.12178529500128862\n",
      "Validation Loss: 0.12135316890871166\n",
      "Epoch 37/100\n",
      "Training Loss: 0.15883406007254475\n",
      "Validation Loss: 0.06597582812701869\n",
      "Epoch 38/100\n",
      "Training Loss: 0.15024304362590118\n",
      "Validation Loss: 0.09616730737126519\n",
      "Epoch 39/100\n",
      "Training Loss: 0.12508188972591766\n",
      "Validation Loss: 0.1554441626694418\n",
      "Epoch 40/100\n",
      "Training Loss: 0.14045180542497185\n",
      "Validation Loss: 0.08373135074988439\n",
      "Epoch 41/100\n",
      "Training Loss: 0.14623723499352642\n",
      "Validation Loss: 0.06506835062844925\n",
      "Epoch 42/100\n",
      "Training Loss: 0.13853383429066848\n",
      "Validation Loss: 0.03677135966886979\n",
      "Epoch 43/100\n",
      "Training Loss: 0.13067662746156097\n",
      "Validation Loss: 0.09768029651664964\n",
      "Epoch 44/100\n",
      "Training Loss: 0.13510888438663637\n",
      "Validation Loss: 0.0863690923076241\n",
      "Epoch 45/100\n",
      "Training Loss: 0.13041062087024965\n",
      "Validation Loss: 0.04420818920482625\n",
      "Epoch 46/100\n",
      "Training Loss: 0.14814733330220378\n",
      "Validation Loss: 0.08905601586358396\n",
      "Epoch 47/100\n",
      "Training Loss: 0.13826109466975853\n",
      "Validation Loss: 0.12940810494369687\n",
      "Epoch 48/100\n",
      "Training Loss: 0.12665719942168252\n",
      "Validation Loss: 0.05216887017173817\n",
      "Epoch 49/100\n",
      "Training Loss: 0.14544079646532942\n",
      "Validation Loss: 0.053188216346840525\n",
      "Epoch 50/100\n",
      "Training Loss: 0.1403994700547595\n",
      "Validation Loss: 0.07070160127184982\n",
      "Epoch 51/100\n",
      "Training Loss: 0.13345930895477318\n",
      "Validation Loss: 0.06261788880847488\n",
      "Epoch 52/100\n",
      "Training Loss: 0.14616096303908574\n",
      "Validation Loss: 0.09670128806149697\n",
      "Epoch 53/100\n",
      "Training Loss: 0.1386859169591444\n",
      "Validation Loss: 0.11406924186222744\n",
      "Epoch 54/100\n",
      "Training Loss: 0.1368923855567164\n",
      "Validation Loss: 0.10244827908403711\n",
      "Epoch 55/100\n",
      "Training Loss: 0.1551423421151473\n",
      "Validation Loss: 0.08027178395042235\n",
      "Epoch 56/100\n",
      "Training Loss: 0.14076182764656547\n",
      "Validation Loss: 0.07684162962714128\n",
      "Epoch 57/100\n",
      "Training Loss: 0.13893531910753837\n",
      "Validation Loss: 0.05314822218640876\n",
      "Epoch 58/100\n",
      "Training Loss: 0.1419897611620705\n",
      "Validation Loss: 0.05462900717883788\n",
      "Epoch 59/100\n",
      "Training Loss: 0.13188598694038844\n",
      "Validation Loss: 0.061084029397907025\n",
      "Epoch 60/100\n",
      "Training Loss: 0.11415845014922871\n",
      "Validation Loss: 0.149884557133775\n",
      "Epoch 61/100\n",
      "Training Loss: 0.1249385364643795\n",
      "Validation Loss: 0.07398649065228845\n",
      "Epoch 62/100\n",
      "Training Loss: 0.16278544546154358\n",
      "Validation Loss: 0.04233623432048585\n",
      "Epoch 63/100\n",
      "Training Loss: 0.1110411719822311\n",
      "Validation Loss: 0.0464050801633591\n",
      "Epoch 64/100\n",
      "Training Loss: 0.13781334471808432\n",
      "Validation Loss: 0.12738401871411734\n",
      "Epoch 65/100\n",
      "Training Loss: 0.13959467685437799\n",
      "Validation Loss: 0.07745825466436382\n",
      "Epoch 66/100\n",
      "Training Loss: 0.12435721800899803\n",
      "Validation Loss: 0.10701825431446181\n",
      "Epoch 67/100\n",
      "Training Loss: 0.11665558828223305\n",
      "Validation Loss: 0.07860472875866324\n",
      "Epoch 68/100\n",
      "Training Loss: 0.1124812563076123\n",
      "Validation Loss: 0.09197931975981079\n",
      "Epoch 69/100\n",
      "Training Loss: 0.10451737492521003\n",
      "Validation Loss: 0.05741214245115289\n",
      "Epoch 70/100\n",
      "Training Loss: 0.10019023070716386\n",
      "Validation Loss: 0.11301031349618018\n",
      "Epoch 71/100\n",
      "Training Loss: 0.12632272287311236\n",
      "Validation Loss: 0.0950713940989058\n",
      "Epoch 72/100\n",
      "Training Loss: 0.11958443177227182\n",
      "Validation Loss: 0.0514706993355787\n",
      "Epoch 73/100\n",
      "Training Loss: 0.12934415670848684\n",
      "Validation Loss: 0.052674895228078965\n",
      "Epoch 74/100\n",
      "Training Loss: 0.11760136838856834\n",
      "Validation Loss: 0.13305143813849735\n",
      "Epoch 75/100\n",
      "Training Loss: 0.10722808561053354\n",
      "Validation Loss: 0.05894740090156895\n",
      "Epoch 76/100\n",
      "Training Loss: 0.11938429094503053\n",
      "Validation Loss: 0.04631019863460316\n",
      "Epoch 77/100\n",
      "Training Loss: 0.10774334298941049\n",
      "Validation Loss: 0.07983355820129791\n",
      "Epoch 78/100\n",
      "Training Loss: 0.1104554102264716\n",
      "Validation Loss: 0.07102977093123508\n",
      "Epoch 79/100\n",
      "Training Loss: 0.11639792034387682\n",
      "Validation Loss: 0.05919694203580259\n",
      "Epoch 80/100\n",
      "Training Loss: 0.1271344328238331\n",
      "Validation Loss: 0.054981074998366\n",
      "Epoch 81/100\n",
      "Training Loss: 0.11161586731974059\n",
      "Validation Loss: 0.0947121848989341\n",
      "Epoch 82/100\n",
      "Training Loss: 0.11505985147103576\n",
      "Validation Loss: 0.043967951888564845\n",
      "Epoch 83/100\n",
      "Training Loss: 0.1131007331153359\n",
      "Validation Loss: 0.08418495664397388\n",
      "Epoch 84/100\n",
      "Training Loss: 0.11864650045904392\n",
      "Validation Loss: 0.09750410674753082\n",
      "Epoch 85/100\n",
      "Training Loss: 0.1111444717839627\n",
      "Validation Loss: 0.06331860206158954\n",
      "Epoch 86/100\n",
      "Training Loss: 0.11981840356472685\n",
      "Validation Loss: 0.11253685556755524\n",
      "Epoch 87/100\n",
      "Training Loss: 0.11585121330235422\n",
      "Validation Loss: 0.08822232631078235\n",
      "Epoch 88/100\n",
      "Training Loss: 0.13352061426323503\n",
      "Validation Loss: 0.04825417265985898\n",
      "Epoch 89/100\n",
      "Training Loss: 0.10569926588294336\n",
      "Validation Loss: 0.09687001408796915\n",
      "Epoch 90/100\n",
      "Training Loss: 0.10212699886912711\n",
      "Validation Loss: 0.0739773268350286\n",
      "Epoch 91/100\n",
      "Training Loss: 0.09642570605558695\n",
      "Validation Loss: 0.13181343085502378\n",
      "Epoch 92/100\n",
      "Training Loss: 0.1139250856228504\n",
      "Validation Loss: 0.1148773322005809\n",
      "Epoch 93/100\n",
      "Training Loss: 0.10038844489232555\n",
      "Validation Loss: 0.0661983494399924\n",
      "Epoch 94/100\n",
      "Training Loss: 0.10841008030226996\n",
      "Validation Loss: 0.05420176617163177\n",
      "Epoch 95/100\n",
      "Training Loss: 0.10156893833261381\n",
      "Validation Loss: 0.06489267169699284\n",
      "Epoch 96/100\n",
      "Training Loss: 0.1038775682896057\n",
      "Validation Loss: 0.044883516195416626\n",
      "Epoch 97/100\n",
      "Training Loss: 0.1026201016453793\n",
      "Validation Loss: 0.08554203590990027\n",
      "Epoch 98/100\n",
      "Training Loss: 0.10505673387900105\n",
      "Validation Loss: 0.04143400098740582\n",
      "Epoch 99/100\n",
      "Training Loss: 0.11174738222977651\n",
      "Validation Loss: 0.058962696461735596\n",
      "Epoch 100/100\n",
      "Training Loss: 0.09589837082474278\n",
      "Validation Loss: 0.06087887955299268\n",
      "    Trial 2/2 for combination 30/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.44714715101413427\n",
      "Validation Loss: 0.4845387939887654\n",
      "Epoch 2/100\n",
      "Training Loss: 0.3421161400887946\n",
      "Validation Loss: 0.09818081203346708\n",
      "Epoch 3/100\n",
      "Training Loss: 0.3019363555457373\n",
      "Validation Loss: 0.14877469559603929\n",
      "Epoch 4/100\n",
      "Training Loss: 0.21116153209804472\n",
      "Validation Loss: 0.1999767654362397\n",
      "Epoch 5/100\n",
      "Training Loss: 0.25123623601333145\n",
      "Validation Loss: 0.13137261324845853\n",
      "Epoch 6/100\n",
      "Training Loss: 0.20153448444840036\n",
      "Validation Loss: 0.08223317055800125\n",
      "Epoch 7/100\n",
      "Training Loss: 0.2370098538221987\n",
      "Validation Loss: 0.09863498841431165\n",
      "Epoch 8/100\n",
      "Training Loss: 0.19899937401341772\n",
      "Validation Loss: 0.2961627333157652\n",
      "Epoch 9/100\n",
      "Training Loss: 0.18773791166253564\n",
      "Validation Loss: 0.07850492682544633\n",
      "Epoch 10/100\n",
      "Training Loss: 0.22613270500034896\n",
      "Validation Loss: 0.2644496299132385\n",
      "Epoch 11/100\n",
      "Training Loss: 0.2372991765597273\n",
      "Validation Loss: 0.07481679796185309\n",
      "Epoch 12/100\n",
      "Training Loss: 0.1848938702205151\n",
      "Validation Loss: 0.15195249519884663\n",
      "Epoch 13/100\n",
      "Training Loss: 0.1782757008606075\n",
      "Validation Loss: 0.17233797975003926\n",
      "Epoch 14/100\n",
      "Training Loss: 0.13948745986073005\n",
      "Validation Loss: 0.09555711555683054\n",
      "Epoch 15/100\n",
      "Training Loss: 0.199089537675558\n",
      "Validation Loss: 0.0777500830734904\n",
      "Epoch 16/100\n",
      "Training Loss: 0.14717624107760163\n",
      "Validation Loss: 0.09477125514810603\n",
      "Epoch 17/100\n",
      "Training Loss: 0.1541274981699522\n",
      "Validation Loss: 0.08427949588954844\n",
      "Epoch 18/100\n",
      "Training Loss: 0.17185278043487387\n",
      "Validation Loss: 0.08955413177801412\n",
      "Epoch 19/100\n",
      "Training Loss: 0.1788457278264076\n",
      "Validation Loss: 0.11090509295824758\n",
      "Epoch 20/100\n",
      "Training Loss: 0.16360911627021765\n",
      "Validation Loss: 0.15202843569328744\n",
      "Epoch 21/100\n",
      "Training Loss: 0.15905830834146986\n",
      "Validation Loss: 0.11958218021008146\n",
      "Epoch 22/100\n",
      "Training Loss: 0.1549466349241256\n",
      "Validation Loss: 0.11824556322483182\n",
      "Epoch 23/100\n",
      "Training Loss: 0.16411299320704717\n",
      "Validation Loss: 0.07016064095788925\n",
      "Epoch 24/100\n",
      "Training Loss: 0.19598969202122363\n",
      "Validation Loss: 0.11145418290492963\n",
      "Epoch 25/100\n",
      "Training Loss: 0.1543050915105778\n",
      "Validation Loss: 0.06483142047130505\n",
      "Epoch 26/100\n",
      "Training Loss: 0.1514293911791603\n",
      "Validation Loss: 0.11397278340934558\n",
      "Epoch 27/100\n",
      "Training Loss: 0.15635445192521874\n",
      "Validation Loss: 0.06025364177633674\n",
      "Epoch 28/100\n",
      "Training Loss: 0.17918679941781113\n",
      "Validation Loss: 0.08688805122885894\n",
      "Epoch 29/100\n",
      "Training Loss: 0.13689233981178597\n",
      "Validation Loss: 0.05578487977933881\n",
      "Epoch 30/100\n",
      "Training Loss: 0.14428354610887634\n",
      "Validation Loss: 0.08040055014330633\n",
      "Epoch 31/100\n",
      "Training Loss: 0.13610684370654394\n",
      "Validation Loss: 0.17909164671028627\n",
      "Epoch 32/100\n",
      "Training Loss: 0.15835277597200464\n",
      "Validation Loss: 0.0726200651087872\n",
      "Epoch 33/100\n",
      "Training Loss: 0.174081132598777\n",
      "Validation Loss: 0.08204180308841588\n",
      "Epoch 34/100\n",
      "Training Loss: 0.16064616217276528\n",
      "Validation Loss: 0.08661975452805352\n",
      "Epoch 35/100\n",
      "Training Loss: 0.14042488333345202\n",
      "Validation Loss: 0.05480267247389657\n",
      "Epoch 36/100\n",
      "Training Loss: 0.153741916452407\n",
      "Validation Loss: 0.07424093593859014\n",
      "Epoch 37/100\n",
      "Training Loss: 0.16460057778409215\n",
      "Validation Loss: 0.0539252409378254\n",
      "Epoch 38/100\n",
      "Training Loss: 0.16715978791796132\n",
      "Validation Loss: 0.14431768400255296\n",
      "Epoch 39/100\n",
      "Training Loss: 0.1348787368628146\n",
      "Validation Loss: 0.07835821100137265\n",
      "Epoch 40/100\n",
      "Training Loss: 0.1424373888428641\n",
      "Validation Loss: 0.06745711665675796\n",
      "Epoch 41/100\n",
      "Training Loss: 0.12946358215709367\n",
      "Validation Loss: 0.09747325752374622\n",
      "Epoch 42/100\n",
      "Training Loss: 0.13973263310476075\n",
      "Validation Loss: 0.16319304584896055\n",
      "Epoch 43/100\n",
      "Training Loss: 0.15631982823719115\n",
      "Validation Loss: 0.06337291056014341\n",
      "Epoch 44/100\n",
      "Training Loss: 0.1454836000613272\n",
      "Validation Loss: 0.07209715348900227\n",
      "Epoch 45/100\n",
      "Training Loss: 0.13462056524255295\n",
      "Validation Loss: 0.0773709485574584\n",
      "Epoch 46/100\n",
      "Training Loss: 0.14372212309566113\n",
      "Validation Loss: 0.05471145276469243\n",
      "Epoch 47/100\n",
      "Training Loss: 0.14653805230867764\n",
      "Validation Loss: 0.1299768325030342\n",
      "Epoch 48/100\n",
      "Training Loss: 0.14770479159381622\n",
      "Validation Loss: 0.08585036420134622\n",
      "Epoch 49/100\n",
      "Training Loss: 0.1495347862505103\n",
      "Validation Loss: 0.14318027858783144\n",
      "Epoch 50/100\n",
      "Training Loss: 0.1303482765876649\n",
      "Validation Loss: 0.09531920952587399\n",
      "Epoch 51/100\n",
      "Training Loss: 0.1345102378987364\n",
      "Validation Loss: 0.08826894591369536\n",
      "Epoch 52/100\n",
      "Training Loss: 0.14680672687074436\n",
      "Validation Loss: 0.08239813007182731\n",
      "Epoch 53/100\n",
      "Training Loss: 0.1549749459131102\n",
      "Validation Loss: 0.08304267904628447\n",
      "Epoch 54/100\n",
      "Training Loss: 0.12591084582464881\n",
      "Validation Loss: 0.07768552240518696\n",
      "Epoch 55/100\n",
      "Training Loss: 0.13624654194988914\n",
      "Validation Loss: 0.07566642579269836\n",
      "Epoch 56/100\n",
      "Training Loss: 0.12394730516758326\n",
      "Validation Loss: 0.07420487553230071\n",
      "Epoch 57/100\n",
      "Training Loss: 0.10592237908068768\n",
      "Validation Loss: 0.13669594767408622\n",
      "Epoch 58/100\n",
      "Training Loss: 0.1501432930940749\n",
      "Validation Loss: 0.05988287703707451\n",
      "Epoch 59/100\n",
      "Training Loss: 0.1415471157086895\n",
      "Validation Loss: 0.07707373123543339\n",
      "Epoch 60/100\n",
      "Training Loss: 0.1191049352023076\n",
      "Validation Loss: 0.1229606972986916\n",
      "Epoch 61/100\n",
      "Training Loss: 0.11901507134151272\n",
      "Validation Loss: 0.0786453707598396\n",
      "Epoch 62/100\n",
      "Training Loss: 0.15085350846046158\n",
      "Validation Loss: 0.07484404273934984\n",
      "Epoch 63/100\n",
      "Training Loss: 0.1288334586221532\n",
      "Validation Loss: 0.0734596862426202\n",
      "Epoch 64/100\n",
      "Training Loss: 0.12941860634449176\n",
      "Validation Loss: 0.06363753869482339\n",
      "Epoch 65/100\n",
      "Training Loss: 0.11420658411208688\n",
      "Validation Loss: 0.0776279788634249\n",
      "Epoch 66/100\n",
      "Training Loss: 0.11907845916383203\n",
      "Validation Loss: 0.1327208745040802\n",
      "Epoch 67/100\n",
      "Training Loss: 0.12613959278692255\n",
      "Validation Loss: 0.05482833579674431\n",
      "Epoch 68/100\n",
      "Training Loss: 0.12627888827050276\n",
      "Validation Loss: 0.10024546940609458\n",
      "Epoch 69/100\n",
      "Training Loss: 0.13027683413170235\n",
      "Validation Loss: 0.05813532721443406\n",
      "Epoch 70/100\n",
      "Training Loss: 0.12359207489504774\n",
      "Validation Loss: 0.05143409813376386\n",
      "Epoch 71/100\n",
      "Training Loss: 0.11740878571508624\n",
      "Validation Loss: 0.09056660855554144\n",
      "Epoch 72/100\n",
      "Training Loss: 0.12266036166805484\n",
      "Validation Loss: 0.07456359754484627\n",
      "Epoch 73/100\n",
      "Training Loss: 0.12323093431923923\n",
      "Validation Loss: 0.10335099098057174\n",
      "Epoch 74/100\n",
      "Training Loss: 0.13573035474611678\n",
      "Validation Loss: 0.06911362569171771\n",
      "Epoch 75/100\n",
      "Training Loss: 0.12123321138900897\n",
      "Validation Loss: 0.07572688642237198\n",
      "Epoch 76/100\n",
      "Training Loss: 0.13182369197840724\n",
      "Validation Loss: 0.06386860826619183\n",
      "Epoch 77/100\n",
      "Training Loss: 0.10328457249012767\n",
      "Validation Loss: 0.044332585497950834\n",
      "Epoch 78/100\n",
      "Training Loss: 0.1271228527617353\n",
      "Validation Loss: 0.0885835142400883\n",
      "Epoch 79/100\n",
      "Training Loss: 0.11200519437791345\n",
      "Validation Loss: 0.08199295262105308\n",
      "Epoch 80/100\n",
      "Training Loss: 0.10813489307219108\n",
      "Validation Loss: 0.0625053922257334\n",
      "Epoch 81/100\n",
      "Training Loss: 0.11588730133253272\n",
      "Validation Loss: 0.08304398209862436\n",
      "Epoch 82/100\n",
      "Training Loss: 0.10596991498469197\n",
      "Validation Loss: 0.03883984791774521\n",
      "Epoch 83/100\n",
      "Training Loss: 0.11408279484288918\n",
      "Validation Loss: 0.07316756500910246\n",
      "Epoch 84/100\n",
      "Training Loss: 0.10748501172152711\n",
      "Validation Loss: 0.08933970146201677\n",
      "Epoch 85/100\n",
      "Training Loss: 0.1282890456254174\n",
      "Validation Loss: 0.06917812081936459\n",
      "Epoch 86/100\n",
      "Training Loss: 0.1064062476845839\n",
      "Validation Loss: 0.05365525041584125\n",
      "Epoch 87/100\n",
      "Training Loss: 0.11778459268622556\n",
      "Validation Loss: 0.06255646049327512\n",
      "Epoch 88/100\n",
      "Training Loss: 0.11916570592000786\n",
      "Validation Loss: 0.04555760963088092\n",
      "Epoch 89/100\n",
      "Training Loss: 0.09675493070420049\n",
      "Validation Loss: 0.12487294829774295\n",
      "Epoch 90/100\n",
      "Training Loss: 0.10606695846051628\n",
      "Validation Loss: 0.05090917978955965\n",
      "Epoch 91/100\n",
      "Training Loss: 0.10982834235301399\n",
      "Validation Loss: 0.08398823028407454\n",
      "Epoch 92/100\n",
      "Training Loss: 0.1049715221262744\n",
      "Validation Loss: 0.043208168821899776\n",
      "Epoch 93/100\n",
      "Training Loss: 0.11639188473610769\n",
      "Validation Loss: 0.095778417853106\n",
      "Epoch 94/100\n",
      "Training Loss: 0.1229324161129603\n",
      "Validation Loss: 0.043608630737592344\n",
      "Epoch 95/100\n",
      "Training Loss: 0.11412714536499702\n",
      "Validation Loss: 0.07083812750058482\n",
      "Epoch 96/100\n",
      "Training Loss: 0.11401606591722913\n",
      "Validation Loss: 0.09203307009704363\n",
      "Epoch 97/100\n",
      "Training Loss: 0.10507830693484266\n",
      "Validation Loss: 0.04463725251197933\n",
      "Epoch 98/100\n",
      "Training Loss: 0.10210422755709556\n",
      "Validation Loss: 0.07965234576031624\n",
      "Epoch 99/100\n",
      "Training Loss: 0.11765207470995838\n",
      "Validation Loss: 0.039384988443504396\n",
      "Epoch 100/100\n",
      "Training Loss: 0.10125635638334843\n",
      "Validation Loss: 0.059114308988850936\n",
      "Combination 30: Avg Training Loss = 0.1480193833517313, Avg Validation Loss = 0.09296584161990554\n",
      "Testing combination 31/48: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.001, 'sequence_length': 6, 'dropout_rate': 0.1}\n",
      "Truncated Train Data Shape: (70, 6, 1)\n",
      "Truncated Validation Data Shape: (10, 6, 1)\n",
      "    Trial 1/2 for combination 31/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.3832523084906264\n",
      "Validation Loss: 0.1981397611032604\n",
      "Epoch 2/100\n",
      "Training Loss: 0.40114791574995723\n",
      "Validation Loss: 0.1627991962912241\n",
      "Epoch 3/100\n",
      "Training Loss: 0.22137204795108778\n",
      "Validation Loss: 0.20109752143490364\n",
      "Epoch 4/100\n",
      "Training Loss: 0.26527006131101644\n",
      "Validation Loss: 0.3974379810040056\n",
      "Epoch 5/100\n",
      "Training Loss: 0.22060216791117887\n",
      "Validation Loss: 0.18660332328044388\n",
      "Epoch 6/100\n",
      "Training Loss: 0.18495370789656387\n",
      "Validation Loss: 0.10823268247192172\n",
      "Epoch 7/100\n",
      "Training Loss: 0.16746968884338706\n",
      "Validation Loss: 0.09398413372746137\n",
      "Epoch 8/100\n",
      "Training Loss: 0.1565695136847561\n",
      "Validation Loss: 0.12471813805596374\n",
      "Epoch 9/100\n",
      "Training Loss: 0.11869992725731507\n",
      "Validation Loss: 0.12238045248726355\n",
      "Epoch 10/100\n",
      "Training Loss: 0.17810307949929202\n",
      "Validation Loss: 0.11514869242629429\n",
      "Epoch 11/100\n",
      "Training Loss: 0.16562243883735278\n",
      "Validation Loss: 0.11447559647425876\n",
      "Epoch 12/100\n",
      "Training Loss: 0.14735187867501723\n",
      "Validation Loss: 0.05195826604845215\n",
      "Epoch 13/100\n",
      "Training Loss: 0.17679799327286483\n",
      "Validation Loss: 0.10795563439808589\n",
      "Epoch 14/100\n",
      "Training Loss: 0.15303093405208348\n",
      "Validation Loss: 0.08143524616397545\n",
      "Epoch 15/100\n",
      "Training Loss: 0.1371448442594494\n",
      "Validation Loss: 0.06910859742019947\n",
      "Epoch 16/100\n",
      "Training Loss: 0.16216204465261608\n",
      "Validation Loss: 0.07585637389005481\n",
      "Epoch 17/100\n",
      "Training Loss: 0.13669838422778682\n",
      "Validation Loss: 0.12524179337468316\n",
      "Epoch 18/100\n",
      "Training Loss: 0.13283194721150618\n",
      "Validation Loss: 0.09187495113772923\n",
      "Epoch 19/100\n",
      "Training Loss: 0.111432511874973\n",
      "Validation Loss: 0.08140264225642595\n",
      "Epoch 20/100\n",
      "Training Loss: 0.11794800486201903\n",
      "Validation Loss: 0.06836127814187434\n",
      "Epoch 21/100\n",
      "Training Loss: 0.11368325262166093\n",
      "Validation Loss: 0.08421291896494978\n",
      "Epoch 22/100\n",
      "Training Loss: 0.11879660780284255\n",
      "Validation Loss: 0.08608999947607805\n",
      "Epoch 23/100\n",
      "Training Loss: 0.10453360557534112\n",
      "Validation Loss: 0.06728494598367188\n",
      "Epoch 24/100\n",
      "Training Loss: 0.13039500457026468\n",
      "Validation Loss: 0.06038372030470869\n",
      "Epoch 25/100\n",
      "Training Loss: 0.10437631080021656\n",
      "Validation Loss: 0.05485855351085367\n",
      "Epoch 26/100\n",
      "Training Loss: 0.10422232768107761\n",
      "Validation Loss: 0.059076931647192375\n",
      "Epoch 27/100\n",
      "Training Loss: 0.10621758866651083\n",
      "Validation Loss: 0.0871837473598762\n",
      "Epoch 28/100\n",
      "Training Loss: 0.11223206756819759\n",
      "Validation Loss: 0.10197596588960538\n",
      "Epoch 29/100\n",
      "Training Loss: 0.10870134635807009\n",
      "Validation Loss: 0.08873340167120233\n",
      "Epoch 30/100\n",
      "Training Loss: 0.1025473884863798\n",
      "Validation Loss: 0.06755701398418974\n",
      "Epoch 31/100\n",
      "Training Loss: 0.10050423889021576\n",
      "Validation Loss: 0.05586494254312728\n",
      "Epoch 32/100\n",
      "Training Loss: 0.09786017948396714\n",
      "Validation Loss: 0.060030975560457354\n",
      "Epoch 33/100\n",
      "Training Loss: 0.09356795598533736\n",
      "Validation Loss: 0.05252684135314578\n",
      "Epoch 34/100\n",
      "Training Loss: 0.08642593534181875\n",
      "Validation Loss: 0.05094661555808085\n",
      "Epoch 35/100\n",
      "Training Loss: 0.08593192273549799\n",
      "Validation Loss: 0.0928054513283444\n",
      "Epoch 36/100\n",
      "Training Loss: 0.09327382874252287\n",
      "Validation Loss: 0.06946610754077044\n",
      "Epoch 37/100\n",
      "Training Loss: 0.08714277714182508\n",
      "Validation Loss: 0.06219378934432827\n",
      "Epoch 38/100\n",
      "Training Loss: 0.07692303641442413\n",
      "Validation Loss: 0.06182222544997802\n",
      "Epoch 39/100\n",
      "Training Loss: 0.08490086460046274\n",
      "Validation Loss: 0.058011332262929996\n",
      "Epoch 40/100\n",
      "Training Loss: 0.08682410628860314\n",
      "Validation Loss: 0.057898272182094965\n",
      "Epoch 41/100\n",
      "Training Loss: 0.09706852190437372\n",
      "Validation Loss: 0.057672172241796216\n",
      "Epoch 42/100\n",
      "Training Loss: 0.09413687483435983\n",
      "Validation Loss: 0.04818009973027863\n",
      "Epoch 43/100\n",
      "Training Loss: 0.0850764153082256\n",
      "Validation Loss: 0.04370413657049439\n",
      "Epoch 44/100\n",
      "Training Loss: 0.07989919361409464\n",
      "Validation Loss: 0.04650032082869045\n",
      "Epoch 45/100\n",
      "Training Loss: 0.08243398753987632\n",
      "Validation Loss: 0.049895156969881574\n",
      "Epoch 46/100\n",
      "Training Loss: 0.08444874625364233\n",
      "Validation Loss: 0.0715490032923928\n",
      "Epoch 47/100\n",
      "Training Loss: 0.08665233238948927\n",
      "Validation Loss: 0.04971147738348865\n",
      "Epoch 48/100\n",
      "Training Loss: 0.08109674611247163\n",
      "Validation Loss: 0.041673862641462237\n",
      "Epoch 49/100\n",
      "Training Loss: 0.09050246091545218\n",
      "Validation Loss: 0.0365905395771326\n",
      "Epoch 50/100\n",
      "Training Loss: 0.07593391129653744\n",
      "Validation Loss: 0.05453456832671938\n",
      "Epoch 51/100\n",
      "Training Loss: 0.07721723857412494\n",
      "Validation Loss: 0.05098282762343588\n",
      "Epoch 52/100\n",
      "Training Loss: 0.08498727580668594\n",
      "Validation Loss: 0.054493556726687355\n",
      "Epoch 53/100\n",
      "Training Loss: 0.08784803998620044\n",
      "Validation Loss: 0.05432238109589248\n",
      "Epoch 54/100\n",
      "Training Loss: 0.08700188199075534\n",
      "Validation Loss: 0.04751196492317587\n",
      "Epoch 55/100\n",
      "Training Loss: 0.07289024063504852\n",
      "Validation Loss: 0.05216313080565891\n",
      "Epoch 56/100\n",
      "Training Loss: 0.07549671302154175\n",
      "Validation Loss: 0.04596068930988704\n",
      "Epoch 57/100\n",
      "Training Loss: 0.08207067668033625\n",
      "Validation Loss: 0.05187178980073397\n",
      "Epoch 58/100\n",
      "Training Loss: 0.0787050954777725\n",
      "Validation Loss: 0.061495668486340804\n",
      "Epoch 59/100\n",
      "Training Loss: 0.08921480310008642\n",
      "Validation Loss: 0.04863374088376589\n",
      "Epoch 60/100\n",
      "Training Loss: 0.07633255376092117\n",
      "Validation Loss: 0.05039525533520639\n",
      "Epoch 61/100\n",
      "Training Loss: 0.08028783222735682\n",
      "Validation Loss: 0.05881605214803863\n",
      "Epoch 62/100\n",
      "Training Loss: 0.07964775796956677\n",
      "Validation Loss: 0.04553042036636097\n",
      "Epoch 63/100\n",
      "Training Loss: 0.07424511007774863\n",
      "Validation Loss: 0.039311131076867845\n",
      "Epoch 64/100\n",
      "Training Loss: 0.07387204326530218\n",
      "Validation Loss: 0.04691049686549263\n",
      "Epoch 65/100\n",
      "Training Loss: 0.08495310167982761\n",
      "Validation Loss: 0.04370979354705261\n",
      "Epoch 66/100\n",
      "Training Loss: 0.08708403120219121\n",
      "Validation Loss: 0.07634583182043778\n",
      "Epoch 67/100\n",
      "Training Loss: 0.08326984645323539\n",
      "Validation Loss: 0.032252265537792126\n",
      "Epoch 68/100\n",
      "Training Loss: 0.07395171700683213\n",
      "Validation Loss: 0.05074522556869424\n",
      "Epoch 69/100\n",
      "Training Loss: 0.08416863784449378\n",
      "Validation Loss: 0.04609572088021223\n",
      "Epoch 70/100\n",
      "Training Loss: 0.07748025836274598\n",
      "Validation Loss: 0.042590825915474306\n",
      "Epoch 71/100\n",
      "Training Loss: 0.07916789025055983\n",
      "Validation Loss: 0.05136930553035257\n",
      "Epoch 72/100\n",
      "Training Loss: 0.08755468694176181\n",
      "Validation Loss: 0.040972518560670475\n",
      "Epoch 73/100\n",
      "Training Loss: 0.07855387892443527\n",
      "Validation Loss: 0.044704902433138086\n",
      "Epoch 74/100\n",
      "Training Loss: 0.07810079857367778\n",
      "Validation Loss: 0.041043167675884215\n",
      "Epoch 75/100\n",
      "Training Loss: 0.07710633894190008\n",
      "Validation Loss: 0.06646553923956078\n",
      "Epoch 76/100\n",
      "Training Loss: 0.08107763856259105\n",
      "Validation Loss: 0.040308535644624506\n",
      "Epoch 77/100\n",
      "Training Loss: 0.0728195263247681\n",
      "Validation Loss: 0.06135260194962786\n",
      "Epoch 78/100\n",
      "Training Loss: 0.07224121862408514\n",
      "Validation Loss: 0.036759426082875066\n",
      "Epoch 79/100\n",
      "Training Loss: 0.07890775975477542\n",
      "Validation Loss: 0.051196587241941524\n",
      "Epoch 80/100\n",
      "Training Loss: 0.08692022861836161\n",
      "Validation Loss: 0.04756032008541713\n",
      "Epoch 81/100\n",
      "Training Loss: 0.08324935011190239\n",
      "Validation Loss: 0.045910099134051745\n",
      "Epoch 82/100\n",
      "Training Loss: 0.08325156443790671\n",
      "Validation Loss: 0.05210790929365354\n",
      "Epoch 83/100\n",
      "Training Loss: 0.0809469313624478\n",
      "Validation Loss: 0.034932780543841926\n",
      "Epoch 84/100\n",
      "Training Loss: 0.0792991255202703\n",
      "Validation Loss: 0.056471255332880735\n",
      "Epoch 85/100\n",
      "Training Loss: 0.07304893492833488\n",
      "Validation Loss: 0.04952743341756556\n",
      "Epoch 86/100\n",
      "Training Loss: 0.08866426655697718\n",
      "Validation Loss: 0.06655848775413253\n",
      "Epoch 87/100\n",
      "Training Loss: 0.07869857030700501\n",
      "Validation Loss: 0.05172771011393782\n",
      "Epoch 88/100\n",
      "Training Loss: 0.07857815897214807\n",
      "Validation Loss: 0.06056289101796597\n",
      "Epoch 89/100\n",
      "Training Loss: 0.08139302289456983\n",
      "Validation Loss: 0.056146509932172795\n",
      "Epoch 90/100\n",
      "Training Loss: 0.07053484679058437\n",
      "Validation Loss: 0.0500551140987025\n",
      "Epoch 91/100\n",
      "Training Loss: 0.07171888768323452\n",
      "Validation Loss: 0.035680282324433904\n",
      "Epoch 92/100\n",
      "Training Loss: 0.08037617851675377\n",
      "Validation Loss: 0.04563820804376083\n",
      "Epoch 93/100\n",
      "Training Loss: 0.08376920591389567\n",
      "Validation Loss: 0.05060170906116316\n",
      "Epoch 94/100\n",
      "Training Loss: 0.07728457425841144\n",
      "Validation Loss: 0.04534975906922212\n",
      "Epoch 95/100\n",
      "Training Loss: 0.08516575061883745\n",
      "Validation Loss: 0.052778156203319805\n",
      "Epoch 96/100\n",
      "Training Loss: 0.08697048231681609\n",
      "Validation Loss: 0.06270804982918389\n",
      "Epoch 97/100\n",
      "Training Loss: 0.08086231171365103\n",
      "Validation Loss: 0.03817801015390696\n",
      "Epoch 98/100\n",
      "Training Loss: 0.060896784828154235\n",
      "Validation Loss: 0.06325054577826383\n",
      "Epoch 99/100\n",
      "Training Loss: 0.08513761972591083\n",
      "Validation Loss: 0.05417653160217547\n",
      "Epoch 100/100\n",
      "Training Loss: 0.0819159903267914\n",
      "Validation Loss: 0.04133655112141501\n",
      "    Trial 2/2 for combination 31/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.35404359491004533\n",
      "Validation Loss: 0.3557458083941197\n",
      "Epoch 2/100\n",
      "Training Loss: 0.3574074839778608\n",
      "Validation Loss: 0.17779053034119988\n",
      "Epoch 3/100\n",
      "Training Loss: 0.2660603134314942\n",
      "Validation Loss: 0.17908443254187714\n",
      "Epoch 4/100\n",
      "Training Loss: 0.21682212792849537\n",
      "Validation Loss: 0.11817915383072038\n",
      "Epoch 5/100\n",
      "Training Loss: 0.2154292446343504\n",
      "Validation Loss: 0.22010465014842628\n",
      "Epoch 6/100\n",
      "Training Loss: 0.21052215480900732\n",
      "Validation Loss: 0.1025963168659371\n",
      "Epoch 7/100\n",
      "Training Loss: 0.19169211096727443\n",
      "Validation Loss: 0.08175404888588768\n",
      "Epoch 8/100\n",
      "Training Loss: 0.19909864559806414\n",
      "Validation Loss: 0.07292760292094322\n",
      "Epoch 9/100\n",
      "Training Loss: 0.18856580495460673\n",
      "Validation Loss: 0.06330887901814365\n",
      "Epoch 10/100\n",
      "Training Loss: 0.15432854311334868\n",
      "Validation Loss: 0.07720635208586271\n",
      "Epoch 11/100\n",
      "Training Loss: 0.13927002151474935\n",
      "Validation Loss: 0.1594639242658216\n",
      "Epoch 12/100\n",
      "Training Loss: 0.1568131647215611\n",
      "Validation Loss: 0.07745376205693635\n",
      "Epoch 13/100\n",
      "Training Loss: 0.112562474088922\n",
      "Validation Loss: 0.10991696385103249\n",
      "Epoch 14/100\n",
      "Training Loss: 0.13724622815048768\n",
      "Validation Loss: 0.08028818801795437\n",
      "Epoch 15/100\n",
      "Training Loss: 0.1271937025887164\n",
      "Validation Loss: 0.07686626837678039\n",
      "Epoch 16/100\n",
      "Training Loss: 0.11900911071864072\n",
      "Validation Loss: 0.09787008565398489\n",
      "Epoch 17/100\n",
      "Training Loss: 0.12337944433236317\n",
      "Validation Loss: 0.05333028238023585\n",
      "Epoch 18/100\n",
      "Training Loss: 0.10793023077417588\n",
      "Validation Loss: 0.07635037531706446\n",
      "Epoch 19/100\n",
      "Training Loss: 0.14054731290374053\n",
      "Validation Loss: 0.06330935838550603\n",
      "Epoch 20/100\n",
      "Training Loss: 0.12412229458301138\n",
      "Validation Loss: 0.07106378314640634\n",
      "Epoch 21/100\n",
      "Training Loss: 0.10985116642080367\n",
      "Validation Loss: 0.08514361762011666\n",
      "Epoch 22/100\n",
      "Training Loss: 0.11407242377632554\n",
      "Validation Loss: 0.08326205367774543\n",
      "Epoch 23/100\n",
      "Training Loss: 0.11880880399204126\n",
      "Validation Loss: 0.1254546161151818\n",
      "Epoch 24/100\n",
      "Training Loss: 0.09553974948904313\n",
      "Validation Loss: 0.06199873640440887\n",
      "Epoch 25/100\n",
      "Training Loss: 0.09998422352724314\n",
      "Validation Loss: 0.0604878225016246\n",
      "Epoch 26/100\n",
      "Training Loss: 0.1207010122536201\n",
      "Validation Loss: 0.06702830390604778\n",
      "Epoch 27/100\n",
      "Training Loss: 0.1046900294880346\n",
      "Validation Loss: 0.05966679554558398\n",
      "Epoch 28/100\n",
      "Training Loss: 0.09285021183657118\n",
      "Validation Loss: 0.05576667743986437\n",
      "Epoch 29/100\n",
      "Training Loss: 0.10246820266643586\n",
      "Validation Loss: 0.055863309513535776\n",
      "Epoch 30/100\n",
      "Training Loss: 0.09106520111571996\n",
      "Validation Loss: 0.06904294691767002\n",
      "Epoch 31/100\n",
      "Training Loss: 0.09513076914710787\n",
      "Validation Loss: 0.07024165831747255\n",
      "Epoch 32/100\n",
      "Training Loss: 0.09371365382260625\n",
      "Validation Loss: 0.06357725940907742\n",
      "Epoch 33/100\n",
      "Training Loss: 0.1009237656843855\n",
      "Validation Loss: 0.041374158392618376\n",
      "Epoch 34/100\n",
      "Training Loss: 0.09022420558287735\n",
      "Validation Loss: 0.054708742522800155\n",
      "Epoch 35/100\n",
      "Training Loss: 0.10532872759906346\n",
      "Validation Loss: 0.052434774600672994\n",
      "Epoch 36/100\n",
      "Training Loss: 0.08858973811635354\n",
      "Validation Loss: 0.0494531923236904\n",
      "Epoch 37/100\n",
      "Training Loss: 0.07814383175294408\n",
      "Validation Loss: 0.05298679390260469\n",
      "Epoch 38/100\n",
      "Training Loss: 0.09135435384517042\n",
      "Validation Loss: 0.056722332510461174\n",
      "Epoch 39/100\n",
      "Training Loss: 0.09311646269643158\n",
      "Validation Loss: 0.04733638808880696\n",
      "Epoch 40/100\n",
      "Training Loss: 0.0976412606314195\n",
      "Validation Loss: 0.05651442825054821\n",
      "Epoch 41/100\n",
      "Training Loss: 0.08553173260303387\n",
      "Validation Loss: 0.050076775360636774\n",
      "Epoch 42/100\n",
      "Training Loss: 0.07373600899566925\n",
      "Validation Loss: 0.05454203614711952\n",
      "Epoch 43/100\n",
      "Training Loss: 0.08986329903389245\n",
      "Validation Loss: 0.04825435000754309\n",
      "Epoch 44/100\n",
      "Training Loss: 0.08187126632900867\n",
      "Validation Loss: 0.04715195727850112\n",
      "Epoch 45/100\n",
      "Training Loss: 0.0850177023197131\n",
      "Validation Loss: 0.056927728504491446\n",
      "Epoch 46/100\n",
      "Training Loss: 0.08630957748137418\n",
      "Validation Loss: 0.07108894877603861\n",
      "Epoch 47/100\n",
      "Training Loss: 0.07183101497569035\n",
      "Validation Loss: 0.06110567473743148\n",
      "Epoch 48/100\n",
      "Training Loss: 0.08393376056228792\n",
      "Validation Loss: 0.07681534621477942\n",
      "Epoch 49/100\n",
      "Training Loss: 0.08159194467462627\n",
      "Validation Loss: 0.0462884933489223\n",
      "Epoch 50/100\n",
      "Training Loss: 0.0802733657174663\n",
      "Validation Loss: 0.04954016303742047\n",
      "Epoch 51/100\n",
      "Training Loss: 0.07072824159744388\n",
      "Validation Loss: 0.05554442049790733\n",
      "Epoch 52/100\n",
      "Training Loss: 0.09213281753018407\n",
      "Validation Loss: 0.04318499350547504\n",
      "Epoch 53/100\n",
      "Training Loss: 0.0831746180982729\n",
      "Validation Loss: 0.04415617955794337\n",
      "Epoch 54/100\n",
      "Training Loss: 0.08680514867604683\n",
      "Validation Loss: 0.05763417188291756\n",
      "Epoch 55/100\n",
      "Training Loss: 0.08710990360411312\n",
      "Validation Loss: 0.050697595730963295\n",
      "Epoch 56/100\n",
      "Training Loss: 0.07664491737396252\n",
      "Validation Loss: 0.042678197259015976\n",
      "Epoch 57/100\n",
      "Training Loss: 0.07932214285938391\n",
      "Validation Loss: 0.03748101113888891\n",
      "Epoch 58/100\n",
      "Training Loss: 0.07215440853521912\n",
      "Validation Loss: 0.047661585180608514\n",
      "Epoch 59/100\n",
      "Training Loss: 0.08905676634885709\n",
      "Validation Loss: 0.03550034228885486\n",
      "Epoch 60/100\n",
      "Training Loss: 0.08364227687507798\n",
      "Validation Loss: 0.04075674684877414\n",
      "Epoch 61/100\n",
      "Training Loss: 0.07663633738605552\n",
      "Validation Loss: 0.0469634301813703\n",
      "Epoch 62/100\n",
      "Training Loss: 0.06582546651757477\n",
      "Validation Loss: 0.06576923395396436\n",
      "Epoch 63/100\n",
      "Training Loss: 0.07469121260611479\n",
      "Validation Loss: 0.04061188889939543\n",
      "Epoch 64/100\n",
      "Training Loss: 0.086164200739964\n",
      "Validation Loss: 0.05693095788778646\n",
      "Epoch 65/100\n",
      "Training Loss: 0.08314524883995829\n",
      "Validation Loss: 0.08381359477442082\n",
      "Epoch 66/100\n",
      "Training Loss: 0.07787678314421825\n",
      "Validation Loss: 0.035502782936774446\n",
      "Epoch 67/100\n",
      "Training Loss: 0.07423093723494419\n",
      "Validation Loss: 0.04287077919831494\n",
      "Epoch 68/100\n",
      "Training Loss: 0.08186214294705838\n",
      "Validation Loss: 0.05293646365275677\n",
      "Epoch 69/100\n",
      "Training Loss: 0.08097179680721767\n",
      "Validation Loss: 0.0365658744866358\n",
      "Epoch 70/100\n",
      "Training Loss: 0.07463648893774381\n",
      "Validation Loss: 0.04703643635279371\n",
      "Epoch 71/100\n",
      "Training Loss: 0.07929658289539296\n",
      "Validation Loss: 0.05006575218147946\n",
      "Epoch 72/100\n",
      "Training Loss: 0.07319587312061232\n",
      "Validation Loss: 0.03933923475721987\n",
      "Epoch 73/100\n",
      "Training Loss: 0.07866207570996979\n",
      "Validation Loss: 0.05328650615020157\n",
      "Epoch 74/100\n",
      "Training Loss: 0.07998317604175567\n",
      "Validation Loss: 0.04834202212267858\n",
      "Epoch 75/100\n",
      "Training Loss: 0.08872987774418752\n",
      "Validation Loss: 0.04803191668497757\n",
      "Epoch 76/100\n",
      "Training Loss: 0.07576116693005805\n",
      "Validation Loss: 0.047861421065417686\n",
      "Epoch 77/100\n",
      "Training Loss: 0.07166165030172932\n",
      "Validation Loss: 0.0527691455792396\n",
      "Epoch 78/100\n",
      "Training Loss: 0.08347690513655466\n",
      "Validation Loss: 0.04153958921632221\n",
      "Epoch 79/100\n",
      "Training Loss: 0.07686969438447078\n",
      "Validation Loss: 0.041508921562711756\n",
      "Epoch 80/100\n",
      "Training Loss: 0.08585481407901667\n",
      "Validation Loss: 0.045958856187807685\n",
      "Epoch 81/100\n",
      "Training Loss: 0.07220171773290171\n",
      "Validation Loss: 0.08252590330665695\n",
      "Epoch 82/100\n",
      "Training Loss: 0.07366377418606726\n",
      "Validation Loss: 0.06908164425804227\n",
      "Epoch 83/100\n",
      "Training Loss: 0.0788476933212762\n",
      "Validation Loss: 0.06060533138431925\n",
      "Epoch 84/100\n",
      "Training Loss: 0.07789651203503295\n",
      "Validation Loss: 0.04626205231741739\n",
      "Epoch 85/100\n",
      "Training Loss: 0.08257791523507144\n",
      "Validation Loss: 0.06868364991175789\n",
      "Epoch 86/100\n",
      "Training Loss: 0.07068633197936103\n",
      "Validation Loss: 0.056672881897275554\n",
      "Epoch 87/100\n",
      "Training Loss: 0.08904992477373098\n",
      "Validation Loss: 0.03877852163872528\n",
      "Epoch 88/100\n",
      "Training Loss: 0.08638863559828942\n",
      "Validation Loss: 0.0469113668610722\n",
      "Epoch 89/100\n",
      "Training Loss: 0.07674446359914903\n",
      "Validation Loss: 0.05312808570375542\n",
      "Epoch 90/100\n",
      "Training Loss: 0.08304025701915227\n",
      "Validation Loss: 0.05539691159006134\n",
      "Epoch 91/100\n",
      "Training Loss: 0.07992176079964257\n",
      "Validation Loss: 0.06632268900130099\n",
      "Epoch 92/100\n",
      "Training Loss: 0.07127351388282362\n",
      "Validation Loss: 0.052763105707297586\n",
      "Epoch 93/100\n",
      "Training Loss: 0.07987365639313403\n",
      "Validation Loss: 0.039692125282932325\n",
      "Epoch 94/100\n",
      "Training Loss: 0.0803330055136999\n",
      "Validation Loss: 0.03390045177945831\n",
      "Epoch 95/100\n",
      "Training Loss: 0.07797166241289595\n",
      "Validation Loss: 0.06723496806893049\n",
      "Epoch 96/100\n",
      "Training Loss: 0.07481029926691267\n",
      "Validation Loss: 0.04480948121809335\n",
      "Epoch 97/100\n",
      "Training Loss: 0.08116174642648925\n",
      "Validation Loss: 0.047330413105139954\n",
      "Epoch 98/100\n",
      "Training Loss: 0.07667369646199167\n",
      "Validation Loss: 0.04105544139331177\n",
      "Epoch 99/100\n",
      "Training Loss: 0.08494476694376277\n",
      "Validation Loss: 0.07431571808758712\n",
      "Epoch 100/100\n",
      "Training Loss: 0.07357481603268105\n",
      "Validation Loss: 0.04491216516120698\n",
      "Combination 31: Avg Training Loss = 0.10498924806162001, Avg Validation Loss = 0.06810783412654836\n",
      "Testing combination 32/48: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.001, 'sequence_length': 6, 'dropout_rate': 0.2}\n",
      "Truncated Train Data Shape: (70, 6, 1)\n",
      "Truncated Validation Data Shape: (10, 6, 1)\n",
      "    Trial 1/2 for combination 32/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.44940474889852194\n",
      "Validation Loss: 0.2328272901161879\n",
      "Epoch 2/100\n",
      "Training Loss: 0.2967941316193021\n",
      "Validation Loss: 0.08271520502578888\n",
      "Epoch 3/100\n",
      "Training Loss: 0.1865324121785101\n",
      "Validation Loss: 0.10657348528161048\n",
      "Epoch 4/100\n",
      "Training Loss: 0.1803099390857225\n",
      "Validation Loss: 0.07348058297927683\n",
      "Epoch 5/100\n",
      "Training Loss: 0.18841332335793792\n",
      "Validation Loss: 0.10364497959243506\n",
      "Epoch 6/100\n",
      "Training Loss: 0.13937698039496602\n",
      "Validation Loss: 0.07722091188527833\n",
      "Epoch 7/100\n",
      "Training Loss: 0.13811868938945374\n",
      "Validation Loss: 0.08403314722187193\n",
      "Epoch 8/100\n",
      "Training Loss: 0.1880591362299233\n",
      "Validation Loss: 0.09241466997891953\n",
      "Epoch 9/100\n",
      "Training Loss: 0.1324641996089737\n",
      "Validation Loss: 0.07217714621075892\n",
      "Epoch 10/100\n",
      "Training Loss: 0.14260387157683455\n",
      "Validation Loss: 0.11417540193954297\n",
      "Epoch 11/100\n",
      "Training Loss: 0.16987239536489745\n",
      "Validation Loss: 0.057216910908661045\n",
      "Epoch 12/100\n",
      "Training Loss: 0.10226858676947496\n",
      "Validation Loss: 0.1079781434482406\n",
      "Epoch 13/100\n",
      "Training Loss: 0.10512632410771462\n",
      "Validation Loss: 0.07849250313628549\n",
      "Epoch 14/100\n",
      "Training Loss: 0.1362068931986081\n",
      "Validation Loss: 0.07203489817222941\n",
      "Epoch 15/100\n",
      "Training Loss: 0.11845310582565556\n",
      "Validation Loss: 0.069394876733092\n",
      "Epoch 16/100\n",
      "Training Loss: 0.1237474878551565\n",
      "Validation Loss: 0.07845045739427955\n",
      "Epoch 17/100\n",
      "Training Loss: 0.11201231666499381\n",
      "Validation Loss: 0.0653548240438554\n",
      "Epoch 18/100\n",
      "Training Loss: 0.12460241913162205\n",
      "Validation Loss: 0.07195120230463334\n",
      "Epoch 19/100\n",
      "Training Loss: 0.0912235238223121\n",
      "Validation Loss: 0.05081491806280296\n",
      "Epoch 20/100\n",
      "Training Loss: 0.09110361970214359\n",
      "Validation Loss: 0.05054491927054293\n",
      "Epoch 21/100\n",
      "Training Loss: 0.10683334954729079\n",
      "Validation Loss: 0.06710599320807269\n",
      "Epoch 22/100\n",
      "Training Loss: 0.08901256551352359\n",
      "Validation Loss: 0.043892092686145745\n",
      "Epoch 23/100\n",
      "Training Loss: 0.09605077361001052\n",
      "Validation Loss: 0.09519943083116479\n",
      "Epoch 24/100\n",
      "Training Loss: 0.09158270649174231\n",
      "Validation Loss: 0.061087367067268764\n",
      "Epoch 25/100\n",
      "Training Loss: 0.10098864660787021\n",
      "Validation Loss: 0.0649080385829351\n",
      "Epoch 26/100\n",
      "Training Loss: 0.08923528438141726\n",
      "Validation Loss: 0.04715063146477192\n",
      "Epoch 27/100\n",
      "Training Loss: 0.1020277158662502\n",
      "Validation Loss: 0.0620477053442474\n",
      "Epoch 28/100\n",
      "Training Loss: 0.09228983902122484\n",
      "Validation Loss: 0.06069838072058995\n",
      "Epoch 29/100\n",
      "Training Loss: 0.08896800788964983\n",
      "Validation Loss: 0.05770367711742351\n",
      "Epoch 30/100\n",
      "Training Loss: 0.10215503199064069\n",
      "Validation Loss: 0.05652941812816742\n",
      "Epoch 31/100\n",
      "Training Loss: 0.08557719137852005\n",
      "Validation Loss: 0.06253702532870123\n",
      "Epoch 32/100\n",
      "Training Loss: 0.09066548812476126\n",
      "Validation Loss: 0.05556010630900669\n",
      "Epoch 33/100\n",
      "Training Loss: 0.0825670751485565\n",
      "Validation Loss: 0.06287103621715365\n",
      "Epoch 34/100\n",
      "Training Loss: 0.08516041986292212\n",
      "Validation Loss: 0.040501466165968475\n",
      "Epoch 35/100\n",
      "Training Loss: 0.08020146161911046\n",
      "Validation Loss: 0.04710049600315482\n",
      "Epoch 36/100\n",
      "Training Loss: 0.07937999174089681\n",
      "Validation Loss: 0.0592992046018994\n",
      "Epoch 37/100\n",
      "Training Loss: 0.09201272057345751\n",
      "Validation Loss: 0.06176539415658381\n",
      "Epoch 38/100\n",
      "Training Loss: 0.08762105072098134\n",
      "Validation Loss: 0.0575206676404009\n",
      "Epoch 39/100\n",
      "Training Loss: 0.0857915038827835\n",
      "Validation Loss: 0.0729331064732375\n",
      "Epoch 40/100\n",
      "Training Loss: 0.07526063720414178\n",
      "Validation Loss: 0.04584292985985022\n",
      "Epoch 41/100\n",
      "Training Loss: 0.07920377587647387\n",
      "Validation Loss: 0.053747155949510864\n",
      "Epoch 42/100\n",
      "Training Loss: 0.08688578742378529\n",
      "Validation Loss: 0.06045156170193783\n",
      "Epoch 43/100\n",
      "Training Loss: 0.08472853555497453\n",
      "Validation Loss: 0.062081649165632656\n",
      "Epoch 44/100\n",
      "Training Loss: 0.07404296181328858\n",
      "Validation Loss: 0.037985322075586456\n",
      "Epoch 45/100\n",
      "Training Loss: 0.07980792473862255\n",
      "Validation Loss: 0.04134815330673141\n",
      "Epoch 46/100\n",
      "Training Loss: 0.07754005664562864\n",
      "Validation Loss: 0.041099172287675116\n",
      "Epoch 47/100\n",
      "Training Loss: 0.078898766166141\n",
      "Validation Loss: 0.05592193418007043\n",
      "Epoch 48/100\n",
      "Training Loss: 0.0700681970469012\n",
      "Validation Loss: 0.03732906628295673\n",
      "Epoch 49/100\n",
      "Training Loss: 0.07789406342152153\n",
      "Validation Loss: 0.03899478781600632\n",
      "Epoch 50/100\n",
      "Training Loss: 0.07761756653251224\n",
      "Validation Loss: 0.03375617470153378\n",
      "Epoch 51/100\n",
      "Training Loss: 0.07898546597379512\n",
      "Validation Loss: 0.05475996764023986\n",
      "Epoch 52/100\n",
      "Training Loss: 0.07494699128733871\n",
      "Validation Loss: 0.04593528161944117\n",
      "Epoch 53/100\n",
      "Training Loss: 0.07211131627304278\n",
      "Validation Loss: 0.04644222826689113\n",
      "Epoch 54/100\n",
      "Training Loss: 0.0825148912677617\n",
      "Validation Loss: 0.06310117703493816\n",
      "Epoch 55/100\n",
      "Training Loss: 0.08354839575526418\n",
      "Validation Loss: 0.0469412228215314\n",
      "Epoch 56/100\n",
      "Training Loss: 0.07155322540377622\n",
      "Validation Loss: 0.04761128303271484\n",
      "Epoch 57/100\n",
      "Training Loss: 0.08311797490875995\n",
      "Validation Loss: 0.06636599988342029\n",
      "Epoch 58/100\n",
      "Training Loss: 0.07949220692494366\n",
      "Validation Loss: 0.0691408075962385\n",
      "Epoch 59/100\n",
      "Training Loss: 0.07096524013553218\n",
      "Validation Loss: 0.055900199095622695\n",
      "Epoch 60/100\n",
      "Training Loss: 0.07830959482331361\n",
      "Validation Loss: 0.05160156857047556\n",
      "Epoch 61/100\n",
      "Training Loss: 0.0698850183564266\n",
      "Validation Loss: 0.06301154928498197\n",
      "Epoch 62/100\n",
      "Training Loss: 0.07514913435317876\n",
      "Validation Loss: 0.04530043775569298\n",
      "Epoch 63/100\n",
      "Training Loss: 0.07796089217760638\n",
      "Validation Loss: 0.0428195952725306\n",
      "Epoch 64/100\n",
      "Training Loss: 0.07509354990011029\n",
      "Validation Loss: 0.05786024517134943\n",
      "Epoch 65/100\n",
      "Training Loss: 0.07428066984839411\n",
      "Validation Loss: 0.049206379475578176\n",
      "Epoch 66/100\n",
      "Training Loss: 0.08580865031057165\n",
      "Validation Loss: 0.04282632098540015\n",
      "Epoch 67/100\n",
      "Training Loss: 0.06561963016663894\n",
      "Validation Loss: 0.036268463488513855\n",
      "Epoch 68/100\n",
      "Training Loss: 0.08028744903333385\n",
      "Validation Loss: 0.04581278044599918\n",
      "Epoch 69/100\n",
      "Training Loss: 0.08305758574491333\n",
      "Validation Loss: 0.06503356807552817\n",
      "Epoch 70/100\n",
      "Training Loss: 0.07435103308332017\n",
      "Validation Loss: 0.04723700728401622\n",
      "Epoch 71/100\n",
      "Training Loss: 0.0805272615932068\n",
      "Validation Loss: 0.041363645906085975\n",
      "Epoch 72/100\n",
      "Training Loss: 0.07671891212685954\n",
      "Validation Loss: 0.05074483324907436\n",
      "Epoch 73/100\n",
      "Training Loss: 0.08169313512921657\n",
      "Validation Loss: 0.04967293434882161\n",
      "Epoch 74/100\n",
      "Training Loss: 0.08099936695295361\n",
      "Validation Loss: 0.032087524091714295\n",
      "Epoch 75/100\n",
      "Training Loss: 0.07807640209466551\n",
      "Validation Loss: 0.05735065848707148\n",
      "Epoch 76/100\n",
      "Training Loss: 0.0771681647794424\n",
      "Validation Loss: 0.04374883107321845\n",
      "Epoch 77/100\n",
      "Training Loss: 0.07197689657852854\n",
      "Validation Loss: 0.0686677689849291\n",
      "Epoch 78/100\n",
      "Training Loss: 0.07151663360072777\n",
      "Validation Loss: 0.055357552088400264\n",
      "Epoch 79/100\n",
      "Training Loss: 0.09194690006248306\n",
      "Validation Loss: 0.05693606097448339\n",
      "Epoch 80/100\n",
      "Training Loss: 0.0733580182454151\n",
      "Validation Loss: 0.03633680287105721\n",
      "Epoch 81/100\n",
      "Training Loss: 0.07775713955827365\n",
      "Validation Loss: 0.05906273359041927\n",
      "Epoch 82/100\n",
      "Training Loss: 0.08526990107192514\n",
      "Validation Loss: 0.04437099520190184\n",
      "Epoch 83/100\n",
      "Training Loss: 0.08131565798450408\n",
      "Validation Loss: 0.04466678890972809\n",
      "Epoch 84/100\n",
      "Training Loss: 0.07938798016121332\n",
      "Validation Loss: 0.09137171127976729\n",
      "Epoch 85/100\n",
      "Training Loss: 0.07729972902895448\n",
      "Validation Loss: 0.04455312114871669\n",
      "Epoch 86/100\n",
      "Training Loss: 0.07460856404001942\n",
      "Validation Loss: 0.04941592353126287\n",
      "Epoch 87/100\n",
      "Training Loss: 0.07455070507026605\n",
      "Validation Loss: 0.06935462169124075\n",
      "Epoch 88/100\n",
      "Training Loss: 0.08540003977868318\n",
      "Validation Loss: 0.045856287540032285\n",
      "Epoch 89/100\n",
      "Training Loss: 0.08161604795906208\n",
      "Validation Loss: 0.07327044946885239\n",
      "Epoch 90/100\n",
      "Training Loss: 0.0708722806640624\n",
      "Validation Loss: 0.04124905041079959\n",
      "Epoch 91/100\n",
      "Training Loss: 0.0790358779447009\n",
      "Validation Loss: 0.059452302464986964\n",
      "Epoch 92/100\n",
      "Training Loss: 0.09196403715486373\n",
      "Validation Loss: 0.04993006738342775\n",
      "Epoch 93/100\n",
      "Training Loss: 0.07351976606462389\n",
      "Validation Loss: 0.05687543945890004\n",
      "Epoch 94/100\n",
      "Training Loss: 0.07396550673905229\n",
      "Validation Loss: 0.08949953179088468\n",
      "Epoch 95/100\n",
      "Training Loss: 0.08624498229734086\n",
      "Validation Loss: 0.04662206435622295\n",
      "Epoch 96/100\n",
      "Training Loss: 0.08952872163731898\n",
      "Validation Loss: 0.04502535269480133\n",
      "Epoch 97/100\n",
      "Training Loss: 0.08538043007462011\n",
      "Validation Loss: 0.09152776045779672\n",
      "Epoch 98/100\n",
      "Training Loss: 0.08867595923749601\n",
      "Validation Loss: 0.06595500163456929\n",
      "Epoch 99/100\n",
      "Training Loss: 0.07834994082014062\n",
      "Validation Loss: 0.08564696532735484\n",
      "Epoch 100/100\n",
      "Training Loss: 0.07918422642938633\n",
      "Validation Loss: 0.06635688161433084\n",
      "    Trial 2/2 for combination 32/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.4096426647494874\n",
      "Validation Loss: 0.43582436485024373\n",
      "Epoch 2/100\n",
      "Training Loss: 0.33267103552516875\n",
      "Validation Loss: 0.09175559343109173\n",
      "Epoch 3/100\n",
      "Training Loss: 0.260555710103516\n",
      "Validation Loss: 0.3457628247674678\n",
      "Epoch 4/100\n",
      "Training Loss: 0.29256073961223533\n",
      "Validation Loss: 0.22808922945727805\n",
      "Epoch 5/100\n",
      "Training Loss: 0.22723942305666323\n",
      "Validation Loss: 0.20877899948979056\n",
      "Epoch 6/100\n",
      "Training Loss: 0.2068769947767052\n",
      "Validation Loss: 0.26773988081606126\n",
      "Epoch 7/100\n",
      "Training Loss: 0.20318600461143904\n",
      "Validation Loss: 0.17287768063530515\n",
      "Epoch 8/100\n",
      "Training Loss: 0.14201688066080848\n",
      "Validation Loss: 0.11501090945053534\n",
      "Epoch 9/100\n",
      "Training Loss: 0.16344561870173788\n",
      "Validation Loss: 0.13126563458245508\n",
      "Epoch 10/100\n",
      "Training Loss: 0.15899399001345543\n",
      "Validation Loss: 0.13761215096817841\n",
      "Epoch 11/100\n",
      "Training Loss: 0.1621636146447916\n",
      "Validation Loss: 0.08961355683854716\n",
      "Epoch 12/100\n",
      "Training Loss: 0.14813265434030679\n",
      "Validation Loss: 0.1057754093075562\n",
      "Epoch 13/100\n",
      "Training Loss: 0.15251278608429789\n",
      "Validation Loss: 0.08531559834130731\n",
      "Epoch 14/100\n",
      "Training Loss: 0.15703828582653215\n",
      "Validation Loss: 0.0944629320316744\n",
      "Epoch 15/100\n",
      "Training Loss: 0.13056515739207092\n",
      "Validation Loss: 0.07719601293225078\n",
      "Epoch 16/100\n",
      "Training Loss: 0.13119844154143162\n",
      "Validation Loss: 0.08587059738046032\n",
      "Epoch 17/100\n",
      "Training Loss: 0.12291499033322274\n",
      "Validation Loss: 0.12059103724238998\n",
      "Epoch 18/100\n",
      "Training Loss: 0.11778017332668568\n",
      "Validation Loss: 0.0689527481448092\n",
      "Epoch 19/100\n",
      "Training Loss: 0.13718316640148076\n",
      "Validation Loss: 0.06549921241123507\n",
      "Epoch 20/100\n",
      "Training Loss: 0.11153827590200709\n",
      "Validation Loss: 0.11999725005024972\n",
      "Epoch 21/100\n",
      "Training Loss: 0.11125171005336472\n",
      "Validation Loss: 0.0528886976800669\n",
      "Epoch 22/100\n",
      "Training Loss: 0.1097852049115418\n",
      "Validation Loss: 0.054845680047923576\n",
      "Epoch 23/100\n",
      "Training Loss: 0.09429107931816272\n",
      "Validation Loss: 0.1118071820855445\n",
      "Epoch 24/100\n",
      "Training Loss: 0.09110143981529646\n",
      "Validation Loss: 0.05411273465433014\n",
      "Epoch 25/100\n",
      "Training Loss: 0.100148541647268\n",
      "Validation Loss: 0.07148258931449408\n",
      "Epoch 26/100\n",
      "Training Loss: 0.09834203779578715\n",
      "Validation Loss: 0.07291409497977518\n",
      "Epoch 27/100\n",
      "Training Loss: 0.10498889917491004\n",
      "Validation Loss: 0.04723429944008553\n",
      "Epoch 28/100\n",
      "Training Loss: 0.11238143574424696\n",
      "Validation Loss: 0.05681257308079444\n",
      "Epoch 29/100\n",
      "Training Loss: 0.08105505176684502\n",
      "Validation Loss: 0.05137737177779479\n",
      "Epoch 30/100\n",
      "Training Loss: 0.0895194582111616\n",
      "Validation Loss: 0.04518254568804138\n",
      "Epoch 31/100\n",
      "Training Loss: 0.08163619968432947\n",
      "Validation Loss: 0.04230219617251206\n",
      "Epoch 32/100\n",
      "Training Loss: 0.09355278108108993\n",
      "Validation Loss: 0.05944584360232512\n",
      "Epoch 33/100\n",
      "Training Loss: 0.09289483091515241\n",
      "Validation Loss: 0.04930456947628533\n",
      "Epoch 34/100\n",
      "Training Loss: 0.10429788232381547\n",
      "Validation Loss: 0.04843706349734582\n",
      "Epoch 35/100\n",
      "Training Loss: 0.07489614440013631\n",
      "Validation Loss: 0.05485946601069901\n",
      "Epoch 36/100\n",
      "Training Loss: 0.07967507664906139\n",
      "Validation Loss: 0.04462913328826969\n",
      "Epoch 37/100\n",
      "Training Loss: 0.0876692968128668\n",
      "Validation Loss: 0.049998859572415635\n",
      "Epoch 38/100\n",
      "Training Loss: 0.08615444841891845\n",
      "Validation Loss: 0.044420368630449306\n",
      "Epoch 39/100\n",
      "Training Loss: 0.07928878459961922\n",
      "Validation Loss: 0.05410450707375984\n",
      "Epoch 40/100\n",
      "Training Loss: 0.08157713632453607\n",
      "Validation Loss: 0.0684131269642256\n",
      "Epoch 41/100\n",
      "Training Loss: 0.07690277461974246\n",
      "Validation Loss: 0.04747327788913945\n",
      "Epoch 42/100\n",
      "Training Loss: 0.08319025291047218\n",
      "Validation Loss: 0.04967937403403541\n",
      "Epoch 43/100\n",
      "Training Loss: 0.0724571951098663\n",
      "Validation Loss: 0.051652720368476276\n",
      "Epoch 44/100\n",
      "Training Loss: 0.08008567865859585\n",
      "Validation Loss: 0.064818682938698\n",
      "Epoch 45/100\n",
      "Training Loss: 0.07874784580834689\n",
      "Validation Loss: 0.05438575093851685\n",
      "Epoch 46/100\n",
      "Training Loss: 0.08256294270632154\n",
      "Validation Loss: 0.05980939619868895\n",
      "Epoch 47/100\n",
      "Training Loss: 0.08143862438836455\n",
      "Validation Loss: 0.05161844823938101\n",
      "Epoch 48/100\n",
      "Training Loss: 0.07750459477746478\n",
      "Validation Loss: 0.039669567424574954\n",
      "Epoch 49/100\n",
      "Training Loss: 0.07983295714972433\n",
      "Validation Loss: 0.060114494530086306\n",
      "Epoch 50/100\n",
      "Training Loss: 0.08198831821705058\n",
      "Validation Loss: 0.0539617102823278\n",
      "Epoch 51/100\n",
      "Training Loss: 0.07270459764984281\n",
      "Validation Loss: 0.05210196795335838\n",
      "Epoch 52/100\n",
      "Training Loss: 0.0812185847880944\n",
      "Validation Loss: 0.050777731454479134\n",
      "Epoch 53/100\n",
      "Training Loss: 0.07491374880557819\n",
      "Validation Loss: 0.045182055702481363\n",
      "Epoch 54/100\n",
      "Training Loss: 0.08583677550609488\n",
      "Validation Loss: 0.05425802743622191\n",
      "Epoch 55/100\n",
      "Training Loss: 0.0751599889355234\n",
      "Validation Loss: 0.03844708033261147\n",
      "Epoch 56/100\n",
      "Training Loss: 0.08110962382798471\n",
      "Validation Loss: 0.045422112658562136\n",
      "Epoch 57/100\n",
      "Training Loss: 0.08280110527899254\n",
      "Validation Loss: 0.05514156944580242\n",
      "Epoch 58/100\n",
      "Training Loss: 0.07020655233846045\n",
      "Validation Loss: 0.046005405278161396\n",
      "Epoch 59/100\n",
      "Training Loss: 0.07353050444455903\n",
      "Validation Loss: 0.041100100147594555\n",
      "Epoch 60/100\n",
      "Training Loss: 0.08290356383997839\n",
      "Validation Loss: 0.05122565230675319\n",
      "Epoch 61/100\n",
      "Training Loss: 0.06976371565152294\n",
      "Validation Loss: 0.0499405692562868\n",
      "Epoch 62/100\n",
      "Training Loss: 0.08088004334601148\n",
      "Validation Loss: 0.05324158353251194\n",
      "Epoch 63/100\n",
      "Training Loss: 0.07057000382758799\n",
      "Validation Loss: 0.06683248263563016\n",
      "Epoch 64/100\n",
      "Training Loss: 0.07821024221520455\n",
      "Validation Loss: 0.06069393140747248\n",
      "Epoch 65/100\n",
      "Training Loss: 0.07570852673205336\n",
      "Validation Loss: 0.0411236506298547\n",
      "Epoch 66/100\n",
      "Training Loss: 0.08010180984631306\n",
      "Validation Loss: 0.0556377255263049\n",
      "Epoch 67/100\n",
      "Training Loss: 0.08618364599874291\n",
      "Validation Loss: 0.052911094611758004\n",
      "Epoch 68/100\n",
      "Training Loss: 0.08037225450317213\n",
      "Validation Loss: 0.05983442924070702\n",
      "Epoch 69/100\n",
      "Training Loss: 0.08485259249107943\n",
      "Validation Loss: 0.04235643388661041\n",
      "Epoch 70/100\n",
      "Training Loss: 0.073959381262697\n",
      "Validation Loss: 0.038018039594689364\n",
      "Epoch 71/100\n",
      "Training Loss: 0.0717549637772194\n",
      "Validation Loss: 0.04285429884048123\n",
      "Epoch 72/100\n",
      "Training Loss: 0.07240493338310007\n",
      "Validation Loss: 0.05325969441958003\n",
      "Epoch 73/100\n",
      "Training Loss: 0.0800611831886851\n",
      "Validation Loss: 0.03737369259968175\n",
      "Epoch 74/100\n",
      "Training Loss: 0.06963191057434182\n",
      "Validation Loss: 0.04117623821110146\n",
      "Epoch 75/100\n",
      "Training Loss: 0.0717437140346924\n",
      "Validation Loss: 0.04873028897535595\n",
      "Epoch 76/100\n",
      "Training Loss: 0.0800762173488976\n",
      "Validation Loss: 0.07521224529656007\n",
      "Epoch 77/100\n",
      "Training Loss: 0.07106716894914464\n",
      "Validation Loss: 0.04379083293626275\n",
      "Epoch 78/100\n",
      "Training Loss: 0.08664438185894184\n",
      "Validation Loss: 0.06461108939132457\n",
      "Epoch 79/100\n",
      "Training Loss: 0.0804039591484292\n",
      "Validation Loss: 0.059942937761328106\n",
      "Epoch 80/100\n",
      "Training Loss: 0.0791749226234683\n",
      "Validation Loss: 0.03541049325383663\n",
      "Epoch 81/100\n",
      "Training Loss: 0.07843728080716365\n",
      "Validation Loss: 0.05687929842810384\n",
      "Epoch 82/100\n",
      "Training Loss: 0.08259458589192319\n",
      "Validation Loss: 0.05140769087141469\n",
      "Epoch 83/100\n",
      "Training Loss: 0.07533325323081766\n",
      "Validation Loss: 0.04936603992124502\n",
      "Epoch 84/100\n",
      "Training Loss: 0.07910788414191174\n",
      "Validation Loss: 0.047951749147406596\n",
      "Epoch 85/100\n",
      "Training Loss: 0.07540396414330638\n",
      "Validation Loss: 0.03474455578234088\n",
      "Epoch 86/100\n",
      "Training Loss: 0.07904812368218708\n",
      "Validation Loss: 0.07433752620534963\n",
      "Epoch 87/100\n",
      "Training Loss: 0.07829095893576843\n",
      "Validation Loss: 0.0864501970231141\n",
      "Epoch 88/100\n",
      "Training Loss: 0.08406687527336974\n",
      "Validation Loss: 0.0522283594364877\n",
      "Epoch 89/100\n",
      "Training Loss: 0.08970484394534123\n",
      "Validation Loss: 0.054373137866688646\n",
      "Epoch 90/100\n",
      "Training Loss: 0.07485028062022321\n",
      "Validation Loss: 0.04896899389616135\n",
      "Epoch 91/100\n",
      "Training Loss: 0.08315265688790958\n",
      "Validation Loss: 0.07460663410067152\n",
      "Epoch 92/100\n",
      "Training Loss: 0.08411028109375615\n",
      "Validation Loss: 0.03911198660589227\n",
      "Epoch 93/100\n",
      "Training Loss: 0.07687915620273085\n",
      "Validation Loss: 0.03580943849426564\n",
      "Epoch 94/100\n",
      "Training Loss: 0.08612521311010744\n",
      "Validation Loss: 0.04686729882918089\n",
      "Epoch 95/100\n",
      "Training Loss: 0.0770917287337884\n",
      "Validation Loss: 0.07037581094065724\n",
      "Epoch 96/100\n",
      "Training Loss: 0.08678054007995584\n",
      "Validation Loss: 0.059189987633553806\n",
      "Epoch 97/100\n",
      "Training Loss: 0.07700690964977236\n",
      "Validation Loss: 0.04033804644558713\n",
      "Epoch 98/100\n",
      "Training Loss: 0.0631277369285261\n",
      "Validation Loss: 0.09820572352716958\n",
      "Epoch 99/100\n",
      "Training Loss: 0.07789855842572925\n",
      "Validation Loss: 0.05203254073809529\n",
      "Epoch 100/100\n",
      "Training Loss: 0.08435541528324504\n",
      "Validation Loss: 0.07613911416336198\n",
      "Combination 32: Avg Training Loss = 0.10038231698314205, Avg Validation Loss = 0.06754858448851349\n",
      "Testing combination 33/48: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.001, 'sequence_length': 20, 'dropout_rate': 0.1}\n",
      "Truncated Train Data Shape: (70, 20, 1)\n",
      "Truncated Validation Data Shape: (10, 20, 1)\n",
      "    Trial 1/2 for combination 33/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.42398697166627936\n",
      "Validation Loss: 0.08671297555894937\n",
      "Epoch 2/100\n",
      "Training Loss: 0.2426282009953009\n",
      "Validation Loss: 0.21268834589185337\n",
      "Epoch 3/100\n",
      "Training Loss: 0.2585657933007526\n",
      "Validation Loss: 0.24137014477260763\n",
      "Epoch 4/100\n",
      "Training Loss: 0.21201998428993477\n",
      "Validation Loss: 0.11522680293797456\n",
      "Epoch 5/100\n",
      "Training Loss: 0.1971820695473974\n",
      "Validation Loss: 0.08451109793615819\n",
      "Epoch 6/100\n",
      "Training Loss: 0.20577812429487305\n",
      "Validation Loss: 0.0812163180854218\n",
      "Epoch 7/100\n",
      "Training Loss: 0.1895855085497975\n",
      "Validation Loss: 0.12119692137553066\n",
      "Epoch 8/100\n",
      "Training Loss: 0.13240349899088008\n",
      "Validation Loss: 0.14156973205284423\n",
      "Epoch 9/100\n",
      "Training Loss: 0.13555305983300797\n",
      "Validation Loss: 0.061745805558404566\n",
      "Epoch 10/100\n",
      "Training Loss: 0.15691728409919278\n",
      "Validation Loss: 0.10640521040845316\n",
      "Epoch 11/100\n",
      "Training Loss: 0.12983697881411285\n",
      "Validation Loss: 0.07750455375285721\n",
      "Epoch 12/100\n",
      "Training Loss: 0.1408513200804268\n",
      "Validation Loss: 0.08386194995657553\n",
      "Epoch 13/100\n",
      "Training Loss: 0.13555775879599435\n",
      "Validation Loss: 0.055232117144861405\n",
      "Epoch 14/100\n",
      "Training Loss: 0.10995458347119237\n",
      "Validation Loss: 0.06652167228314514\n",
      "Epoch 15/100\n",
      "Training Loss: 0.1233014230681094\n",
      "Validation Loss: 0.10263379487887951\n",
      "Epoch 16/100\n",
      "Training Loss: 0.09579852871306194\n",
      "Validation Loss: 0.04572065686919165\n",
      "Epoch 17/100\n",
      "Training Loss: 0.09630224574559759\n",
      "Validation Loss: 0.05968209952323944\n",
      "Epoch 18/100\n",
      "Training Loss: 0.10666073604466358\n",
      "Validation Loss: 0.05893506993119722\n",
      "Epoch 19/100\n",
      "Training Loss: 0.09581101700551044\n",
      "Validation Loss: 0.06712647384253372\n",
      "Epoch 20/100\n",
      "Training Loss: 0.09428279099824206\n",
      "Validation Loss: 0.054216278516393226\n",
      "Epoch 21/100\n",
      "Training Loss: 0.09095982669484472\n",
      "Validation Loss: 0.051217796353780286\n",
      "Epoch 22/100\n",
      "Training Loss: 0.0885111144680054\n",
      "Validation Loss: 0.04934100131181955\n",
      "Epoch 23/100\n",
      "Training Loss: 0.08708096208444566\n",
      "Validation Loss: 0.0634618389481616\n",
      "Epoch 24/100\n",
      "Training Loss: 0.08150840567129461\n",
      "Validation Loss: 0.049335252623003624\n",
      "Epoch 25/100\n",
      "Training Loss: 0.06747687536307008\n",
      "Validation Loss: 0.04651653800981777\n",
      "Epoch 26/100\n",
      "Training Loss: 0.0783735552528365\n",
      "Validation Loss: 0.06651768494172114\n",
      "Epoch 27/100\n",
      "Training Loss: 0.0821647652212846\n",
      "Validation Loss: 0.059726488073820125\n",
      "Epoch 28/100\n",
      "Training Loss: 0.06939832511886604\n",
      "Validation Loss: 0.06384242163210226\n",
      "Epoch 29/100\n",
      "Training Loss: 0.08900014983532835\n",
      "Validation Loss: 0.06487949382318273\n",
      "Epoch 30/100\n",
      "Training Loss: 0.08335069435440234\n",
      "Validation Loss: 0.04763891791170606\n",
      "Epoch 31/100\n",
      "Training Loss: 0.07049056270339545\n",
      "Validation Loss: 0.0671082172139735\n",
      "Epoch 32/100\n",
      "Training Loss: 0.07267740473375069\n",
      "Validation Loss: 0.039351408678347966\n",
      "Epoch 33/100\n",
      "Training Loss: 0.07412182021727093\n",
      "Validation Loss: 0.03927280352282598\n",
      "Epoch 34/100\n",
      "Training Loss: 0.0769806728589304\n",
      "Validation Loss: 0.07969988838561137\n",
      "Epoch 35/100\n",
      "Training Loss: 0.0689453120904327\n",
      "Validation Loss: 0.03858692235513291\n",
      "Epoch 36/100\n",
      "Training Loss: 0.0702122679819586\n",
      "Validation Loss: 0.047396935948254615\n",
      "Epoch 37/100\n",
      "Training Loss: 0.061986329337726395\n",
      "Validation Loss: 0.04107122884425279\n",
      "Epoch 38/100\n",
      "Training Loss: 0.0692170323356135\n",
      "Validation Loss: 0.05907540138476421\n",
      "Epoch 39/100\n",
      "Training Loss: 0.06433400794566874\n",
      "Validation Loss: 0.053259098254481343\n",
      "Epoch 40/100\n",
      "Training Loss: 0.07911856658999494\n",
      "Validation Loss: 0.03851098815693588\n",
      "Epoch 41/100\n",
      "Training Loss: 0.06372912968689005\n",
      "Validation Loss: 0.0385242814261116\n",
      "Epoch 42/100\n",
      "Training Loss: 0.06927384980579965\n",
      "Validation Loss: 0.06868851024127459\n",
      "Epoch 43/100\n",
      "Training Loss: 0.06984361372937284\n",
      "Validation Loss: 0.06500314717959234\n",
      "Epoch 44/100\n",
      "Training Loss: 0.06805321446038277\n",
      "Validation Loss: 0.07875377431712835\n",
      "Epoch 45/100\n",
      "Training Loss: 0.07768348341658753\n",
      "Validation Loss: 0.04150652409406891\n",
      "Epoch 46/100\n",
      "Training Loss: 0.066482883355322\n",
      "Validation Loss: 0.04949233139090419\n",
      "Epoch 47/100\n",
      "Training Loss: 0.07015726248017179\n",
      "Validation Loss: 0.06417087593425455\n",
      "Epoch 48/100\n",
      "Training Loss: 0.07206553857375189\n",
      "Validation Loss: 0.05484550862155797\n",
      "Epoch 49/100\n",
      "Training Loss: 0.0650144536632366\n",
      "Validation Loss: 0.04908267111410343\n",
      "Epoch 50/100\n",
      "Training Loss: 0.06394855958925914\n",
      "Validation Loss: 0.04117787809978922\n",
      "Epoch 51/100\n",
      "Training Loss: 0.06851053374867719\n",
      "Validation Loss: 0.03772697412221078\n",
      "Epoch 52/100\n",
      "Training Loss: 0.07904516291566044\n",
      "Validation Loss: 0.04066604035794489\n",
      "Epoch 53/100\n",
      "Training Loss: 0.05828306846758047\n",
      "Validation Loss: 0.12197821248637135\n",
      "Epoch 54/100\n",
      "Training Loss: 0.07248353125776674\n",
      "Validation Loss: 0.07549022938501918\n",
      "Epoch 55/100\n",
      "Training Loss: 0.05784220366334313\n",
      "Validation Loss: 0.03721484699944324\n",
      "Epoch 56/100\n",
      "Training Loss: 0.08484358632596702\n",
      "Validation Loss: 0.08189251168945531\n",
      "Epoch 57/100\n",
      "Training Loss: 0.07106392279383292\n",
      "Validation Loss: 0.11259362344292645\n",
      "Epoch 58/100\n",
      "Training Loss: 0.08147455669075315\n",
      "Validation Loss: 0.05750083071699762\n",
      "Epoch 59/100\n",
      "Training Loss: 0.07522163912630986\n",
      "Validation Loss: 0.04318158269699931\n",
      "Epoch 60/100\n",
      "Training Loss: 0.07391383641032637\n",
      "Validation Loss: 0.0757319926461439\n",
      "Epoch 61/100\n",
      "Training Loss: 0.06952490291894196\n",
      "Validation Loss: 0.04617114772766136\n",
      "Epoch 62/100\n",
      "Training Loss: 0.10144614639439013\n",
      "Validation Loss: 0.06273053739974908\n",
      "Epoch 63/100\n",
      "Training Loss: 0.07002302521735677\n",
      "Validation Loss: 0.04501567043453186\n",
      "Epoch 64/100\n",
      "Training Loss: 0.07301649759638475\n",
      "Validation Loss: 0.030226458716564965\n",
      "Epoch 65/100\n",
      "Training Loss: 0.07080753026693873\n",
      "Validation Loss: 0.10459158064890943\n",
      "Epoch 66/100\n",
      "Training Loss: 0.07749002264092995\n",
      "Validation Loss: 0.04310928879542811\n",
      "Epoch 67/100\n",
      "Training Loss: 0.08353661464502453\n",
      "Validation Loss: 0.018138101618703172\n",
      "Epoch 68/100\n",
      "Training Loss: 0.07854592947917904\n",
      "Validation Loss: 0.06302997854631712\n",
      "Epoch 69/100\n",
      "Training Loss: 0.06065758371987041\n",
      "Validation Loss: 0.061452140532624135\n",
      "Epoch 70/100\n",
      "Training Loss: 0.0785048160275602\n",
      "Validation Loss: 0.06570867672879563\n",
      "Epoch 71/100\n",
      "Training Loss: 0.08420851269304214\n",
      "Validation Loss: 0.08480367063306274\n",
      "Epoch 72/100\n",
      "Training Loss: 0.08130604623758234\n",
      "Validation Loss: 0.09502891861195314\n",
      "Epoch 73/100\n",
      "Training Loss: 0.07854120498339047\n",
      "Validation Loss: 0.03825446384092792\n",
      "Epoch 74/100\n",
      "Training Loss: 0.08486408472089114\n",
      "Validation Loss: 0.05635564542054091\n",
      "Epoch 75/100\n",
      "Training Loss: 0.08860301978200262\n",
      "Validation Loss: 0.049159516158671084\n",
      "Epoch 76/100\n",
      "Training Loss: 0.06919442862396621\n",
      "Validation Loss: 0.028280137250368275\n",
      "Epoch 77/100\n",
      "Training Loss: 0.09290955120044521\n",
      "Validation Loss: 0.031041652396663937\n",
      "Epoch 78/100\n",
      "Training Loss: 0.06734385911974233\n",
      "Validation Loss: 0.03807749047607446\n",
      "Epoch 79/100\n",
      "Training Loss: 0.06958402487076372\n",
      "Validation Loss: 0.06857596881393276\n",
      "Epoch 80/100\n",
      "Training Loss: 0.08627289176023067\n",
      "Validation Loss: 0.04302441067667674\n",
      "Epoch 81/100\n",
      "Training Loss: 0.08626221883205976\n",
      "Validation Loss: 0.062470666666149685\n",
      "Epoch 82/100\n",
      "Training Loss: 0.06984122758476376\n",
      "Validation Loss: 0.040877225700804445\n",
      "Epoch 83/100\n",
      "Training Loss: 0.06780912001170036\n",
      "Validation Loss: 0.06040232166212576\n",
      "Epoch 84/100\n",
      "Training Loss: 0.0656299415387157\n",
      "Validation Loss: 0.04101885711256921\n",
      "Epoch 85/100\n",
      "Training Loss: 0.08183163108387813\n",
      "Validation Loss: 0.03987480098728067\n",
      "Epoch 86/100\n",
      "Training Loss: 0.07763190977290375\n",
      "Validation Loss: 0.06584402969785427\n",
      "Epoch 87/100\n",
      "Training Loss: 0.0879066211793684\n",
      "Validation Loss: 0.038278978187465416\n",
      "Epoch 88/100\n",
      "Training Loss: 0.09946065157193815\n",
      "Validation Loss: 0.10532892176830164\n",
      "Epoch 89/100\n",
      "Training Loss: 0.08910519183309225\n",
      "Validation Loss: 0.023273284724598404\n",
      "Epoch 90/100\n",
      "Training Loss: 0.07696514854984693\n",
      "Validation Loss: 0.07384389610753869\n",
      "Epoch 91/100\n",
      "Training Loss: 0.08331790657409374\n",
      "Validation Loss: 0.06613565784043023\n",
      "Epoch 92/100\n",
      "Training Loss: 0.0800285038466076\n",
      "Validation Loss: 0.0813053734698642\n",
      "Epoch 93/100\n",
      "Training Loss: 0.08523735085929258\n",
      "Validation Loss: 0.05038600680932827\n",
      "Epoch 94/100\n",
      "Training Loss: 0.07688463552143336\n",
      "Validation Loss: 0.06429174427263001\n",
      "Epoch 95/100\n",
      "Training Loss: 0.08475966784670018\n",
      "Validation Loss: 0.08445698177301478\n",
      "Epoch 96/100\n",
      "Training Loss: 0.06511583465770991\n",
      "Validation Loss: 0.0978788564725704\n",
      "Epoch 97/100\n",
      "Training Loss: 0.0776490812304228\n",
      "Validation Loss: 0.06782552715871967\n",
      "Epoch 98/100\n",
      "Training Loss: 0.07169128671483163\n",
      "Validation Loss: 0.039744870022348715\n",
      "Epoch 99/100\n",
      "Training Loss: 0.0893589575858592\n",
      "Validation Loss: 0.08299192800152672\n",
      "Epoch 100/100\n",
      "Training Loss: 0.06880406421742162\n",
      "Validation Loss: 0.03509425473431447\n",
      "    Trial 2/2 for combination 33/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.31893561100759177\n",
      "Validation Loss: 0.11839475360258445\n",
      "Epoch 2/100\n",
      "Training Loss: 0.28005080438716284\n",
      "Validation Loss: 0.171814436546048\n",
      "Epoch 3/100\n",
      "Training Loss: 0.23469149539534914\n",
      "Validation Loss: 0.16362602096509682\n",
      "Epoch 4/100\n",
      "Training Loss: 0.2276396296881758\n",
      "Validation Loss: 0.08804256336864877\n",
      "Epoch 5/100\n",
      "Training Loss: 0.1844566341696926\n",
      "Validation Loss: 0.06865182446101446\n",
      "Epoch 6/100\n",
      "Training Loss: 0.20571597545608838\n",
      "Validation Loss: 0.08882921517165411\n",
      "Epoch 7/100\n",
      "Training Loss: 0.1675534539982594\n",
      "Validation Loss: 0.0851461954494982\n",
      "Epoch 8/100\n",
      "Training Loss: 0.17223907033271138\n",
      "Validation Loss: 0.055015642618831\n",
      "Epoch 9/100\n",
      "Training Loss: 0.11926636541239578\n",
      "Validation Loss: 0.09482480874479367\n",
      "Epoch 10/100\n",
      "Training Loss: 0.14145154262183146\n",
      "Validation Loss: 0.07700312195987172\n",
      "Epoch 11/100\n",
      "Training Loss: 0.1252981252134735\n",
      "Validation Loss: 0.07813803850845058\n",
      "Epoch 12/100\n",
      "Training Loss: 0.14304414902616017\n",
      "Validation Loss: 0.05851967289767721\n",
      "Epoch 13/100\n",
      "Training Loss: 0.11498079524567194\n",
      "Validation Loss: 0.04666851160239349\n",
      "Epoch 14/100\n",
      "Training Loss: 0.13001416492648898\n",
      "Validation Loss: 0.08109401659246573\n",
      "Epoch 15/100\n",
      "Training Loss: 0.13033442213997162\n",
      "Validation Loss: 0.06495389591591413\n",
      "Epoch 16/100\n",
      "Training Loss: 0.11031924884182902\n",
      "Validation Loss: 0.03879775969989009\n",
      "Epoch 17/100\n",
      "Training Loss: 0.10321947430916582\n",
      "Validation Loss: 0.08774636118126411\n",
      "Epoch 18/100\n",
      "Training Loss: 0.10758549214814053\n",
      "Validation Loss: 0.07622284836887136\n",
      "Epoch 19/100\n",
      "Training Loss: 0.09943289618749002\n",
      "Validation Loss: 0.07694856425710998\n",
      "Epoch 20/100\n",
      "Training Loss: 0.11035144642222218\n",
      "Validation Loss: 0.033783661494453616\n",
      "Epoch 21/100\n",
      "Training Loss: 0.08853493182423695\n",
      "Validation Loss: 0.04699142265411405\n",
      "Epoch 22/100\n",
      "Training Loss: 0.09947709984911102\n",
      "Validation Loss: 0.10486572240544689\n",
      "Epoch 23/100\n",
      "Training Loss: 0.0851647378993262\n",
      "Validation Loss: 0.05105091034352018\n",
      "Epoch 24/100\n",
      "Training Loss: 0.08983095617216921\n",
      "Validation Loss: 0.03862874674731335\n",
      "Epoch 25/100\n",
      "Training Loss: 0.09046568824248906\n",
      "Validation Loss: 0.05413737693955066\n",
      "Epoch 26/100\n",
      "Training Loss: 0.07050814111114248\n",
      "Validation Loss: 0.06285058671004072\n",
      "Epoch 27/100\n",
      "Training Loss: 0.0739891009741259\n",
      "Validation Loss: 0.04137601768286348\n",
      "Epoch 28/100\n",
      "Training Loss: 0.08015040246967575\n",
      "Validation Loss: 0.04402312199504262\n",
      "Epoch 29/100\n",
      "Training Loss: 0.0803343373131348\n",
      "Validation Loss: 0.045439060860166566\n",
      "Epoch 30/100\n",
      "Training Loss: 0.08142711202958147\n",
      "Validation Loss: 0.035398009604626024\n",
      "Epoch 31/100\n",
      "Training Loss: 0.07325995769369018\n",
      "Validation Loss: 0.06391350490098488\n",
      "Epoch 32/100\n",
      "Training Loss: 0.08323289127515508\n",
      "Validation Loss: 0.04719500239461537\n",
      "Epoch 33/100\n",
      "Training Loss: 0.07726771384941933\n",
      "Validation Loss: 0.04877856779681907\n",
      "Epoch 34/100\n",
      "Training Loss: 0.0819171323631618\n",
      "Validation Loss: 0.06695685824322209\n",
      "Epoch 35/100\n",
      "Training Loss: 0.07134536518595064\n",
      "Validation Loss: 0.05085310759108701\n",
      "Epoch 36/100\n",
      "Training Loss: 0.06926248323089852\n",
      "Validation Loss: 0.047871836660845477\n",
      "Epoch 37/100\n",
      "Training Loss: 0.06876610054403048\n",
      "Validation Loss: 0.06031079254280221\n",
      "Epoch 38/100\n",
      "Training Loss: 0.07949262584034106\n",
      "Validation Loss: 0.07170385992746925\n",
      "Epoch 39/100\n",
      "Training Loss: 0.07893638508262509\n",
      "Validation Loss: 0.05575223012358688\n",
      "Epoch 40/100\n",
      "Training Loss: 0.08152854259508051\n",
      "Validation Loss: 0.044550092606436206\n",
      "Epoch 41/100\n",
      "Training Loss: 0.07233224142778254\n",
      "Validation Loss: 0.05348006965125236\n",
      "Epoch 42/100\n",
      "Training Loss: 0.07791790410379072\n",
      "Validation Loss: 0.09640596130120024\n",
      "Epoch 43/100\n",
      "Training Loss: 0.07124323944273989\n",
      "Validation Loss: 0.037615583606820456\n",
      "Epoch 44/100\n",
      "Training Loss: 0.07842060060630347\n",
      "Validation Loss: 0.05412255918253904\n",
      "Epoch 45/100\n",
      "Training Loss: 0.07447668363753202\n",
      "Validation Loss: 0.05644355575232929\n",
      "Epoch 46/100\n",
      "Training Loss: 0.07632118991087444\n",
      "Validation Loss: 0.06562131795635164\n",
      "Epoch 47/100\n",
      "Training Loss: 0.08008167773852234\n",
      "Validation Loss: 0.03634485583762219\n",
      "Epoch 48/100\n",
      "Training Loss: 0.06081999905679957\n",
      "Validation Loss: 0.04152679518061643\n",
      "Epoch 49/100\n",
      "Training Loss: 0.07910544146936843\n",
      "Validation Loss: 0.04147407938237435\n",
      "Epoch 50/100\n",
      "Training Loss: 0.08291752497672962\n",
      "Validation Loss: 0.0778868551813099\n",
      "Epoch 51/100\n",
      "Training Loss: 0.06484607020241626\n",
      "Validation Loss: 0.044129463010256595\n",
      "Epoch 52/100\n",
      "Training Loss: 0.08470964073067862\n",
      "Validation Loss: 0.060515124451001566\n",
      "Epoch 53/100\n",
      "Training Loss: 0.08212940599017673\n",
      "Validation Loss: 0.0537381118792379\n",
      "Epoch 54/100\n",
      "Training Loss: 0.08212929577300276\n",
      "Validation Loss: 0.05846093234319416\n",
      "Epoch 55/100\n",
      "Training Loss: 0.07028097506328385\n",
      "Validation Loss: 0.028494016565742897\n",
      "Epoch 56/100\n",
      "Training Loss: 0.06002820335969185\n",
      "Validation Loss: 0.08161633636803096\n",
      "Epoch 57/100\n",
      "Training Loss: 0.0751880859848422\n",
      "Validation Loss: 0.027444526801025004\n",
      "Epoch 58/100\n",
      "Training Loss: 0.06349364444240861\n",
      "Validation Loss: 0.07176468228079022\n",
      "Epoch 59/100\n",
      "Training Loss: 0.06634360461964708\n",
      "Validation Loss: 0.0394735527558431\n",
      "Epoch 60/100\n",
      "Training Loss: 0.07932969683261251\n",
      "Validation Loss: 0.0770088145261282\n",
      "Epoch 61/100\n",
      "Training Loss: 0.0725699044087497\n",
      "Validation Loss: 0.0551333104604501\n",
      "Epoch 62/100\n",
      "Training Loss: 0.061828402756681965\n",
      "Validation Loss: 0.058557756272093364\n",
      "Epoch 63/100\n",
      "Training Loss: 0.053735862262397516\n",
      "Validation Loss: 0.04265407280009224\n",
      "Epoch 64/100\n",
      "Training Loss: 0.07643221475481776\n",
      "Validation Loss: 0.04383827592132776\n",
      "Epoch 65/100\n",
      "Training Loss: 0.0780784903424811\n",
      "Validation Loss: 0.04414900333407723\n",
      "Epoch 66/100\n",
      "Training Loss: 0.07423642061614924\n",
      "Validation Loss: 0.05546919132573754\n",
      "Epoch 67/100\n",
      "Training Loss: 0.07586219117131957\n",
      "Validation Loss: 0.07807460509857884\n",
      "Epoch 68/100\n",
      "Training Loss: 0.07264260394664211\n",
      "Validation Loss: 0.04531430259864424\n",
      "Epoch 69/100\n",
      "Training Loss: 0.05112655909879224\n",
      "Validation Loss: 0.04464664400216577\n",
      "Epoch 70/100\n",
      "Training Loss: 0.07395244587519842\n",
      "Validation Loss: 0.05801722179665546\n",
      "Epoch 71/100\n",
      "Training Loss: 0.058058109178534004\n",
      "Validation Loss: 0.05765687155123067\n",
      "Epoch 72/100\n",
      "Training Loss: 0.08030352471010418\n",
      "Validation Loss: 0.04104421811050261\n",
      "Epoch 73/100\n",
      "Training Loss: 0.07858087242599433\n",
      "Validation Loss: 0.023658407663794748\n",
      "Epoch 74/100\n",
      "Training Loss: 0.07559175822643324\n",
      "Validation Loss: 0.05393447574872258\n",
      "Epoch 75/100\n",
      "Training Loss: 0.0639481619802136\n",
      "Validation Loss: 0.03178903049824803\n",
      "Epoch 76/100\n",
      "Training Loss: 0.0728579666360641\n",
      "Validation Loss: 0.03868647935942545\n",
      "Epoch 77/100\n",
      "Training Loss: 0.07778522600355332\n",
      "Validation Loss: 0.06400251147933214\n",
      "Epoch 78/100\n",
      "Training Loss: 0.07880061793834141\n",
      "Validation Loss: 0.048093743668273944\n",
      "Epoch 79/100\n",
      "Training Loss: 0.09227353039800924\n",
      "Validation Loss: 0.054547694759459385\n",
      "Epoch 80/100\n",
      "Training Loss: 0.0752232872584921\n",
      "Validation Loss: 0.08171403506316986\n",
      "Epoch 81/100\n",
      "Training Loss: 0.07327892065776491\n",
      "Validation Loss: 0.05483473350018111\n",
      "Epoch 82/100\n",
      "Training Loss: 0.064418269448243\n",
      "Validation Loss: 0.08295366210525716\n",
      "Epoch 83/100\n",
      "Training Loss: 0.07596453235528178\n",
      "Validation Loss: 0.02469728233460956\n",
      "Epoch 84/100\n",
      "Training Loss: 0.0696001867260689\n",
      "Validation Loss: 0.04954181050698048\n",
      "Epoch 85/100\n",
      "Training Loss: 0.07292706965297188\n",
      "Validation Loss: 0.08507879197458583\n",
      "Epoch 86/100\n",
      "Training Loss: 0.0793270471623574\n",
      "Validation Loss: 0.08172446633014338\n",
      "Epoch 87/100\n",
      "Training Loss: 0.057453611775274624\n",
      "Validation Loss: 0.07160973338953092\n",
      "Epoch 88/100\n",
      "Training Loss: 0.07045899233595161\n",
      "Validation Loss: 0.05263573344057815\n",
      "Epoch 89/100\n",
      "Training Loss: 0.07238573912539553\n",
      "Validation Loss: 0.0344305981204383\n",
      "Epoch 90/100\n",
      "Training Loss: 0.07576114178634703\n",
      "Validation Loss: 0.04006445688838425\n",
      "Epoch 91/100\n",
      "Training Loss: 0.08649331249188545\n",
      "Validation Loss: 0.0498305282090712\n",
      "Epoch 92/100\n",
      "Training Loss: 0.0736809754102309\n",
      "Validation Loss: 0.06246951118468318\n",
      "Epoch 93/100\n",
      "Training Loss: 0.09079256124732395\n",
      "Validation Loss: 0.04819387095128262\n",
      "Epoch 94/100\n",
      "Training Loss: 0.08850085014546165\n",
      "Validation Loss: 0.037693236454123466\n",
      "Epoch 95/100\n",
      "Training Loss: 0.08824027971212736\n",
      "Validation Loss: 0.03107948927232205\n",
      "Epoch 96/100\n",
      "Training Loss: 0.07232680355711546\n",
      "Validation Loss: 0.03169390333086501\n",
      "Epoch 97/100\n",
      "Training Loss: 0.09858721252068915\n",
      "Validation Loss: 0.050842394305182025\n",
      "Epoch 98/100\n",
      "Training Loss: 0.08890400605096348\n",
      "Validation Loss: 0.08975926061886509\n",
      "Epoch 99/100\n",
      "Training Loss: 0.07210519796372926\n",
      "Validation Loss: 0.040239363346854955\n",
      "Epoch 100/100\n",
      "Training Loss: 0.0776779899200326\n",
      "Validation Loss: 0.08636972713002564\n",
      "Combination 33: Avg Training Loss = 0.0933792634055716, Avg Validation Loss = 0.062431853371336776\n",
      "Testing combination 34/48: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.001, 'sequence_length': 20, 'dropout_rate': 0.2}\n",
      "Truncated Train Data Shape: (70, 20, 1)\n",
      "Truncated Validation Data Shape: (10, 20, 1)\n",
      "    Trial 1/2 for combination 34/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.30927363234916283\n",
      "Validation Loss: 0.19541108047983347\n",
      "Epoch 2/100\n",
      "Training Loss: 0.2556381596821187\n",
      "Validation Loss: 0.2638787912514831\n",
      "Epoch 3/100\n",
      "Training Loss: 0.23454901424360056\n",
      "Validation Loss: 0.1460926837460791\n",
      "Epoch 4/100\n",
      "Training Loss: 0.16090980263129998\n",
      "Validation Loss: 0.12513404660339272\n",
      "Epoch 5/100\n",
      "Training Loss: 0.19813431642645735\n",
      "Validation Loss: 0.08504238388755167\n",
      "Epoch 6/100\n",
      "Training Loss: 0.1472076813862476\n",
      "Validation Loss: 0.06919302632580568\n",
      "Epoch 7/100\n",
      "Training Loss: 0.15165644896245953\n",
      "Validation Loss: 0.08741405346024503\n",
      "Epoch 8/100\n",
      "Training Loss: 0.14038479345639845\n",
      "Validation Loss: 0.08931950372144029\n",
      "Epoch 9/100\n",
      "Training Loss: 0.14643539693011737\n",
      "Validation Loss: 0.06946201179160551\n",
      "Epoch 10/100\n",
      "Training Loss: 0.12099587525577356\n",
      "Validation Loss: 0.06263925018427427\n",
      "Epoch 11/100\n",
      "Training Loss: 0.10786928076592249\n",
      "Validation Loss: 0.11503689255676275\n",
      "Epoch 12/100\n",
      "Training Loss: 0.11510974497678415\n",
      "Validation Loss: 0.05873554554681436\n",
      "Epoch 13/100\n",
      "Training Loss: 0.11109232860560765\n",
      "Validation Loss: 0.07588195256596116\n",
      "Epoch 14/100\n",
      "Training Loss: 0.09987012268417639\n",
      "Validation Loss: 0.06943553746417767\n",
      "Epoch 15/100\n",
      "Training Loss: 0.09262768419536734\n",
      "Validation Loss: 0.05067150257979515\n",
      "Epoch 16/100\n",
      "Training Loss: 0.09421160886436368\n",
      "Validation Loss: 0.07241965497297964\n",
      "Epoch 17/100\n",
      "Training Loss: 0.0903244475814011\n",
      "Validation Loss: 0.04958995347360802\n",
      "Epoch 18/100\n",
      "Training Loss: 0.08646661372168679\n",
      "Validation Loss: 0.06803331829290447\n",
      "Epoch 19/100\n",
      "Training Loss: 0.07362671672113827\n",
      "Validation Loss: 0.061474100046692445\n",
      "Epoch 20/100\n",
      "Training Loss: 0.07447973436970662\n",
      "Validation Loss: 0.058055891575072495\n",
      "Epoch 21/100\n",
      "Training Loss: 0.07537071507056246\n",
      "Validation Loss: 0.037815701240517134\n",
      "Epoch 22/100\n",
      "Training Loss: 0.08907962468522047\n",
      "Validation Loss: 0.05632162865423125\n",
      "Epoch 23/100\n",
      "Training Loss: 0.07972257016603519\n",
      "Validation Loss: 0.0510305092736222\n",
      "Epoch 24/100\n",
      "Training Loss: 0.08214874596753272\n",
      "Validation Loss: 0.03978936157227051\n",
      "Epoch 25/100\n",
      "Training Loss: 0.06977368112214205\n",
      "Validation Loss: 0.08411805060098135\n",
      "Epoch 26/100\n",
      "Training Loss: 0.0692844209367304\n",
      "Validation Loss: 0.058291070744121286\n",
      "Epoch 27/100\n",
      "Training Loss: 0.08093710192979565\n",
      "Validation Loss: 0.05570052076845551\n",
      "Epoch 28/100\n",
      "Training Loss: 0.07407212447756918\n",
      "Validation Loss: 0.038392376411887294\n",
      "Epoch 29/100\n",
      "Training Loss: 0.07117509005601064\n",
      "Validation Loss: 0.038608908429647745\n",
      "Epoch 30/100\n",
      "Training Loss: 0.06577256707073022\n",
      "Validation Loss: 0.04263365909564757\n",
      "Epoch 31/100\n",
      "Training Loss: 0.06694875828031453\n",
      "Validation Loss: 0.05156164291112798\n",
      "Epoch 32/100\n",
      "Training Loss: 0.06578326855725494\n",
      "Validation Loss: 0.04494169296072447\n",
      "Epoch 33/100\n",
      "Training Loss: 0.07346891869205747\n",
      "Validation Loss: 0.05487785472466052\n",
      "Epoch 34/100\n",
      "Training Loss: 0.06916620687783906\n",
      "Validation Loss: 0.06637323119107959\n",
      "Epoch 35/100\n",
      "Training Loss: 0.0659428834523615\n",
      "Validation Loss: 0.05397487234617088\n",
      "Epoch 36/100\n",
      "Training Loss: 0.07015629319471957\n",
      "Validation Loss: 0.04153055892216494\n",
      "Epoch 37/100\n",
      "Training Loss: 0.06695437433019473\n",
      "Validation Loss: 0.046154337938843645\n",
      "Epoch 38/100\n",
      "Training Loss: 0.076613070451327\n",
      "Validation Loss: 0.0555761033571321\n",
      "Epoch 39/100\n",
      "Training Loss: 0.06238531327748065\n",
      "Validation Loss: 0.034388486928026096\n",
      "Epoch 40/100\n",
      "Training Loss: 0.06707262838131153\n",
      "Validation Loss: 0.04003065738625176\n",
      "Epoch 41/100\n",
      "Training Loss: 0.07097665497184485\n",
      "Validation Loss: 0.05178091553468254\n",
      "Epoch 42/100\n",
      "Training Loss: 0.0690786136064834\n",
      "Validation Loss: 0.06067183388787528\n",
      "Epoch 43/100\n",
      "Training Loss: 0.07843142970507726\n",
      "Validation Loss: 0.04562777155586058\n",
      "Epoch 44/100\n",
      "Training Loss: 0.07516731493782408\n",
      "Validation Loss: 0.050822485334417244\n",
      "Epoch 45/100\n",
      "Training Loss: 0.06822846416523205\n",
      "Validation Loss: 0.05316055509964178\n",
      "Epoch 46/100\n",
      "Training Loss: 0.06576919971503964\n",
      "Validation Loss: 0.05576127988716336\n",
      "Epoch 47/100\n",
      "Training Loss: 0.06325655931933422\n",
      "Validation Loss: 0.0663829374089092\n",
      "Epoch 48/100\n",
      "Training Loss: 0.07644168142274275\n",
      "Validation Loss: 0.045929244733157945\n",
      "Epoch 49/100\n",
      "Training Loss: 0.06468474982451536\n",
      "Validation Loss: 0.04761069071715499\n",
      "Epoch 50/100\n",
      "Training Loss: 0.06873005343772713\n",
      "Validation Loss: 0.06169978322406283\n",
      "Epoch 51/100\n",
      "Training Loss: 0.06222332280970486\n",
      "Validation Loss: 0.04239005054842497\n",
      "Epoch 52/100\n",
      "Training Loss: 0.06852043164845464\n",
      "Validation Loss: 0.04471473773781351\n",
      "Epoch 53/100\n",
      "Training Loss: 0.07282727910714662\n",
      "Validation Loss: 0.02610234250165335\n",
      "Epoch 54/100\n",
      "Training Loss: 0.06403675628477266\n",
      "Validation Loss: 0.030633062800856942\n",
      "Epoch 55/100\n",
      "Training Loss: 0.07511359812171281\n",
      "Validation Loss: 0.0783209081393953\n",
      "Epoch 56/100\n",
      "Training Loss: 0.06460956819863291\n",
      "Validation Loss: 0.05378049379316409\n",
      "Epoch 57/100\n",
      "Training Loss: 0.07243708790128643\n",
      "Validation Loss: 0.03804817493363073\n",
      "Epoch 58/100\n",
      "Training Loss: 0.07682237598992313\n",
      "Validation Loss: 0.04987325431462941\n",
      "Epoch 59/100\n",
      "Training Loss: 0.0742771217334775\n",
      "Validation Loss: 0.06721422345481178\n",
      "Epoch 60/100\n",
      "Training Loss: 0.06786740792912756\n",
      "Validation Loss: 0.08471909135562133\n",
      "Epoch 61/100\n",
      "Training Loss: 0.0692260576268422\n",
      "Validation Loss: 0.060381152228432745\n",
      "Epoch 62/100\n",
      "Training Loss: 0.06873653980900359\n",
      "Validation Loss: 0.06889539749162425\n",
      "Epoch 63/100\n",
      "Training Loss: 0.08411614187580593\n",
      "Validation Loss: 0.061343055089213436\n",
      "Epoch 64/100\n",
      "Training Loss: 0.06957896746150473\n",
      "Validation Loss: 0.029358054819078355\n",
      "Epoch 65/100\n",
      "Training Loss: 0.07057448095340088\n",
      "Validation Loss: 0.06445656547015714\n",
      "Epoch 66/100\n",
      "Training Loss: 0.06895388339876325\n",
      "Validation Loss: 0.042087185789929246\n",
      "Epoch 67/100\n",
      "Training Loss: 0.08407457357919647\n",
      "Validation Loss: 0.06531846831054419\n",
      "Epoch 68/100\n",
      "Training Loss: 0.05822188783084691\n",
      "Validation Loss: 0.0676308409325292\n",
      "Epoch 69/100\n",
      "Training Loss: 0.06178348477289195\n",
      "Validation Loss: 0.026081555544369462\n",
      "Epoch 70/100\n",
      "Training Loss: 0.0607682512994156\n",
      "Validation Loss: 0.1323290244421003\n",
      "Epoch 71/100\n",
      "Training Loss: 0.06948231314739178\n",
      "Validation Loss: 0.06939413897970276\n",
      "Epoch 72/100\n",
      "Training Loss: 0.06601370667106059\n",
      "Validation Loss: 0.07679773984305863\n",
      "Epoch 73/100\n",
      "Training Loss: 0.07058937575021969\n",
      "Validation Loss: 0.07452265830659201\n",
      "Epoch 74/100\n",
      "Training Loss: 0.06794800209622043\n",
      "Validation Loss: 0.07220251310590242\n",
      "Epoch 75/100\n",
      "Training Loss: 0.06010064486700902\n",
      "Validation Loss: 0.04501774382741902\n",
      "Epoch 76/100\n",
      "Training Loss: 0.0768785159459436\n",
      "Validation Loss: 0.06324734843790969\n",
      "Epoch 77/100\n",
      "Training Loss: 0.0787372784175475\n",
      "Validation Loss: 0.039512512906404076\n",
      "Epoch 78/100\n",
      "Training Loss: 0.08097041605538398\n",
      "Validation Loss: 0.05187458522500088\n",
      "Epoch 79/100\n",
      "Training Loss: 0.06889232918506243\n",
      "Validation Loss: 0.06564893392480854\n",
      "Epoch 80/100\n",
      "Training Loss: 0.07164698054549595\n",
      "Validation Loss: 0.04878554755076392\n",
      "Epoch 81/100\n",
      "Training Loss: 0.07592709139535954\n",
      "Validation Loss: 0.14754614078326228\n",
      "Epoch 82/100\n",
      "Training Loss: 0.05475164704449526\n",
      "Validation Loss: 0.10716223912548806\n",
      "Epoch 83/100\n",
      "Training Loss: 0.06622066492848133\n",
      "Validation Loss: 0.07294852949107397\n",
      "Epoch 84/100\n",
      "Training Loss: 0.07112089048922321\n",
      "Validation Loss: 0.06678915790203813\n",
      "Epoch 85/100\n",
      "Training Loss: 0.069067288194681\n",
      "Validation Loss: 0.08472490411377354\n",
      "Epoch 86/100\n",
      "Training Loss: 0.08310633392467\n",
      "Validation Loss: 0.050455986078297946\n",
      "Epoch 87/100\n",
      "Training Loss: 0.08393334117805984\n",
      "Validation Loss: 0.09823374249880505\n",
      "Epoch 88/100\n",
      "Training Loss: 0.06864118391161077\n",
      "Validation Loss: 0.06136551042274698\n",
      "Epoch 89/100\n",
      "Training Loss: 0.07690244054120787\n",
      "Validation Loss: 0.05463326130232461\n",
      "Epoch 90/100\n",
      "Training Loss: 0.07823074136220567\n",
      "Validation Loss: 0.05592270136588767\n",
      "Epoch 91/100\n",
      "Training Loss: 0.08622526995105806\n",
      "Validation Loss: 0.07980987583959744\n",
      "Epoch 92/100\n",
      "Training Loss: 0.08815048282350615\n",
      "Validation Loss: 0.03147146278521061\n",
      "Epoch 93/100\n",
      "Training Loss: 0.07764696627874135\n",
      "Validation Loss: 0.058112464752317336\n",
      "Epoch 94/100\n",
      "Training Loss: 0.07082708977240357\n",
      "Validation Loss: 0.06460986109907359\n",
      "Epoch 95/100\n",
      "Training Loss: 0.06857663482203144\n",
      "Validation Loss: 0.054182254196427436\n",
      "Epoch 96/100\n",
      "Training Loss: 0.07742687865650481\n",
      "Validation Loss: 0.04023837050659765\n",
      "Epoch 97/100\n",
      "Training Loss: 0.06720606126304231\n",
      "Validation Loss: 0.07650622007169065\n",
      "Epoch 98/100\n",
      "Training Loss: 0.07369559162429817\n",
      "Validation Loss: 0.03091755617858895\n",
      "Epoch 99/100\n",
      "Training Loss: 0.0827886452002456\n",
      "Validation Loss: 0.06173539641923392\n",
      "Epoch 100/100\n",
      "Training Loss: 0.06751332408816034\n",
      "Validation Loss: 0.0385126652553069\n",
      "    Trial 2/2 for combination 34/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.3047849976338239\n",
      "Validation Loss: 0.1658762610898437\n",
      "Epoch 2/100\n",
      "Training Loss: 0.22525356365196933\n",
      "Validation Loss: 0.08638208375797103\n",
      "Epoch 3/100\n",
      "Training Loss: 0.18811933636786285\n",
      "Validation Loss: 0.15225843724455404\n",
      "Epoch 4/100\n",
      "Training Loss: 0.16166408633554302\n",
      "Validation Loss: 0.11825436597263035\n",
      "Epoch 5/100\n",
      "Training Loss: 0.16331245302347366\n",
      "Validation Loss: 0.13320363105220637\n",
      "Epoch 6/100\n",
      "Training Loss: 0.1496992165063103\n",
      "Validation Loss: 0.08796744197101096\n",
      "Epoch 7/100\n",
      "Training Loss: 0.14526335774812957\n",
      "Validation Loss: 0.09708526089065592\n",
      "Epoch 8/100\n",
      "Training Loss: 0.1433194462727738\n",
      "Validation Loss: 0.10449818407223088\n",
      "Epoch 9/100\n",
      "Training Loss: 0.11460540307386477\n",
      "Validation Loss: 0.07183739279810222\n",
      "Epoch 10/100\n",
      "Training Loss: 0.11141322723552025\n",
      "Validation Loss: 0.04673181718127024\n",
      "Epoch 11/100\n",
      "Training Loss: 0.09739738468950515\n",
      "Validation Loss: 0.09411912771663185\n",
      "Epoch 12/100\n",
      "Training Loss: 0.09776070559007828\n",
      "Validation Loss: 0.061226196620163834\n",
      "Epoch 13/100\n",
      "Training Loss: 0.11050038151966428\n",
      "Validation Loss: 0.04777569004863522\n",
      "Epoch 14/100\n",
      "Training Loss: 0.10194883824796531\n",
      "Validation Loss: 0.08348562970141682\n",
      "Epoch 15/100\n",
      "Training Loss: 0.11007706991312602\n",
      "Validation Loss: 0.047334543889657035\n",
      "Epoch 16/100\n",
      "Training Loss: 0.08600368162543227\n",
      "Validation Loss: 0.04442161036828358\n",
      "Epoch 17/100\n",
      "Training Loss: 0.08330336907916625\n",
      "Validation Loss: 0.06793579903517749\n",
      "Epoch 18/100\n",
      "Training Loss: 0.09304900590683937\n",
      "Validation Loss: 0.07273814072203572\n",
      "Epoch 19/100\n",
      "Training Loss: 0.08665716952627356\n",
      "Validation Loss: 0.09194977144694007\n",
      "Epoch 20/100\n",
      "Training Loss: 0.08755252708293638\n",
      "Validation Loss: 0.04634981867526302\n",
      "Epoch 21/100\n",
      "Training Loss: 0.0763661421612666\n",
      "Validation Loss: 0.07537971670543321\n",
      "Epoch 22/100\n",
      "Training Loss: 0.08569354739063385\n",
      "Validation Loss: 0.06802974310625387\n",
      "Epoch 23/100\n",
      "Training Loss: 0.09118431406747875\n",
      "Validation Loss: 0.047401229584053586\n",
      "Epoch 24/100\n",
      "Training Loss: 0.07076852197964494\n",
      "Validation Loss: 0.07414316361158305\n",
      "Epoch 25/100\n",
      "Training Loss: 0.07359872541048502\n",
      "Validation Loss: 0.06121659740511315\n",
      "Epoch 26/100\n",
      "Training Loss: 0.08022268048669194\n",
      "Validation Loss: 0.07788916878782334\n",
      "Epoch 27/100\n",
      "Training Loss: 0.07796626108526793\n",
      "Validation Loss: 0.06944624844000077\n",
      "Epoch 28/100\n",
      "Training Loss: 0.06718751064542701\n",
      "Validation Loss: 0.044970978983532015\n",
      "Epoch 29/100\n",
      "Training Loss: 0.07134311934818267\n",
      "Validation Loss: 0.0683912024987257\n",
      "Epoch 30/100\n",
      "Training Loss: 0.07788141795745042\n",
      "Validation Loss: 0.047647240790312344\n",
      "Epoch 31/100\n",
      "Training Loss: 0.07309352092919008\n",
      "Validation Loss: 0.04332713465906214\n",
      "Epoch 32/100\n",
      "Training Loss: 0.07439079236733774\n",
      "Validation Loss: 0.03543216642055401\n",
      "Epoch 33/100\n",
      "Training Loss: 0.07009644336691252\n",
      "Validation Loss: 0.0487833334658659\n",
      "Epoch 34/100\n",
      "Training Loss: 0.06819355386898815\n",
      "Validation Loss: 0.057327707076937476\n",
      "Epoch 35/100\n",
      "Training Loss: 0.07260614179883663\n",
      "Validation Loss: 0.048165850638643024\n",
      "Epoch 36/100\n",
      "Training Loss: 0.07066771515437657\n",
      "Validation Loss: 0.07947976486746014\n",
      "Epoch 37/100\n",
      "Training Loss: 0.07023174187711156\n",
      "Validation Loss: 0.06326709927212758\n",
      "Epoch 38/100\n",
      "Training Loss: 0.0672272978369295\n",
      "Validation Loss: 0.04728197897456079\n",
      "Epoch 39/100\n",
      "Training Loss: 0.07394187696796123\n",
      "Validation Loss: 0.051078042031969625\n",
      "Epoch 40/100\n",
      "Training Loss: 0.06536205571347595\n",
      "Validation Loss: 0.04045003024888296\n",
      "Epoch 41/100\n",
      "Training Loss: 0.07127139221446664\n",
      "Validation Loss: 0.059965602240637336\n",
      "Epoch 42/100\n",
      "Training Loss: 0.07169212177947609\n",
      "Validation Loss: 0.052587560262505786\n",
      "Epoch 43/100\n",
      "Training Loss: 0.06675115030666609\n",
      "Validation Loss: 0.04864480350482431\n",
      "Epoch 44/100\n",
      "Training Loss: 0.062234357444426965\n",
      "Validation Loss: 0.030778917112268784\n",
      "Epoch 45/100\n",
      "Training Loss: 0.07461471633359419\n",
      "Validation Loss: 0.051243174366273944\n",
      "Epoch 46/100\n",
      "Training Loss: 0.08876229475571511\n",
      "Validation Loss: 0.05972830868488754\n",
      "Epoch 47/100\n",
      "Training Loss: 0.0644831986365247\n",
      "Validation Loss: 0.07563453877031848\n",
      "Epoch 48/100\n",
      "Training Loss: 0.07080506273257102\n",
      "Validation Loss: 0.04768911503024083\n",
      "Epoch 49/100\n",
      "Training Loss: 0.07429818791956454\n",
      "Validation Loss: 0.07270511798998494\n",
      "Epoch 50/100\n",
      "Training Loss: 0.06508017802177449\n",
      "Validation Loss: 0.05772571596018272\n",
      "Epoch 51/100\n",
      "Training Loss: 0.06381214061349888\n",
      "Validation Loss: 0.06261070888245789\n",
      "Epoch 52/100\n",
      "Training Loss: 0.06951059698417635\n",
      "Validation Loss: 0.040386475995635836\n",
      "Epoch 53/100\n",
      "Training Loss: 0.08017073390887872\n",
      "Validation Loss: 0.04385450884042791\n",
      "Epoch 54/100\n",
      "Training Loss: 0.07293264537134733\n",
      "Validation Loss: 0.0488110505617044\n",
      "Epoch 55/100\n",
      "Training Loss: 0.0621502218018218\n",
      "Validation Loss: 0.07448187089404706\n",
      "Epoch 56/100\n",
      "Training Loss: 0.08211565653198884\n",
      "Validation Loss: 0.05032669578462253\n",
      "Epoch 57/100\n",
      "Training Loss: 0.0774352076990843\n",
      "Validation Loss: 0.05278168387221038\n",
      "Epoch 58/100\n",
      "Training Loss: 0.07006407125432508\n",
      "Validation Loss: 0.0641110995577473\n",
      "Epoch 59/100\n",
      "Training Loss: 0.08190796483028324\n",
      "Validation Loss: 0.03060203888188192\n",
      "Epoch 60/100\n",
      "Training Loss: 0.06918960419717514\n",
      "Validation Loss: 0.04095598022102155\n",
      "Epoch 61/100\n",
      "Training Loss: 0.06707643017657577\n",
      "Validation Loss: 0.03618412269614214\n",
      "Epoch 62/100\n",
      "Training Loss: 0.06680558202007157\n",
      "Validation Loss: 0.047864115545721196\n",
      "Epoch 63/100\n",
      "Training Loss: 0.062403093472064124\n",
      "Validation Loss: 0.028913846696343403\n",
      "Epoch 64/100\n",
      "Training Loss: 0.0707543650497487\n",
      "Validation Loss: 0.05474962392216566\n",
      "Epoch 65/100\n",
      "Training Loss: 0.07119773113042505\n",
      "Validation Loss: 0.06202572920027679\n",
      "Epoch 66/100\n",
      "Training Loss: 0.07110558668257125\n",
      "Validation Loss: 0.08949656336644357\n",
      "Epoch 67/100\n",
      "Training Loss: 0.06419613648527893\n",
      "Validation Loss: 0.04971895597859579\n",
      "Epoch 68/100\n",
      "Training Loss: 0.07368263571545448\n",
      "Validation Loss: 0.09085728140835911\n",
      "Epoch 69/100\n",
      "Training Loss: 0.06431390641451178\n",
      "Validation Loss: 0.0780616236565631\n",
      "Epoch 70/100\n",
      "Training Loss: 0.0777054401575208\n",
      "Validation Loss: 0.09520570395254377\n",
      "Epoch 71/100\n",
      "Training Loss: 0.06688989064283948\n",
      "Validation Loss: 0.0708062794972518\n",
      "Epoch 72/100\n",
      "Training Loss: 0.07705971653894728\n",
      "Validation Loss: 0.052592965868079475\n",
      "Epoch 73/100\n",
      "Training Loss: 0.08471055415102915\n",
      "Validation Loss: 0.04847086279679238\n",
      "Epoch 74/100\n",
      "Training Loss: 0.07667182250588116\n",
      "Validation Loss: 0.05408999637256775\n",
      "Epoch 75/100\n",
      "Training Loss: 0.08110603558893399\n",
      "Validation Loss: 0.04098820768150493\n",
      "Epoch 76/100\n",
      "Training Loss: 0.07813763423667293\n",
      "Validation Loss: 0.05251796520881604\n",
      "Epoch 77/100\n",
      "Training Loss: 0.06807963593295484\n",
      "Validation Loss: 0.09361717168068227\n",
      "Epoch 78/100\n",
      "Training Loss: 0.08215366290768229\n",
      "Validation Loss: 0.0397498373782586\n",
      "Epoch 79/100\n",
      "Training Loss: 0.07568806423668747\n",
      "Validation Loss: 0.14135434998574747\n",
      "Epoch 80/100\n",
      "Training Loss: 0.07278382275702122\n",
      "Validation Loss: 0.08483178702940336\n",
      "Epoch 81/100\n",
      "Training Loss: 0.08738544818094855\n",
      "Validation Loss: 0.06478042596834328\n",
      "Epoch 82/100\n",
      "Training Loss: 0.09036301428560914\n",
      "Validation Loss: 0.07623512069046104\n",
      "Epoch 83/100\n",
      "Training Loss: 0.07311345510273357\n",
      "Validation Loss: 0.04994128749824779\n",
      "Epoch 84/100\n",
      "Training Loss: 0.08131595912065771\n",
      "Validation Loss: 0.03460218441866335\n",
      "Epoch 85/100\n",
      "Training Loss: 0.08577654526958527\n",
      "Validation Loss: 0.05976341879991861\n",
      "Epoch 86/100\n",
      "Training Loss: 0.08533112698910271\n",
      "Validation Loss: 0.06453220961941558\n",
      "Epoch 87/100\n",
      "Training Loss: 0.0649864135200445\n",
      "Validation Loss: 0.06698970575765918\n",
      "Epoch 88/100\n",
      "Training Loss: 0.05980188843694074\n",
      "Validation Loss: 0.042228702201380766\n",
      "Epoch 89/100\n",
      "Training Loss: 0.08924942163611704\n",
      "Validation Loss: 0.06122741950286755\n",
      "Epoch 90/100\n",
      "Training Loss: 0.06255005742494639\n",
      "Validation Loss: 0.04399422769581525\n",
      "Epoch 91/100\n",
      "Training Loss: 0.07853990277307729\n",
      "Validation Loss: 0.07201536489359585\n",
      "Epoch 92/100\n",
      "Training Loss: 0.08002407090239015\n",
      "Validation Loss: 0.0881294540782219\n",
      "Epoch 93/100\n",
      "Training Loss: 0.06781604092060162\n",
      "Validation Loss: 0.15381904739314772\n",
      "Epoch 94/100\n",
      "Training Loss: 0.08028210087845807\n",
      "Validation Loss: 0.07505829062595124\n",
      "Epoch 95/100\n",
      "Training Loss: 0.08938169097691379\n",
      "Validation Loss: 0.05135234988346218\n",
      "Epoch 96/100\n",
      "Training Loss: 0.0872701876254476\n",
      "Validation Loss: 0.05998909582640956\n",
      "Epoch 97/100\n",
      "Training Loss: 0.0899645364134811\n",
      "Validation Loss: 0.028342719249011283\n",
      "Epoch 98/100\n",
      "Training Loss: 0.08623637149366412\n",
      "Validation Loss: 0.034304492319427074\n",
      "Epoch 99/100\n",
      "Training Loss: 0.06818956774165069\n",
      "Validation Loss: 0.07097780887732036\n",
      "Epoch 100/100\n",
      "Training Loss: 0.07900914711511564\n",
      "Validation Loss: 0.08116172704944474\n",
      "Combination 34: Avg Training Loss = 0.08582771314373831, Avg Validation Loss = 0.06504373088434365\n",
      "Testing combination 35/48: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.001, 'sequence_length': 50, 'dropout_rate': 0.1}\n",
      "Truncated Train Data Shape: (70, 50, 1)\n",
      "Truncated Validation Data Shape: (10, 50, 1)\n",
      "    Trial 1/2 for combination 35/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.41182547120751856\n",
      "Validation Loss: 0.1386880277926688\n",
      "Epoch 2/100\n",
      "Training Loss: 0.4160574924965306\n",
      "Validation Loss: 0.12426748755146574\n",
      "Epoch 3/100\n",
      "Training Loss: 0.25615058188937845\n",
      "Validation Loss: 0.1569653500202627\n",
      "Epoch 4/100\n",
      "Training Loss: 0.2753799960762908\n",
      "Validation Loss: 0.062441420553555324\n",
      "Epoch 5/100\n",
      "Training Loss: 0.21722682986337397\n",
      "Validation Loss: 0.17712379967025085\n",
      "Epoch 6/100\n",
      "Training Loss: 0.20418923639456263\n",
      "Validation Loss: 0.1262427988959581\n",
      "Epoch 7/100\n",
      "Training Loss: 0.1539484762116173\n",
      "Validation Loss: 0.09898258140150153\n",
      "Epoch 8/100\n",
      "Training Loss: 0.16469073326714473\n",
      "Validation Loss: 0.10692651980315278\n",
      "Epoch 9/100\n",
      "Training Loss: 0.134560447761093\n",
      "Validation Loss: 0.05427449822253209\n",
      "Epoch 10/100\n",
      "Training Loss: 0.11530028411823925\n",
      "Validation Loss: 0.0982424390469097\n",
      "Epoch 11/100\n",
      "Training Loss: 0.11230153719591839\n",
      "Validation Loss: 0.04894821124709971\n",
      "Epoch 12/100\n",
      "Training Loss: 0.11717506518904232\n",
      "Validation Loss: 0.06898293374843807\n",
      "Epoch 13/100\n",
      "Training Loss: 0.12034301182216411\n",
      "Validation Loss: 0.05524688304729678\n",
      "Epoch 14/100\n",
      "Training Loss: 0.0890804643262678\n",
      "Validation Loss: 0.09754568480028515\n",
      "Epoch 15/100\n",
      "Training Loss: 0.08934052754464965\n",
      "Validation Loss: 0.06166112322275188\n",
      "Epoch 16/100\n",
      "Training Loss: 0.0970082026099777\n",
      "Validation Loss: 0.0387689678410497\n",
      "Epoch 17/100\n",
      "Training Loss: 0.08661893637205031\n",
      "Validation Loss: 0.07195047982443739\n",
      "Epoch 18/100\n",
      "Training Loss: 0.08600479809259177\n",
      "Validation Loss: 0.049112473731193027\n",
      "Epoch 19/100\n",
      "Training Loss: 0.08441853418541252\n",
      "Validation Loss: 0.04005998379333543\n",
      "Epoch 20/100\n",
      "Training Loss: 0.08307170673378803\n",
      "Validation Loss: 0.04538813080855696\n",
      "Epoch 21/100\n",
      "Training Loss: 0.06859038385040388\n",
      "Validation Loss: 0.047426624738108755\n",
      "Epoch 22/100\n",
      "Training Loss: 0.07231817226425179\n",
      "Validation Loss: 0.046560124111541305\n",
      "Epoch 23/100\n",
      "Training Loss: 0.07638103007300868\n",
      "Validation Loss: 0.04248779916508898\n",
      "Epoch 24/100\n",
      "Training Loss: 0.06569408302184783\n",
      "Validation Loss: 0.06606996345648639\n",
      "Epoch 25/100\n",
      "Training Loss: 0.06947639778580711\n",
      "Validation Loss: 0.06026718175167711\n",
      "Epoch 26/100\n",
      "Training Loss: 0.06682121478368631\n",
      "Validation Loss: 0.04816231675676399\n",
      "Epoch 27/100\n",
      "Training Loss: 0.06914743190026154\n",
      "Validation Loss: 0.05715303880274777\n",
      "Epoch 28/100\n",
      "Training Loss: 0.0740356563397517\n",
      "Validation Loss: 0.04143512149076352\n",
      "Epoch 29/100\n",
      "Training Loss: 0.07022346824056484\n",
      "Validation Loss: 0.044888033980646024\n",
      "Epoch 30/100\n",
      "Training Loss: 0.06746094436685367\n",
      "Validation Loss: 0.06253400455078449\n",
      "Epoch 31/100\n",
      "Training Loss: 0.06223046522810973\n",
      "Validation Loss: 0.03146458586699401\n",
      "Epoch 32/100\n",
      "Training Loss: 0.07220337545434223\n",
      "Validation Loss: 0.053404671069289145\n",
      "Epoch 33/100\n",
      "Training Loss: 0.05814841018246396\n",
      "Validation Loss: 0.06340143833316632\n",
      "Epoch 34/100\n",
      "Training Loss: 0.06870575123993272\n",
      "Validation Loss: 0.06712498076937759\n",
      "Epoch 35/100\n",
      "Training Loss: 0.06309792038609059\n",
      "Validation Loss: 0.07079452470766771\n",
      "Epoch 36/100\n",
      "Training Loss: 0.06834029430367085\n",
      "Validation Loss: 0.025267157775260497\n",
      "Epoch 37/100\n",
      "Training Loss: 0.07157332947977878\n",
      "Validation Loss: 0.0463180605235803\n",
      "Epoch 38/100\n",
      "Training Loss: 0.07322687314337964\n",
      "Validation Loss: 0.03175639175141241\n",
      "Epoch 39/100\n",
      "Training Loss: 0.05880452822799207\n",
      "Validation Loss: 0.08833619814344501\n",
      "Epoch 40/100\n",
      "Training Loss: 0.07032460589726904\n",
      "Validation Loss: 0.050649991526399187\n",
      "Epoch 41/100\n",
      "Training Loss: 0.07284023880708328\n",
      "Validation Loss: 0.056485087413568905\n",
      "Epoch 42/100\n",
      "Training Loss: 0.06507881771534706\n",
      "Validation Loss: 0.05656490241890692\n",
      "Epoch 43/100\n",
      "Training Loss: 0.06549168503343088\n",
      "Validation Loss: 0.03438145131278072\n",
      "Epoch 44/100\n",
      "Training Loss: 0.06701347776055526\n",
      "Validation Loss: 0.06731302634840064\n",
      "Epoch 45/100\n",
      "Training Loss: 0.06116957081263513\n",
      "Validation Loss: 0.0704166328985804\n",
      "Epoch 46/100\n",
      "Training Loss: 0.07283086062689154\n",
      "Validation Loss: 0.027597187049660965\n",
      "Epoch 47/100\n",
      "Training Loss: 0.06954250209408844\n",
      "Validation Loss: 0.052016611061837914\n",
      "Epoch 48/100\n",
      "Training Loss: 0.06858910306578085\n",
      "Validation Loss: 0.05051040036872353\n",
      "Epoch 49/100\n",
      "Training Loss: 0.06059612281775321\n",
      "Validation Loss: 0.04436121926118628\n",
      "Epoch 50/100\n",
      "Training Loss: 0.07720103515048257\n",
      "Validation Loss: 0.07269626866576451\n",
      "Epoch 51/100\n",
      "Training Loss: 0.07006086719808576\n",
      "Validation Loss: 0.03055040100139807\n",
      "Epoch 52/100\n",
      "Training Loss: 0.07667104552288283\n",
      "Validation Loss: 0.024937866039288912\n",
      "Epoch 53/100\n",
      "Training Loss: 0.07979457263116746\n",
      "Validation Loss: 0.05836903995756222\n",
      "Epoch 54/100\n",
      "Training Loss: 0.06669779522195488\n",
      "Validation Loss: 0.07946215626654565\n",
      "Epoch 55/100\n",
      "Training Loss: 0.0742696572821116\n",
      "Validation Loss: 0.05358184065470686\n",
      "Epoch 56/100\n",
      "Training Loss: 0.06537867127383502\n",
      "Validation Loss: 0.05914574987675555\n",
      "Epoch 57/100\n",
      "Training Loss: 0.07295999524798454\n",
      "Validation Loss: 0.045108419689227554\n",
      "Epoch 58/100\n",
      "Training Loss: 0.05920503829531429\n",
      "Validation Loss: 0.04149663722424822\n",
      "Epoch 59/100\n",
      "Training Loss: 0.05658247875707879\n",
      "Validation Loss: 0.0502277439367795\n",
      "Epoch 60/100\n",
      "Training Loss: 0.056568848515649334\n",
      "Validation Loss: 0.05413980623406196\n",
      "Epoch 61/100\n",
      "Training Loss: 0.0854121522429654\n",
      "Validation Loss: 0.07652337388254846\n",
      "Epoch 62/100\n",
      "Training Loss: 0.05778479186100162\n",
      "Validation Loss: 0.06536858993354748\n",
      "Epoch 63/100\n",
      "Training Loss: 0.0678538590873654\n",
      "Validation Loss: 0.035690391891657305\n",
      "Epoch 64/100\n",
      "Training Loss: 0.08535978843604927\n",
      "Validation Loss: 0.02803108480589301\n",
      "Epoch 65/100\n",
      "Training Loss: 0.07879296269532923\n",
      "Validation Loss: 0.04057006653522587\n",
      "Epoch 66/100\n",
      "Training Loss: 0.08100082826600684\n",
      "Validation Loss: 0.05727330589647371\n",
      "Epoch 67/100\n",
      "Training Loss: 0.08916592834363404\n",
      "Validation Loss: 0.08912124838595877\n",
      "Epoch 68/100\n",
      "Training Loss: 0.07815560948799626\n",
      "Validation Loss: 0.05516003308596231\n",
      "Epoch 69/100\n",
      "Training Loss: 0.09390868912253077\n",
      "Validation Loss: 0.031823769719470915\n",
      "Epoch 70/100\n",
      "Training Loss: 0.07089624240108644\n",
      "Validation Loss: 0.04456271891852341\n",
      "Epoch 71/100\n",
      "Training Loss: 0.07009324241518\n",
      "Validation Loss: 0.03484813639335814\n",
      "Epoch 72/100\n",
      "Training Loss: 0.0664131816648907\n",
      "Validation Loss: 0.03517477958524375\n",
      "Epoch 73/100\n",
      "Training Loss: 0.06733567929484646\n",
      "Validation Loss: 0.05440170720461407\n",
      "Epoch 74/100\n",
      "Training Loss: 0.07356071949076017\n",
      "Validation Loss: 0.02737770605903019\n",
      "Epoch 75/100\n",
      "Training Loss: 0.05987746414639983\n",
      "Validation Loss: 0.024961034095640624\n",
      "Epoch 76/100\n",
      "Training Loss: 0.06935621639923242\n",
      "Validation Loss: 0.06267727438743029\n",
      "Epoch 77/100\n",
      "Training Loss: 0.08340962419130014\n",
      "Validation Loss: 0.03438173559382195\n",
      "Epoch 78/100\n",
      "Training Loss: 0.06505397858070407\n",
      "Validation Loss: 0.03618993485366132\n",
      "Epoch 79/100\n",
      "Training Loss: 0.07759731381110035\n",
      "Validation Loss: 0.03242322456919345\n",
      "Epoch 80/100\n",
      "Training Loss: 0.06649252502374416\n",
      "Validation Loss: 0.03735860793013861\n",
      "Epoch 81/100\n",
      "Training Loss: 0.08104051292635779\n",
      "Validation Loss: 0.061259025945223956\n",
      "Epoch 82/100\n",
      "Training Loss: 0.07390052889394344\n",
      "Validation Loss: 0.04582297995209793\n",
      "Epoch 83/100\n",
      "Training Loss: 0.07564645654279316\n",
      "Validation Loss: 0.045926321974539115\n",
      "Epoch 84/100\n",
      "Training Loss: 0.07112175853961149\n",
      "Validation Loss: 0.08694071205931173\n",
      "Epoch 85/100\n",
      "Training Loss: 0.07724760009170004\n",
      "Validation Loss: 0.05002061448120437\n",
      "Epoch 86/100\n",
      "Training Loss: 0.06220787425443307\n",
      "Validation Loss: 0.045589916520696466\n",
      "Epoch 87/100\n",
      "Training Loss: 0.0792564386984375\n",
      "Validation Loss: 0.09806745887154814\n",
      "Epoch 88/100\n",
      "Training Loss: 0.0722690592772595\n",
      "Validation Loss: 0.04381740671380048\n",
      "Epoch 89/100\n",
      "Training Loss: 0.0658791900194379\n",
      "Validation Loss: 0.05558985132023696\n",
      "Epoch 90/100\n",
      "Training Loss: 0.07779020579044989\n",
      "Validation Loss: 0.06869304451544848\n",
      "Epoch 91/100\n",
      "Training Loss: 0.0748711510644033\n",
      "Validation Loss: 0.06552871763989057\n",
      "Epoch 92/100\n",
      "Training Loss: 0.057666648877886965\n",
      "Validation Loss: 0.06675854054562448\n",
      "Epoch 93/100\n",
      "Training Loss: 0.06492208528443598\n",
      "Validation Loss: 0.08028634375339735\n",
      "Epoch 94/100\n",
      "Training Loss: 0.07809257865290181\n",
      "Validation Loss: 0.05313714033138757\n",
      "Epoch 95/100\n",
      "Training Loss: 0.0773294326503924\n",
      "Validation Loss: 0.03041765225901692\n",
      "Epoch 96/100\n",
      "Training Loss: 0.07795087891663997\n",
      "Validation Loss: 0.0856289527631636\n",
      "Epoch 97/100\n",
      "Training Loss: 0.07643432741607992\n",
      "Validation Loss: 0.08857456473939081\n",
      "Epoch 98/100\n",
      "Training Loss: 0.07817018933685357\n",
      "Validation Loss: 0.0320386366027859\n",
      "Epoch 99/100\n",
      "Training Loss: 0.08046155852524485\n",
      "Validation Loss: 0.05777370983427603\n",
      "Epoch 100/100\n",
      "Training Loss: 0.07110001663252492\n",
      "Validation Loss: 0.050347057722660495\n",
      "    Trial 2/2 for combination 35/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.4261535313110181\n",
      "Validation Loss: 0.2072974405980673\n",
      "Epoch 2/100\n",
      "Training Loss: 0.23735598927415558\n",
      "Validation Loss: 0.21770167733793272\n",
      "Epoch 3/100\n",
      "Training Loss: 0.22791203600232443\n",
      "Validation Loss: 0.07427509362565518\n",
      "Epoch 4/100\n",
      "Training Loss: 0.18255726876327558\n",
      "Validation Loss: 0.07478079651811034\n",
      "Epoch 5/100\n",
      "Training Loss: 0.15918510572464492\n",
      "Validation Loss: 0.06987697974485192\n",
      "Epoch 6/100\n",
      "Training Loss: 0.1690084044442596\n",
      "Validation Loss: 0.04923507425058583\n",
      "Epoch 7/100\n",
      "Training Loss: 0.15457822360818563\n",
      "Validation Loss: 0.08884167652247035\n",
      "Epoch 8/100\n",
      "Training Loss: 0.14397864429720128\n",
      "Validation Loss: 0.0713950842479991\n",
      "Epoch 9/100\n",
      "Training Loss: 0.14187502652534273\n",
      "Validation Loss: 0.07978359543834734\n",
      "Epoch 10/100\n",
      "Training Loss: 0.12838659649744993\n",
      "Validation Loss: 0.06493344412745816\n",
      "Epoch 11/100\n",
      "Training Loss: 0.11489363778796147\n",
      "Validation Loss: 0.09468214532719721\n",
      "Epoch 12/100\n",
      "Training Loss: 0.125435559158996\n",
      "Validation Loss: 0.048783588051999095\n",
      "Epoch 13/100\n",
      "Training Loss: 0.111909896167678\n",
      "Validation Loss: 0.06337454245163165\n",
      "Epoch 14/100\n",
      "Training Loss: 0.10252688037056057\n",
      "Validation Loss: 0.06256973478793143\n",
      "Epoch 15/100\n",
      "Training Loss: 0.10010449181940294\n",
      "Validation Loss: 0.06734494143415187\n",
      "Epoch 16/100\n",
      "Training Loss: 0.09019011989836227\n",
      "Validation Loss: 0.04994957152107127\n",
      "Epoch 17/100\n",
      "Training Loss: 0.07963756696249653\n",
      "Validation Loss: 0.07190866320330473\n",
      "Epoch 18/100\n",
      "Training Loss: 0.09134959960954582\n",
      "Validation Loss: 0.06702494436546193\n",
      "Epoch 19/100\n",
      "Training Loss: 0.07691231023246049\n",
      "Validation Loss: 0.037390259931583035\n",
      "Epoch 20/100\n",
      "Training Loss: 0.07884682425181917\n",
      "Validation Loss: 0.05327405110854527\n",
      "Epoch 21/100\n",
      "Training Loss: 0.08315187493003834\n",
      "Validation Loss: 0.08157250363597279\n",
      "Epoch 22/100\n",
      "Training Loss: 0.07559463935071045\n",
      "Validation Loss: 0.08441053868252367\n",
      "Epoch 23/100\n",
      "Training Loss: 0.06607423451568593\n",
      "Validation Loss: 0.039336336428079675\n",
      "Epoch 24/100\n",
      "Training Loss: 0.08262888756228627\n",
      "Validation Loss: 0.06389070410751345\n",
      "Epoch 25/100\n",
      "Training Loss: 0.07322790250912653\n",
      "Validation Loss: 0.041896179652415586\n",
      "Epoch 26/100\n",
      "Training Loss: 0.07285864944273353\n",
      "Validation Loss: 0.04684553899893497\n",
      "Epoch 27/100\n",
      "Training Loss: 0.06987863246921468\n",
      "Validation Loss: 0.07819242665177537\n",
      "Epoch 28/100\n",
      "Training Loss: 0.0696288957349601\n",
      "Validation Loss: 0.049855195600144446\n",
      "Epoch 29/100\n",
      "Training Loss: 0.0618626903434496\n",
      "Validation Loss: 0.06578768915660985\n",
      "Epoch 30/100\n",
      "Training Loss: 0.06861687805543495\n",
      "Validation Loss: 0.05329158250433238\n",
      "Epoch 31/100\n",
      "Training Loss: 0.06865241288769677\n",
      "Validation Loss: 0.03955917309794956\n",
      "Epoch 32/100\n",
      "Training Loss: 0.0663571120561653\n",
      "Validation Loss: 0.055277768899510714\n",
      "Epoch 33/100\n",
      "Training Loss: 0.05180285024590123\n",
      "Validation Loss: 0.06856401185260833\n",
      "Epoch 34/100\n",
      "Training Loss: 0.0635621292890914\n",
      "Validation Loss: 0.06620585572772454\n",
      "Epoch 35/100\n",
      "Training Loss: 0.06971149904198544\n",
      "Validation Loss: 0.03239287229311518\n",
      "Epoch 36/100\n",
      "Training Loss: 0.07266483604502938\n",
      "Validation Loss: 0.043080783047888566\n",
      "Epoch 37/100\n",
      "Training Loss: 0.06330701760279087\n",
      "Validation Loss: 0.06495401652345327\n",
      "Epoch 38/100\n",
      "Training Loss: 0.07373547464895015\n",
      "Validation Loss: 0.03239701203231986\n",
      "Epoch 39/100\n",
      "Training Loss: 0.06522466128443645\n",
      "Validation Loss: 0.057419487941669144\n",
      "Epoch 40/100\n",
      "Training Loss: 0.07273567470556719\n",
      "Validation Loss: 0.053961647076091725\n",
      "Epoch 41/100\n",
      "Training Loss: 0.06936423783719468\n",
      "Validation Loss: 0.041191807149394445\n",
      "Epoch 42/100\n",
      "Training Loss: 0.061764784429757164\n",
      "Validation Loss: 0.029544533677229208\n",
      "Epoch 43/100\n",
      "Training Loss: 0.06322571475439671\n",
      "Validation Loss: 0.07354261399690373\n",
      "Epoch 44/100\n",
      "Training Loss: 0.06665202972792011\n",
      "Validation Loss: 0.061526564272739524\n",
      "Epoch 45/100\n",
      "Training Loss: 0.07079831969114055\n",
      "Validation Loss: 0.05340608295454356\n",
      "Epoch 46/100\n",
      "Training Loss: 0.07234583908539627\n",
      "Validation Loss: 0.07288331520205951\n",
      "Epoch 47/100\n",
      "Training Loss: 0.06940793299693071\n",
      "Validation Loss: 0.029189178755923362\n",
      "Epoch 48/100\n",
      "Training Loss: 0.05674132453168539\n",
      "Validation Loss: 0.04542762568149229\n",
      "Epoch 49/100\n",
      "Training Loss: 0.07536138906482347\n",
      "Validation Loss: 0.032763023852205864\n",
      "Epoch 50/100\n",
      "Training Loss: 0.06262464950944611\n",
      "Validation Loss: 0.10566577121160611\n",
      "Epoch 51/100\n",
      "Training Loss: 0.0630736784549996\n",
      "Validation Loss: 0.027542715425054053\n",
      "Epoch 52/100\n",
      "Training Loss: 0.06254230091161683\n",
      "Validation Loss: 0.04280799506224657\n",
      "Epoch 53/100\n",
      "Training Loss: 0.06611542230366574\n",
      "Validation Loss: 0.028393800094192623\n",
      "Epoch 54/100\n",
      "Training Loss: 0.07781783703854571\n",
      "Validation Loss: 0.035107527787646794\n",
      "Epoch 55/100\n",
      "Training Loss: 0.05683972014613088\n",
      "Validation Loss: 0.024088794923012694\n",
      "Epoch 56/100\n",
      "Training Loss: 0.06599527814113246\n",
      "Validation Loss: 0.07133113933632754\n",
      "Epoch 57/100\n",
      "Training Loss: 0.07225155290003216\n",
      "Validation Loss: 0.04085469690593233\n",
      "Epoch 58/100\n",
      "Training Loss: 0.06164459695894025\n",
      "Validation Loss: 0.05592550838077577\n",
      "Epoch 59/100\n",
      "Training Loss: 0.0696581591022081\n",
      "Validation Loss: 0.051659049326427585\n",
      "Epoch 60/100\n",
      "Training Loss: 0.0668646969152763\n",
      "Validation Loss: 0.025523527851779186\n",
      "Epoch 61/100\n",
      "Training Loss: 0.055618592220198416\n",
      "Validation Loss: 0.05076239951363535\n",
      "Epoch 62/100\n",
      "Training Loss: 0.07587813763921422\n",
      "Validation Loss: 0.04277524824114647\n",
      "Epoch 63/100\n",
      "Training Loss: 0.07666861036100829\n",
      "Validation Loss: 0.04974203935305171\n",
      "Epoch 64/100\n",
      "Training Loss: 0.07597878516418438\n",
      "Validation Loss: 0.0310080615891914\n",
      "Epoch 65/100\n",
      "Training Loss: 0.07819821583217068\n",
      "Validation Loss: 0.04402549135904675\n",
      "Epoch 66/100\n",
      "Training Loss: 0.065214645324009\n",
      "Validation Loss: 0.048125084100177185\n",
      "Epoch 67/100\n",
      "Training Loss: 0.07198770638465873\n",
      "Validation Loss: 0.05262780364204546\n",
      "Epoch 68/100\n",
      "Training Loss: 0.09377401653384804\n",
      "Validation Loss: 0.044257230717758556\n",
      "Epoch 69/100\n",
      "Training Loss: 0.06345407568604511\n",
      "Validation Loss: 0.04297200619216256\n",
      "Epoch 70/100\n",
      "Training Loss: 0.0763947482179763\n",
      "Validation Loss: 0.05016318646033345\n",
      "Epoch 71/100\n",
      "Training Loss: 0.06637905384173985\n",
      "Validation Loss: 0.030582620010750967\n",
      "Epoch 72/100\n",
      "Training Loss: 0.06857708814131037\n",
      "Validation Loss: 0.0410414266291874\n",
      "Epoch 73/100\n",
      "Training Loss: 0.06301996832161426\n",
      "Validation Loss: 0.04210079990227691\n",
      "Epoch 74/100\n",
      "Training Loss: 0.07420680426934755\n",
      "Validation Loss: 0.03589897132534862\n",
      "Epoch 75/100\n",
      "Training Loss: 0.07387745015303789\n",
      "Validation Loss: 0.04749956423513774\n",
      "Epoch 76/100\n",
      "Training Loss: 0.06808078765658716\n",
      "Validation Loss: 0.13659168241884187\n",
      "Epoch 77/100\n",
      "Training Loss: 0.07042874978794396\n",
      "Validation Loss: 0.04730633890714615\n",
      "Epoch 78/100\n",
      "Training Loss: 0.06384160434357845\n",
      "Validation Loss: 0.05058594878450627\n",
      "Epoch 79/100\n",
      "Training Loss: 0.09432744131048136\n",
      "Validation Loss: 0.03474450857734442\n",
      "Epoch 80/100\n",
      "Training Loss: 0.08309533136085527\n",
      "Validation Loss: 0.056840611206632495\n",
      "Epoch 81/100\n",
      "Training Loss: 0.07636892654370302\n",
      "Validation Loss: 0.029027384636215032\n",
      "Epoch 82/100\n",
      "Training Loss: 0.06740996931232977\n",
      "Validation Loss: 0.07765457179215576\n",
      "Epoch 83/100\n",
      "Training Loss: 0.06029454257724845\n",
      "Validation Loss: 0.030981115498382096\n",
      "Epoch 84/100\n",
      "Training Loss: 0.08510192354694723\n",
      "Validation Loss: 0.027890780797911956\n",
      "Epoch 85/100\n",
      "Training Loss: 0.06518757625452011\n",
      "Validation Loss: 0.06364924881456344\n",
      "Epoch 86/100\n",
      "Training Loss: 0.06997978219772184\n",
      "Validation Loss: 0.07043003374152636\n",
      "Epoch 87/100\n",
      "Training Loss: 0.07274411366446772\n",
      "Validation Loss: 0.09209097600276547\n",
      "Epoch 88/100\n",
      "Training Loss: 0.06528323119693567\n",
      "Validation Loss: 0.059745776232288536\n",
      "Epoch 89/100\n",
      "Training Loss: 0.07939420256646755\n",
      "Validation Loss: 0.04580090907162321\n",
      "Epoch 90/100\n",
      "Training Loss: 0.07456591851860062\n",
      "Validation Loss: 0.06137839890847376\n",
      "Epoch 91/100\n",
      "Training Loss: 0.062389475892957846\n",
      "Validation Loss: 0.06633712895626372\n",
      "Epoch 92/100\n",
      "Training Loss: 0.061767402912895585\n",
      "Validation Loss: 0.060137406720069064\n",
      "Epoch 93/100\n",
      "Training Loss: 0.07493205998967903\n",
      "Validation Loss: 0.04407307426927849\n",
      "Epoch 94/100\n",
      "Training Loss: 0.06359964526912174\n",
      "Validation Loss: 0.037682049249835196\n",
      "Epoch 95/100\n",
      "Training Loss: 0.07544931203262126\n",
      "Validation Loss: 0.03143279621926008\n",
      "Epoch 96/100\n",
      "Training Loss: 0.06221416656563458\n",
      "Validation Loss: 0.03576557893029415\n",
      "Epoch 97/100\n",
      "Training Loss: 0.06246437673314339\n",
      "Validation Loss: 0.02568868962544854\n",
      "Epoch 98/100\n",
      "Training Loss: 0.0716913269803224\n",
      "Validation Loss: 0.066344778428754\n",
      "Epoch 99/100\n",
      "Training Loss: 0.07012434250415431\n",
      "Validation Loss: 0.03596210083187814\n",
      "Epoch 100/100\n",
      "Training Loss: 0.0653865579564019\n",
      "Validation Loss: 0.02592483246043232\n",
      "Combination 35: Avg Training Loss = 0.08755716569203209, Avg Validation Loss = 0.05781080189476302\n",
      "Testing combination 36/48: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.001, 'sequence_length': 50, 'dropout_rate': 0.2}\n",
      "Truncated Train Data Shape: (70, 50, 1)\n",
      "Truncated Validation Data Shape: (10, 50, 1)\n",
      "    Trial 1/2 for combination 36/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.30945998848148654\n",
      "Validation Loss: 0.23702646271285693\n",
      "Epoch 2/100\n",
      "Training Loss: 0.2395025581188529\n",
      "Validation Loss: 0.2412420040570366\n",
      "Epoch 3/100\n",
      "Training Loss: 0.2389828970477106\n",
      "Validation Loss: 0.15753802789079757\n",
      "Epoch 4/100\n",
      "Training Loss: 0.24658037780751282\n",
      "Validation Loss: 0.1955334735684681\n",
      "Epoch 5/100\n",
      "Training Loss: 0.18273006864930433\n",
      "Validation Loss: 0.06947874542338661\n",
      "Epoch 6/100\n",
      "Training Loss: 0.16019442755357013\n",
      "Validation Loss: 0.1282759548791096\n",
      "Epoch 7/100\n",
      "Training Loss: 0.1816282739222118\n",
      "Validation Loss: 0.15783922959107338\n",
      "Epoch 8/100\n",
      "Training Loss: 0.13441840314019007\n",
      "Validation Loss: 0.06140210495631693\n",
      "Epoch 9/100\n",
      "Training Loss: 0.13972000666995474\n",
      "Validation Loss: 0.07217829463928854\n",
      "Epoch 10/100\n",
      "Training Loss: 0.1153314114345434\n",
      "Validation Loss: 0.07897241412119556\n",
      "Epoch 11/100\n",
      "Training Loss: 0.11397564207271378\n",
      "Validation Loss: 0.07997007730805221\n",
      "Epoch 12/100\n",
      "Training Loss: 0.09830213417552951\n",
      "Validation Loss: 0.04017236231745311\n",
      "Epoch 13/100\n",
      "Training Loss: 0.10951044033939006\n",
      "Validation Loss: 0.054239096714858825\n",
      "Epoch 14/100\n",
      "Training Loss: 0.09981115535000841\n",
      "Validation Loss: 0.0614456764041094\n",
      "Epoch 15/100\n",
      "Training Loss: 0.08721831200914532\n",
      "Validation Loss: 0.05071007619836486\n",
      "Epoch 16/100\n",
      "Training Loss: 0.0664999109035192\n",
      "Validation Loss: 0.04103265749598186\n",
      "Epoch 17/100\n",
      "Training Loss: 0.07568241828944142\n",
      "Validation Loss: 0.050984806530179935\n",
      "Epoch 18/100\n",
      "Training Loss: 0.08190913443915453\n",
      "Validation Loss: 0.05809134852673936\n",
      "Epoch 19/100\n",
      "Training Loss: 0.07571026569138581\n",
      "Validation Loss: 0.06141894744168266\n",
      "Epoch 20/100\n",
      "Training Loss: 0.06113213161864327\n",
      "Validation Loss: 0.04784585629062079\n",
      "Epoch 21/100\n",
      "Training Loss: 0.06650377132414892\n",
      "Validation Loss: 0.09516432212084498\n",
      "Epoch 22/100\n",
      "Training Loss: 0.07081222792905191\n",
      "Validation Loss: 0.036150536638770006\n",
      "Epoch 23/100\n",
      "Training Loss: 0.05914372939843817\n",
      "Validation Loss: 0.02412500441246371\n",
      "Epoch 24/100\n",
      "Training Loss: 0.06338147127989141\n",
      "Validation Loss: 0.05510584065687596\n",
      "Epoch 25/100\n",
      "Training Loss: 0.0620410880490482\n",
      "Validation Loss: 0.042212843038002296\n",
      "Epoch 26/100\n",
      "Training Loss: 0.06168865337321241\n",
      "Validation Loss: 0.07988915945824118\n",
      "Epoch 27/100\n",
      "Training Loss: 0.05803188705991903\n",
      "Validation Loss: 0.06351202799787967\n",
      "Epoch 28/100\n",
      "Training Loss: 0.06724451611826543\n",
      "Validation Loss: 0.057786133819766404\n",
      "Epoch 29/100\n",
      "Training Loss: 0.058494631457557046\n",
      "Validation Loss: 0.028676280876985955\n",
      "Epoch 30/100\n",
      "Training Loss: 0.06574578463251408\n",
      "Validation Loss: 0.0477108884430751\n",
      "Epoch 31/100\n",
      "Training Loss: 0.06114642567578083\n",
      "Validation Loss: 0.055002378486384465\n",
      "Epoch 32/100\n",
      "Training Loss: 0.066257434645284\n",
      "Validation Loss: 0.0402130389311862\n",
      "Epoch 33/100\n",
      "Training Loss: 0.060577412110765116\n",
      "Validation Loss: 0.09344909718024808\n",
      "Epoch 34/100\n",
      "Training Loss: 0.06051046452897093\n",
      "Validation Loss: 0.03041694106362964\n",
      "Epoch 35/100\n",
      "Training Loss: 0.0729212939438311\n",
      "Validation Loss: 0.06058295645115932\n",
      "Epoch 36/100\n",
      "Training Loss: 0.06540206718388493\n",
      "Validation Loss: 0.04270895920215158\n",
      "Epoch 37/100\n",
      "Training Loss: 0.06057088990714372\n",
      "Validation Loss: 0.015859094967124433\n",
      "Epoch 38/100\n",
      "Training Loss: 0.07089958701253922\n",
      "Validation Loss: 0.044730512530987705\n",
      "Epoch 39/100\n",
      "Training Loss: 0.07547823847953014\n",
      "Validation Loss: 0.04747640802063067\n",
      "Epoch 40/100\n",
      "Training Loss: 0.07876843688779626\n",
      "Validation Loss: 0.04986671230749086\n",
      "Epoch 41/100\n",
      "Training Loss: 0.061599427153184685\n",
      "Validation Loss: 0.08330387407680653\n",
      "Epoch 42/100\n",
      "Training Loss: 0.06977618816121467\n",
      "Validation Loss: 0.041753930047194285\n",
      "Epoch 43/100\n",
      "Training Loss: 0.06911722193575864\n",
      "Validation Loss: 0.03323756495065463\n",
      "Epoch 44/100\n",
      "Training Loss: 0.06280965548923159\n",
      "Validation Loss: 0.08479723717168806\n",
      "Epoch 45/100\n",
      "Training Loss: 0.07612087025387007\n",
      "Validation Loss: 0.05418237908877861\n",
      "Epoch 46/100\n",
      "Training Loss: 0.06541409150243792\n",
      "Validation Loss: 0.0420524118460426\n",
      "Epoch 47/100\n",
      "Training Loss: 0.06705046883141445\n",
      "Validation Loss: 0.06400518813053316\n",
      "Epoch 48/100\n",
      "Training Loss: 0.07482294560921436\n",
      "Validation Loss: 0.06837591788758092\n",
      "Epoch 49/100\n",
      "Training Loss: 0.07067003941357597\n",
      "Validation Loss: 0.0442228226682664\n",
      "Epoch 50/100\n",
      "Training Loss: 0.06934196266610877\n",
      "Validation Loss: 0.035855412471853886\n",
      "Epoch 51/100\n",
      "Training Loss: 0.06970175821322294\n",
      "Validation Loss: 0.07853758440964123\n",
      "Epoch 52/100\n",
      "Training Loss: 0.06525173616708105\n",
      "Validation Loss: 0.038021388969851334\n",
      "Epoch 53/100\n",
      "Training Loss: 0.07562481659758055\n",
      "Validation Loss: 0.02867492221570548\n",
      "Epoch 54/100\n",
      "Training Loss: 0.06841153408844582\n",
      "Validation Loss: 0.057905174749791\n",
      "Epoch 55/100\n",
      "Training Loss: 0.09256741065599283\n",
      "Validation Loss: 0.03905389309317643\n",
      "Epoch 56/100\n",
      "Training Loss: 0.06715890512902978\n",
      "Validation Loss: 0.036246645911178994\n",
      "Epoch 57/100\n",
      "Training Loss: 0.05684677178531784\n",
      "Validation Loss: 0.04892143085251667\n",
      "Epoch 58/100\n",
      "Training Loss: 0.07207200874216375\n",
      "Validation Loss: 0.051443060647261486\n",
      "Epoch 59/100\n",
      "Training Loss: 0.07244701352292311\n",
      "Validation Loss: 0.04852820884483388\n",
      "Epoch 60/100\n",
      "Training Loss: 0.08253237272518661\n",
      "Validation Loss: 0.04137913695230915\n",
      "Epoch 61/100\n",
      "Training Loss: 0.07211591639418312\n",
      "Validation Loss: 0.06266216425596154\n",
      "Epoch 62/100\n",
      "Training Loss: 0.0710584566573526\n",
      "Validation Loss: 0.04496669154833081\n",
      "Epoch 63/100\n",
      "Training Loss: 0.07715267641682738\n",
      "Validation Loss: 0.05430805445531449\n",
      "Epoch 64/100\n",
      "Training Loss: 0.06860966482832798\n",
      "Validation Loss: 0.05892600978107652\n",
      "Epoch 65/100\n",
      "Training Loss: 0.07421387847128158\n",
      "Validation Loss: 0.04242505255082312\n",
      "Epoch 66/100\n",
      "Training Loss: 0.08525264859340108\n",
      "Validation Loss: 0.042499999416587324\n",
      "Epoch 67/100\n",
      "Training Loss: 0.08425056396570178\n",
      "Validation Loss: 0.04137519415092189\n",
      "Epoch 68/100\n",
      "Training Loss: 0.08362046165524491\n",
      "Validation Loss: 0.0644150148772539\n",
      "Epoch 69/100\n",
      "Training Loss: 0.08523342385481279\n",
      "Validation Loss: 0.03466815819453614\n",
      "Epoch 70/100\n",
      "Training Loss: 0.08193297103790657\n",
      "Validation Loss: 0.04554084615337077\n",
      "Epoch 71/100\n",
      "Training Loss: 0.0700827688383016\n",
      "Validation Loss: 0.044835795954172274\n",
      "Epoch 72/100\n",
      "Training Loss: 0.08730842835313758\n",
      "Validation Loss: 0.05419050919819979\n",
      "Epoch 73/100\n",
      "Training Loss: 0.08063332938507888\n",
      "Validation Loss: 0.06484649961252768\n",
      "Epoch 74/100\n",
      "Training Loss: 0.07165528237693992\n",
      "Validation Loss: 0.05512500284597216\n",
      "Epoch 75/100\n",
      "Training Loss: 0.07843405853079453\n",
      "Validation Loss: 0.04975327694244683\n",
      "Epoch 76/100\n",
      "Training Loss: 0.07426706053874976\n",
      "Validation Loss: 0.03177222255111354\n",
      "Epoch 77/100\n",
      "Training Loss: 0.06809255849170083\n",
      "Validation Loss: 0.05740654549811112\n",
      "Epoch 78/100\n",
      "Training Loss: 0.07049715629607357\n",
      "Validation Loss: 0.06532388173568358\n",
      "Epoch 79/100\n",
      "Training Loss: 0.08188162311256349\n",
      "Validation Loss: 0.02125227498149667\n",
      "Epoch 80/100\n",
      "Training Loss: 0.06408512718771214\n",
      "Validation Loss: 0.07865469307562634\n",
      "Epoch 81/100\n",
      "Training Loss: 0.08229699923518266\n",
      "Validation Loss: 0.06577431728691108\n",
      "Epoch 82/100\n",
      "Training Loss: 0.07602773295428641\n",
      "Validation Loss: 0.06409929412012923\n",
      "Epoch 83/100\n",
      "Training Loss: 0.0686687296947164\n",
      "Validation Loss: 0.039053872083257946\n",
      "Epoch 84/100\n",
      "Training Loss: 0.06848758006420679\n",
      "Validation Loss: 0.07227682160311984\n",
      "Epoch 85/100\n",
      "Training Loss: 0.06750634406257236\n",
      "Validation Loss: 0.03812823763102734\n",
      "Epoch 86/100\n",
      "Training Loss: 0.079661247394934\n",
      "Validation Loss: 0.11201255817979176\n",
      "Epoch 87/100\n",
      "Training Loss: 0.0909481047379457\n",
      "Validation Loss: 0.04386559786598112\n",
      "Epoch 88/100\n",
      "Training Loss: 0.06773860615685519\n",
      "Validation Loss: 0.0487302677587866\n",
      "Epoch 89/100\n",
      "Training Loss: 0.07715552780000098\n",
      "Validation Loss: 0.055846452247775624\n",
      "Epoch 90/100\n",
      "Training Loss: 0.06997392231632164\n",
      "Validation Loss: 0.0666179802496352\n",
      "Epoch 91/100\n",
      "Training Loss: 0.07461044055311893\n",
      "Validation Loss: 0.04280533956514106\n",
      "Epoch 92/100\n",
      "Training Loss: 0.07313028386782192\n",
      "Validation Loss: 0.09467441484421704\n",
      "Epoch 93/100\n",
      "Training Loss: 0.07584309057191851\n",
      "Validation Loss: 0.05051811569263499\n",
      "Epoch 94/100\n",
      "Training Loss: 0.06999465675711761\n",
      "Validation Loss: 0.05790554821039847\n",
      "Epoch 95/100\n",
      "Training Loss: 0.07183777169151531\n",
      "Validation Loss: 0.0867600531260336\n",
      "Epoch 96/100\n",
      "Training Loss: 0.06897434719691574\n",
      "Validation Loss: 0.05840878402153711\n",
      "Epoch 97/100\n",
      "Training Loss: 0.0598838126135835\n",
      "Validation Loss: 0.06328036901516972\n",
      "Epoch 98/100\n",
      "Training Loss: 0.08803649833270828\n",
      "Validation Loss: 0.07022353881867786\n",
      "Epoch 99/100\n",
      "Training Loss: 0.08433816960482754\n",
      "Validation Loss: 0.0586001857035733\n",
      "Epoch 100/100\n",
      "Training Loss: 0.08237409875005529\n",
      "Validation Loss: 0.035118981396281744\n",
      "    Trial 2/2 for combination 36/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.26574073489895\n",
      "Validation Loss: 0.19927452158823025\n",
      "Epoch 2/100\n",
      "Training Loss: 0.24911408246490296\n",
      "Validation Loss: 0.1745330054332221\n",
      "Epoch 3/100\n",
      "Training Loss: 0.19986272795808555\n",
      "Validation Loss: 0.16846615855136543\n",
      "Epoch 4/100\n",
      "Training Loss: 0.1765678882150275\n",
      "Validation Loss: 0.11103716962078883\n",
      "Epoch 5/100\n",
      "Training Loss: 0.1401221517680621\n",
      "Validation Loss: 0.06450441683742099\n",
      "Epoch 6/100\n",
      "Training Loss: 0.18969342761725622\n",
      "Validation Loss: 0.15789942803999316\n",
      "Epoch 7/100\n",
      "Training Loss: 0.14153091573188817\n",
      "Validation Loss: 0.0929606604354416\n",
      "Epoch 8/100\n",
      "Training Loss: 0.1218610918489424\n",
      "Validation Loss: 0.06962577200411448\n",
      "Epoch 9/100\n",
      "Training Loss: 0.1117068676921788\n",
      "Validation Loss: 0.04801567312261165\n",
      "Epoch 10/100\n",
      "Training Loss: 0.08866177537198708\n",
      "Validation Loss: 0.05495495142824782\n",
      "Epoch 11/100\n",
      "Training Loss: 0.08973238281080492\n",
      "Validation Loss: 0.04117596738450039\n",
      "Epoch 12/100\n",
      "Training Loss: 0.10617233889360309\n",
      "Validation Loss: 0.08897364929799335\n",
      "Epoch 13/100\n",
      "Training Loss: 0.08731476191286697\n",
      "Validation Loss: 0.044991383841823704\n",
      "Epoch 14/100\n",
      "Training Loss: 0.08831447667681463\n",
      "Validation Loss: 0.03922971628244075\n",
      "Epoch 15/100\n",
      "Training Loss: 0.07966752119002785\n",
      "Validation Loss: 0.05920378004204927\n",
      "Epoch 16/100\n",
      "Training Loss: 0.07278643129930709\n",
      "Validation Loss: 0.0328675254443927\n",
      "Epoch 17/100\n",
      "Training Loss: 0.08119382764093462\n",
      "Validation Loss: 0.08455898447220142\n",
      "Epoch 18/100\n",
      "Training Loss: 0.0668620637134191\n",
      "Validation Loss: 0.03253515161289829\n",
      "Epoch 19/100\n",
      "Training Loss: 0.06574216132293904\n",
      "Validation Loss: 0.050251307437161254\n",
      "Epoch 20/100\n",
      "Training Loss: 0.06643516818960146\n",
      "Validation Loss: 0.04355099126400035\n",
      "Epoch 21/100\n",
      "Training Loss: 0.07030472904689593\n",
      "Validation Loss: 0.030230504651309175\n",
      "Epoch 22/100\n",
      "Training Loss: 0.06528265657433703\n",
      "Validation Loss: 0.02572435229406425\n",
      "Epoch 23/100\n",
      "Training Loss: 0.06998641621862349\n",
      "Validation Loss: 0.0648324248118198\n",
      "Epoch 24/100\n",
      "Training Loss: 0.053513053571304856\n",
      "Validation Loss: 0.05090374600382186\n",
      "Epoch 25/100\n",
      "Training Loss: 0.06266628317641224\n",
      "Validation Loss: 0.0378129666209857\n",
      "Epoch 26/100\n",
      "Training Loss: 0.05947269462580964\n",
      "Validation Loss: 0.0491558743046127\n",
      "Epoch 27/100\n",
      "Training Loss: 0.05345692793327598\n",
      "Validation Loss: 0.07829837490235152\n",
      "Epoch 28/100\n",
      "Training Loss: 0.06785664990712664\n",
      "Validation Loss: 0.05666053373733802\n",
      "Epoch 29/100\n",
      "Training Loss: 0.07029510314359447\n",
      "Validation Loss: 0.08561017262396892\n",
      "Epoch 30/100\n",
      "Training Loss: 0.06615319390927604\n",
      "Validation Loss: 0.0962955404756041\n",
      "Epoch 31/100\n",
      "Training Loss: 0.07805254776885888\n",
      "Validation Loss: 0.04558082761285154\n",
      "Epoch 32/100\n",
      "Training Loss: 0.06479052262699274\n",
      "Validation Loss: 0.07997553626602853\n",
      "Epoch 33/100\n",
      "Training Loss: 0.06002768785073922\n",
      "Validation Loss: 0.02778589704323804\n",
      "Epoch 34/100\n",
      "Training Loss: 0.06993290684909324\n",
      "Validation Loss: 0.07106993764768764\n",
      "Epoch 35/100\n",
      "Training Loss: 0.07394749529349248\n",
      "Validation Loss: 0.043772382506921816\n",
      "Epoch 36/100\n",
      "Training Loss: 0.07408989436350047\n",
      "Validation Loss: 0.029806488383266865\n",
      "Epoch 37/100\n",
      "Training Loss: 0.0727637317879219\n",
      "Validation Loss: 0.07937954114114898\n",
      "Epoch 38/100\n",
      "Training Loss: 0.06444960021636127\n",
      "Validation Loss: 0.03383800340155617\n",
      "Epoch 39/100\n",
      "Training Loss: 0.06106748402168227\n",
      "Validation Loss: 0.047014466890811776\n",
      "Epoch 40/100\n",
      "Training Loss: 0.0780612964830403\n",
      "Validation Loss: 0.04613909073500125\n",
      "Epoch 41/100\n",
      "Training Loss: 0.07744376053406161\n",
      "Validation Loss: 0.0495843074382256\n",
      "Epoch 42/100\n",
      "Training Loss: 0.058613084665628916\n",
      "Validation Loss: 0.022126564371878103\n",
      "Epoch 43/100\n",
      "Training Loss: 0.08666316372645416\n",
      "Validation Loss: 0.03624997578799756\n",
      "Epoch 44/100\n",
      "Training Loss: 0.07595314925371031\n",
      "Validation Loss: 0.08490789198289272\n",
      "Epoch 45/100\n",
      "Training Loss: 0.0735035656260004\n",
      "Validation Loss: 0.06114547156294895\n",
      "Epoch 46/100\n",
      "Training Loss: 0.06631945717555958\n",
      "Validation Loss: 0.04912676752909899\n",
      "Epoch 47/100\n",
      "Training Loss: 0.06744440429212631\n",
      "Validation Loss: 0.06405427477626002\n",
      "Epoch 48/100\n",
      "Training Loss: 0.0737666555684821\n",
      "Validation Loss: 0.03232525117744442\n",
      "Epoch 49/100\n",
      "Training Loss: 0.0701170347497737\n",
      "Validation Loss: 0.05426672056245093\n",
      "Epoch 50/100\n",
      "Training Loss: 0.06543490493155203\n",
      "Validation Loss: 0.06478009959753904\n",
      "Epoch 51/100\n",
      "Training Loss: 0.07133423327081709\n",
      "Validation Loss: 0.057541496169245844\n",
      "Epoch 52/100\n",
      "Training Loss: 0.05836135555070917\n",
      "Validation Loss: 0.04096841158597965\n",
      "Epoch 53/100\n",
      "Training Loss: 0.09293429644866529\n",
      "Validation Loss: 0.030288274651694626\n",
      "Epoch 54/100\n",
      "Training Loss: 0.068782009690466\n",
      "Validation Loss: 0.08280381270392581\n",
      "Epoch 55/100\n",
      "Training Loss: 0.06193492803220285\n",
      "Validation Loss: 0.08253763260807234\n",
      "Epoch 56/100\n",
      "Training Loss: 0.07438185559254887\n",
      "Validation Loss: 0.04457932332263035\n",
      "Epoch 57/100\n",
      "Training Loss: 0.0636494427084202\n",
      "Validation Loss: 0.08689706806307211\n",
      "Epoch 58/100\n",
      "Training Loss: 0.09041079310779089\n",
      "Validation Loss: 0.04166203712873508\n",
      "Epoch 59/100\n",
      "Training Loss: 0.0832517518051645\n",
      "Validation Loss: 0.04845415933243866\n",
      "Epoch 60/100\n",
      "Training Loss: 0.06349262606630206\n",
      "Validation Loss: 0.06508205268274574\n",
      "Epoch 61/100\n",
      "Training Loss: 0.06491414666629391\n",
      "Validation Loss: 0.1026674612678539\n",
      "Epoch 62/100\n",
      "Training Loss: 0.07620146436580338\n",
      "Validation Loss: 0.06441582378485973\n",
      "Epoch 63/100\n",
      "Training Loss: 0.08232067525340687\n",
      "Validation Loss: 0.0454306021796076\n",
      "Epoch 64/100\n",
      "Training Loss: 0.08975243507470303\n",
      "Validation Loss: 0.06844261006200118\n",
      "Epoch 65/100\n",
      "Training Loss: 0.09064835219805907\n",
      "Validation Loss: 0.08207711107485341\n",
      "Epoch 66/100\n",
      "Training Loss: 0.08731818673768518\n",
      "Validation Loss: 0.06202117210783066\n",
      "Epoch 67/100\n",
      "Training Loss: 0.07091535818296896\n",
      "Validation Loss: 0.04345566745827177\n",
      "Epoch 68/100\n",
      "Training Loss: 0.09392681425689328\n",
      "Validation Loss: 0.06536498365852139\n",
      "Epoch 69/100\n",
      "Training Loss: 0.07560666963245845\n",
      "Validation Loss: 0.05828843716695649\n",
      "Epoch 70/100\n",
      "Training Loss: 0.07133026872828968\n",
      "Validation Loss: 0.05529805929417011\n",
      "Epoch 71/100\n",
      "Training Loss: 0.08117221081032835\n",
      "Validation Loss: 0.05743093237506049\n",
      "Epoch 72/100\n",
      "Training Loss: 0.07697150453700899\n",
      "Validation Loss: 0.03531124995211958\n",
      "Epoch 73/100\n",
      "Training Loss: 0.06603189321837702\n",
      "Validation Loss: 0.034721599208747435\n",
      "Epoch 74/100\n",
      "Training Loss: 0.07433143894547478\n",
      "Validation Loss: 0.06747180263101284\n",
      "Epoch 75/100\n",
      "Training Loss: 0.07301763556310806\n",
      "Validation Loss: 0.049411074755063907\n",
      "Epoch 76/100\n",
      "Training Loss: 0.09210130961521315\n",
      "Validation Loss: 0.03369119971305811\n",
      "Epoch 77/100\n",
      "Training Loss: 0.06710926261152216\n",
      "Validation Loss: 0.03947210448568957\n",
      "Epoch 78/100\n",
      "Training Loss: 0.07704841527095908\n",
      "Validation Loss: 0.05624143445460489\n",
      "Epoch 79/100\n",
      "Training Loss: 0.07965673690602294\n",
      "Validation Loss: 0.05731111027145132\n",
      "Epoch 80/100\n",
      "Training Loss: 0.08086887577197958\n",
      "Validation Loss: 0.08279496738070292\n",
      "Epoch 81/100\n",
      "Training Loss: 0.06564024087153464\n",
      "Validation Loss: 0.07450890343808217\n",
      "Epoch 82/100\n",
      "Training Loss: 0.07209154597222732\n",
      "Validation Loss: 0.041086605270565846\n",
      "Epoch 83/100\n",
      "Training Loss: 0.09337872689436592\n",
      "Validation Loss: 0.0807126607329438\n",
      "Epoch 84/100\n",
      "Training Loss: 0.08586044677129859\n",
      "Validation Loss: 0.11414172021739377\n",
      "Epoch 85/100\n",
      "Training Loss: 0.08312731018378526\n",
      "Validation Loss: 0.07929767347888175\n",
      "Epoch 86/100\n",
      "Training Loss: 0.08261984105827445\n",
      "Validation Loss: 0.07305645549476861\n",
      "Epoch 87/100\n",
      "Training Loss: 0.08138845901694484\n",
      "Validation Loss: 0.07001489753569763\n",
      "Epoch 88/100\n",
      "Training Loss: 0.06788212309794305\n",
      "Validation Loss: 0.07892488543815528\n",
      "Epoch 89/100\n",
      "Training Loss: 0.07062564304996966\n",
      "Validation Loss: 0.05666893130107094\n",
      "Epoch 90/100\n",
      "Training Loss: 0.08949125447980792\n",
      "Validation Loss: 0.09784935841412698\n",
      "Epoch 91/100\n",
      "Training Loss: 0.08128483993568077\n",
      "Validation Loss: 0.051140585820796405\n",
      "Epoch 92/100\n",
      "Training Loss: 0.07485711445312077\n",
      "Validation Loss: 0.04337588493000562\n",
      "Epoch 93/100\n",
      "Training Loss: 0.07830225781566487\n",
      "Validation Loss: 0.08938206542708303\n",
      "Epoch 94/100\n",
      "Training Loss: 0.07005481408209493\n",
      "Validation Loss: 0.07611142406287116\n",
      "Epoch 95/100\n",
      "Training Loss: 0.08200047178702291\n",
      "Validation Loss: 0.1314886330038509\n",
      "Epoch 96/100\n",
      "Training Loss: 0.07808433220432502\n",
      "Validation Loss: 0.08787280772503095\n",
      "Epoch 97/100\n",
      "Training Loss: 0.08200371716363268\n",
      "Validation Loss: 0.03642250738550494\n",
      "Epoch 98/100\n",
      "Training Loss: 0.08296801099881324\n",
      "Validation Loss: 0.07438620325141666\n",
      "Epoch 99/100\n",
      "Training Loss: 0.08820132066108843\n",
      "Validation Loss: 0.06421679300278314\n",
      "Epoch 100/100\n",
      "Training Loss: 0.061152059274826635\n",
      "Validation Loss: 0.0322610899211722\n",
      "Combination 36: Avg Training Loss = 0.08486213758099422, Avg Validation Loss = 0.06334150757110972\n",
      "Testing combination 37/48: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0001, 'sequence_length': 6, 'dropout_rate': 0.1}\n",
      "Truncated Train Data Shape: (70, 6, 1)\n",
      "Truncated Validation Data Shape: (10, 6, 1)\n",
      "    Trial 1/2 for combination 37/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.7143622146617613\n",
      "Validation Loss: 0.7009715411817432\n",
      "Epoch 2/100\n",
      "Training Loss: 0.49456881845994605\n",
      "Validation Loss: 0.5027006488287513\n",
      "Epoch 3/100\n",
      "Training Loss: 0.4656517426852198\n",
      "Validation Loss: 0.31677218685511865\n",
      "Epoch 4/100\n",
      "Training Loss: 0.45613249146954377\n",
      "Validation Loss: 0.4140573982895327\n",
      "Epoch 5/100\n",
      "Training Loss: 0.3907443990118926\n",
      "Validation Loss: 0.17465762000646803\n",
      "Epoch 6/100\n",
      "Training Loss: 0.42728200487876106\n",
      "Validation Loss: 0.11820248236641177\n",
      "Epoch 7/100\n",
      "Training Loss: 0.2932822981193777\n",
      "Validation Loss: 0.32383948531594287\n",
      "Epoch 8/100\n",
      "Training Loss: 0.37966679325226754\n",
      "Validation Loss: 0.18635173203506147\n",
      "Epoch 9/100\n",
      "Training Loss: 0.3177089512509475\n",
      "Validation Loss: 0.29700725032166814\n",
      "Epoch 10/100\n",
      "Training Loss: 0.3973105423138182\n",
      "Validation Loss: 0.1752577082840743\n",
      "Epoch 11/100\n",
      "Training Loss: 0.2776445075504932\n",
      "Validation Loss: 0.10440107091486553\n",
      "Epoch 12/100\n",
      "Training Loss: 0.3896077894314669\n",
      "Validation Loss: 0.20540773067091336\n",
      "Epoch 13/100\n",
      "Training Loss: 0.2915092059506646\n",
      "Validation Loss: 0.3028846257784038\n",
      "Epoch 14/100\n",
      "Training Loss: 0.3014090108356676\n",
      "Validation Loss: 0.37222566371593646\n",
      "Epoch 15/100\n",
      "Training Loss: 0.26620852814718016\n",
      "Validation Loss: 0.2046471925477466\n",
      "Epoch 16/100\n",
      "Training Loss: 0.31308003318846594\n",
      "Validation Loss: 0.2718834513762206\n",
      "Epoch 17/100\n",
      "Training Loss: 0.29657611816899\n",
      "Validation Loss: 0.1500407165520803\n",
      "Epoch 18/100\n",
      "Training Loss: 0.2909284378252952\n",
      "Validation Loss: 0.17544061879245837\n",
      "Epoch 19/100\n",
      "Training Loss: 0.2811348859653938\n",
      "Validation Loss: 0.17937965255686553\n",
      "Epoch 20/100\n",
      "Training Loss: 0.20602444343355258\n",
      "Validation Loss: 0.6529684382962814\n",
      "Epoch 21/100\n",
      "Training Loss: 0.26860321551398364\n",
      "Validation Loss: 0.24765532644838456\n",
      "Epoch 22/100\n",
      "Training Loss: 0.2564309103260513\n",
      "Validation Loss: 0.15918790511009576\n",
      "Epoch 23/100\n",
      "Training Loss: 0.2080006944455279\n",
      "Validation Loss: 0.15722513300296315\n",
      "Epoch 24/100\n",
      "Training Loss: 0.2514693689806442\n",
      "Validation Loss: 0.3156278581616417\n",
      "Epoch 25/100\n",
      "Training Loss: 0.2439847904850115\n",
      "Validation Loss: 0.2843537656748998\n",
      "Epoch 26/100\n",
      "Training Loss: 0.23801047306676937\n",
      "Validation Loss: 0.21300757178780444\n",
      "Epoch 27/100\n",
      "Training Loss: 0.20851209701749654\n",
      "Validation Loss: 0.1765990806872915\n",
      "Epoch 28/100\n",
      "Training Loss: 0.19861756619218116\n",
      "Validation Loss: 0.18375104430445904\n",
      "Epoch 29/100\n",
      "Training Loss: 0.20532692437423944\n",
      "Validation Loss: 0.11528362289383756\n",
      "Epoch 30/100\n",
      "Training Loss: 0.2118694141312504\n",
      "Validation Loss: 0.21741925604282927\n",
      "Epoch 31/100\n",
      "Training Loss: 0.22884710268559008\n",
      "Validation Loss: 0.24161888291260944\n",
      "Epoch 32/100\n",
      "Training Loss: 0.22332817449774003\n",
      "Validation Loss: 0.1802440280337674\n",
      "Epoch 33/100\n",
      "Training Loss: 0.2044618590148734\n",
      "Validation Loss: 0.18408082088602146\n",
      "Epoch 34/100\n",
      "Training Loss: 0.2139498003718574\n",
      "Validation Loss: 0.15537527282312832\n",
      "Epoch 35/100\n",
      "Training Loss: 0.2081764346358719\n",
      "Validation Loss: 0.111293292360931\n",
      "Epoch 36/100\n",
      "Training Loss: 0.21760262821417556\n",
      "Validation Loss: 0.24238257908531757\n",
      "Epoch 37/100\n",
      "Training Loss: 0.21622489622033778\n",
      "Validation Loss: 0.17390866890656168\n",
      "Epoch 38/100\n",
      "Training Loss: 0.22351426328029564\n",
      "Validation Loss: 0.21498765449888238\n",
      "Epoch 39/100\n",
      "Training Loss: 0.20022932335403004\n",
      "Validation Loss: 0.09224098893710928\n",
      "Epoch 40/100\n",
      "Training Loss: 0.19662077935822575\n",
      "Validation Loss: 0.20444282149279308\n",
      "Epoch 41/100\n",
      "Training Loss: 0.1947593918410305\n",
      "Validation Loss: 0.16966124138294542\n",
      "Epoch 42/100\n",
      "Training Loss: 0.21750859957874621\n",
      "Validation Loss: 0.15022884574963813\n",
      "Epoch 43/100\n",
      "Training Loss: 0.1740844119999647\n",
      "Validation Loss: 0.1236386815322732\n",
      "Epoch 44/100\n",
      "Training Loss: 0.1895240144235753\n",
      "Validation Loss: 0.20736386680703936\n",
      "Epoch 45/100\n",
      "Training Loss: 0.19579648981841638\n",
      "Validation Loss: 0.1470011695395304\n",
      "Epoch 46/100\n",
      "Training Loss: 0.21141301586109937\n",
      "Validation Loss: 0.22710788635689383\n",
      "Epoch 47/100\n",
      "Training Loss: 0.19750009608177652\n",
      "Validation Loss: 0.132739365797439\n",
      "Epoch 48/100\n",
      "Training Loss: 0.18526803887912194\n",
      "Validation Loss: 0.22833094846469573\n",
      "Epoch 49/100\n",
      "Training Loss: 0.17764399481692295\n",
      "Validation Loss: 0.13283839870667088\n",
      "Epoch 50/100\n",
      "Training Loss: 0.20524312330562006\n",
      "Validation Loss: 0.11294636023840261\n",
      "Epoch 51/100\n",
      "Training Loss: 0.1824167399479972\n",
      "Validation Loss: 0.09238168210610267\n",
      "Epoch 52/100\n",
      "Training Loss: 0.14932229284666815\n",
      "Validation Loss: 0.10699854460509635\n",
      "Epoch 53/100\n",
      "Training Loss: 0.18740158871832605\n",
      "Validation Loss: 0.13811519809409994\n",
      "Epoch 54/100\n",
      "Training Loss: 0.1986166721949824\n",
      "Validation Loss: 0.12176029845764827\n",
      "Epoch 55/100\n",
      "Training Loss: 0.16631358796001866\n",
      "Validation Loss: 0.14540868324103814\n",
      "Epoch 56/100\n",
      "Training Loss: 0.18502112595618073\n",
      "Validation Loss: 0.07253543068982268\n",
      "Epoch 57/100\n",
      "Training Loss: 0.17555636129104862\n",
      "Validation Loss: 0.11130406939901749\n",
      "Epoch 58/100\n",
      "Training Loss: 0.17203448528261336\n",
      "Validation Loss: 0.12663999183208424\n",
      "Epoch 59/100\n",
      "Training Loss: 0.1445254071776121\n",
      "Validation Loss: 0.1031944062794575\n",
      "Epoch 60/100\n",
      "Training Loss: 0.19799975920391838\n",
      "Validation Loss: 0.12840888596660222\n",
      "Epoch 61/100\n",
      "Training Loss: 0.1606721106492825\n",
      "Validation Loss: 0.088916082216787\n",
      "Epoch 62/100\n",
      "Training Loss: 0.17382442467712947\n",
      "Validation Loss: 0.11418904723627729\n",
      "Epoch 63/100\n",
      "Training Loss: 0.18173338197091346\n",
      "Validation Loss: 0.14061079331872525\n",
      "Epoch 64/100\n",
      "Training Loss: 0.1647017667929169\n",
      "Validation Loss: 0.08154007084837979\n",
      "Epoch 65/100\n",
      "Training Loss: 0.17180563795765716\n",
      "Validation Loss: 0.10834979000116715\n",
      "Epoch 66/100\n",
      "Training Loss: 0.14423343971659874\n",
      "Validation Loss: 0.1757546180164545\n",
      "Epoch 67/100\n",
      "Training Loss: 0.1370437348416466\n",
      "Validation Loss: 0.1584413721047914\n",
      "Epoch 68/100\n",
      "Training Loss: 0.1569570889373543\n",
      "Validation Loss: 0.13768942352421976\n",
      "Epoch 69/100\n",
      "Training Loss: 0.16218948798148797\n",
      "Validation Loss: 0.2048850581502078\n",
      "Epoch 70/100\n",
      "Training Loss: 0.1805650013016491\n",
      "Validation Loss: 0.14072116266116624\n",
      "Epoch 71/100\n",
      "Training Loss: 0.16685259600384603\n",
      "Validation Loss: 0.11967044288961934\n",
      "Epoch 72/100\n",
      "Training Loss: 0.15838830841117563\n",
      "Validation Loss: 0.10493232373655945\n",
      "Epoch 73/100\n",
      "Training Loss: 0.1686302500364015\n",
      "Validation Loss: 0.09490994848729598\n",
      "Epoch 74/100\n",
      "Training Loss: 0.14247620293009347\n",
      "Validation Loss: 0.16004567920464624\n",
      "Epoch 75/100\n",
      "Training Loss: 0.1446314570010998\n",
      "Validation Loss: 0.11140741510502208\n",
      "Epoch 76/100\n",
      "Training Loss: 0.15701319903559158\n",
      "Validation Loss: 0.11319791503496082\n",
      "Epoch 77/100\n",
      "Training Loss: 0.1421008297855714\n",
      "Validation Loss: 0.11826955294844751\n",
      "Epoch 78/100\n",
      "Training Loss: 0.16175561002043132\n",
      "Validation Loss: 0.09941159522999449\n",
      "Epoch 79/100\n",
      "Training Loss: 0.1486456442004737\n",
      "Validation Loss: 0.09661561705766777\n",
      "Epoch 80/100\n",
      "Training Loss: 0.15890815055373692\n",
      "Validation Loss: 0.09658476234845996\n",
      "Epoch 81/100\n",
      "Training Loss: 0.15833381859051773\n",
      "Validation Loss: 0.13269721038857715\n",
      "Epoch 82/100\n",
      "Training Loss: 0.14145489872433964\n",
      "Validation Loss: 0.09495842681850615\n",
      "Epoch 83/100\n",
      "Training Loss: 0.15427914209547353\n",
      "Validation Loss: 0.10009160961434846\n",
      "Epoch 84/100\n",
      "Training Loss: 0.17285789248147268\n",
      "Validation Loss: 0.057184615252509265\n",
      "Epoch 85/100\n",
      "Training Loss: 0.16129636832933872\n",
      "Validation Loss: 0.076394055728628\n",
      "Epoch 86/100\n",
      "Training Loss: 0.13079471989521863\n",
      "Validation Loss: 0.14654752532527937\n",
      "Epoch 87/100\n",
      "Training Loss: 0.14840434325508167\n",
      "Validation Loss: 0.11190497978140426\n",
      "Epoch 88/100\n",
      "Training Loss: 0.14461944138798655\n",
      "Validation Loss: 0.09309051263510446\n",
      "Epoch 89/100\n",
      "Training Loss: 0.15031225260215195\n",
      "Validation Loss: 0.07644929584654599\n",
      "Epoch 90/100\n",
      "Training Loss: 0.1411456342965945\n",
      "Validation Loss: 0.0766612742969868\n",
      "Epoch 91/100\n",
      "Training Loss: 0.1445682394475302\n",
      "Validation Loss: 0.12210031864937579\n",
      "Epoch 92/100\n",
      "Training Loss: 0.11496103549204223\n",
      "Validation Loss: 0.1322498798348221\n",
      "Epoch 93/100\n",
      "Training Loss: 0.1417307894143162\n",
      "Validation Loss: 0.09031464784997732\n",
      "Epoch 94/100\n",
      "Training Loss: 0.13565637210541923\n",
      "Validation Loss: 0.15352320521986412\n",
      "Epoch 95/100\n",
      "Training Loss: 0.14884757122904752\n",
      "Validation Loss: 0.08057593917685249\n",
      "Epoch 96/100\n",
      "Training Loss: 0.14676703585249193\n",
      "Validation Loss: 0.10307264812650482\n",
      "Epoch 97/100\n",
      "Training Loss: 0.13459249137245424\n",
      "Validation Loss: 0.08652409620972532\n",
      "Epoch 98/100\n",
      "Training Loss: 0.13331370148817911\n",
      "Validation Loss: 0.06461444707406974\n",
      "Epoch 99/100\n",
      "Training Loss: 0.13426481877425778\n",
      "Validation Loss: 0.06763374056203773\n",
      "Epoch 100/100\n",
      "Training Loss: 0.1369127519894784\n",
      "Validation Loss: 0.08450117075533536\n",
      "    Trial 2/2 for combination 37/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.4992170060204216\n",
      "Validation Loss: 0.49615753382348454\n",
      "Epoch 2/100\n",
      "Training Loss: 0.5083485202110574\n",
      "Validation Loss: 0.4765448159273512\n",
      "Epoch 3/100\n",
      "Training Loss: 0.4669145483041785\n",
      "Validation Loss: 0.5328465624351317\n",
      "Epoch 4/100\n",
      "Training Loss: 0.39213872577998715\n",
      "Validation Loss: 0.47438590973302475\n",
      "Epoch 5/100\n",
      "Training Loss: 0.42171026885203655\n",
      "Validation Loss: 0.4280973474605446\n",
      "Epoch 6/100\n",
      "Training Loss: 0.2953649257839454\n",
      "Validation Loss: 0.4053984217477695\n",
      "Epoch 7/100\n",
      "Training Loss: 0.3571533500203384\n",
      "Validation Loss: 0.0998304565002175\n",
      "Epoch 8/100\n",
      "Training Loss: 0.2928860809221738\n",
      "Validation Loss: 0.38259973620088744\n",
      "Epoch 9/100\n",
      "Training Loss: 0.35337546049483676\n",
      "Validation Loss: 0.2416633833475951\n",
      "Epoch 10/100\n",
      "Training Loss: 0.3289896407632972\n",
      "Validation Loss: 0.14105612714907062\n",
      "Epoch 11/100\n",
      "Training Loss: 0.3357624372604799\n",
      "Validation Loss: 0.3578752053781309\n",
      "Epoch 12/100\n",
      "Training Loss: 0.3547281692080747\n",
      "Validation Loss: 0.25474387669155546\n",
      "Epoch 13/100\n",
      "Training Loss: 0.255340593964945\n",
      "Validation Loss: 0.39450774582036924\n",
      "Epoch 14/100\n",
      "Training Loss: 0.2843626683875133\n",
      "Validation Loss: 0.2051828240880274\n",
      "Epoch 15/100\n",
      "Training Loss: 0.32125591480212995\n",
      "Validation Loss: 0.241399918981641\n",
      "Epoch 16/100\n",
      "Training Loss: 0.27047424983471374\n",
      "Validation Loss: 0.194895501506192\n",
      "Epoch 17/100\n",
      "Training Loss: 0.26631492592931455\n",
      "Validation Loss: 0.1610708082257991\n",
      "Epoch 18/100\n",
      "Training Loss: 0.2818280770323959\n",
      "Validation Loss: 0.20649652188391188\n",
      "Epoch 19/100\n",
      "Training Loss: 0.24743470996409112\n",
      "Validation Loss: 0.14303924280078995\n",
      "Epoch 20/100\n",
      "Training Loss: 0.23780821056398857\n",
      "Validation Loss: 0.15956798972340702\n",
      "Epoch 21/100\n",
      "Training Loss: 0.19968898612192484\n",
      "Validation Loss: 0.1450846642788707\n",
      "Epoch 22/100\n",
      "Training Loss: 0.24403244538985536\n",
      "Validation Loss: 0.16374474500505892\n",
      "Epoch 23/100\n",
      "Training Loss: 0.2565906744445752\n",
      "Validation Loss: 0.23223942100625022\n",
      "Epoch 24/100\n",
      "Training Loss: 0.19074646542809734\n",
      "Validation Loss: 0.2848432020269752\n",
      "Epoch 25/100\n",
      "Training Loss: 0.19320184564171194\n",
      "Validation Loss: 0.14174818469151423\n",
      "Epoch 26/100\n",
      "Training Loss: 0.2820617372951301\n",
      "Validation Loss: 0.14615949094155922\n",
      "Epoch 27/100\n",
      "Training Loss: 0.2278801629804376\n",
      "Validation Loss: 0.20649992799453334\n",
      "Epoch 28/100\n",
      "Training Loss: 0.22276848709088215\n",
      "Validation Loss: 0.14052032905374262\n",
      "Epoch 29/100\n",
      "Training Loss: 0.1908770074974075\n",
      "Validation Loss: 0.26914426175393813\n",
      "Epoch 30/100\n",
      "Training Loss: 0.21217071358153397\n",
      "Validation Loss: 0.20045318562694697\n",
      "Epoch 31/100\n",
      "Training Loss: 0.21504979722431763\n",
      "Validation Loss: 0.13723917975900693\n",
      "Epoch 32/100\n",
      "Training Loss: 0.17892925835074347\n",
      "Validation Loss: 0.1277001972707483\n",
      "Epoch 33/100\n",
      "Training Loss: 0.17757506155964423\n",
      "Validation Loss: 0.21670678141032163\n",
      "Epoch 34/100\n",
      "Training Loss: 0.23337039071900767\n",
      "Validation Loss: 0.14154535235691823\n",
      "Epoch 35/100\n",
      "Training Loss: 0.22224847990933652\n",
      "Validation Loss: 0.17620045142656665\n",
      "Epoch 36/100\n",
      "Training Loss: 0.19852216418455462\n",
      "Validation Loss: 0.1563725025617245\n",
      "Epoch 37/100\n",
      "Training Loss: 0.2146396076336\n",
      "Validation Loss: 0.12475062288347441\n",
      "Epoch 38/100\n",
      "Training Loss: 0.20188988948948997\n",
      "Validation Loss: 0.35077254456009405\n",
      "Epoch 39/100\n",
      "Training Loss: 0.22993809049497124\n",
      "Validation Loss: 0.17068361165868084\n",
      "Epoch 40/100\n",
      "Training Loss: 0.17230879448218706\n",
      "Validation Loss: 0.17744275107591614\n",
      "Epoch 41/100\n",
      "Training Loss: 0.19379058305686137\n",
      "Validation Loss: 0.19118517321674594\n",
      "Epoch 42/100\n",
      "Training Loss: 0.18619768227038927\n",
      "Validation Loss: 0.15668383315485662\n",
      "Epoch 43/100\n",
      "Training Loss: 0.18117820442118204\n",
      "Validation Loss: 0.1191321845425789\n",
      "Epoch 44/100\n",
      "Training Loss: 0.19427304996759706\n",
      "Validation Loss: 0.10310496105178538\n",
      "Epoch 45/100\n",
      "Training Loss: 0.16310259256098267\n",
      "Validation Loss: 0.23458705441776342\n",
      "Epoch 46/100\n",
      "Training Loss: 0.1811587054347344\n",
      "Validation Loss: 0.1343629901248694\n",
      "Epoch 47/100\n",
      "Training Loss: 0.18294625404111461\n",
      "Validation Loss: 0.31304944785456884\n",
      "Epoch 48/100\n",
      "Training Loss: 0.18684036697433823\n",
      "Validation Loss: 0.14041501133389508\n",
      "Epoch 49/100\n",
      "Training Loss: 0.18237390020463926\n",
      "Validation Loss: 0.13780553672944576\n",
      "Epoch 50/100\n",
      "Training Loss: 0.16204596150964828\n",
      "Validation Loss: 0.1962117462035166\n",
      "Epoch 51/100\n",
      "Training Loss: 0.18580383010155188\n",
      "Validation Loss: 0.13181505482345793\n",
      "Epoch 52/100\n",
      "Training Loss: 0.19869049115388307\n",
      "Validation Loss: 0.10870658841656027\n",
      "Epoch 53/100\n",
      "Training Loss: 0.16876522812011652\n",
      "Validation Loss: 0.08256913060921164\n",
      "Epoch 54/100\n",
      "Training Loss: 0.19282731295877087\n",
      "Validation Loss: 0.09658064774262173\n",
      "Epoch 55/100\n",
      "Training Loss: 0.16373149875244605\n",
      "Validation Loss: 0.15649188214058984\n",
      "Epoch 56/100\n",
      "Training Loss: 0.21113964466524132\n",
      "Validation Loss: 0.11115233652141292\n",
      "Epoch 57/100\n",
      "Training Loss: 0.18324427541487234\n",
      "Validation Loss: 0.12462948938836524\n",
      "Epoch 58/100\n",
      "Training Loss: 0.16714393489047683\n",
      "Validation Loss: 0.14013875871797743\n",
      "Epoch 59/100\n",
      "Training Loss: 0.15773870708168602\n",
      "Validation Loss: 0.09199328853207663\n",
      "Epoch 60/100\n",
      "Training Loss: 0.15041845793718686\n",
      "Validation Loss: 0.19975548763057752\n",
      "Epoch 61/100\n",
      "Training Loss: 0.14474013301062777\n",
      "Validation Loss: 0.13018809810748555\n",
      "Epoch 62/100\n",
      "Training Loss: 0.16100384851832528\n",
      "Validation Loss: 0.09114942656688327\n",
      "Epoch 63/100\n",
      "Training Loss: 0.172735878754993\n",
      "Validation Loss: 0.15082576456505714\n",
      "Epoch 64/100\n",
      "Training Loss: 0.19360260491544698\n",
      "Validation Loss: 0.1003399603055131\n",
      "Epoch 65/100\n",
      "Training Loss: 0.16016928296161898\n",
      "Validation Loss: 0.17666439642136036\n",
      "Epoch 66/100\n",
      "Training Loss: 0.1370562567959659\n",
      "Validation Loss: 0.0957392335719465\n",
      "Epoch 67/100\n",
      "Training Loss: 0.16386920943179473\n",
      "Validation Loss: 0.13082268569848446\n",
      "Epoch 68/100\n",
      "Training Loss: 0.1384267580372161\n",
      "Validation Loss: 0.08267592510420975\n",
      "Epoch 69/100\n",
      "Training Loss: 0.16357513318758804\n",
      "Validation Loss: 0.12087647677078923\n",
      "Epoch 70/100\n",
      "Training Loss: 0.14033892434053524\n",
      "Validation Loss: 0.14782327646986443\n",
      "Epoch 71/100\n",
      "Training Loss: 0.17808850584444338\n",
      "Validation Loss: 0.07930155056404384\n",
      "Epoch 72/100\n",
      "Training Loss: 0.15998515703620583\n",
      "Validation Loss: 0.12013109912898072\n",
      "Epoch 73/100\n",
      "Training Loss: 0.19840107114295427\n",
      "Validation Loss: 0.08048739439743083\n",
      "Epoch 74/100\n",
      "Training Loss: 0.1573560966349187\n",
      "Validation Loss: 0.16139497477355863\n",
      "Epoch 75/100\n",
      "Training Loss: 0.14334606337725372\n",
      "Validation Loss: 0.07436890442358955\n",
      "Epoch 76/100\n",
      "Training Loss: 0.1405193717658079\n",
      "Validation Loss: 0.1082417325802918\n",
      "Epoch 77/100\n",
      "Training Loss: 0.14914455214021732\n",
      "Validation Loss: 0.14479344596941712\n",
      "Epoch 78/100\n",
      "Training Loss: 0.14881930329598544\n",
      "Validation Loss: 0.08927925154342924\n",
      "Epoch 79/100\n",
      "Training Loss: 0.14672331316692255\n",
      "Validation Loss: 0.11300327533233041\n",
      "Epoch 80/100\n",
      "Training Loss: 0.15048009749607322\n",
      "Validation Loss: 0.07991291097657055\n",
      "Epoch 81/100\n",
      "Training Loss: 0.15005692170633533\n",
      "Validation Loss: 0.10597803720723627\n",
      "Epoch 82/100\n",
      "Training Loss: 0.15351173173314073\n",
      "Validation Loss: 0.13447767228439816\n",
      "Epoch 83/100\n",
      "Training Loss: 0.1237409584444893\n",
      "Validation Loss: 0.08745108683605682\n",
      "Epoch 84/100\n",
      "Training Loss: 0.1579256987684037\n",
      "Validation Loss: 0.12468142838429197\n",
      "Epoch 85/100\n",
      "Training Loss: 0.15480228257215936\n",
      "Validation Loss: 0.14498599528100717\n",
      "Epoch 86/100\n",
      "Training Loss: 0.15190569732790626\n",
      "Validation Loss: 0.09670027978284776\n",
      "Epoch 87/100\n",
      "Training Loss: 0.13290603735622916\n",
      "Validation Loss: 0.10688853521828105\n",
      "Epoch 88/100\n",
      "Training Loss: 0.16050250406402916\n",
      "Validation Loss: 0.0816688686647973\n",
      "Epoch 89/100\n",
      "Training Loss: 0.14865562750837913\n",
      "Validation Loss: 0.08196562683970121\n",
      "Epoch 90/100\n",
      "Training Loss: 0.13781963084686852\n",
      "Validation Loss: 0.09215857739815729\n",
      "Epoch 91/100\n",
      "Training Loss: 0.1621988881182476\n",
      "Validation Loss: 0.0942443654932433\n",
      "Epoch 92/100\n",
      "Training Loss: 0.1575999335736138\n",
      "Validation Loss: 0.10263982307638968\n",
      "Epoch 93/100\n",
      "Training Loss: 0.17146321174714893\n",
      "Validation Loss: 0.10311028913459516\n",
      "Epoch 94/100\n",
      "Training Loss: 0.13936798498669675\n",
      "Validation Loss: 0.06357587867185491\n",
      "Epoch 95/100\n",
      "Training Loss: 0.13313838805497064\n",
      "Validation Loss: 0.12746245480780022\n",
      "Epoch 96/100\n",
      "Training Loss: 0.14106757693019523\n",
      "Validation Loss: 0.11939405262749689\n",
      "Epoch 97/100\n",
      "Training Loss: 0.14280488674683162\n",
      "Validation Loss: 0.07567885017349887\n",
      "Epoch 98/100\n",
      "Training Loss: 0.15283451937994227\n",
      "Validation Loss: 0.1170819963991789\n",
      "Epoch 99/100\n",
      "Training Loss: 0.14050636786670903\n",
      "Validation Loss: 0.1259763275824306\n",
      "Epoch 100/100\n",
      "Training Loss: 0.15410596111574618\n",
      "Validation Loss: 0.0581852052283189\n",
      "Combination 37: Avg Training Loss = 0.2109270518371047, Avg Validation Loss = 0.16947763043767558\n",
      "Testing combination 38/48: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0001, 'sequence_length': 6, 'dropout_rate': 0.2}\n",
      "Truncated Train Data Shape: (70, 6, 1)\n",
      "Truncated Validation Data Shape: (10, 6, 1)\n",
      "    Trial 1/2 for combination 38/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.4375462131929021\n",
      "Validation Loss: 0.4140454969697682\n",
      "Epoch 2/100\n",
      "Training Loss: 0.46323940728260776\n",
      "Validation Loss: 0.6364341218596097\n",
      "Epoch 3/100\n",
      "Training Loss: 0.4381424101213635\n",
      "Validation Loss: 0.30006503631669473\n",
      "Epoch 4/100\n",
      "Training Loss: 0.3618306983000633\n",
      "Validation Loss: 0.3057577482212237\n",
      "Epoch 5/100\n",
      "Training Loss: 0.31769958840327783\n",
      "Validation Loss: 0.2772528085032494\n",
      "Epoch 6/100\n",
      "Training Loss: 0.29472956145770196\n",
      "Validation Loss: 0.16783839120838695\n",
      "Epoch 7/100\n",
      "Training Loss: 0.32902653539360105\n",
      "Validation Loss: 0.4682509434219293\n",
      "Epoch 8/100\n",
      "Training Loss: 0.2645530611857251\n",
      "Validation Loss: 0.33112059428950386\n",
      "Epoch 9/100\n",
      "Training Loss: 0.31550568012855884\n",
      "Validation Loss: 0.18924218330542258\n",
      "Epoch 10/100\n",
      "Training Loss: 0.3396777834292237\n",
      "Validation Loss: 0.2391021146146397\n",
      "Epoch 11/100\n",
      "Training Loss: 0.2642604733701356\n",
      "Validation Loss: 0.169308008125382\n",
      "Epoch 12/100\n",
      "Training Loss: 0.2755370236027674\n",
      "Validation Loss: 0.17930435690506025\n",
      "Epoch 13/100\n",
      "Training Loss: 0.23436409694433447\n",
      "Validation Loss: 0.14154438346890907\n",
      "Epoch 14/100\n",
      "Training Loss: 0.23954079786348334\n",
      "Validation Loss: 0.24826552560669865\n",
      "Epoch 15/100\n",
      "Training Loss: 0.2289873288221294\n",
      "Validation Loss: 0.2936204755894616\n",
      "Epoch 16/100\n",
      "Training Loss: 0.2202133363273628\n",
      "Validation Loss: 0.2405321044972034\n",
      "Epoch 17/100\n",
      "Training Loss: 0.24260897907597867\n",
      "Validation Loss: 0.2362051706436313\n",
      "Epoch 18/100\n",
      "Training Loss: 0.23114052258567433\n",
      "Validation Loss: 0.2108777291863609\n",
      "Epoch 19/100\n",
      "Training Loss: 0.22245209791482545\n",
      "Validation Loss: 0.21501385875746495\n",
      "Epoch 20/100\n",
      "Training Loss: 0.20713477786220377\n",
      "Validation Loss: 0.2615394189036042\n",
      "Epoch 21/100\n",
      "Training Loss: 0.1642581893628874\n",
      "Validation Loss: 0.28464012284869383\n",
      "Epoch 22/100\n",
      "Training Loss: 0.18508707415108114\n",
      "Validation Loss: 0.11714438466593906\n",
      "Epoch 23/100\n",
      "Training Loss: 0.24208034189730457\n",
      "Validation Loss: 0.14381755460752027\n",
      "Epoch 24/100\n",
      "Training Loss: 0.2076226687531389\n",
      "Validation Loss: 0.16474317785730352\n",
      "Epoch 25/100\n",
      "Training Loss: 0.202382896029122\n",
      "Validation Loss: 0.23650252018605628\n",
      "Epoch 26/100\n",
      "Training Loss: 0.21669531510850487\n",
      "Validation Loss: 0.27515729916743725\n",
      "Epoch 27/100\n",
      "Training Loss: 0.19265177440788545\n",
      "Validation Loss: 0.09491424819377117\n",
      "Epoch 28/100\n",
      "Training Loss: 0.1897258879715634\n",
      "Validation Loss: 0.10925662365626188\n",
      "Epoch 29/100\n",
      "Training Loss: 0.21297606751000026\n",
      "Validation Loss: 0.16072535626050033\n",
      "Epoch 30/100\n",
      "Training Loss: 0.1761420594795981\n",
      "Validation Loss: 0.12115138545812787\n",
      "Epoch 31/100\n",
      "Training Loss: 0.1816076670597779\n",
      "Validation Loss: 0.12241832238533024\n",
      "Epoch 32/100\n",
      "Training Loss: 0.19996085493669677\n",
      "Validation Loss: 0.18534369477175155\n",
      "Epoch 33/100\n",
      "Training Loss: 0.16601618061543938\n",
      "Validation Loss: 0.14893762985063766\n",
      "Epoch 34/100\n",
      "Training Loss: 0.21370382078175817\n",
      "Validation Loss: 0.18252727004723854\n",
      "Epoch 35/100\n",
      "Training Loss: 0.17137071985867328\n",
      "Validation Loss: 0.10285513244831543\n",
      "Epoch 36/100\n",
      "Training Loss: 0.18944720960169656\n",
      "Validation Loss: 0.1452491308210515\n",
      "Epoch 37/100\n",
      "Training Loss: 0.14616192105443618\n",
      "Validation Loss: 0.22653490132579565\n",
      "Epoch 38/100\n",
      "Training Loss: 0.19141301891142365\n",
      "Validation Loss: 0.15797863754680982\n",
      "Epoch 39/100\n",
      "Training Loss: 0.17816222212377125\n",
      "Validation Loss: 0.17173212300275348\n",
      "Epoch 40/100\n",
      "Training Loss: 0.213905561813196\n",
      "Validation Loss: 0.18766651275060361\n",
      "Epoch 41/100\n",
      "Training Loss: 0.18731602821550294\n",
      "Validation Loss: 0.14128294814911063\n",
      "Epoch 42/100\n",
      "Training Loss: 0.16381016373223872\n",
      "Validation Loss: 0.168154460309554\n",
      "Epoch 43/100\n",
      "Training Loss: 0.18138901092764392\n",
      "Validation Loss: 0.22242447084446124\n",
      "Epoch 44/100\n",
      "Training Loss: 0.17627082889782114\n",
      "Validation Loss: 0.2664784184319569\n",
      "Epoch 45/100\n",
      "Training Loss: 0.18040064524462335\n",
      "Validation Loss: 0.19382152323343643\n",
      "Epoch 46/100\n",
      "Training Loss: 0.1532202531372196\n",
      "Validation Loss: 0.13424826913103108\n",
      "Epoch 47/100\n",
      "Training Loss: 0.14585310848520314\n",
      "Validation Loss: 0.15224446820104395\n",
      "Epoch 48/100\n",
      "Training Loss: 0.16793411533544103\n",
      "Validation Loss: 0.1276488826924083\n",
      "Epoch 49/100\n",
      "Training Loss: 0.19484909520533758\n",
      "Validation Loss: 0.18504678990226697\n",
      "Epoch 50/100\n",
      "Training Loss: 0.14866767904362282\n",
      "Validation Loss: 0.21164680905194894\n",
      "Epoch 51/100\n",
      "Training Loss: 0.1598105312579358\n",
      "Validation Loss: 0.23848656030288629\n",
      "Epoch 52/100\n",
      "Training Loss: 0.1630505357047593\n",
      "Validation Loss: 0.14583119082859006\n",
      "Epoch 53/100\n",
      "Training Loss: 0.14511706747018097\n",
      "Validation Loss: 0.23400078008086647\n",
      "Epoch 54/100\n",
      "Training Loss: 0.1632724040290708\n",
      "Validation Loss: 0.11077682704942844\n",
      "Epoch 55/100\n",
      "Training Loss: 0.15205531841385583\n",
      "Validation Loss: 0.18335230655378146\n",
      "Epoch 56/100\n",
      "Training Loss: 0.14365703221449996\n",
      "Validation Loss: 0.12631620137630964\n",
      "Epoch 57/100\n",
      "Training Loss: 0.15352832947714404\n",
      "Validation Loss: 0.14320870379918677\n",
      "Epoch 58/100\n",
      "Training Loss: 0.16496757188020594\n",
      "Validation Loss: 0.09869485077654401\n",
      "Epoch 59/100\n",
      "Training Loss: 0.13646622348024615\n",
      "Validation Loss: 0.13370557679887712\n",
      "Epoch 60/100\n",
      "Training Loss: 0.17179203887487174\n",
      "Validation Loss: 0.1508507596743056\n",
      "Epoch 61/100\n",
      "Training Loss: 0.16893166574667096\n",
      "Validation Loss: 0.09211743287180049\n",
      "Epoch 62/100\n",
      "Training Loss: 0.16006249783953355\n",
      "Validation Loss: 0.11191935593019267\n",
      "Epoch 63/100\n",
      "Training Loss: 0.1775304468291664\n",
      "Validation Loss: 0.11105373827023177\n",
      "Epoch 64/100\n",
      "Training Loss: 0.154006246887414\n",
      "Validation Loss: 0.11081917138443036\n",
      "Epoch 65/100\n",
      "Training Loss: 0.1554456489513823\n",
      "Validation Loss: 0.13843515458302597\n",
      "Epoch 66/100\n",
      "Training Loss: 0.1612339191132981\n",
      "Validation Loss: 0.09822160766210511\n",
      "Epoch 67/100\n",
      "Training Loss: 0.14704104507378699\n",
      "Validation Loss: 0.11531595747534529\n",
      "Epoch 68/100\n",
      "Training Loss: 0.13296651646098084\n",
      "Validation Loss: 0.09827844628227127\n",
      "Epoch 69/100\n",
      "Training Loss: 0.13593651992901107\n",
      "Validation Loss: 0.12140726378099702\n",
      "Epoch 70/100\n",
      "Training Loss: 0.1495513152084175\n",
      "Validation Loss: 0.17352783496881952\n",
      "Epoch 71/100\n",
      "Training Loss: 0.1563992368850259\n",
      "Validation Loss: 0.13735249651835185\n",
      "Epoch 72/100\n",
      "Training Loss: 0.14652060475011408\n",
      "Validation Loss: 0.09204819632396403\n",
      "Epoch 73/100\n",
      "Training Loss: 0.14069341158950363\n",
      "Validation Loss: 0.11915415527717339\n",
      "Epoch 74/100\n",
      "Training Loss: 0.14785786909453144\n",
      "Validation Loss: 0.14397780381714523\n",
      "Epoch 75/100\n",
      "Training Loss: 0.13226178825363635\n",
      "Validation Loss: 0.19144504717867145\n",
      "Epoch 76/100\n",
      "Training Loss: 0.13486651951836182\n",
      "Validation Loss: 0.0787887749703136\n",
      "Epoch 77/100\n",
      "Training Loss: 0.127923937006361\n",
      "Validation Loss: 0.13465308616147428\n",
      "Epoch 78/100\n",
      "Training Loss: 0.14084193061249492\n",
      "Validation Loss: 0.10555305916346683\n",
      "Epoch 79/100\n",
      "Training Loss: 0.14246212744632267\n",
      "Validation Loss: 0.12833554817301396\n",
      "Epoch 80/100\n",
      "Training Loss: 0.12978546034096766\n",
      "Validation Loss: 0.08784945085923986\n",
      "Epoch 81/100\n",
      "Training Loss: 0.16393937843834408\n",
      "Validation Loss: 0.08184040976262717\n",
      "Epoch 82/100\n",
      "Training Loss: 0.13677942972601295\n",
      "Validation Loss: 0.09239121961346132\n",
      "Epoch 83/100\n",
      "Training Loss: 0.12995830154690707\n",
      "Validation Loss: 0.12071952118882727\n",
      "Epoch 84/100\n",
      "Training Loss: 0.13425890720216596\n",
      "Validation Loss: 0.14667647864815747\n",
      "Epoch 85/100\n",
      "Training Loss: 0.14177019204095043\n",
      "Validation Loss: 0.0917416298251649\n",
      "Epoch 86/100\n",
      "Training Loss: 0.13172367844637461\n",
      "Validation Loss: 0.08865359312222379\n",
      "Epoch 87/100\n",
      "Training Loss: 0.11661995148234455\n",
      "Validation Loss: 0.08601790068800383\n",
      "Epoch 88/100\n",
      "Training Loss: 0.12948019948315054\n",
      "Validation Loss: 0.08926840911106083\n",
      "Epoch 89/100\n",
      "Training Loss: 0.14410392136662073\n",
      "Validation Loss: 0.08058497862072914\n",
      "Epoch 90/100\n",
      "Training Loss: 0.14975505544994228\n",
      "Validation Loss: 0.09212783004805121\n",
      "Epoch 91/100\n",
      "Training Loss: 0.13968053170367128\n",
      "Validation Loss: 0.12231368465423176\n",
      "Epoch 92/100\n",
      "Training Loss: 0.1381364051198594\n",
      "Validation Loss: 0.10937594299586344\n",
      "Epoch 93/100\n",
      "Training Loss: 0.13972460118976834\n",
      "Validation Loss: 0.0724074715476863\n",
      "Epoch 94/100\n",
      "Training Loss: 0.12503960203300518\n",
      "Validation Loss: 0.08312733325392642\n",
      "Epoch 95/100\n",
      "Training Loss: 0.13353477516666948\n",
      "Validation Loss: 0.09460930185542538\n",
      "Epoch 96/100\n",
      "Training Loss: 0.1614555493658807\n",
      "Validation Loss: 0.08104076894188053\n",
      "Epoch 97/100\n",
      "Training Loss: 0.1157162814005953\n",
      "Validation Loss: 0.0739864247813062\n",
      "Epoch 98/100\n",
      "Training Loss: 0.12286423278175572\n",
      "Validation Loss: 0.11693228283799813\n",
      "Epoch 99/100\n",
      "Training Loss: 0.11416963816508349\n",
      "Validation Loss: 0.0893462636218789\n",
      "Epoch 100/100\n",
      "Training Loss: 0.1528615077509595\n",
      "Validation Loss: 0.12910869927060417\n",
      "    Trial 2/2 for combination 38/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.5739683679491999\n",
      "Validation Loss: 0.5072520809546488\n",
      "Epoch 2/100\n",
      "Training Loss: 0.41133828605576234\n",
      "Validation Loss: 0.37458826183870875\n",
      "Epoch 3/100\n",
      "Training Loss: 0.498523530227239\n",
      "Validation Loss: 0.5516561128882493\n",
      "Epoch 4/100\n",
      "Training Loss: 0.3654852541236408\n",
      "Validation Loss: 0.20211094647513045\n",
      "Epoch 5/100\n",
      "Training Loss: 0.3463889263400556\n",
      "Validation Loss: 0.23282451567679\n",
      "Epoch 6/100\n",
      "Training Loss: 0.36443997059887295\n",
      "Validation Loss: 0.29520587119786224\n",
      "Epoch 7/100\n",
      "Training Loss: 0.3247984714536403\n",
      "Validation Loss: 0.13567283424947515\n",
      "Epoch 8/100\n",
      "Training Loss: 0.32460668915052765\n",
      "Validation Loss: 0.2927842628502079\n",
      "Epoch 9/100\n",
      "Training Loss: 0.2601372220518004\n",
      "Validation Loss: 0.34680862704983\n",
      "Epoch 10/100\n",
      "Training Loss: 0.27762230684272815\n",
      "Validation Loss: 0.2675439382123647\n",
      "Epoch 11/100\n",
      "Training Loss: 0.27167130952111346\n",
      "Validation Loss: 0.31412605403642113\n",
      "Epoch 12/100\n",
      "Training Loss: 0.21534249706198438\n",
      "Validation Loss: 0.1804031353815734\n",
      "Epoch 13/100\n",
      "Training Loss: 0.2690189313463238\n",
      "Validation Loss: 0.2702568943948006\n",
      "Epoch 14/100\n",
      "Training Loss: 0.28974333246244854\n",
      "Validation Loss: 0.25803308574839046\n",
      "Epoch 15/100\n",
      "Training Loss: 0.27434374723128885\n",
      "Validation Loss: 0.1318160234798369\n",
      "Epoch 16/100\n",
      "Training Loss: 0.2726272269736893\n",
      "Validation Loss: 0.22947595440270438\n",
      "Epoch 17/100\n",
      "Training Loss: 0.20705081024366964\n",
      "Validation Loss: 0.15744729870637172\n",
      "Epoch 18/100\n",
      "Training Loss: 0.21337609477445993\n",
      "Validation Loss: 0.2027789628311134\n",
      "Epoch 19/100\n",
      "Training Loss: 0.22118143043992972\n",
      "Validation Loss: 0.23334560439333885\n",
      "Epoch 20/100\n",
      "Training Loss: 0.20295420229187178\n",
      "Validation Loss: 0.248165138731027\n",
      "Epoch 21/100\n",
      "Training Loss: 0.24420723155752663\n",
      "Validation Loss: 0.2730607798804801\n",
      "Epoch 22/100\n",
      "Training Loss: 0.24082399796236997\n",
      "Validation Loss: 0.12182983017790908\n",
      "Epoch 23/100\n",
      "Training Loss: 0.23485399179376287\n",
      "Validation Loss: 0.19017938983187435\n",
      "Epoch 24/100\n",
      "Training Loss: 0.1954541555252043\n",
      "Validation Loss: 0.1468145153492726\n",
      "Epoch 25/100\n",
      "Training Loss: 0.1813385295924428\n",
      "Validation Loss: 0.1356396540256946\n",
      "Epoch 26/100\n",
      "Training Loss: 0.17960802825023936\n",
      "Validation Loss: 0.13494846065508687\n",
      "Epoch 27/100\n",
      "Training Loss: 0.20713375527246708\n",
      "Validation Loss: 0.1958420823068871\n",
      "Epoch 28/100\n",
      "Training Loss: 0.20793932316462105\n",
      "Validation Loss: 0.23761535006695653\n",
      "Epoch 29/100\n",
      "Training Loss: 0.18275952402083107\n",
      "Validation Loss: 0.22081455986621332\n",
      "Epoch 30/100\n",
      "Training Loss: 0.19090723585036018\n",
      "Validation Loss: 0.15744905385877986\n",
      "Epoch 31/100\n",
      "Training Loss: 0.2184304360161577\n",
      "Validation Loss: 0.1961167582276424\n",
      "Epoch 32/100\n",
      "Training Loss: 0.19863864297637696\n",
      "Validation Loss: 0.167635711034155\n",
      "Epoch 33/100\n",
      "Training Loss: 0.18944351874053036\n",
      "Validation Loss: 0.24543315254882664\n",
      "Epoch 34/100\n",
      "Training Loss: 0.19723435937867806\n",
      "Validation Loss: 0.13517519656168725\n",
      "Epoch 35/100\n",
      "Training Loss: 0.17280719732640484\n",
      "Validation Loss: 0.1770133229575826\n",
      "Epoch 36/100\n",
      "Training Loss: 0.18765178688244713\n",
      "Validation Loss: 0.1311396822174778\n",
      "Epoch 37/100\n",
      "Training Loss: 0.21479911335931004\n",
      "Validation Loss: 0.18170620760232187\n",
      "Epoch 38/100\n",
      "Training Loss: 0.18122467308855536\n",
      "Validation Loss: 0.1863438818714848\n",
      "Epoch 39/100\n",
      "Training Loss: 0.18253998908221672\n",
      "Validation Loss: 0.16893600846297144\n",
      "Epoch 40/100\n",
      "Training Loss: 0.207668535156636\n",
      "Validation Loss: 0.16039417825618354\n",
      "Epoch 41/100\n",
      "Training Loss: 0.1598341572000629\n",
      "Validation Loss: 0.13139176390199686\n",
      "Epoch 42/100\n",
      "Training Loss: 0.2015067291920869\n",
      "Validation Loss: 0.1518780656958726\n",
      "Epoch 43/100\n",
      "Training Loss: 0.17812609399937426\n",
      "Validation Loss: 0.22536583487157613\n",
      "Epoch 44/100\n",
      "Training Loss: 0.18787742811594815\n",
      "Validation Loss: 0.24232439531127575\n",
      "Epoch 45/100\n",
      "Training Loss: 0.1926877275486686\n",
      "Validation Loss: 0.13946960544137388\n",
      "Epoch 46/100\n",
      "Training Loss: 0.18836965258646807\n",
      "Validation Loss: 0.10139954738896388\n",
      "Epoch 47/100\n",
      "Training Loss: 0.18518644010996255\n",
      "Validation Loss: 0.1586829220541337\n",
      "Epoch 48/100\n",
      "Training Loss: 0.1650667494774192\n",
      "Validation Loss: 0.1162587357830411\n",
      "Epoch 49/100\n",
      "Training Loss: 0.1744569002951591\n",
      "Validation Loss: 0.2934699172792195\n",
      "Epoch 50/100\n",
      "Training Loss: 0.17447491249008637\n",
      "Validation Loss: 0.12746379370327643\n",
      "Epoch 51/100\n",
      "Training Loss: 0.20357511760807143\n",
      "Validation Loss: 0.16944911309889857\n",
      "Epoch 52/100\n",
      "Training Loss: 0.18115723536694348\n",
      "Validation Loss: 0.18021981399342182\n",
      "Epoch 53/100\n",
      "Training Loss: 0.1663025823995956\n",
      "Validation Loss: 0.14186047057851686\n",
      "Epoch 54/100\n",
      "Training Loss: 0.1493081331227939\n",
      "Validation Loss: 0.18130049226484032\n",
      "Epoch 55/100\n",
      "Training Loss: 0.17515860311190692\n",
      "Validation Loss: 0.12762443558413566\n",
      "Epoch 56/100\n",
      "Training Loss: 0.16218703898198045\n",
      "Validation Loss: 0.18506255118713955\n",
      "Epoch 57/100\n",
      "Training Loss: 0.1374469422173935\n",
      "Validation Loss: 0.13641304678546953\n",
      "Epoch 58/100\n",
      "Training Loss: 0.15103220159465838\n",
      "Validation Loss: 0.20734325055375233\n",
      "Epoch 59/100\n",
      "Training Loss: 0.16638342682312657\n",
      "Validation Loss: 0.18512786093590813\n",
      "Epoch 60/100\n",
      "Training Loss: 0.16299487419917885\n",
      "Validation Loss: 0.14141870488410463\n",
      "Epoch 61/100\n",
      "Training Loss: 0.15219876017235243\n",
      "Validation Loss: 0.16761305825408088\n",
      "Epoch 62/100\n",
      "Training Loss: 0.14064696247418232\n",
      "Validation Loss: 0.1186988206581315\n",
      "Epoch 63/100\n",
      "Training Loss: 0.1497673575428351\n",
      "Validation Loss: 0.11124095793031659\n",
      "Epoch 64/100\n",
      "Training Loss: 0.14947882465589155\n",
      "Validation Loss: 0.11314618073110379\n",
      "Epoch 65/100\n",
      "Training Loss: 0.14694541462613758\n",
      "Validation Loss: 0.12636383633636614\n",
      "Epoch 66/100\n",
      "Training Loss: 0.14561662922764812\n",
      "Validation Loss: 0.10773697218888659\n",
      "Epoch 67/100\n",
      "Training Loss: 0.14256621228964383\n",
      "Validation Loss: 0.20238463439050552\n",
      "Epoch 68/100\n",
      "Training Loss: 0.14786266484088315\n",
      "Validation Loss: 0.08435068676562477\n",
      "Epoch 69/100\n",
      "Training Loss: 0.1682392324487982\n",
      "Validation Loss: 0.12274705241537673\n",
      "Epoch 70/100\n",
      "Training Loss: 0.17540339357065604\n",
      "Validation Loss: 0.1445128582359804\n",
      "Epoch 71/100\n",
      "Training Loss: 0.15799652504356793\n",
      "Validation Loss: 0.19286968045941538\n",
      "Epoch 72/100\n",
      "Training Loss: 0.13193300006079187\n",
      "Validation Loss: 0.12304580420370419\n",
      "Epoch 73/100\n",
      "Training Loss: 0.15314500325257988\n",
      "Validation Loss: 0.12535779317927717\n",
      "Epoch 74/100\n",
      "Training Loss: 0.13850004661163526\n",
      "Validation Loss: 0.06704365158826187\n",
      "Epoch 75/100\n",
      "Training Loss: 0.12977841383063246\n",
      "Validation Loss: 0.18577420207058187\n",
      "Epoch 76/100\n",
      "Training Loss: 0.13841637901946716\n",
      "Validation Loss: 0.16755420651296654\n",
      "Epoch 77/100\n",
      "Training Loss: 0.14848300206994502\n",
      "Validation Loss: 0.11890569965804951\n",
      "Epoch 78/100\n",
      "Training Loss: 0.1569397404959794\n",
      "Validation Loss: 0.10707884276054358\n",
      "Epoch 79/100\n",
      "Training Loss: 0.15901583127516128\n",
      "Validation Loss: 0.09089184325083488\n",
      "Epoch 80/100\n",
      "Training Loss: 0.17592461995400963\n",
      "Validation Loss: 0.20072069753137942\n",
      "Epoch 81/100\n",
      "Training Loss: 0.13201005341351327\n",
      "Validation Loss: 0.10375025174882717\n",
      "Epoch 82/100\n",
      "Training Loss: 0.13626803366361634\n",
      "Validation Loss: 0.07129176093697331\n",
      "Epoch 83/100\n",
      "Training Loss: 0.14993336162091095\n",
      "Validation Loss: 0.19191015947460346\n",
      "Epoch 84/100\n",
      "Training Loss: 0.13649704020206\n",
      "Validation Loss: 0.09458372752180019\n",
      "Epoch 85/100\n",
      "Training Loss: 0.1387368166203191\n",
      "Validation Loss: 0.10648896840680404\n",
      "Epoch 86/100\n",
      "Training Loss: 0.1354252616501769\n",
      "Validation Loss: 0.10803105982977752\n",
      "Epoch 87/100\n",
      "Training Loss: 0.12834921480206898\n",
      "Validation Loss: 0.12970807471221144\n",
      "Epoch 88/100\n",
      "Training Loss: 0.12543355161611794\n",
      "Validation Loss: 0.12380176809704477\n",
      "Epoch 89/100\n",
      "Training Loss: 0.12297871561780731\n",
      "Validation Loss: 0.11912680979611738\n",
      "Epoch 90/100\n",
      "Training Loss: 0.1506140622210679\n",
      "Validation Loss: 0.1018748642711846\n",
      "Epoch 91/100\n",
      "Training Loss: 0.1347096455098914\n",
      "Validation Loss: 0.11512128359167331\n",
      "Epoch 92/100\n",
      "Training Loss: 0.1234225778251237\n",
      "Validation Loss: 0.1247416855813229\n",
      "Epoch 93/100\n",
      "Training Loss: 0.13606453413274933\n",
      "Validation Loss: 0.06594880928722531\n",
      "Epoch 94/100\n",
      "Training Loss: 0.16373511219990533\n",
      "Validation Loss: 0.07970581955658432\n",
      "Epoch 95/100\n",
      "Training Loss: 0.13636943027607423\n",
      "Validation Loss: 0.08120317446379194\n",
      "Epoch 96/100\n",
      "Training Loss: 0.1526195325013631\n",
      "Validation Loss: 0.08142345419874553\n",
      "Epoch 97/100\n",
      "Training Loss: 0.12196233677127695\n",
      "Validation Loss: 0.07877802527440828\n",
      "Epoch 98/100\n",
      "Training Loss: 0.13533513078486234\n",
      "Validation Loss: 0.12682061697885944\n",
      "Epoch 99/100\n",
      "Training Loss: 0.12618425366899427\n",
      "Validation Loss: 0.10267946467420463\n",
      "Epoch 100/100\n",
      "Training Loss: 0.13189023735471672\n",
      "Validation Loss: 0.08787674380924994\n",
      "Combination 38: Avg Training Loss = 0.19038746573805015, Avg Validation Loss = 0.1683908636168164\n",
      "Testing combination 39/48: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0001, 'sequence_length': 20, 'dropout_rate': 0.1}\n",
      "Truncated Train Data Shape: (70, 20, 1)\n",
      "Truncated Validation Data Shape: (10, 20, 1)\n",
      "    Trial 1/2 for combination 39/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.5671040475858499\n",
      "Validation Loss: 0.18724119859959168\n",
      "Epoch 2/100\n",
      "Training Loss: 0.37842189857426256\n",
      "Validation Loss: 0.2677079596645152\n",
      "Epoch 3/100\n",
      "Training Loss: 0.29101001553994965\n",
      "Validation Loss: 0.29613778897916576\n",
      "Epoch 4/100\n",
      "Training Loss: 0.3214457380324368\n",
      "Validation Loss: 0.16133574217776736\n",
      "Epoch 5/100\n",
      "Training Loss: 0.26432563725886077\n",
      "Validation Loss: 0.14466313244495016\n",
      "Epoch 6/100\n",
      "Training Loss: 0.25178416604331527\n",
      "Validation Loss: 0.17766045236602857\n",
      "Epoch 7/100\n",
      "Training Loss: 0.2406371386685325\n",
      "Validation Loss: 0.1593642954374375\n",
      "Epoch 8/100\n",
      "Training Loss: 0.23507005036157602\n",
      "Validation Loss: 0.18529833995163672\n",
      "Epoch 9/100\n",
      "Training Loss: 0.2267778873844986\n",
      "Validation Loss: 0.16899684662139042\n",
      "Epoch 10/100\n",
      "Training Loss: 0.21050313207634888\n",
      "Validation Loss: 0.10300298856376615\n",
      "Epoch 11/100\n",
      "Training Loss: 0.22909201489821385\n",
      "Validation Loss: 0.2157098200854425\n",
      "Epoch 12/100\n",
      "Training Loss: 0.23398055008912208\n",
      "Validation Loss: 0.17434299365409872\n",
      "Epoch 13/100\n",
      "Training Loss: 0.26454783934267817\n",
      "Validation Loss: 0.0613072850756605\n",
      "Epoch 14/100\n",
      "Training Loss: 0.20529936683843614\n",
      "Validation Loss: 0.16273543915060557\n",
      "Epoch 15/100\n",
      "Training Loss: 0.2145463021172337\n",
      "Validation Loss: 0.09803206025342935\n",
      "Epoch 16/100\n",
      "Training Loss: 0.18419528951891778\n",
      "Validation Loss: 0.10113988223610057\n",
      "Epoch 17/100\n",
      "Training Loss: 0.18298862583315628\n",
      "Validation Loss: 0.12535527544978903\n",
      "Epoch 18/100\n",
      "Training Loss: 0.20605432040015975\n",
      "Validation Loss: 0.10529956039392263\n",
      "Epoch 19/100\n",
      "Training Loss: 0.1808502363048828\n",
      "Validation Loss: 0.10239737264901286\n",
      "Epoch 20/100\n",
      "Training Loss: 0.18655425406783258\n",
      "Validation Loss: 0.10574974942818513\n",
      "Epoch 21/100\n",
      "Training Loss: 0.19015986896900083\n",
      "Validation Loss: 0.18233929006145316\n",
      "Epoch 22/100\n",
      "Training Loss: 0.15739468920674407\n",
      "Validation Loss: 0.12907247144561954\n",
      "Epoch 23/100\n",
      "Training Loss: 0.207544424386972\n",
      "Validation Loss: 0.10506326450586725\n",
      "Epoch 24/100\n",
      "Training Loss: 0.1916837430355603\n",
      "Validation Loss: 0.09163956887247113\n",
      "Epoch 25/100\n",
      "Training Loss: 0.17449044753653167\n",
      "Validation Loss: 0.11952333305956095\n",
      "Epoch 26/100\n",
      "Training Loss: 0.15789877041911374\n",
      "Validation Loss: 0.06524697611432298\n",
      "Epoch 27/100\n",
      "Training Loss: 0.16130046856745558\n",
      "Validation Loss: 0.1547372726351362\n",
      "Epoch 28/100\n",
      "Training Loss: 0.17287888148034808\n",
      "Validation Loss: 0.11236118262463231\n",
      "Epoch 29/100\n",
      "Training Loss: 0.19008154121127901\n",
      "Validation Loss: 0.09622127712916617\n",
      "Epoch 30/100\n",
      "Training Loss: 0.17903960802579108\n",
      "Validation Loss: 0.0906064168866007\n",
      "Epoch 31/100\n",
      "Training Loss: 0.17360141996196324\n",
      "Validation Loss: 0.09014256228232276\n",
      "Epoch 32/100\n",
      "Training Loss: 0.17709722280911158\n",
      "Validation Loss: 0.10018558157900989\n",
      "Epoch 33/100\n",
      "Training Loss: 0.16907594240247956\n",
      "Validation Loss: 0.11065368096220522\n",
      "Epoch 34/100\n",
      "Training Loss: 0.14133484792360212\n",
      "Validation Loss: 0.10282565265009744\n",
      "Epoch 35/100\n",
      "Training Loss: 0.1562393843503132\n",
      "Validation Loss: 0.10815619024557328\n",
      "Epoch 36/100\n",
      "Training Loss: 0.1763495814943863\n",
      "Validation Loss: 0.06520590355302736\n",
      "Epoch 37/100\n",
      "Training Loss: 0.16155992266075278\n",
      "Validation Loss: 0.06906083268264224\n",
      "Epoch 38/100\n",
      "Training Loss: 0.17121464424806035\n",
      "Validation Loss: 0.09040375638234767\n",
      "Epoch 39/100\n",
      "Training Loss: 0.16424906655787117\n",
      "Validation Loss: 0.0671522548175735\n",
      "Epoch 40/100\n",
      "Training Loss: 0.14760003546424594\n",
      "Validation Loss: 0.08289256806691452\n",
      "Epoch 41/100\n",
      "Training Loss: 0.1561865971473162\n",
      "Validation Loss: 0.07176216363745763\n",
      "Epoch 42/100\n",
      "Training Loss: 0.17850893562948503\n",
      "Validation Loss: 0.12825296317338003\n",
      "Epoch 43/100\n",
      "Training Loss: 0.15937515237372543\n",
      "Validation Loss: 0.08584430533006902\n",
      "Epoch 44/100\n",
      "Training Loss: 0.1464993583184438\n",
      "Validation Loss: 0.10799906126054085\n",
      "Epoch 45/100\n",
      "Training Loss: 0.1327025140164898\n",
      "Validation Loss: 0.13335785373134795\n",
      "Epoch 46/100\n",
      "Training Loss: 0.1544496920132596\n",
      "Validation Loss: 0.09591727843115976\n",
      "Epoch 47/100\n",
      "Training Loss: 0.1427626585912703\n",
      "Validation Loss: 0.060140930273604655\n",
      "Epoch 48/100\n",
      "Training Loss: 0.14628906647824935\n",
      "Validation Loss: 0.11798926653126918\n",
      "Epoch 49/100\n",
      "Training Loss: 0.15916311845944622\n",
      "Validation Loss: 0.0861235166026163\n",
      "Epoch 50/100\n",
      "Training Loss: 0.1610107373641063\n",
      "Validation Loss: 0.09755815429495429\n",
      "Epoch 51/100\n",
      "Training Loss: 0.15629256057395552\n",
      "Validation Loss: 0.06542667159205828\n",
      "Epoch 52/100\n",
      "Training Loss: 0.14027751323693563\n",
      "Validation Loss: 0.05280871348514151\n",
      "Epoch 53/100\n",
      "Training Loss: 0.15704462111057427\n",
      "Validation Loss: 0.08920789923844107\n",
      "Epoch 54/100\n",
      "Training Loss: 0.1593340239264116\n",
      "Validation Loss: 0.12517902448757076\n",
      "Epoch 55/100\n",
      "Training Loss: 0.11639202656800265\n",
      "Validation Loss: 0.11916089531170518\n",
      "Epoch 56/100\n",
      "Training Loss: 0.1375445576182002\n",
      "Validation Loss: 0.11491207806115811\n",
      "Epoch 57/100\n",
      "Training Loss: 0.1552301800613082\n",
      "Validation Loss: 0.0467591740230464\n",
      "Epoch 58/100\n",
      "Training Loss: 0.13423050173006065\n",
      "Validation Loss: 0.07027218092689488\n",
      "Epoch 59/100\n",
      "Training Loss: 0.15021921316066367\n",
      "Validation Loss: 0.08512375229004962\n",
      "Epoch 60/100\n",
      "Training Loss: 0.13515297092639425\n",
      "Validation Loss: 0.08836658580468956\n",
      "Epoch 61/100\n",
      "Training Loss: 0.1370367271636252\n",
      "Validation Loss: 0.08706891650984822\n",
      "Epoch 62/100\n",
      "Training Loss: 0.1251526404072119\n",
      "Validation Loss: 0.10212285388279205\n",
      "Epoch 63/100\n",
      "Training Loss: 0.14867282931756642\n",
      "Validation Loss: 0.05311097483923759\n",
      "Epoch 64/100\n",
      "Training Loss: 0.1243799392137733\n",
      "Validation Loss: 0.09773193791928642\n",
      "Epoch 65/100\n",
      "Training Loss: 0.12795776757153077\n",
      "Validation Loss: 0.11319809923621885\n",
      "Epoch 66/100\n",
      "Training Loss: 0.1391214945510339\n",
      "Validation Loss: 0.15260093132088876\n",
      "Epoch 67/100\n",
      "Training Loss: 0.12932276575534565\n",
      "Validation Loss: 0.0788479773008465\n",
      "Epoch 68/100\n",
      "Training Loss: 0.141247169907601\n",
      "Validation Loss: 0.07116900485125549\n",
      "Epoch 69/100\n",
      "Training Loss: 0.15306856309210187\n",
      "Validation Loss: 0.102828146509336\n",
      "Epoch 70/100\n",
      "Training Loss: 0.1261703474658398\n",
      "Validation Loss: 0.0730175809841638\n",
      "Epoch 71/100\n",
      "Training Loss: 0.1292613510403756\n",
      "Validation Loss: 0.1265007026845528\n",
      "Epoch 72/100\n",
      "Training Loss: 0.12716482449128944\n",
      "Validation Loss: 0.08168794947202515\n",
      "Epoch 73/100\n",
      "Training Loss: 0.14259854103808794\n",
      "Validation Loss: 0.07687739011507586\n",
      "Epoch 74/100\n",
      "Training Loss: 0.12787492557576913\n",
      "Validation Loss: 0.11030209384477693\n",
      "Epoch 75/100\n",
      "Training Loss: 0.13633524290020654\n",
      "Validation Loss: 0.08166160807994931\n",
      "Epoch 76/100\n",
      "Training Loss: 0.10547621390999995\n",
      "Validation Loss: 0.06235916967315532\n",
      "Epoch 77/100\n",
      "Training Loss: 0.1526672255866409\n",
      "Validation Loss: 0.08402970497498652\n",
      "Epoch 78/100\n",
      "Training Loss: 0.11773283809470884\n",
      "Validation Loss: 0.08681007644042868\n",
      "Epoch 79/100\n",
      "Training Loss: 0.13080099752149132\n",
      "Validation Loss: 0.11988079319286749\n",
      "Epoch 80/100\n",
      "Training Loss: 0.15124571318953642\n",
      "Validation Loss: 0.09433770380315512\n",
      "Epoch 81/100\n",
      "Training Loss: 0.14234963136875858\n",
      "Validation Loss: 0.07362099807852004\n",
      "Epoch 82/100\n",
      "Training Loss: 0.1372598448068574\n",
      "Validation Loss: 0.05193098949449736\n",
      "Epoch 83/100\n",
      "Training Loss: 0.14189301237881424\n",
      "Validation Loss: 0.11077135438853822\n",
      "Epoch 84/100\n",
      "Training Loss: 0.13511803280553444\n",
      "Validation Loss: 0.07765630355560209\n",
      "Epoch 85/100\n",
      "Training Loss: 0.1315333588440126\n",
      "Validation Loss: 0.10432545871714324\n",
      "Epoch 86/100\n",
      "Training Loss: 0.12758326825803298\n",
      "Validation Loss: 0.0860336404049312\n",
      "Epoch 87/100\n",
      "Training Loss: 0.11995150913188583\n",
      "Validation Loss: 0.07343314324818104\n",
      "Epoch 88/100\n",
      "Training Loss: 0.11832621129680165\n",
      "Validation Loss: 0.08062341505123459\n",
      "Epoch 89/100\n",
      "Training Loss: 0.13286849272199447\n",
      "Validation Loss: 0.07613612391364814\n",
      "Epoch 90/100\n",
      "Training Loss: 0.11682284376254472\n",
      "Validation Loss: 0.08531895088308936\n",
      "Epoch 91/100\n",
      "Training Loss: 0.1268005027436256\n",
      "Validation Loss: 0.05663861396333338\n",
      "Epoch 92/100\n",
      "Training Loss: 0.12291063442978421\n",
      "Validation Loss: 0.08012780181691759\n",
      "Epoch 93/100\n",
      "Training Loss: 0.12233460283991208\n",
      "Validation Loss: 0.08079445036874593\n",
      "Epoch 94/100\n",
      "Training Loss: 0.12620929545159001\n",
      "Validation Loss: 0.049901775146409685\n",
      "Epoch 95/100\n",
      "Training Loss: 0.11890341745598493\n",
      "Validation Loss: 0.07302188526321893\n",
      "Epoch 96/100\n",
      "Training Loss: 0.1275943833525872\n",
      "Validation Loss: 0.07605210419226868\n",
      "Epoch 97/100\n",
      "Training Loss: 0.11006317585203239\n",
      "Validation Loss: 0.061759016587093506\n",
      "Epoch 98/100\n",
      "Training Loss: 0.11381461057462194\n",
      "Validation Loss: 0.09807456746033913\n",
      "Epoch 99/100\n",
      "Training Loss: 0.12743795878951147\n",
      "Validation Loss: 0.07315163569748266\n",
      "Epoch 100/100\n",
      "Training Loss: 0.11734950369471644\n",
      "Validation Loss: 0.06119786255090002\n",
      "    Trial 2/2 for combination 39/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.4516045876609992\n",
      "Validation Loss: 0.4023403505214769\n",
      "Epoch 2/100\n",
      "Training Loss: 0.3868747256327122\n",
      "Validation Loss: 0.277432604370407\n",
      "Epoch 3/100\n",
      "Training Loss: 0.24991400449858153\n",
      "Validation Loss: 0.38423262417185255\n",
      "Epoch 4/100\n",
      "Training Loss: 0.2785325326587854\n",
      "Validation Loss: 0.25192144119689797\n",
      "Epoch 5/100\n",
      "Training Loss: 0.2641861463743333\n",
      "Validation Loss: 0.2279208229153214\n",
      "Epoch 6/100\n",
      "Training Loss: 0.2637423490015716\n",
      "Validation Loss: 0.247757590798357\n",
      "Epoch 7/100\n",
      "Training Loss: 0.268300019938741\n",
      "Validation Loss: 0.18714185507655154\n",
      "Epoch 8/100\n",
      "Training Loss: 0.22876568323597815\n",
      "Validation Loss: 0.1201947012923518\n",
      "Epoch 9/100\n",
      "Training Loss: 0.21156145962912906\n",
      "Validation Loss: 0.11534802719184281\n",
      "Epoch 10/100\n",
      "Training Loss: 0.22846129447916144\n",
      "Validation Loss: 0.12678810792590456\n",
      "Epoch 11/100\n",
      "Training Loss: 0.19357520491815597\n",
      "Validation Loss: 0.15109257410831156\n",
      "Epoch 12/100\n",
      "Training Loss: 0.21845687335099467\n",
      "Validation Loss: 0.15280370371367274\n",
      "Epoch 13/100\n",
      "Training Loss: 0.2412164684806781\n",
      "Validation Loss: 0.12030242408322374\n",
      "Epoch 14/100\n",
      "Training Loss: 0.22056056499398052\n",
      "Validation Loss: 0.11949870128518594\n",
      "Epoch 15/100\n",
      "Training Loss: 0.18076308223444779\n",
      "Validation Loss: 0.17076305219549187\n",
      "Epoch 16/100\n",
      "Training Loss: 0.18667217315621068\n",
      "Validation Loss: 0.11027299480191081\n",
      "Epoch 17/100\n",
      "Training Loss: 0.18053923652653067\n",
      "Validation Loss: 0.08478345047309688\n",
      "Epoch 18/100\n",
      "Training Loss: 0.18779234356591618\n",
      "Validation Loss: 0.11016739736445409\n",
      "Epoch 19/100\n",
      "Training Loss: 0.16168176589235253\n",
      "Validation Loss: 0.1932013183744836\n",
      "Epoch 20/100\n",
      "Training Loss: 0.18447022547090183\n",
      "Validation Loss: 0.09183004508950689\n",
      "Epoch 21/100\n",
      "Training Loss: 0.16041493698224052\n",
      "Validation Loss: 0.07356234327316327\n",
      "Epoch 22/100\n",
      "Training Loss: 0.180915350262707\n",
      "Validation Loss: 0.04851754824891174\n",
      "Epoch 23/100\n",
      "Training Loss: 0.13137897458455142\n",
      "Validation Loss: 0.1384221080778462\n",
      "Epoch 24/100\n",
      "Training Loss: 0.16019814295817997\n",
      "Validation Loss: 0.15917489374018023\n",
      "Epoch 25/100\n",
      "Training Loss: 0.15278974596887465\n",
      "Validation Loss: 0.11411396707941128\n",
      "Epoch 26/100\n",
      "Training Loss: 0.18038454924105615\n",
      "Validation Loss: 0.1245631209607814\n",
      "Epoch 27/100\n",
      "Training Loss: 0.15442136235071752\n",
      "Validation Loss: 0.12944443251499774\n",
      "Epoch 28/100\n",
      "Training Loss: 0.16209793584531065\n",
      "Validation Loss: 0.06270537265654538\n",
      "Epoch 29/100\n",
      "Training Loss: 0.18983982824494763\n",
      "Validation Loss: 0.07763585631475277\n",
      "Epoch 30/100\n",
      "Training Loss: 0.16192992826402688\n",
      "Validation Loss: 0.1921400071493466\n",
      "Epoch 31/100\n",
      "Training Loss: 0.14445581702869853\n",
      "Validation Loss: 0.10799224348627916\n",
      "Epoch 32/100\n",
      "Training Loss: 0.14390763091737346\n",
      "Validation Loss: 0.061568648397957934\n",
      "Epoch 33/100\n",
      "Training Loss: 0.13861833372341456\n",
      "Validation Loss: 0.12442564234016293\n",
      "Epoch 34/100\n",
      "Training Loss: 0.16152318709740895\n",
      "Validation Loss: 0.09786569921186099\n",
      "Epoch 35/100\n",
      "Training Loss: 0.13767960060734585\n",
      "Validation Loss: 0.05104331416374712\n",
      "Epoch 36/100\n",
      "Training Loss: 0.15426925148827145\n",
      "Validation Loss: 0.08523051200641249\n",
      "Epoch 37/100\n",
      "Training Loss: 0.15181867991503417\n",
      "Validation Loss: 0.07686938028277478\n",
      "Epoch 38/100\n",
      "Training Loss: 0.14210513705805067\n",
      "Validation Loss: 0.06817169551744855\n",
      "Epoch 39/100\n",
      "Training Loss: 0.14106467329770997\n",
      "Validation Loss: 0.07875991762496726\n",
      "Epoch 40/100\n",
      "Training Loss: 0.1463117937728164\n",
      "Validation Loss: 0.07586518392559238\n",
      "Epoch 41/100\n",
      "Training Loss: 0.13355838561269\n",
      "Validation Loss: 0.105027189545993\n",
      "Epoch 42/100\n",
      "Training Loss: 0.13450460532031197\n",
      "Validation Loss: 0.08723842383751716\n",
      "Epoch 43/100\n",
      "Training Loss: 0.13195331693604062\n",
      "Validation Loss: 0.07860447495875036\n",
      "Epoch 44/100\n",
      "Training Loss: 0.14198455417764147\n",
      "Validation Loss: 0.0708800874667917\n",
      "Epoch 45/100\n",
      "Training Loss: 0.14124205170416165\n",
      "Validation Loss: 0.10558278637430649\n",
      "Epoch 46/100\n",
      "Training Loss: 0.14844809267309803\n",
      "Validation Loss: 0.11644700097930268\n",
      "Epoch 47/100\n",
      "Training Loss: 0.13493120078229165\n",
      "Validation Loss: 0.06771378414026283\n",
      "Epoch 48/100\n",
      "Training Loss: 0.14652008934988117\n",
      "Validation Loss: 0.15299697964720982\n",
      "Epoch 49/100\n",
      "Training Loss: 0.15446698479244503\n",
      "Validation Loss: 0.08035753181646417\n",
      "Epoch 50/100\n",
      "Training Loss: 0.1275589249061301\n",
      "Validation Loss: 0.07628384145571379\n",
      "Epoch 51/100\n",
      "Training Loss: 0.12533048980847655\n",
      "Validation Loss: 0.10017808077982471\n",
      "Epoch 52/100\n",
      "Training Loss: 0.14458478276142891\n",
      "Validation Loss: 0.10100823314031267\n",
      "Epoch 53/100\n",
      "Training Loss: 0.12612184951931507\n",
      "Validation Loss: 0.15056273945055829\n",
      "Epoch 54/100\n",
      "Training Loss: 0.14857667518392903\n",
      "Validation Loss: 0.08720079900059018\n",
      "Epoch 55/100\n",
      "Training Loss: 0.11424033383180189\n",
      "Validation Loss: 0.04214535117062815\n",
      "Epoch 56/100\n",
      "Training Loss: 0.11499349914465845\n",
      "Validation Loss: 0.0942550028045062\n",
      "Epoch 57/100\n",
      "Training Loss: 0.1296228676407215\n",
      "Validation Loss: 0.05513550553601276\n",
      "Epoch 58/100\n",
      "Training Loss: 0.14205448135509943\n",
      "Validation Loss: 0.04608409492784528\n",
      "Epoch 59/100\n",
      "Training Loss: 0.13320723274879384\n",
      "Validation Loss: 0.07963621719966885\n",
      "Epoch 60/100\n",
      "Training Loss: 0.14421178599430104\n",
      "Validation Loss: 0.07472475411494053\n",
      "Epoch 61/100\n",
      "Training Loss: 0.11782354261293493\n",
      "Validation Loss: 0.07418074226993833\n",
      "Epoch 62/100\n",
      "Training Loss: 0.12854713712535\n",
      "Validation Loss: 0.08767554134379976\n",
      "Epoch 63/100\n",
      "Training Loss: 0.13345128281707286\n",
      "Validation Loss: 0.06321901872417467\n",
      "Epoch 64/100\n",
      "Training Loss: 0.13679226927926233\n",
      "Validation Loss: 0.06653013532946248\n",
      "Epoch 65/100\n",
      "Training Loss: 0.12374244473500023\n",
      "Validation Loss: 0.08394273049749149\n",
      "Epoch 66/100\n",
      "Training Loss: 0.12394857325061552\n",
      "Validation Loss: 0.0893677747449539\n",
      "Epoch 67/100\n",
      "Training Loss: 0.11701948038112664\n",
      "Validation Loss: 0.043020389244049786\n",
      "Epoch 68/100\n",
      "Training Loss: 0.13032743001837452\n",
      "Validation Loss: 0.053476411445962546\n",
      "Epoch 69/100\n",
      "Training Loss: 0.1215786998851281\n",
      "Validation Loss: 0.07953776431204071\n",
      "Epoch 70/100\n",
      "Training Loss: 0.1231813418895731\n",
      "Validation Loss: 0.08986869594525407\n",
      "Epoch 71/100\n",
      "Training Loss: 0.12524824202459586\n",
      "Validation Loss: 0.0915770886531216\n",
      "Epoch 72/100\n",
      "Training Loss: 0.12265814339214699\n",
      "Validation Loss: 0.08045525369683242\n",
      "Epoch 73/100\n",
      "Training Loss: 0.1209829230604858\n",
      "Validation Loss: 0.0648244011216958\n",
      "Epoch 74/100\n",
      "Training Loss: 0.11196474702053057\n",
      "Validation Loss: 0.0796193804143765\n",
      "Epoch 75/100\n",
      "Training Loss: 0.1186588123607425\n",
      "Validation Loss: 0.07552954380836341\n",
      "Epoch 76/100\n",
      "Training Loss: 0.12346709925055117\n",
      "Validation Loss: 0.08135013868381333\n",
      "Epoch 77/100\n",
      "Training Loss: 0.1114103741459067\n",
      "Validation Loss: 0.06204393440635998\n",
      "Epoch 78/100\n",
      "Training Loss: 0.12207214929939185\n",
      "Validation Loss: 0.07834212895215772\n",
      "Epoch 79/100\n",
      "Training Loss: 0.10844988213171633\n",
      "Validation Loss: 0.06414835098890304\n",
      "Epoch 80/100\n",
      "Training Loss: 0.12078631760685692\n",
      "Validation Loss: 0.07960336662943066\n",
      "Epoch 81/100\n",
      "Training Loss: 0.1125481579137779\n",
      "Validation Loss: 0.09625081620205415\n",
      "Epoch 82/100\n",
      "Training Loss: 0.1037116750682819\n",
      "Validation Loss: 0.06940530868732228\n",
      "Epoch 83/100\n",
      "Training Loss: 0.10709690934077287\n",
      "Validation Loss: 0.04951362221929863\n",
      "Epoch 84/100\n",
      "Training Loss: 0.11524680926213193\n",
      "Validation Loss: 0.06587695454151457\n",
      "Epoch 85/100\n",
      "Training Loss: 0.13760658698347017\n",
      "Validation Loss: 0.05840425852797919\n",
      "Epoch 86/100\n",
      "Training Loss: 0.12128110798184209\n",
      "Validation Loss: 0.05877918465749364\n",
      "Epoch 87/100\n",
      "Training Loss: 0.12207360497256971\n",
      "Validation Loss: 0.062093632877644175\n",
      "Epoch 88/100\n",
      "Training Loss: 0.10368507685568451\n",
      "Validation Loss: 0.09684573996725095\n",
      "Epoch 89/100\n",
      "Training Loss: 0.12402614452496533\n",
      "Validation Loss: 0.05806111032552476\n",
      "Epoch 90/100\n",
      "Training Loss: 0.10845052308927293\n",
      "Validation Loss: 0.06064109162349013\n",
      "Epoch 91/100\n",
      "Training Loss: 0.1093872837180308\n",
      "Validation Loss: 0.052119281347122945\n",
      "Epoch 92/100\n",
      "Training Loss: 0.09470680323417974\n",
      "Validation Loss: 0.07103589189534745\n",
      "Epoch 93/100\n",
      "Training Loss: 0.11306689342733627\n",
      "Validation Loss: 0.034503900169670955\n",
      "Epoch 94/100\n",
      "Training Loss: 0.13031764599003332\n",
      "Validation Loss: 0.07689331461213159\n",
      "Epoch 95/100\n",
      "Training Loss: 0.11422375340563876\n",
      "Validation Loss: 0.039444447401913094\n",
      "Epoch 96/100\n",
      "Training Loss: 0.13609268929172103\n",
      "Validation Loss: 0.053915900407324455\n",
      "Epoch 97/100\n",
      "Training Loss: 0.11752816633648415\n",
      "Validation Loss: 0.06542131923998792\n",
      "Epoch 98/100\n",
      "Training Loss: 0.10840264036062842\n",
      "Validation Loss: 0.08502536107509713\n",
      "Epoch 99/100\n",
      "Training Loss: 0.10680280387231261\n",
      "Validation Loss: 0.08065638019693336\n",
      "Epoch 100/100\n",
      "Training Loss: 0.10271072823109062\n",
      "Validation Loss: 0.07966995864132059\n",
      "Combination 39: Avg Training Loss = 0.15999023065502593, Avg Validation Loss = 0.10267321606371191\n",
      "Testing combination 40/48: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0001, 'sequence_length': 20, 'dropout_rate': 0.2}\n",
      "Truncated Train Data Shape: (70, 20, 1)\n",
      "Truncated Validation Data Shape: (10, 20, 1)\n",
      "    Trial 1/2 for combination 40/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.4815252339630766\n",
      "Validation Loss: 0.38045692609392806\n",
      "Epoch 2/100\n",
      "Training Loss: 0.3687910688688949\n",
      "Validation Loss: 0.21408226732586563\n",
      "Epoch 3/100\n",
      "Training Loss: 0.30277611182959646\n",
      "Validation Loss: 0.228432523775991\n",
      "Epoch 4/100\n",
      "Training Loss: 0.315178621583185\n",
      "Validation Loss: 0.2735402816349599\n",
      "Epoch 5/100\n",
      "Training Loss: 0.24044075310497978\n",
      "Validation Loss: 0.2588958932647145\n",
      "Epoch 6/100\n",
      "Training Loss: 0.2592237763815114\n",
      "Validation Loss: 0.11543856221968776\n",
      "Epoch 7/100\n",
      "Training Loss: 0.2568861684324939\n",
      "Validation Loss: 0.21005666632490172\n",
      "Epoch 8/100\n",
      "Training Loss: 0.23046555547680775\n",
      "Validation Loss: 0.2542702963147978\n",
      "Epoch 9/100\n",
      "Training Loss: 0.20232569483845855\n",
      "Validation Loss: 0.13354849866060842\n",
      "Epoch 10/100\n",
      "Training Loss: 0.21801675711494428\n",
      "Validation Loss: 0.11434690868875166\n",
      "Epoch 11/100\n",
      "Training Loss: 0.20695933664615224\n",
      "Validation Loss: 0.3186399004565649\n",
      "Epoch 12/100\n",
      "Training Loss: 0.20156621548841525\n",
      "Validation Loss: 0.16375864890300312\n",
      "Epoch 13/100\n",
      "Training Loss: 0.22283398059106257\n",
      "Validation Loss: 0.1312282139327888\n",
      "Epoch 14/100\n",
      "Training Loss: 0.1876175939256795\n",
      "Validation Loss: 0.1727942606801068\n",
      "Epoch 15/100\n",
      "Training Loss: 0.2132637213491683\n",
      "Validation Loss: 0.17321334328236568\n",
      "Epoch 16/100\n",
      "Training Loss: 0.18410760336915835\n",
      "Validation Loss: 0.13238808150963838\n",
      "Epoch 17/100\n",
      "Training Loss: 0.18141349482139407\n",
      "Validation Loss: 0.21778044528581414\n",
      "Epoch 18/100\n",
      "Training Loss: 0.20867631336812698\n",
      "Validation Loss: 0.15002715352262777\n",
      "Epoch 19/100\n",
      "Training Loss: 0.16590289968544109\n",
      "Validation Loss: 0.2047437172817373\n",
      "Epoch 20/100\n",
      "Training Loss: 0.15286856870025076\n",
      "Validation Loss: 0.16070999166512437\n",
      "Epoch 21/100\n",
      "Training Loss: 0.20176386645526176\n",
      "Validation Loss: 0.16739935861153438\n",
      "Epoch 22/100\n",
      "Training Loss: 0.17863565085606498\n",
      "Validation Loss: 0.11840451309435088\n",
      "Epoch 23/100\n",
      "Training Loss: 0.174112717916621\n",
      "Validation Loss: 0.23466204779968983\n",
      "Epoch 24/100\n",
      "Training Loss: 0.164591016103859\n",
      "Validation Loss: 0.11699665654845895\n",
      "Epoch 25/100\n",
      "Training Loss: 0.16724893427315976\n",
      "Validation Loss: 0.11285533533573193\n",
      "Epoch 26/100\n",
      "Training Loss: 0.15259822196739253\n",
      "Validation Loss: 0.10503240083806593\n",
      "Epoch 27/100\n",
      "Training Loss: 0.1801124142486798\n",
      "Validation Loss: 0.12429815134292395\n",
      "Epoch 28/100\n",
      "Training Loss: 0.1721827993198\n",
      "Validation Loss: 0.1374397837617771\n",
      "Epoch 29/100\n",
      "Training Loss: 0.14368757668808593\n",
      "Validation Loss: 0.0782994197961361\n",
      "Epoch 30/100\n",
      "Training Loss: 0.15276866655471943\n",
      "Validation Loss: 0.1301583580741013\n",
      "Epoch 31/100\n",
      "Training Loss: 0.15919486735402227\n",
      "Validation Loss: 0.09240337241116534\n",
      "Epoch 32/100\n",
      "Training Loss: 0.15418776527428646\n",
      "Validation Loss: 0.1037287491561025\n",
      "Epoch 33/100\n",
      "Training Loss: 0.1496668036996369\n",
      "Validation Loss: 0.13618153416532416\n",
      "Epoch 34/100\n",
      "Training Loss: 0.15887374108694835\n",
      "Validation Loss: 0.09870740154659409\n",
      "Epoch 35/100\n",
      "Training Loss: 0.1552454485059752\n",
      "Validation Loss: 0.11921125753287036\n",
      "Epoch 36/100\n",
      "Training Loss: 0.15876099748304814\n",
      "Validation Loss: 0.07634616446829112\n",
      "Epoch 37/100\n",
      "Training Loss: 0.16379100759012563\n",
      "Validation Loss: 0.08701806871079021\n",
      "Epoch 38/100\n",
      "Training Loss: 0.16610586179119888\n",
      "Validation Loss: 0.065080877704322\n",
      "Epoch 39/100\n",
      "Training Loss: 0.1621726244895884\n",
      "Validation Loss: 0.079237944465272\n",
      "Epoch 40/100\n",
      "Training Loss: 0.1317008798897121\n",
      "Validation Loss: 0.09985932047328003\n",
      "Epoch 41/100\n",
      "Training Loss: 0.1471815541129279\n",
      "Validation Loss: 0.1275315763378006\n",
      "Epoch 42/100\n",
      "Training Loss: 0.14502906768757598\n",
      "Validation Loss: 0.17460870022985503\n",
      "Epoch 43/100\n",
      "Training Loss: 0.14368344349728993\n",
      "Validation Loss: 0.11236539919920523\n",
      "Epoch 44/100\n",
      "Training Loss: 0.13783426886669586\n",
      "Validation Loss: 0.07699601414675328\n",
      "Epoch 45/100\n",
      "Training Loss: 0.13500761511548012\n",
      "Validation Loss: 0.10696254025411031\n",
      "Epoch 46/100\n",
      "Training Loss: 0.13352367277782606\n",
      "Validation Loss: 0.12850230011385075\n",
      "Epoch 47/100\n",
      "Training Loss: 0.14420037887307025\n",
      "Validation Loss: 0.10198467683175276\n",
      "Epoch 48/100\n",
      "Training Loss: 0.14716936248302728\n",
      "Validation Loss: 0.10461544648765767\n",
      "Epoch 49/100\n",
      "Training Loss: 0.12101542808022646\n",
      "Validation Loss: 0.1142273509193776\n",
      "Epoch 50/100\n",
      "Training Loss: 0.1368546518823726\n",
      "Validation Loss: 0.09637678290664052\n",
      "Epoch 51/100\n",
      "Training Loss: 0.14543421471967197\n",
      "Validation Loss: 0.08412058426462142\n",
      "Epoch 52/100\n",
      "Training Loss: 0.14321797552616988\n",
      "Validation Loss: 0.07858390502713711\n",
      "Epoch 53/100\n",
      "Training Loss: 0.13367336326554716\n",
      "Validation Loss: 0.08040534237316878\n",
      "Epoch 54/100\n",
      "Training Loss: 0.13541142880504822\n",
      "Validation Loss: 0.12938828854074946\n",
      "Epoch 55/100\n",
      "Training Loss: 0.1355371895677679\n",
      "Validation Loss: 0.13398875945120067\n",
      "Epoch 56/100\n",
      "Training Loss: 0.13386554856245805\n",
      "Validation Loss: 0.10893604681168627\n",
      "Epoch 57/100\n",
      "Training Loss: 0.12494434006952164\n",
      "Validation Loss: 0.0845349930598385\n",
      "Epoch 58/100\n",
      "Training Loss: 0.12857611241304626\n",
      "Validation Loss: 0.1306226415961113\n",
      "Epoch 59/100\n",
      "Training Loss: 0.12844425263038964\n",
      "Validation Loss: 0.07791134387766425\n",
      "Epoch 60/100\n",
      "Training Loss: 0.1295977274161158\n",
      "Validation Loss: 0.08848125773143418\n",
      "Epoch 61/100\n",
      "Training Loss: 0.13176924616928343\n",
      "Validation Loss: 0.08378486386700976\n",
      "Epoch 62/100\n",
      "Training Loss: 0.12627410316043156\n",
      "Validation Loss: 0.06970520215793499\n",
      "Epoch 63/100\n",
      "Training Loss: 0.13781309880495163\n",
      "Validation Loss: 0.10269495874737118\n",
      "Epoch 64/100\n",
      "Training Loss: 0.12815079061834836\n",
      "Validation Loss: 0.07162413276290207\n",
      "Epoch 65/100\n",
      "Training Loss: 0.12857223074006813\n",
      "Validation Loss: 0.09756539864412492\n",
      "Epoch 66/100\n",
      "Training Loss: 0.1278973129904305\n",
      "Validation Loss: 0.1399221712290251\n",
      "Epoch 67/100\n",
      "Training Loss: 0.13126038479220856\n",
      "Validation Loss: 0.1096258715988188\n",
      "Epoch 68/100\n",
      "Training Loss: 0.12561747657262634\n",
      "Validation Loss: 0.07039000031244633\n",
      "Epoch 69/100\n",
      "Training Loss: 0.12786499448773814\n",
      "Validation Loss: 0.09539097579211608\n",
      "Epoch 70/100\n",
      "Training Loss: 0.12051619616348155\n",
      "Validation Loss: 0.07505717454060587\n",
      "Epoch 71/100\n",
      "Training Loss: 0.1112109187987919\n",
      "Validation Loss: 0.101040201636178\n",
      "Epoch 72/100\n",
      "Training Loss: 0.1265093258845966\n",
      "Validation Loss: 0.09145803889758788\n",
      "Epoch 73/100\n",
      "Training Loss: 0.12134036621598251\n",
      "Validation Loss: 0.06855460250945829\n",
      "Epoch 74/100\n",
      "Training Loss: 0.11779748426455414\n",
      "Validation Loss: 0.08402515105480787\n",
      "Epoch 75/100\n",
      "Training Loss: 0.11485092826468561\n",
      "Validation Loss: 0.05044775133215877\n",
      "Epoch 76/100\n",
      "Training Loss: 0.10875665816629553\n",
      "Validation Loss: 0.07050769344328955\n",
      "Epoch 77/100\n",
      "Training Loss: 0.11236422537493958\n",
      "Validation Loss: 0.05027316459822288\n",
      "Epoch 78/100\n",
      "Training Loss: 0.12124569696050247\n",
      "Validation Loss: 0.07550428690493513\n",
      "Epoch 79/100\n",
      "Training Loss: 0.10996269111773885\n",
      "Validation Loss: 0.057800936630172106\n",
      "Epoch 80/100\n",
      "Training Loss: 0.12100380810968321\n",
      "Validation Loss: 0.08213147406367032\n",
      "Epoch 81/100\n",
      "Training Loss: 0.11269693595607694\n",
      "Validation Loss: 0.1158728563827133\n",
      "Epoch 82/100\n",
      "Training Loss: 0.11020264126265926\n",
      "Validation Loss: 0.0688615278982939\n",
      "Epoch 83/100\n",
      "Training Loss: 0.12509192953664935\n",
      "Validation Loss: 0.05299521556791331\n",
      "Epoch 84/100\n",
      "Training Loss: 0.12275433566461517\n",
      "Validation Loss: 0.06318190070069371\n",
      "Epoch 85/100\n",
      "Training Loss: 0.11227819631613886\n",
      "Validation Loss: 0.06712572771486172\n",
      "Epoch 86/100\n",
      "Training Loss: 0.11802337881581472\n",
      "Validation Loss: 0.07020223761191102\n",
      "Epoch 87/100\n",
      "Training Loss: 0.11207679534977107\n",
      "Validation Loss: 0.057031382482569205\n",
      "Epoch 88/100\n",
      "Training Loss: 0.11736218889972234\n",
      "Validation Loss: 0.07117503728135616\n",
      "Epoch 89/100\n",
      "Training Loss: 0.10077149108352294\n",
      "Validation Loss: 0.05371402295280381\n",
      "Epoch 90/100\n",
      "Training Loss: 0.10646746035946009\n",
      "Validation Loss: 0.06856887598944819\n",
      "Epoch 91/100\n",
      "Training Loss: 0.10768851569914833\n",
      "Validation Loss: 0.07244511953927434\n",
      "Epoch 92/100\n",
      "Training Loss: 0.11688936519433127\n",
      "Validation Loss: 0.07751154169800692\n",
      "Epoch 93/100\n",
      "Training Loss: 0.10853193393324394\n",
      "Validation Loss: 0.06530605774996007\n",
      "Epoch 94/100\n",
      "Training Loss: 0.09846838045136991\n",
      "Validation Loss: 0.0720481892345827\n",
      "Epoch 95/100\n",
      "Training Loss: 0.10181309718148013\n",
      "Validation Loss: 0.07929007139677319\n",
      "Epoch 96/100\n",
      "Training Loss: 0.09229625841668432\n",
      "Validation Loss: 0.06423226395299023\n",
      "Epoch 97/100\n",
      "Training Loss: 0.11049710614805437\n",
      "Validation Loss: 0.10658604728494614\n",
      "Epoch 98/100\n",
      "Training Loss: 0.10761250822180564\n",
      "Validation Loss: 0.06900506584209305\n",
      "Epoch 99/100\n",
      "Training Loss: 0.10129291778977115\n",
      "Validation Loss: 0.06337178965711662\n",
      "Epoch 100/100\n",
      "Training Loss: 0.09980760692774956\n",
      "Validation Loss: 0.09562424795145259\n",
      "    Trial 2/2 for combination 40/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.4160153982471331\n",
      "Validation Loss: 0.4001831413046163\n",
      "Epoch 2/100\n",
      "Training Loss: 0.3443844939949244\n",
      "Validation Loss: 0.185610680728521\n",
      "Epoch 3/100\n",
      "Training Loss: 0.3307462926599281\n",
      "Validation Loss: 0.47642453266979634\n",
      "Epoch 4/100\n",
      "Training Loss: 0.2354655278824519\n",
      "Validation Loss: 0.2538803668651658\n",
      "Epoch 5/100\n",
      "Training Loss: 0.3097527838296925\n",
      "Validation Loss: 0.12153949870643335\n",
      "Epoch 6/100\n",
      "Training Loss: 0.23535334122789295\n",
      "Validation Loss: 0.20324809847347045\n",
      "Epoch 7/100\n",
      "Training Loss: 0.23448909668973056\n",
      "Validation Loss: 0.15251154240461212\n",
      "Epoch 8/100\n",
      "Training Loss: 0.229806953680963\n",
      "Validation Loss: 0.23913589458996148\n",
      "Epoch 9/100\n",
      "Training Loss: 0.23076989888190375\n",
      "Validation Loss: 0.17490337443512766\n",
      "Epoch 10/100\n",
      "Training Loss: 0.21025019853396565\n",
      "Validation Loss: 0.20088899254859297\n",
      "Epoch 11/100\n",
      "Training Loss: 0.19098319550374207\n",
      "Validation Loss: 0.09823454370208408\n",
      "Epoch 12/100\n",
      "Training Loss: 0.18519963076971857\n",
      "Validation Loss: 0.11356226751232024\n",
      "Epoch 13/100\n",
      "Training Loss: 0.2091878553517911\n",
      "Validation Loss: 0.14892167703124276\n",
      "Epoch 14/100\n",
      "Training Loss: 0.18537197340404213\n",
      "Validation Loss: 0.1359331269429215\n",
      "Epoch 15/100\n",
      "Training Loss: 0.1934476853104464\n",
      "Validation Loss: 0.13633478497499957\n",
      "Epoch 16/100\n",
      "Training Loss: 0.1840414907861393\n",
      "Validation Loss: 0.14231135792894317\n",
      "Epoch 17/100\n",
      "Training Loss: 0.17388388743127073\n",
      "Validation Loss: 0.15591329500350792\n",
      "Epoch 18/100\n",
      "Training Loss: 0.18509596243159748\n",
      "Validation Loss: 0.19501559387718137\n",
      "Epoch 19/100\n",
      "Training Loss: 0.15908198735445123\n",
      "Validation Loss: 0.10836800740617156\n",
      "Epoch 20/100\n",
      "Training Loss: 0.15595807190615835\n",
      "Validation Loss: 0.06745977498071085\n",
      "Epoch 21/100\n",
      "Training Loss: 0.1641583688200457\n",
      "Validation Loss: 0.14784620804565551\n",
      "Epoch 22/100\n",
      "Training Loss: 0.17210468060361833\n",
      "Validation Loss: 0.13667830986758409\n",
      "Epoch 23/100\n",
      "Training Loss: 0.17643210582356744\n",
      "Validation Loss: 0.1606746027962474\n",
      "Epoch 24/100\n",
      "Training Loss: 0.16659982586875546\n",
      "Validation Loss: 0.26640780012325554\n",
      "Epoch 25/100\n",
      "Training Loss: 0.15313302168941406\n",
      "Validation Loss: 0.09408389949821787\n",
      "Epoch 26/100\n",
      "Training Loss: 0.14248491241632094\n",
      "Validation Loss: 0.15661723168360955\n",
      "Epoch 27/100\n",
      "Training Loss: 0.16230817546115434\n",
      "Validation Loss: 0.17229209458714861\n",
      "Epoch 28/100\n",
      "Training Loss: 0.16777271625588433\n",
      "Validation Loss: 0.09488545274357076\n",
      "Epoch 29/100\n",
      "Training Loss: 0.15366100663873594\n",
      "Validation Loss: 0.11903419086177444\n",
      "Epoch 30/100\n",
      "Training Loss: 0.1740793834298637\n",
      "Validation Loss: 0.25005109492350674\n",
      "Epoch 31/100\n",
      "Training Loss: 0.17955334202194245\n",
      "Validation Loss: 0.17312434298946594\n",
      "Epoch 32/100\n",
      "Training Loss: 0.14045703851831093\n",
      "Validation Loss: 0.09129688170523202\n",
      "Epoch 33/100\n",
      "Training Loss: 0.14939430148180502\n",
      "Validation Loss: 0.10339447627071138\n",
      "Epoch 34/100\n",
      "Training Loss: 0.15422322392437138\n",
      "Validation Loss: 0.14367994061131448\n",
      "Epoch 35/100\n",
      "Training Loss: 0.13597717283907299\n",
      "Validation Loss: 0.08211563911607558\n",
      "Epoch 36/100\n",
      "Training Loss: 0.13439382673876446\n",
      "Validation Loss: 0.16207655896956413\n",
      "Epoch 37/100\n",
      "Training Loss: 0.14682438504126677\n",
      "Validation Loss: 0.09204580843104121\n",
      "Epoch 38/100\n",
      "Training Loss: 0.12797493653339673\n",
      "Validation Loss: 0.09775508337689069\n",
      "Epoch 39/100\n",
      "Training Loss: 0.16929751281410987\n",
      "Validation Loss: 0.13840574414614354\n",
      "Epoch 40/100\n",
      "Training Loss: 0.13643058183171422\n",
      "Validation Loss: 0.08673917114282335\n",
      "Epoch 41/100\n",
      "Training Loss: 0.14326891951378984\n",
      "Validation Loss: 0.07445143505854894\n",
      "Epoch 42/100\n",
      "Training Loss: 0.15163094069457056\n",
      "Validation Loss: 0.09951753500845364\n",
      "Epoch 43/100\n",
      "Training Loss: 0.1435343759332741\n",
      "Validation Loss: 0.07366672413528606\n",
      "Epoch 44/100\n",
      "Training Loss: 0.13307083313581175\n",
      "Validation Loss: 0.1153013025991427\n",
      "Epoch 45/100\n",
      "Training Loss: 0.1281536133678721\n",
      "Validation Loss: 0.10771212021201466\n",
      "Epoch 46/100\n",
      "Training Loss: 0.12998838080224462\n",
      "Validation Loss: 0.08805167672318229\n",
      "Epoch 47/100\n",
      "Training Loss: 0.1272370349022551\n",
      "Validation Loss: 0.10438416717202556\n",
      "Epoch 48/100\n",
      "Training Loss: 0.12345307819576348\n",
      "Validation Loss: 0.07532998955177257\n",
      "Epoch 49/100\n",
      "Training Loss: 0.14106717575598332\n",
      "Validation Loss: 0.12390325861241695\n",
      "Epoch 50/100\n",
      "Training Loss: 0.1310192969218099\n",
      "Validation Loss: 0.09438333899579003\n",
      "Epoch 51/100\n",
      "Training Loss: 0.13443108677462906\n",
      "Validation Loss: 0.06520042109043431\n",
      "Epoch 52/100\n",
      "Training Loss: 0.13050502343100703\n",
      "Validation Loss: 0.08834788711606083\n",
      "Epoch 53/100\n",
      "Training Loss: 0.1337455126078043\n",
      "Validation Loss: 0.10996831830978578\n",
      "Epoch 54/100\n",
      "Training Loss: 0.1431392973636741\n",
      "Validation Loss: 0.13569598493395282\n",
      "Epoch 55/100\n",
      "Training Loss: 0.13797791437519472\n",
      "Validation Loss: 0.09894174052720797\n",
      "Epoch 56/100\n",
      "Training Loss: 0.1360693047165967\n",
      "Validation Loss: 0.11880222545601257\n",
      "Epoch 57/100\n",
      "Training Loss: 0.13776325783698726\n",
      "Validation Loss: 0.07621166120225684\n",
      "Epoch 58/100\n",
      "Training Loss: 0.122631699477765\n",
      "Validation Loss: 0.11692611638235481\n",
      "Epoch 59/100\n",
      "Training Loss: 0.12100769211046324\n",
      "Validation Loss: 0.0668589094644413\n",
      "Epoch 60/100\n",
      "Training Loss: 0.14260703006185324\n",
      "Validation Loss: 0.09769599606687603\n",
      "Epoch 61/100\n",
      "Training Loss: 0.10557877545319262\n",
      "Validation Loss: 0.08062462222527261\n",
      "Epoch 62/100\n",
      "Training Loss: 0.12775906363038445\n",
      "Validation Loss: 0.06417845744677639\n",
      "Epoch 63/100\n",
      "Training Loss: 0.1347678033163173\n",
      "Validation Loss: 0.07727380440734162\n",
      "Epoch 64/100\n",
      "Training Loss: 0.12471260229667673\n",
      "Validation Loss: 0.09982784916874048\n",
      "Epoch 65/100\n",
      "Training Loss: 0.13917653146364536\n",
      "Validation Loss: 0.0845297010583829\n",
      "Epoch 66/100\n",
      "Training Loss: 0.11682171841576187\n",
      "Validation Loss: 0.08204124020033815\n",
      "Epoch 67/100\n",
      "Training Loss: 0.12031952934709185\n",
      "Validation Loss: 0.05821381032175914\n",
      "Epoch 68/100\n",
      "Training Loss: 0.11764640047565779\n",
      "Validation Loss: 0.07869153262853604\n",
      "Epoch 69/100\n",
      "Training Loss: 0.1231180442600944\n",
      "Validation Loss: 0.07208978817446292\n",
      "Epoch 70/100\n",
      "Training Loss: 0.11666704403303754\n",
      "Validation Loss: 0.046610496011362565\n",
      "Epoch 71/100\n",
      "Training Loss: 0.13292519530259586\n",
      "Validation Loss: 0.047857078747614565\n",
      "Epoch 72/100\n",
      "Training Loss: 0.11570140122906135\n",
      "Validation Loss: 0.10953196380267634\n",
      "Epoch 73/100\n",
      "Training Loss: 0.12488296852699655\n",
      "Validation Loss: 0.07926843984790936\n",
      "Epoch 74/100\n",
      "Training Loss: 0.11049155629412226\n",
      "Validation Loss: 0.0648939571848856\n",
      "Epoch 75/100\n",
      "Training Loss: 0.1108616198360845\n",
      "Validation Loss: 0.08221103153831585\n",
      "Epoch 76/100\n",
      "Training Loss: 0.11394302849186357\n",
      "Validation Loss: 0.05942340089765154\n",
      "Epoch 77/100\n",
      "Training Loss: 0.10616135546470522\n",
      "Validation Loss: 0.05715435113294357\n",
      "Epoch 78/100\n",
      "Training Loss: 0.11408846528155393\n",
      "Validation Loss: 0.056046322422901085\n",
      "Epoch 79/100\n",
      "Training Loss: 0.11700203343420348\n",
      "Validation Loss: 0.06560356760894141\n",
      "Epoch 80/100\n",
      "Training Loss: 0.116896201228699\n",
      "Validation Loss: 0.06495389955093663\n",
      "Epoch 81/100\n",
      "Training Loss: 0.11372517806655785\n",
      "Validation Loss: 0.08362899147791417\n",
      "Epoch 82/100\n",
      "Training Loss: 0.12539604135852866\n",
      "Validation Loss: 0.0574671882525594\n",
      "Epoch 83/100\n",
      "Training Loss: 0.10878932845814024\n",
      "Validation Loss: 0.07116374522667707\n",
      "Epoch 84/100\n",
      "Training Loss: 0.11447169277627078\n",
      "Validation Loss: 0.06244524301343747\n",
      "Epoch 85/100\n",
      "Training Loss: 0.11495525503907274\n",
      "Validation Loss: 0.057005882833176605\n",
      "Epoch 86/100\n",
      "Training Loss: 0.10357829571531085\n",
      "Validation Loss: 0.06601468487767584\n",
      "Epoch 87/100\n",
      "Training Loss: 0.11597434548186127\n",
      "Validation Loss: 0.054994999998146596\n",
      "Epoch 88/100\n",
      "Training Loss: 0.11637363884586924\n",
      "Validation Loss: 0.062250535813387686\n",
      "Epoch 89/100\n",
      "Training Loss: 0.11735994373560289\n",
      "Validation Loss: 0.05008751676292907\n",
      "Epoch 90/100\n",
      "Training Loss: 0.11764110396793974\n",
      "Validation Loss: 0.06938117398345682\n",
      "Epoch 91/100\n",
      "Training Loss: 0.11599402969373226\n",
      "Validation Loss: 0.08828716727105908\n",
      "Epoch 92/100\n",
      "Training Loss: 0.1169915299729262\n",
      "Validation Loss: 0.08128117187824296\n",
      "Epoch 93/100\n",
      "Training Loss: 0.09825648103160446\n",
      "Validation Loss: 0.07381829242562286\n",
      "Epoch 94/100\n",
      "Training Loss: 0.10334355336586845\n",
      "Validation Loss: 0.0828308438437009\n",
      "Epoch 95/100\n",
      "Training Loss: 0.10443070714338053\n",
      "Validation Loss: 0.08071144089835727\n",
      "Epoch 96/100\n",
      "Training Loss: 0.10826448262159828\n",
      "Validation Loss: 0.07313314186446551\n",
      "Epoch 97/100\n",
      "Training Loss: 0.11630355747826296\n",
      "Validation Loss: 0.06407981292078521\n",
      "Epoch 98/100\n",
      "Training Loss: 0.09918610292776575\n",
      "Validation Loss: 0.06214459055756818\n",
      "Epoch 99/100\n",
      "Training Loss: 0.09775378508698573\n",
      "Validation Loss: 0.050222420176924985\n",
      "Epoch 100/100\n",
      "Training Loss: 0.11349170701235796\n",
      "Validation Loss: 0.05957518417836048\n",
      "Combination 40: Avg Training Loss = 0.15211246859237704, Avg Validation Loss = 0.113634778558302\n",
      "Testing combination 41/48: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0001, 'sequence_length': 50, 'dropout_rate': 0.1}\n",
      "Truncated Train Data Shape: (70, 50, 1)\n",
      "Truncated Validation Data Shape: (10, 50, 1)\n",
      "    Trial 1/2 for combination 41/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.39370768695573766\n",
      "Validation Loss: 0.13834076460000472\n",
      "Epoch 2/100\n",
      "Training Loss: 0.24287842957430783\n",
      "Validation Loss: 0.14877824528663353\n",
      "Epoch 3/100\n",
      "Training Loss: 0.33517986785431536\n",
      "Validation Loss: 0.2150176352211907\n",
      "Epoch 4/100\n",
      "Training Loss: 0.19019996456084257\n",
      "Validation Loss: 0.17045611899175328\n",
      "Epoch 5/100\n",
      "Training Loss: 0.23539567287725138\n",
      "Validation Loss: 0.234152217847233\n",
      "Epoch 6/100\n",
      "Training Loss: 0.20205078070458315\n",
      "Validation Loss: 0.08987243699165612\n",
      "Epoch 7/100\n",
      "Training Loss: 0.18961580615394766\n",
      "Validation Loss: 0.13111176532708874\n",
      "Epoch 8/100\n",
      "Training Loss: 0.183688410223826\n",
      "Validation Loss: 0.08624446131239419\n",
      "Epoch 9/100\n",
      "Training Loss: 0.20755038344604776\n",
      "Validation Loss: 0.06080974475090987\n",
      "Epoch 10/100\n",
      "Training Loss: 0.17539806295595817\n",
      "Validation Loss: 0.0771719558291208\n",
      "Epoch 11/100\n",
      "Training Loss: 0.18977676830653398\n",
      "Validation Loss: 0.06658891918215443\n",
      "Epoch 12/100\n",
      "Training Loss: 0.19329701714538425\n",
      "Validation Loss: 0.21633333837863664\n",
      "Epoch 13/100\n",
      "Training Loss: 0.15127464438774885\n",
      "Validation Loss: 0.15203548464488262\n",
      "Epoch 14/100\n",
      "Training Loss: 0.136577282572239\n",
      "Validation Loss: 0.08403624748847716\n",
      "Epoch 15/100\n",
      "Training Loss: 0.15414691790303747\n",
      "Validation Loss: 0.07768134129970153\n",
      "Epoch 16/100\n",
      "Training Loss: 0.13595581171968277\n",
      "Validation Loss: 0.13230300629411434\n",
      "Epoch 17/100\n",
      "Training Loss: 0.15613221336217975\n",
      "Validation Loss: 0.061712252270234455\n",
      "Epoch 18/100\n",
      "Training Loss: 0.1671370469558432\n",
      "Validation Loss: 0.144956056236453\n",
      "Epoch 19/100\n",
      "Training Loss: 0.1382192695639349\n",
      "Validation Loss: 0.09071494443095632\n",
      "Epoch 20/100\n",
      "Training Loss: 0.17303925734069764\n",
      "Validation Loss: 0.10129740314298008\n",
      "Epoch 21/100\n",
      "Training Loss: 0.1504726090690696\n",
      "Validation Loss: 0.06989889423642934\n",
      "Epoch 22/100\n",
      "Training Loss: 0.16177619931831197\n",
      "Validation Loss: 0.08262539761699897\n",
      "Epoch 23/100\n",
      "Training Loss: 0.18732395729265194\n",
      "Validation Loss: 0.11443418784819888\n",
      "Epoch 24/100\n",
      "Training Loss: 0.16764403732364255\n",
      "Validation Loss: 0.140578643578892\n",
      "Epoch 25/100\n",
      "Training Loss: 0.1522523682888698\n",
      "Validation Loss: 0.06634070208405377\n",
      "Epoch 26/100\n",
      "Training Loss: 0.14254193119930758\n",
      "Validation Loss: 0.07031199000527452\n",
      "Epoch 27/100\n",
      "Training Loss: 0.17515580608480094\n",
      "Validation Loss: 0.11747433965833869\n",
      "Epoch 28/100\n",
      "Training Loss: 0.1285572956003898\n",
      "Validation Loss: 0.08215933853845553\n",
      "Epoch 29/100\n",
      "Training Loss: 0.14199042764974282\n",
      "Validation Loss: 0.08253481529618606\n",
      "Epoch 30/100\n",
      "Training Loss: 0.15152116040936806\n",
      "Validation Loss: 0.08088353000402577\n",
      "Epoch 31/100\n",
      "Training Loss: 0.1558525839908089\n",
      "Validation Loss: 0.07652929570649111\n",
      "Epoch 32/100\n",
      "Training Loss: 0.13954552871482437\n",
      "Validation Loss: 0.07660679860324537\n",
      "Epoch 33/100\n",
      "Training Loss: 0.13305789795055906\n",
      "Validation Loss: 0.07820966288683721\n",
      "Epoch 34/100\n",
      "Training Loss: 0.17489760302273735\n",
      "Validation Loss: 0.08786726441294417\n",
      "Epoch 35/100\n",
      "Training Loss: 0.1388698119810408\n",
      "Validation Loss: 0.07088307277264491\n",
      "Epoch 36/100\n",
      "Training Loss: 0.1508050387416688\n",
      "Validation Loss: 0.0723573318717873\n",
      "Epoch 37/100\n",
      "Training Loss: 0.13603652577933165\n",
      "Validation Loss: 0.059823447470739574\n",
      "Epoch 38/100\n",
      "Training Loss: 0.14025348614819674\n",
      "Validation Loss: 0.03806811672411231\n",
      "Epoch 39/100\n",
      "Training Loss: 0.13804237348471524\n",
      "Validation Loss: 0.06534757832743539\n",
      "Epoch 40/100\n",
      "Training Loss: 0.11770517319836456\n",
      "Validation Loss: 0.06529185448214492\n",
      "Epoch 41/100\n",
      "Training Loss: 0.13093550375386734\n",
      "Validation Loss: 0.0662692980930792\n",
      "Epoch 42/100\n",
      "Training Loss: 0.12674806997885868\n",
      "Validation Loss: 0.038275701776848504\n",
      "Epoch 43/100\n",
      "Training Loss: 0.1385049127811428\n",
      "Validation Loss: 0.03961597909772644\n",
      "Epoch 44/100\n",
      "Training Loss: 0.12020312120385761\n",
      "Validation Loss: 0.07542562641722309\n",
      "Epoch 45/100\n",
      "Training Loss: 0.12007055091371244\n",
      "Validation Loss: 0.05426107121896708\n",
      "Epoch 46/100\n",
      "Training Loss: 0.12497459506025209\n",
      "Validation Loss: 0.1465528639161938\n",
      "Epoch 47/100\n",
      "Training Loss: 0.11823045997147348\n",
      "Validation Loss: 0.04680678528440777\n",
      "Epoch 48/100\n",
      "Training Loss: 0.13316588514142458\n",
      "Validation Loss: 0.059452649102594055\n",
      "Epoch 49/100\n",
      "Training Loss: 0.1257396374347407\n",
      "Validation Loss: 0.0507076523713864\n",
      "Epoch 50/100\n",
      "Training Loss: 0.12070494043264939\n",
      "Validation Loss: 0.04593959223435166\n",
      "Epoch 51/100\n",
      "Training Loss: 0.10378430016986624\n",
      "Validation Loss: 0.08577212800554854\n",
      "Epoch 52/100\n",
      "Training Loss: 0.15594905459344774\n",
      "Validation Loss: 0.0744726409079715\n",
      "Epoch 53/100\n",
      "Training Loss: 0.10260897476261342\n",
      "Validation Loss: 0.051975609561758965\n",
      "Epoch 54/100\n",
      "Training Loss: 0.11585872512202948\n",
      "Validation Loss: 0.04876968490605812\n",
      "Epoch 55/100\n",
      "Training Loss: 0.10810374959109392\n",
      "Validation Loss: 0.08051946267887325\n",
      "Epoch 56/100\n",
      "Training Loss: 0.10998488281565096\n",
      "Validation Loss: 0.073692015702835\n",
      "Epoch 57/100\n",
      "Training Loss: 0.10352760464862214\n",
      "Validation Loss: 0.07677197994393445\n",
      "Epoch 58/100\n",
      "Training Loss: 0.11094727690906821\n",
      "Validation Loss: 0.03926010707060348\n",
      "Epoch 59/100\n",
      "Training Loss: 0.10731230177638071\n",
      "Validation Loss: 0.04651282828799254\n",
      "Epoch 60/100\n",
      "Training Loss: 0.10682335346982484\n",
      "Validation Loss: 0.060213788622527686\n",
      "Epoch 61/100\n",
      "Training Loss: 0.09623886082921365\n",
      "Validation Loss: 0.08604175751034863\n",
      "Epoch 62/100\n",
      "Training Loss: 0.11180695691260974\n",
      "Validation Loss: 0.05332419661039548\n",
      "Epoch 63/100\n",
      "Training Loss: 0.11538101236476546\n",
      "Validation Loss: 0.04501577066935553\n",
      "Epoch 64/100\n",
      "Training Loss: 0.09669720030067268\n",
      "Validation Loss: 0.04021951422870217\n",
      "Epoch 65/100\n",
      "Training Loss: 0.1066272792261786\n",
      "Validation Loss: 0.05445470780397631\n",
      "Epoch 66/100\n",
      "Training Loss: 0.10469041463285797\n",
      "Validation Loss: 0.044978678317375266\n",
      "Epoch 67/100\n",
      "Training Loss: 0.10128784416163392\n",
      "Validation Loss: 0.04490885627475709\n",
      "Epoch 68/100\n",
      "Training Loss: 0.10285799611415213\n",
      "Validation Loss: 0.07014830268855583\n",
      "Epoch 69/100\n",
      "Training Loss: 0.1050189357128463\n",
      "Validation Loss: 0.05961709527934458\n",
      "Epoch 70/100\n",
      "Training Loss: 0.10320402608909945\n",
      "Validation Loss: 0.047933155267616295\n",
      "Epoch 71/100\n",
      "Training Loss: 0.09867769952269806\n",
      "Validation Loss: 0.04208350240891142\n",
      "Epoch 72/100\n",
      "Training Loss: 0.11049073915619485\n",
      "Validation Loss: 0.07091171050024593\n",
      "Epoch 73/100\n",
      "Training Loss: 0.11492094016498001\n",
      "Validation Loss: 0.04253884036087675\n",
      "Epoch 74/100\n",
      "Training Loss: 0.09570560554234295\n",
      "Validation Loss: 0.048976824038039705\n",
      "Epoch 75/100\n",
      "Training Loss: 0.11284686031351117\n",
      "Validation Loss: 0.0759983731854992\n",
      "Epoch 76/100\n",
      "Training Loss: 0.10507084757355592\n",
      "Validation Loss: 0.04421972248867402\n",
      "Epoch 77/100\n",
      "Training Loss: 0.10273453632754927\n",
      "Validation Loss: 0.03758415735206102\n",
      "Epoch 78/100\n",
      "Training Loss: 0.09542168520402229\n",
      "Validation Loss: 0.05905627024347019\n",
      "Epoch 79/100\n",
      "Training Loss: 0.10198705501866673\n",
      "Validation Loss: 0.03255983034018288\n",
      "Epoch 80/100\n",
      "Training Loss: 0.09547423733679357\n",
      "Validation Loss: 0.040784860246291424\n",
      "Epoch 81/100\n",
      "Training Loss: 0.09953602376706773\n",
      "Validation Loss: 0.03203891757614234\n",
      "Epoch 82/100\n",
      "Training Loss: 0.10282343497456078\n",
      "Validation Loss: 0.03861347876690422\n",
      "Epoch 83/100\n",
      "Training Loss: 0.09801431779735692\n",
      "Validation Loss: 0.05784487835630179\n",
      "Epoch 84/100\n",
      "Training Loss: 0.10118991668111292\n",
      "Validation Loss: 0.06671778265012276\n",
      "Epoch 85/100\n",
      "Training Loss: 0.09063890996442647\n",
      "Validation Loss: 0.03990664409939807\n",
      "Epoch 86/100\n",
      "Training Loss: 0.1020329811021485\n",
      "Validation Loss: 0.06249876116287225\n",
      "Epoch 87/100\n",
      "Training Loss: 0.0877564529414085\n",
      "Validation Loss: 0.03968692178777124\n",
      "Epoch 88/100\n",
      "Training Loss: 0.09393672015222265\n",
      "Validation Loss: 0.023156762422250902\n",
      "Epoch 89/100\n",
      "Training Loss: 0.09377751169658644\n",
      "Validation Loss: 0.04580093197739675\n",
      "Epoch 90/100\n",
      "Training Loss: 0.08841029744261136\n",
      "Validation Loss: 0.07461300288939265\n",
      "Epoch 91/100\n",
      "Training Loss: 0.09829196248944982\n",
      "Validation Loss: 0.04674211984570094\n",
      "Epoch 92/100\n",
      "Training Loss: 0.08722068503744469\n",
      "Validation Loss: 0.061913150090388266\n",
      "Epoch 93/100\n",
      "Training Loss: 0.09037315670553732\n",
      "Validation Loss: 0.056359816328420985\n",
      "Epoch 94/100\n",
      "Training Loss: 0.10593161692414976\n",
      "Validation Loss: 0.05904891921882065\n",
      "Epoch 95/100\n",
      "Training Loss: 0.09488511143647993\n",
      "Validation Loss: 0.04733124262687456\n",
      "Epoch 96/100\n",
      "Training Loss: 0.08733350218518417\n",
      "Validation Loss: 0.043087400134846056\n",
      "Epoch 97/100\n",
      "Training Loss: 0.0940070439373859\n",
      "Validation Loss: 0.09599552617640657\n",
      "Epoch 98/100\n",
      "Training Loss: 0.09304064089645118\n",
      "Validation Loss: 0.04211370406850536\n",
      "Epoch 99/100\n",
      "Training Loss: 0.09316909934512661\n",
      "Validation Loss: 0.044164590055863616\n",
      "Epoch 100/100\n",
      "Training Loss: 0.08603536097647015\n",
      "Validation Loss: 0.03900125432575916\n",
      "    Trial 2/2 for combination 41/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.3967142438834041\n",
      "Validation Loss: 0.2184416393773024\n",
      "Epoch 2/100\n",
      "Training Loss: 0.34375897097912533\n",
      "Validation Loss: 0.2770267652285598\n",
      "Epoch 3/100\n",
      "Training Loss: 0.2731047296995937\n",
      "Validation Loss: 0.09515560261763899\n",
      "Epoch 4/100\n",
      "Training Loss: 0.27350586312450265\n",
      "Validation Loss: 0.16284184288237558\n",
      "Epoch 5/100\n",
      "Training Loss: 0.24907957820273718\n",
      "Validation Loss: 0.19096762137211573\n",
      "Epoch 6/100\n",
      "Training Loss: 0.1662476461095976\n",
      "Validation Loss: 0.15123145247894226\n",
      "Epoch 7/100\n",
      "Training Loss: 0.23160136045036436\n",
      "Validation Loss: 0.06713578598075534\n",
      "Epoch 8/100\n",
      "Training Loss: 0.19115365128244705\n",
      "Validation Loss: 0.20944678639715844\n",
      "Epoch 9/100\n",
      "Training Loss: 0.1822986338530154\n",
      "Validation Loss: 0.09920917826684236\n",
      "Epoch 10/100\n",
      "Training Loss: 0.1683069854896081\n",
      "Validation Loss: 0.11190456676468923\n",
      "Epoch 11/100\n",
      "Training Loss: 0.17587507228772134\n",
      "Validation Loss: 0.07707790504391868\n",
      "Epoch 12/100\n",
      "Training Loss: 0.16649301433279087\n",
      "Validation Loss: 0.10888388765850474\n",
      "Epoch 13/100\n",
      "Training Loss: 0.16399897630583676\n",
      "Validation Loss: 0.17334578178981236\n",
      "Epoch 14/100\n",
      "Training Loss: 0.17890385402647033\n",
      "Validation Loss: 0.1460988032021627\n",
      "Epoch 15/100\n",
      "Training Loss: 0.19553647124550017\n",
      "Validation Loss: 0.16991711925759265\n",
      "Epoch 16/100\n",
      "Training Loss: 0.1612653387113437\n",
      "Validation Loss: 0.15187208415445563\n",
      "Epoch 17/100\n",
      "Training Loss: 0.15178425604851123\n",
      "Validation Loss: 0.17801396950501944\n",
      "Epoch 18/100\n",
      "Training Loss: 0.16075228299086863\n",
      "Validation Loss: 0.11588001587005263\n",
      "Epoch 19/100\n",
      "Training Loss: 0.15972563297543207\n",
      "Validation Loss: 0.061043213892210145\n",
      "Epoch 20/100\n",
      "Training Loss: 0.1788210611729482\n",
      "Validation Loss: 0.08258225217381017\n",
      "Epoch 21/100\n",
      "Training Loss: 0.1513877988500213\n",
      "Validation Loss: 0.06455782405528572\n",
      "Epoch 22/100\n",
      "Training Loss: 0.1647123416363573\n",
      "Validation Loss: 0.10911660742718583\n",
      "Epoch 23/100\n",
      "Training Loss: 0.13795696575458724\n",
      "Validation Loss: 0.05731340170104138\n",
      "Epoch 24/100\n",
      "Training Loss: 0.1646473771005909\n",
      "Validation Loss: 0.0712783427585355\n",
      "Epoch 25/100\n",
      "Training Loss: 0.15330935553406141\n",
      "Validation Loss: 0.12171802781334481\n",
      "Epoch 26/100\n",
      "Training Loss: 0.14299620444217326\n",
      "Validation Loss: 0.08060437795541844\n",
      "Epoch 27/100\n",
      "Training Loss: 0.16277963127434247\n",
      "Validation Loss: 0.12149024875727825\n",
      "Epoch 28/100\n",
      "Training Loss: 0.1331881859580104\n",
      "Validation Loss: 0.05571663384836234\n",
      "Epoch 29/100\n",
      "Training Loss: 0.14600086785049046\n",
      "Validation Loss: 0.07566804494243747\n",
      "Epoch 30/100\n",
      "Training Loss: 0.1336545114125782\n",
      "Validation Loss: 0.062378420754515426\n",
      "Epoch 31/100\n",
      "Training Loss: 0.14869418995967415\n",
      "Validation Loss: 0.08588998361254704\n",
      "Epoch 32/100\n",
      "Training Loss: 0.1392435495619054\n",
      "Validation Loss: 0.09361995078839393\n",
      "Epoch 33/100\n",
      "Training Loss: 0.15478522886459398\n",
      "Validation Loss: 0.07456212483148661\n",
      "Epoch 34/100\n",
      "Training Loss: 0.14498226154955046\n",
      "Validation Loss: 0.11122248317885015\n",
      "Epoch 35/100\n",
      "Training Loss: 0.16543293855455235\n",
      "Validation Loss: 0.10184259762561812\n",
      "Epoch 36/100\n",
      "Training Loss: 0.1413010250510136\n",
      "Validation Loss: 0.04044047078476561\n",
      "Epoch 37/100\n",
      "Training Loss: 0.14946673013438155\n",
      "Validation Loss: 0.07604493278954108\n",
      "Epoch 38/100\n",
      "Training Loss: 0.13618618683717307\n",
      "Validation Loss: 0.08445341237838329\n",
      "Epoch 39/100\n",
      "Training Loss: 0.13778646165050762\n",
      "Validation Loss: 0.054920131021018594\n",
      "Epoch 40/100\n",
      "Training Loss: 0.14793150159085955\n",
      "Validation Loss: 0.04521165969685798\n",
      "Epoch 41/100\n",
      "Training Loss: 0.11761842638987229\n",
      "Validation Loss: 0.04606419374835473\n",
      "Epoch 42/100\n",
      "Training Loss: 0.12888125804289696\n",
      "Validation Loss: 0.06682502804484672\n",
      "Epoch 43/100\n",
      "Training Loss: 0.12367785522380555\n",
      "Validation Loss: 0.057693365495867156\n",
      "Epoch 44/100\n",
      "Training Loss: 0.14307318701954047\n",
      "Validation Loss: 0.03188851970929329\n",
      "Epoch 45/100\n",
      "Training Loss: 0.12285643000328622\n",
      "Validation Loss: 0.06275454793373472\n",
      "Epoch 46/100\n",
      "Training Loss: 0.136695776618528\n",
      "Validation Loss: 0.0606175657415858\n",
      "Epoch 47/100\n",
      "Training Loss: 0.13608054754731114\n",
      "Validation Loss: 0.08061037305014702\n",
      "Epoch 48/100\n",
      "Training Loss: 0.11971251591269161\n",
      "Validation Loss: 0.11747680294289524\n",
      "Epoch 49/100\n",
      "Training Loss: 0.14738070532140687\n",
      "Validation Loss: 0.0941070352482561\n",
      "Epoch 50/100\n",
      "Training Loss: 0.13025306023886754\n",
      "Validation Loss: 0.05608154092589555\n",
      "Epoch 51/100\n",
      "Training Loss: 0.11699998668805178\n",
      "Validation Loss: 0.05265174109290521\n",
      "Epoch 52/100\n",
      "Training Loss: 0.12973559733825107\n",
      "Validation Loss: 0.09103693862556561\n",
      "Epoch 53/100\n",
      "Training Loss: 0.11833801750752307\n",
      "Validation Loss: 0.08896337819425838\n",
      "Epoch 54/100\n",
      "Training Loss: 0.1310931628693291\n",
      "Validation Loss: 0.039785964406493554\n",
      "Epoch 55/100\n",
      "Training Loss: 0.12659677131057734\n",
      "Validation Loss: 0.0673957342475079\n",
      "Epoch 56/100\n",
      "Training Loss: 0.12741527047226112\n",
      "Validation Loss: 0.0752026301127261\n",
      "Epoch 57/100\n",
      "Training Loss: 0.1374717746004774\n",
      "Validation Loss: 0.0966718847310835\n",
      "Epoch 58/100\n",
      "Training Loss: 0.13132357101761707\n",
      "Validation Loss: 0.10793843184403837\n",
      "Epoch 59/100\n",
      "Training Loss: 0.11567087625427337\n",
      "Validation Loss: 0.13420708223202732\n",
      "Epoch 60/100\n",
      "Training Loss: 0.12799653263283062\n",
      "Validation Loss: 0.14904200656443967\n",
      "Epoch 61/100\n",
      "Training Loss: 0.11766021778708124\n",
      "Validation Loss: 0.09318669399257229\n",
      "Epoch 62/100\n",
      "Training Loss: 0.13118496121096695\n",
      "Validation Loss: 0.0931268252046892\n",
      "Epoch 63/100\n",
      "Training Loss: 0.13259657675040454\n",
      "Validation Loss: 0.07923063872711084\n",
      "Epoch 64/100\n",
      "Training Loss: 0.11855296973800125\n",
      "Validation Loss: 0.05610545350579103\n",
      "Epoch 65/100\n",
      "Training Loss: 0.12243157737736686\n",
      "Validation Loss: 0.0743446286309157\n",
      "Epoch 66/100\n",
      "Training Loss: 0.11718555154564181\n",
      "Validation Loss: 0.08615542223106011\n",
      "Epoch 67/100\n",
      "Training Loss: 0.11478269851580793\n",
      "Validation Loss: 0.08176303033132762\n",
      "Epoch 68/100\n",
      "Training Loss: 0.12319850324302646\n",
      "Validation Loss: 0.09732096634407733\n",
      "Epoch 69/100\n",
      "Training Loss: 0.1176469267888068\n",
      "Validation Loss: 0.11842734265693951\n",
      "Epoch 70/100\n",
      "Training Loss: 0.10470931534014637\n",
      "Validation Loss: 0.06805185535766187\n",
      "Epoch 71/100\n",
      "Training Loss: 0.11719879084599566\n",
      "Validation Loss: 0.059465920494612946\n",
      "Epoch 72/100\n",
      "Training Loss: 0.10269571951525772\n",
      "Validation Loss: 0.15679693163553557\n",
      "Epoch 73/100\n",
      "Training Loss: 0.11681497473924189\n",
      "Validation Loss: 0.10639985646596233\n",
      "Epoch 74/100\n",
      "Training Loss: 0.11976886014129244\n",
      "Validation Loss: 0.04103534026964543\n",
      "Epoch 75/100\n",
      "Training Loss: 0.12745792894139488\n",
      "Validation Loss: 0.0824148468054732\n",
      "Epoch 76/100\n",
      "Training Loss: 0.11847632442003206\n",
      "Validation Loss: 0.03267113767210349\n",
      "Epoch 77/100\n",
      "Training Loss: 0.11619092645210599\n",
      "Validation Loss: 0.04125105667318461\n",
      "Epoch 78/100\n",
      "Training Loss: 0.13269462390788364\n",
      "Validation Loss: 0.08300581630783503\n",
      "Epoch 79/100\n",
      "Training Loss: 0.12289985389632499\n",
      "Validation Loss: 0.0919322512811153\n",
      "Epoch 80/100\n",
      "Training Loss: 0.11665902164985335\n",
      "Validation Loss: 0.08962548192655317\n",
      "Epoch 81/100\n",
      "Training Loss: 0.10334514461160349\n",
      "Validation Loss: 0.03883779597109031\n",
      "Epoch 82/100\n",
      "Training Loss: 0.1105495093893293\n",
      "Validation Loss: 0.05655734174462218\n",
      "Epoch 83/100\n",
      "Training Loss: 0.10368428405705019\n",
      "Validation Loss: 0.0392387498427613\n",
      "Epoch 84/100\n",
      "Training Loss: 0.09893268947255361\n",
      "Validation Loss: 0.05486081039296488\n",
      "Epoch 85/100\n",
      "Training Loss: 0.10398091017169071\n",
      "Validation Loss: 0.07783570866400494\n",
      "Epoch 86/100\n",
      "Training Loss: 0.10052109536613724\n",
      "Validation Loss: 0.09895868104795326\n",
      "Epoch 87/100\n",
      "Training Loss: 0.11155148129859725\n",
      "Validation Loss: 0.05819875868604529\n",
      "Epoch 88/100\n",
      "Training Loss: 0.10961402307867618\n",
      "Validation Loss: 0.04498346171851467\n",
      "Epoch 89/100\n",
      "Training Loss: 0.09464183765312308\n",
      "Validation Loss: 0.054092077635334\n",
      "Epoch 90/100\n",
      "Training Loss: 0.10153456646700038\n",
      "Validation Loss: 0.09220833216747498\n",
      "Epoch 91/100\n",
      "Training Loss: 0.09798770354959929\n",
      "Validation Loss: 0.05909669289806443\n",
      "Epoch 92/100\n",
      "Training Loss: 0.0946011307295015\n",
      "Validation Loss: 0.049005622817612324\n",
      "Epoch 93/100\n",
      "Training Loss: 0.10722398294867187\n",
      "Validation Loss: 0.12220112217861785\n",
      "Epoch 94/100\n",
      "Training Loss: 0.09883078292772464\n",
      "Validation Loss: 0.06514815905693377\n",
      "Epoch 95/100\n",
      "Training Loss: 0.09611231059954091\n",
      "Validation Loss: 0.07471681288940148\n",
      "Epoch 96/100\n",
      "Training Loss: 0.10440739215243955\n",
      "Validation Loss: 0.08297102376363397\n",
      "Epoch 97/100\n",
      "Training Loss: 0.11617165739799018\n",
      "Validation Loss: 0.13971703657955944\n",
      "Epoch 98/100\n",
      "Training Loss: 0.09907277133975266\n",
      "Validation Loss: 0.13181656211756568\n",
      "Epoch 99/100\n",
      "Training Loss: 0.08836907648497085\n",
      "Validation Loss: 0.05468172313910078\n",
      "Epoch 100/100\n",
      "Training Loss: 0.10006442683041301\n",
      "Validation Loss: 0.04681078275927637\n",
      "Combination 41: Avg Training Loss = 0.13705745924191126, Avg Validation Loss = 0.0824224473266946\n",
      "Testing combination 42/48: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0001, 'sequence_length': 50, 'dropout_rate': 0.2}\n",
      "Truncated Train Data Shape: (70, 50, 1)\n",
      "Truncated Validation Data Shape: (10, 50, 1)\n",
      "    Trial 1/2 for combination 42/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.34326485277812885\n",
      "Validation Loss: 0.1528779563570169\n",
      "Epoch 2/100\n",
      "Training Loss: 0.29009918724087047\n",
      "Validation Loss: 0.21800218654305198\n",
      "Epoch 3/100\n",
      "Training Loss: 0.2122677185325527\n",
      "Validation Loss: 0.183828914222178\n",
      "Epoch 4/100\n",
      "Training Loss: 0.20957272278091635\n",
      "Validation Loss: 0.21920419227711752\n",
      "Epoch 5/100\n",
      "Training Loss: 0.2107393201042533\n",
      "Validation Loss: 0.1408204683376698\n",
      "Epoch 6/100\n",
      "Training Loss: 0.18715053791605676\n",
      "Validation Loss: 0.1982414784900855\n",
      "Epoch 7/100\n",
      "Training Loss: 0.20813337498468348\n",
      "Validation Loss: 0.1036187451139153\n",
      "Epoch 8/100\n",
      "Training Loss: 0.1914030532954874\n",
      "Validation Loss: 0.16785379754269694\n",
      "Epoch 9/100\n",
      "Training Loss: 0.18801502019853014\n",
      "Validation Loss: 0.10012603643900501\n",
      "Epoch 10/100\n",
      "Training Loss: 0.18013385191361403\n",
      "Validation Loss: 0.11504908603582116\n",
      "Epoch 11/100\n",
      "Training Loss: 0.14626589247673014\n",
      "Validation Loss: 0.12193770515585511\n",
      "Epoch 12/100\n",
      "Training Loss: 0.15471980350374612\n",
      "Validation Loss: 0.13112632257478743\n",
      "Epoch 13/100\n",
      "Training Loss: 0.15064764450424792\n",
      "Validation Loss: 0.12498762116375048\n",
      "Epoch 14/100\n",
      "Training Loss: 0.1703178651146583\n",
      "Validation Loss: 0.13531721399084692\n",
      "Epoch 15/100\n",
      "Training Loss: 0.16036146525255227\n",
      "Validation Loss: 0.11432583006495838\n",
      "Epoch 16/100\n",
      "Training Loss: 0.16682946299527798\n",
      "Validation Loss: 0.10727728492876847\n",
      "Epoch 17/100\n",
      "Training Loss: 0.16433449362361227\n",
      "Validation Loss: 0.10106734799959198\n",
      "Epoch 18/100\n",
      "Training Loss: 0.16224217672594765\n",
      "Validation Loss: 0.10285887850410336\n",
      "Epoch 19/100\n",
      "Training Loss: 0.16260096893515008\n",
      "Validation Loss: 0.09417906235956489\n",
      "Epoch 20/100\n",
      "Training Loss: 0.1550752921385096\n",
      "Validation Loss: 0.06788934663858774\n",
      "Epoch 21/100\n",
      "Training Loss: 0.14554466402560587\n",
      "Validation Loss: 0.14611223225011133\n",
      "Epoch 22/100\n",
      "Training Loss: 0.16257949435758942\n",
      "Validation Loss: 0.07032862130833303\n",
      "Epoch 23/100\n",
      "Training Loss: 0.1377596224370729\n",
      "Validation Loss: 0.0986042181093706\n",
      "Epoch 24/100\n",
      "Training Loss: 0.1437626933948035\n",
      "Validation Loss: 0.11999993759977759\n",
      "Epoch 25/100\n",
      "Training Loss: 0.1503090365448047\n",
      "Validation Loss: 0.09877924220524939\n",
      "Epoch 26/100\n",
      "Training Loss: 0.15626270036879228\n",
      "Validation Loss: 0.1313604686773619\n",
      "Epoch 27/100\n",
      "Training Loss: 0.1477904451708499\n",
      "Validation Loss: 0.10385489267165533\n",
      "Epoch 28/100\n",
      "Training Loss: 0.1242403008647377\n",
      "Validation Loss: 0.07644522751927219\n",
      "Epoch 29/100\n",
      "Training Loss: 0.15105319773707482\n",
      "Validation Loss: 0.0746747292142106\n",
      "Epoch 30/100\n",
      "Training Loss: 0.15168500086467523\n",
      "Validation Loss: 0.07247963033628288\n",
      "Epoch 31/100\n",
      "Training Loss: 0.16714986642977686\n",
      "Validation Loss: 0.0768147931373194\n",
      "Epoch 32/100\n",
      "Training Loss: 0.12717581893041274\n",
      "Validation Loss: 0.14967156546837718\n",
      "Epoch 33/100\n",
      "Training Loss: 0.14391942043400427\n",
      "Validation Loss: 0.10902257677898489\n",
      "Epoch 34/100\n",
      "Training Loss: 0.13843886296880895\n",
      "Validation Loss: 0.0957429048529329\n",
      "Epoch 35/100\n",
      "Training Loss: 0.1486596648308543\n",
      "Validation Loss: 0.0639907591051958\n",
      "Epoch 36/100\n",
      "Training Loss: 0.13531003386256385\n",
      "Validation Loss: 0.10248610316699444\n",
      "Epoch 37/100\n",
      "Training Loss: 0.1348944387814497\n",
      "Validation Loss: 0.076840726218887\n",
      "Epoch 38/100\n",
      "Training Loss: 0.1427089066836218\n",
      "Validation Loss: 0.04158711863168339\n",
      "Epoch 39/100\n",
      "Training Loss: 0.1336458292334823\n",
      "Validation Loss: 0.05890144667122017\n",
      "Epoch 40/100\n",
      "Training Loss: 0.14076920155086492\n",
      "Validation Loss: 0.048290179241013594\n",
      "Epoch 41/100\n",
      "Training Loss: 0.1345738159838631\n",
      "Validation Loss: 0.16303894914119565\n",
      "Epoch 42/100\n",
      "Training Loss: 0.12306262618693128\n",
      "Validation Loss: 0.05403615262763363\n",
      "Epoch 43/100\n",
      "Training Loss: 0.12121115290935854\n",
      "Validation Loss: 0.09255585881660625\n",
      "Epoch 44/100\n",
      "Training Loss: 0.1253747653168623\n",
      "Validation Loss: 0.12395719025915361\n",
      "Epoch 45/100\n",
      "Training Loss: 0.11503339816614482\n",
      "Validation Loss: 0.0957081022575319\n",
      "Epoch 46/100\n",
      "Training Loss: 0.1266603077378791\n",
      "Validation Loss: 0.04650701710658114\n",
      "Epoch 47/100\n",
      "Training Loss: 0.12854485225710452\n",
      "Validation Loss: 0.049705303576190366\n",
      "Epoch 48/100\n",
      "Training Loss: 0.11768465582197234\n",
      "Validation Loss: 0.09332447804026975\n",
      "Epoch 49/100\n",
      "Training Loss: 0.11304014817791001\n",
      "Validation Loss: 0.06968098073585151\n",
      "Epoch 50/100\n",
      "Training Loss: 0.1285098951405227\n",
      "Validation Loss: 0.11965052751643149\n",
      "Epoch 51/100\n",
      "Training Loss: 0.12969593629658033\n",
      "Validation Loss: 0.14611988104900386\n",
      "Epoch 52/100\n",
      "Training Loss: 0.11012327003010025\n",
      "Validation Loss: 0.10086545456894969\n",
      "Epoch 53/100\n",
      "Training Loss: 0.10898404499301215\n",
      "Validation Loss: 0.14105905409899172\n",
      "Epoch 54/100\n",
      "Training Loss: 0.1188146825612619\n",
      "Validation Loss: 0.1108547860200585\n",
      "Epoch 55/100\n",
      "Training Loss: 0.13087293985421414\n",
      "Validation Loss: 0.07073939502237982\n",
      "Epoch 56/100\n",
      "Training Loss: 0.12802625723645666\n",
      "Validation Loss: 0.12561377812178734\n",
      "Epoch 57/100\n",
      "Training Loss: 0.1038140699460887\n",
      "Validation Loss: 0.1417592600377421\n",
      "Epoch 58/100\n",
      "Training Loss: 0.12737949007596455\n",
      "Validation Loss: 0.10191077156711843\n",
      "Epoch 59/100\n",
      "Training Loss: 0.13024774846342674\n",
      "Validation Loss: 0.11594853800212221\n",
      "Epoch 60/100\n",
      "Training Loss: 0.11888972927875772\n",
      "Validation Loss: 0.16459766696529837\n",
      "Epoch 61/100\n",
      "Training Loss: 0.1337857139810089\n",
      "Validation Loss: 0.08948460260449884\n",
      "Epoch 62/100\n",
      "Training Loss: 0.10779293976084466\n",
      "Validation Loss: 0.09230675251238558\n",
      "Epoch 63/100\n",
      "Training Loss: 0.11370576349459378\n",
      "Validation Loss: 0.1257029769059114\n",
      "Epoch 64/100\n",
      "Training Loss: 0.11698404201660052\n",
      "Validation Loss: 0.08921623924075128\n",
      "Epoch 65/100\n",
      "Training Loss: 0.11526091139875655\n",
      "Validation Loss: 0.12443029666192176\n",
      "Epoch 66/100\n",
      "Training Loss: 0.11693572662355259\n",
      "Validation Loss: 0.1251192439583747\n",
      "Epoch 67/100\n",
      "Training Loss: 0.10837526387854962\n",
      "Validation Loss: 0.13501411306742384\n",
      "Epoch 68/100\n",
      "Training Loss: 0.1086367730338676\n",
      "Validation Loss: 0.11877539060771758\n",
      "Epoch 69/100\n",
      "Training Loss: 0.100798024874767\n",
      "Validation Loss: 0.12692342435548915\n",
      "Epoch 70/100\n",
      "Training Loss: 0.11070837222165988\n",
      "Validation Loss: 0.08296769010817495\n",
      "Epoch 71/100\n",
      "Training Loss: 0.10811606998956781\n",
      "Validation Loss: 0.11311417969679285\n",
      "Epoch 72/100\n",
      "Training Loss: 0.11276217314823926\n",
      "Validation Loss: 0.11781630633679993\n",
      "Epoch 73/100\n",
      "Training Loss: 0.10563584414288073\n",
      "Validation Loss: 0.06359818209602475\n",
      "Epoch 74/100\n",
      "Training Loss: 0.11440946003881375\n",
      "Validation Loss: 0.08960912209216851\n",
      "Epoch 75/100\n",
      "Training Loss: 0.10713247337145\n",
      "Validation Loss: 0.1682691246518245\n",
      "Epoch 76/100\n",
      "Training Loss: 0.09249383858255655\n",
      "Validation Loss: 0.1644984953593974\n",
      "Epoch 77/100\n",
      "Training Loss: 0.10292745549031848\n",
      "Validation Loss: 0.13375030430651752\n",
      "Epoch 78/100\n",
      "Training Loss: 0.1084803568659112\n",
      "Validation Loss: 0.1122861838441215\n",
      "Epoch 79/100\n",
      "Training Loss: 0.09349834177079992\n",
      "Validation Loss: 0.10268230644021652\n",
      "Epoch 80/100\n",
      "Training Loss: 0.090600483278703\n",
      "Validation Loss: 0.20268601954623078\n",
      "Epoch 81/100\n",
      "Training Loss: 0.10109774893263758\n",
      "Validation Loss: 0.06431069005577181\n",
      "Epoch 82/100\n",
      "Training Loss: 0.08859157813636634\n",
      "Validation Loss: 0.10198179955690465\n",
      "Epoch 83/100\n",
      "Training Loss: 0.0994224254710381\n",
      "Validation Loss: 0.1320181521610303\n",
      "Epoch 84/100\n",
      "Training Loss: 0.09457594205497932\n",
      "Validation Loss: 0.1889774419387264\n",
      "Epoch 85/100\n",
      "Training Loss: 0.10064381125197383\n",
      "Validation Loss: 0.046768596088123254\n",
      "Epoch 86/100\n",
      "Training Loss: 0.10048663086897472\n",
      "Validation Loss: 0.11132264350860588\n",
      "Epoch 87/100\n",
      "Training Loss: 0.09205039689372024\n",
      "Validation Loss: 0.09391760563975629\n",
      "Epoch 88/100\n",
      "Training Loss: 0.10256899697063263\n",
      "Validation Loss: 0.07652764030926776\n",
      "Epoch 89/100\n",
      "Training Loss: 0.0908087872265429\n",
      "Validation Loss: 0.05837527463461835\n",
      "Epoch 90/100\n",
      "Training Loss: 0.09273423047547531\n",
      "Validation Loss: 0.15987532792438583\n",
      "Epoch 91/100\n",
      "Training Loss: 0.09101407104902791\n",
      "Validation Loss: 0.05805563545205281\n",
      "Epoch 92/100\n",
      "Training Loss: 0.09264183602076882\n",
      "Validation Loss: 0.08885757516920825\n",
      "Epoch 93/100\n",
      "Training Loss: 0.08805057721258623\n",
      "Validation Loss: 0.1377448421936998\n",
      "Epoch 94/100\n",
      "Training Loss: 0.08506317892673276\n",
      "Validation Loss: 0.14793044752753093\n",
      "Epoch 95/100\n",
      "Training Loss: 0.0873184858222415\n",
      "Validation Loss: 0.08568774086410644\n",
      "Epoch 96/100\n",
      "Training Loss: 0.0904633795683922\n",
      "Validation Loss: 0.11766166759146209\n",
      "Epoch 97/100\n",
      "Training Loss: 0.08449425571150562\n",
      "Validation Loss: 0.17066799487540435\n",
      "Epoch 98/100\n",
      "Training Loss: 0.08851304806043296\n",
      "Validation Loss: 0.14948883457630938\n",
      "Epoch 99/100\n",
      "Training Loss: 0.09221771280345792\n",
      "Validation Loss: 0.06520869985883339\n",
      "Epoch 100/100\n",
      "Training Loss: 0.07993478845509118\n",
      "Validation Loss: 0.1487741818379718\n",
      "    Trial 2/2 for combination 42/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.38389728322269173\n",
      "Validation Loss: 0.25395079375874946\n",
      "Epoch 2/100\n",
      "Training Loss: 0.26907893910705355\n",
      "Validation Loss: 0.12913903340191374\n",
      "Epoch 3/100\n",
      "Training Loss: 0.22930774406952387\n",
      "Validation Loss: 0.21449898996230848\n",
      "Epoch 4/100\n",
      "Training Loss: 0.23367109589626078\n",
      "Validation Loss: 0.16632323915350924\n",
      "Epoch 5/100\n",
      "Training Loss: 0.18184271804715393\n",
      "Validation Loss: 0.1839693469408453\n",
      "Epoch 6/100\n",
      "Training Loss: 0.15702022681818842\n",
      "Validation Loss: 0.17881428205472208\n",
      "Epoch 7/100\n",
      "Training Loss: 0.19375519335886185\n",
      "Validation Loss: 0.07863059073578661\n",
      "Epoch 8/100\n",
      "Training Loss: 0.18154562888615713\n",
      "Validation Loss: 0.0871139592394736\n",
      "Epoch 9/100\n",
      "Training Loss: 0.16660240380351388\n",
      "Validation Loss: 0.09583216969381395\n",
      "Epoch 10/100\n",
      "Training Loss: 0.15007426573916474\n",
      "Validation Loss: 0.10443567991401839\n",
      "Epoch 11/100\n",
      "Training Loss: 0.16906393377832354\n",
      "Validation Loss: 0.05620301083920953\n",
      "Epoch 12/100\n",
      "Training Loss: 0.15443674752686948\n",
      "Validation Loss: 0.17604132371115497\n",
      "Epoch 13/100\n",
      "Training Loss: 0.1616813788773247\n",
      "Validation Loss: 0.10160346611356921\n",
      "Epoch 14/100\n",
      "Training Loss: 0.1563914011852216\n",
      "Validation Loss: 0.11951455814384489\n",
      "Epoch 15/100\n",
      "Training Loss: 0.1545113075436992\n",
      "Validation Loss: 0.049564925514195175\n",
      "Epoch 16/100\n",
      "Training Loss: 0.13178497411386883\n",
      "Validation Loss: 0.10486919660677445\n",
      "Epoch 17/100\n",
      "Training Loss: 0.15987047235411325\n",
      "Validation Loss: 0.12657274249853975\n",
      "Epoch 18/100\n",
      "Training Loss: 0.13058740193068633\n",
      "Validation Loss: 0.11341322305473774\n",
      "Epoch 19/100\n",
      "Training Loss: 0.15029096809658452\n",
      "Validation Loss: 0.1589305766888557\n",
      "Epoch 20/100\n",
      "Training Loss: 0.14977377221566815\n",
      "Validation Loss: 0.07884813469962722\n",
      "Epoch 21/100\n",
      "Training Loss: 0.13216511116487092\n",
      "Validation Loss: 0.06779222924192439\n",
      "Epoch 22/100\n",
      "Training Loss: 0.1340321809839457\n",
      "Validation Loss: 0.07079139073945269\n",
      "Epoch 23/100\n",
      "Training Loss: 0.1562291373924425\n",
      "Validation Loss: 0.08697078503685184\n",
      "Epoch 24/100\n",
      "Training Loss: 0.13110343278277017\n",
      "Validation Loss: 0.06682657074170714\n",
      "Epoch 25/100\n",
      "Training Loss: 0.12880169948047077\n",
      "Validation Loss: 0.07119488884477176\n",
      "Epoch 26/100\n",
      "Training Loss: 0.14114698008380513\n",
      "Validation Loss: 0.06468068713091238\n",
      "Epoch 27/100\n",
      "Training Loss: 0.14737224328520623\n",
      "Validation Loss: 0.09801235465674457\n",
      "Epoch 28/100\n",
      "Training Loss: 0.13908844895682354\n",
      "Validation Loss: 0.07850811937529978\n",
      "Epoch 29/100\n",
      "Training Loss: 0.12986816334419526\n",
      "Validation Loss: 0.0897160468195613\n",
      "Epoch 30/100\n",
      "Training Loss: 0.13540749882879008\n",
      "Validation Loss: 0.15021340703179464\n",
      "Epoch 31/100\n",
      "Training Loss: 0.13282714476321156\n",
      "Validation Loss: 0.11287573537053493\n",
      "Epoch 32/100\n",
      "Training Loss: 0.13000576273870376\n",
      "Validation Loss: 0.10321356857634896\n",
      "Epoch 33/100\n",
      "Training Loss: 0.11796260350609973\n",
      "Validation Loss: 0.08335545277204906\n",
      "Epoch 34/100\n",
      "Training Loss: 0.13140054857466196\n",
      "Validation Loss: 0.05355116240740001\n",
      "Epoch 35/100\n",
      "Training Loss: 0.13863456731138002\n",
      "Validation Loss: 0.06480342599091338\n",
      "Epoch 36/100\n",
      "Training Loss: 0.14427190329178097\n",
      "Validation Loss: 0.08763773021236906\n",
      "Epoch 37/100\n",
      "Training Loss: 0.13571882236895183\n",
      "Validation Loss: 0.10441984374191586\n",
      "Epoch 38/100\n",
      "Training Loss: 0.11783002320296883\n",
      "Validation Loss: 0.05133790592629108\n",
      "Epoch 39/100\n",
      "Training Loss: 0.12580769991915172\n",
      "Validation Loss: 0.12533654563583213\n",
      "Epoch 40/100\n",
      "Training Loss: 0.12700552512816263\n",
      "Validation Loss: 0.0489909964413331\n",
      "Epoch 41/100\n",
      "Training Loss: 0.12443580137590438\n",
      "Validation Loss: 0.10012976054122032\n",
      "Epoch 42/100\n",
      "Training Loss: 0.11683397512739953\n",
      "Validation Loss: 0.03602470266338986\n",
      "Epoch 43/100\n",
      "Training Loss: 0.12193001805693868\n",
      "Validation Loss: 0.08417909601194175\n",
      "Epoch 44/100\n",
      "Training Loss: 0.11303416771989734\n",
      "Validation Loss: 0.11334187905362492\n",
      "Epoch 45/100\n",
      "Training Loss: 0.12100099614366651\n",
      "Validation Loss: 0.0808823948078812\n",
      "Epoch 46/100\n",
      "Training Loss: 0.127803552195676\n",
      "Validation Loss: 0.06620686137424506\n",
      "Epoch 47/100\n",
      "Training Loss: 0.11595943036272206\n",
      "Validation Loss: 0.07460518570996824\n",
      "Epoch 48/100\n",
      "Training Loss: 0.1311272227350398\n",
      "Validation Loss: 0.07604315227510236\n",
      "Epoch 49/100\n",
      "Training Loss: 0.12963613224311235\n",
      "Validation Loss: 0.07277577641578323\n",
      "Epoch 50/100\n",
      "Training Loss: 0.12142142164701838\n",
      "Validation Loss: 0.050877790335846954\n",
      "Epoch 51/100\n",
      "Training Loss: 0.10965184486241182\n",
      "Validation Loss: 0.07516324853651184\n",
      "Epoch 52/100\n",
      "Training Loss: 0.12019602180383626\n",
      "Validation Loss: 0.06583452486467174\n",
      "Epoch 53/100\n",
      "Training Loss: 0.11968866651774204\n",
      "Validation Loss: 0.15192441852893873\n",
      "Epoch 54/100\n",
      "Training Loss: 0.10477255110721821\n",
      "Validation Loss: 0.06147668134699101\n",
      "Epoch 55/100\n",
      "Training Loss: 0.11691069178053602\n",
      "Validation Loss: 0.08633713088151812\n",
      "Epoch 56/100\n",
      "Training Loss: 0.11744341962360087\n",
      "Validation Loss: 0.05627481226962381\n",
      "Epoch 57/100\n",
      "Training Loss: 0.11244082589321618\n",
      "Validation Loss: 0.06762005150868801\n",
      "Epoch 58/100\n",
      "Training Loss: 0.1126602490793688\n",
      "Validation Loss: 0.07033623737802092\n",
      "Epoch 59/100\n",
      "Training Loss: 0.1110962373023839\n",
      "Validation Loss: 0.08787020691656285\n",
      "Epoch 60/100\n",
      "Training Loss: 0.11068947844478043\n",
      "Validation Loss: 0.0868354550214566\n",
      "Epoch 61/100\n",
      "Training Loss: 0.10422441910919789\n",
      "Validation Loss: 0.10482172525182543\n",
      "Epoch 62/100\n",
      "Training Loss: 0.11722181217352404\n",
      "Validation Loss: 0.05298569655255041\n",
      "Epoch 63/100\n",
      "Training Loss: 0.10720387611448581\n",
      "Validation Loss: 0.10759807276064619\n",
      "Epoch 64/100\n",
      "Training Loss: 0.11009971906337011\n",
      "Validation Loss: 0.15023377129742066\n",
      "Epoch 65/100\n",
      "Training Loss: 0.10866290639600211\n",
      "Validation Loss: 0.13634285977270008\n",
      "Epoch 66/100\n",
      "Training Loss: 0.11333727640463762\n",
      "Validation Loss: 0.13482868754970986\n",
      "Epoch 67/100\n",
      "Training Loss: 0.09791383932299054\n",
      "Validation Loss: 0.1200379363824585\n",
      "Epoch 68/100\n",
      "Training Loss: 0.09707037030059582\n",
      "Validation Loss: 0.22629736065164052\n",
      "Epoch 69/100\n",
      "Training Loss: 0.1058994557394932\n",
      "Validation Loss: 0.13396230951942417\n",
      "Epoch 70/100\n",
      "Training Loss: 0.08929577246851013\n",
      "Validation Loss: 0.13283751098998756\n",
      "Epoch 71/100\n",
      "Training Loss: 0.09867340686715545\n",
      "Validation Loss: 0.039683333963085196\n",
      "Epoch 72/100\n",
      "Training Loss: 0.08998981287320086\n",
      "Validation Loss: 0.10250512862356846\n",
      "Epoch 73/100\n",
      "Training Loss: 0.10088429074382471\n",
      "Validation Loss: 0.07135505991773666\n",
      "Epoch 74/100\n",
      "Training Loss: 0.10980652351036228\n",
      "Validation Loss: 0.13308273360922263\n",
      "Epoch 75/100\n",
      "Training Loss: 0.08885506307769256\n",
      "Validation Loss: 0.11497093528624902\n",
      "Epoch 76/100\n",
      "Training Loss: 0.12199200543382988\n",
      "Validation Loss: 0.09015364945804673\n",
      "Epoch 77/100\n",
      "Training Loss: 0.08903381218642449\n",
      "Validation Loss: 0.16279576277902533\n",
      "Epoch 78/100\n",
      "Training Loss: 0.09993623334585497\n",
      "Validation Loss: 0.1963259391467418\n",
      "Epoch 79/100\n",
      "Training Loss: 0.10307243829638242\n",
      "Validation Loss: 0.10744929609425882\n",
      "Epoch 80/100\n",
      "Training Loss: 0.09195651463130518\n",
      "Validation Loss: 0.19481360538479717\n",
      "Epoch 81/100\n",
      "Training Loss: 0.09721116199302936\n",
      "Validation Loss: 0.06038391622370735\n",
      "Epoch 82/100\n",
      "Training Loss: 0.09099066630309527\n",
      "Validation Loss: 0.10098021731265176\n",
      "Epoch 83/100\n",
      "Training Loss: 0.08743348915855609\n",
      "Validation Loss: 0.0856378787119207\n",
      "Epoch 84/100\n",
      "Training Loss: 0.09335620289624731\n",
      "Validation Loss: 0.1399009487854186\n",
      "Epoch 85/100\n",
      "Training Loss: 0.09383849283230616\n",
      "Validation Loss: 0.07716051438413432\n",
      "Epoch 86/100\n",
      "Training Loss: 0.08542868976484529\n",
      "Validation Loss: 0.13495553194038662\n",
      "Epoch 87/100\n",
      "Training Loss: 0.08777725487397868\n",
      "Validation Loss: 0.09880011896882997\n",
      "Epoch 88/100\n",
      "Training Loss: 0.0954088850977721\n",
      "Validation Loss: 0.25235150469442436\n",
      "Epoch 89/100\n",
      "Training Loss: 0.08673023838057904\n",
      "Validation Loss: 0.17699176719780815\n",
      "Epoch 90/100\n",
      "Training Loss: 0.08318845151796682\n",
      "Validation Loss: 0.1395662167137282\n",
      "Epoch 91/100\n",
      "Training Loss: 0.07976581648246832\n",
      "Validation Loss: 0.22720172534653096\n",
      "Epoch 92/100\n",
      "Training Loss: 0.09292843695610453\n",
      "Validation Loss: 0.11836069556213083\n",
      "Epoch 93/100\n",
      "Training Loss: 0.08303574864714548\n",
      "Validation Loss: 0.16986589783312694\n",
      "Epoch 94/100\n",
      "Training Loss: 0.08217681492319666\n",
      "Validation Loss: 0.12767380165028888\n",
      "Epoch 95/100\n",
      "Training Loss: 0.09538768839894425\n",
      "Validation Loss: 0.14838963710859426\n",
      "Epoch 96/100\n",
      "Training Loss: 0.07892041992180529\n",
      "Validation Loss: 0.18147564581976844\n",
      "Epoch 97/100\n",
      "Training Loss: 0.06863595179585524\n",
      "Validation Loss: 0.16611645420625423\n",
      "Epoch 98/100\n",
      "Training Loss: 0.09015961321571958\n",
      "Validation Loss: 0.08301227812213388\n",
      "Epoch 99/100\n",
      "Training Loss: 0.08050346290195402\n",
      "Validation Loss: 0.11392084630866921\n",
      "Epoch 100/100\n",
      "Training Loss: 0.08327134600041612\n",
      "Validation Loss: 0.11240308250742499\n",
      "Combination 42: Avg Training Loss = 0.12815994019310212, Avg Validation Loss = 0.11139040624920846\n",
      "Testing combination 43/48: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.001, 'sequence_length': 6, 'dropout_rate': 0.1}\n",
      "Truncated Train Data Shape: (70, 6, 1)\n",
      "Truncated Validation Data Shape: (10, 6, 1)\n",
      "    Trial 1/2 for combination 43/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.4651118793388784\n",
      "Validation Loss: 0.2203918764304444\n",
      "Epoch 2/100\n",
      "Training Loss: 0.2628136570520623\n",
      "Validation Loss: 0.19151178640627228\n",
      "Epoch 3/100\n",
      "Training Loss: 0.23147985913748725\n",
      "Validation Loss: 0.22100732353817207\n",
      "Epoch 4/100\n",
      "Training Loss: 0.19851671855646288\n",
      "Validation Loss: 0.22702290207801762\n",
      "Epoch 5/100\n",
      "Training Loss: 0.2010819581442033\n",
      "Validation Loss: 0.14612175203065936\n",
      "Epoch 6/100\n",
      "Training Loss: 0.17622621361336305\n",
      "Validation Loss: 0.11860380900553236\n",
      "Epoch 7/100\n",
      "Training Loss: 0.19634569472046462\n",
      "Validation Loss: 0.14902988046507884\n",
      "Epoch 8/100\n",
      "Training Loss: 0.14298158305537134\n",
      "Validation Loss: 0.08427057539536248\n",
      "Epoch 9/100\n",
      "Training Loss: 0.13307270643081537\n",
      "Validation Loss: 0.0757045210417852\n",
      "Epoch 10/100\n",
      "Training Loss: 0.1715828089665789\n",
      "Validation Loss: 0.14518396442049183\n",
      "Epoch 11/100\n",
      "Training Loss: 0.14139716643446318\n",
      "Validation Loss: 0.20638903072877338\n",
      "Epoch 12/100\n",
      "Training Loss: 0.14063775091163322\n",
      "Validation Loss: 0.11812915795581651\n",
      "Epoch 13/100\n",
      "Training Loss: 0.11937335339515125\n",
      "Validation Loss: 0.11745294361990474\n",
      "Epoch 14/100\n",
      "Training Loss: 0.11439347847036423\n",
      "Validation Loss: 0.078555646180696\n",
      "Epoch 15/100\n",
      "Training Loss: 0.11615289477612062\n",
      "Validation Loss: 0.07892562820788179\n",
      "Epoch 16/100\n",
      "Training Loss: 0.12444234124976838\n",
      "Validation Loss: 0.06214422036024122\n",
      "Epoch 17/100\n",
      "Training Loss: 0.10446594632012608\n",
      "Validation Loss: 0.06943315459994627\n",
      "Epoch 18/100\n",
      "Training Loss: 0.10643300060529885\n",
      "Validation Loss: 0.07640409107094105\n",
      "Epoch 19/100\n",
      "Training Loss: 0.11259316809241308\n",
      "Validation Loss: 0.10919037868999681\n",
      "Epoch 20/100\n",
      "Training Loss: 0.10045119511341266\n",
      "Validation Loss: 0.06779226759090819\n",
      "Epoch 21/100\n",
      "Training Loss: 0.11654110256465464\n",
      "Validation Loss: 0.07619058763329786\n",
      "Epoch 22/100\n",
      "Training Loss: 0.1081003729580428\n",
      "Validation Loss: 0.06330997693824311\n",
      "Epoch 23/100\n",
      "Training Loss: 0.09731803223385492\n",
      "Validation Loss: 0.06101586856916992\n",
      "Epoch 24/100\n",
      "Training Loss: 0.09373091973952506\n",
      "Validation Loss: 0.06600146906635126\n",
      "Epoch 25/100\n",
      "Training Loss: 0.09360417385841714\n",
      "Validation Loss: 0.04418147078150724\n",
      "Epoch 26/100\n",
      "Training Loss: 0.10414168614950195\n",
      "Validation Loss: 0.07450213307944206\n",
      "Epoch 27/100\n",
      "Training Loss: 0.0951192631599711\n",
      "Validation Loss: 0.039503287835191005\n",
      "Epoch 28/100\n",
      "Training Loss: 0.08658085228778782\n",
      "Validation Loss: 0.05019954823094749\n",
      "Epoch 29/100\n",
      "Training Loss: 0.08718209357490475\n",
      "Validation Loss: 0.041783730665580956\n",
      "Epoch 30/100\n",
      "Training Loss: 0.0962167280127088\n",
      "Validation Loss: 0.049571838762210864\n",
      "Epoch 31/100\n",
      "Training Loss: 0.08121273374845898\n",
      "Validation Loss: 0.047753682623194196\n",
      "Epoch 32/100\n",
      "Training Loss: 0.07898106595652396\n",
      "Validation Loss: 0.05421422910224808\n",
      "Epoch 33/100\n",
      "Training Loss: 0.08184620392286683\n",
      "Validation Loss: 0.03763332788499908\n",
      "Epoch 34/100\n",
      "Training Loss: 0.08412213054083745\n",
      "Validation Loss: 0.04272875178339487\n",
      "Epoch 35/100\n",
      "Training Loss: 0.09188220887382621\n",
      "Validation Loss: 0.05765144676292402\n",
      "Epoch 36/100\n",
      "Training Loss: 0.08709978675859828\n",
      "Validation Loss: 0.0506467128095873\n",
      "Epoch 37/100\n",
      "Training Loss: 0.07293304573924642\n",
      "Validation Loss: 0.05179270459301953\n",
      "Epoch 38/100\n",
      "Training Loss: 0.08220242477180514\n",
      "Validation Loss: 0.04274118321144337\n",
      "Epoch 39/100\n",
      "Training Loss: 0.07523663547713566\n",
      "Validation Loss: 0.05519835346637157\n",
      "Epoch 40/100\n",
      "Training Loss: 0.07800368469402974\n",
      "Validation Loss: 0.04513517461285883\n",
      "Epoch 41/100\n",
      "Training Loss: 0.07925761432344444\n",
      "Validation Loss: 0.07361957603997163\n",
      "Epoch 42/100\n",
      "Training Loss: 0.07179128404597562\n",
      "Validation Loss: 0.03668604543386023\n",
      "Epoch 43/100\n",
      "Training Loss: 0.08572387352461769\n",
      "Validation Loss: 0.040522303664052066\n",
      "Epoch 44/100\n",
      "Training Loss: 0.07982191451803283\n",
      "Validation Loss: 0.0422618827696826\n",
      "Epoch 45/100\n",
      "Training Loss: 0.0688593018943456\n",
      "Validation Loss: 0.037426811330199414\n",
      "Epoch 46/100\n",
      "Training Loss: 0.07364209722088418\n",
      "Validation Loss: 0.03515597171438281\n",
      "Epoch 47/100\n",
      "Training Loss: 0.06812059865222228\n",
      "Validation Loss: 0.05122927194761736\n",
      "Epoch 48/100\n",
      "Training Loss: 0.0797115241273895\n",
      "Validation Loss: 0.047116017040293\n",
      "Epoch 49/100\n",
      "Training Loss: 0.086169957617722\n",
      "Validation Loss: 0.05295243424707139\n",
      "Epoch 50/100\n",
      "Training Loss: 0.08300995116751464\n",
      "Validation Loss: 0.07538672831814677\n",
      "Epoch 51/100\n",
      "Training Loss: 0.061814642194675036\n",
      "Validation Loss: 0.03408686348345423\n",
      "Epoch 52/100\n",
      "Training Loss: 0.08324460481163481\n",
      "Validation Loss: 0.03915710133125087\n",
      "Epoch 53/100\n",
      "Training Loss: 0.08053543141076776\n",
      "Validation Loss: 0.050932088737605165\n",
      "Epoch 54/100\n",
      "Training Loss: 0.07024346167299927\n",
      "Validation Loss: 0.025448157926129887\n",
      "Epoch 55/100\n",
      "Training Loss: 0.0862634399674479\n",
      "Validation Loss: 0.04323089144058009\n",
      "Epoch 56/100\n",
      "Training Loss: 0.06846393711521655\n",
      "Validation Loss: 0.026919276865213938\n",
      "Epoch 57/100\n",
      "Training Loss: 0.07159375363473694\n",
      "Validation Loss: 0.05979414828672667\n",
      "Epoch 58/100\n",
      "Training Loss: 0.05510891363696559\n",
      "Validation Loss: 0.061850752135312825\n",
      "Epoch 59/100\n",
      "Training Loss: 0.07271579824905672\n",
      "Validation Loss: 0.04352273524574786\n",
      "Epoch 60/100\n",
      "Training Loss: 0.0799581802802598\n",
      "Validation Loss: 0.03038863515597843\n",
      "Epoch 61/100\n",
      "Training Loss: 0.08209548035095264\n",
      "Validation Loss: 0.05467602015551848\n",
      "Epoch 62/100\n",
      "Training Loss: 0.07360452311035882\n",
      "Validation Loss: 0.06569734039303007\n",
      "Epoch 63/100\n",
      "Training Loss: 0.06861382435476841\n",
      "Validation Loss: 0.04747509645281016\n",
      "Epoch 64/100\n",
      "Training Loss: 0.07621017561434808\n",
      "Validation Loss: 0.0765627566045329\n",
      "Epoch 65/100\n",
      "Training Loss: 0.07968570292939928\n",
      "Validation Loss: 0.06742133717707198\n",
      "Epoch 66/100\n",
      "Training Loss: 0.08089512829383494\n",
      "Validation Loss: 0.04686585066311589\n",
      "Epoch 67/100\n",
      "Training Loss: 0.07920366118871751\n",
      "Validation Loss: 0.0719479635565903\n",
      "Epoch 68/100\n",
      "Training Loss: 0.07641380458582542\n",
      "Validation Loss: 0.029218767581499903\n",
      "Epoch 69/100\n",
      "Training Loss: 0.07717357479791179\n",
      "Validation Loss: 0.057648443283220094\n",
      "Epoch 70/100\n",
      "Training Loss: 0.08085007516687034\n",
      "Validation Loss: 0.05450315928848182\n",
      "Epoch 71/100\n",
      "Training Loss: 0.07156786445936689\n",
      "Validation Loss: 0.047498605412601\n",
      "Epoch 72/100\n",
      "Training Loss: 0.05963591016325558\n",
      "Validation Loss: 0.050843098885862546\n",
      "Epoch 73/100\n",
      "Training Loss: 0.07180336560961373\n",
      "Validation Loss: 0.05895713292046558\n",
      "Epoch 74/100\n",
      "Training Loss: 0.07252287108858468\n",
      "Validation Loss: 0.07159890432306006\n",
      "Epoch 75/100\n",
      "Training Loss: 0.07080223448524685\n",
      "Validation Loss: 0.04780238063873863\n",
      "Epoch 76/100\n",
      "Training Loss: 0.07446356323589268\n",
      "Validation Loss: 0.046132392446337704\n",
      "Epoch 77/100\n",
      "Training Loss: 0.07726240117920863\n",
      "Validation Loss: 0.0560676202558958\n",
      "Epoch 78/100\n",
      "Training Loss: 0.07149452740690702\n",
      "Validation Loss: 0.04070046035169746\n",
      "Epoch 79/100\n",
      "Training Loss: 0.07628323127709478\n",
      "Validation Loss: 0.05178296426818037\n",
      "Epoch 80/100\n",
      "Training Loss: 0.0932653185081937\n",
      "Validation Loss: 0.050013928486658324\n",
      "Epoch 81/100\n",
      "Training Loss: 0.08094383663648114\n",
      "Validation Loss: 0.05682009817311293\n",
      "Epoch 82/100\n",
      "Training Loss: 0.0902629974300867\n",
      "Validation Loss: 0.04203282097251897\n",
      "Epoch 83/100\n",
      "Training Loss: 0.08203230926730729\n",
      "Validation Loss: 0.09032462294886033\n",
      "Epoch 84/100\n",
      "Training Loss: 0.07886554256412698\n",
      "Validation Loss: 0.0568281835474939\n",
      "Epoch 85/100\n",
      "Training Loss: 0.07552779213960796\n",
      "Validation Loss: 0.07110405288565476\n",
      "Epoch 86/100\n",
      "Training Loss: 0.07248455106411081\n",
      "Validation Loss: 0.051479272234302466\n",
      "Epoch 87/100\n",
      "Training Loss: 0.06696414394523882\n",
      "Validation Loss: 0.08603182955057725\n",
      "Epoch 88/100\n",
      "Training Loss: 0.09423823284938551\n",
      "Validation Loss: 0.04643184561962253\n",
      "Epoch 89/100\n",
      "Training Loss: 0.0744939727160819\n",
      "Validation Loss: 0.05541235295091429\n",
      "Epoch 90/100\n",
      "Training Loss: 0.0774224507506971\n",
      "Validation Loss: 0.024535054635822735\n",
      "Epoch 91/100\n",
      "Training Loss: 0.0766471476233692\n",
      "Validation Loss: 0.08503786995130798\n",
      "Epoch 92/100\n",
      "Training Loss: 0.08334698709898758\n",
      "Validation Loss: 0.12471812710261562\n",
      "Epoch 93/100\n",
      "Training Loss: 0.06097856035020144\n",
      "Validation Loss: 0.061578875032296844\n",
      "Epoch 94/100\n",
      "Training Loss: 0.06856549708740559\n",
      "Validation Loss: 0.07276497114539653\n",
      "Epoch 95/100\n",
      "Training Loss: 0.09023876155881487\n",
      "Validation Loss: 0.06923040959866092\n",
      "Epoch 96/100\n",
      "Training Loss: 0.06862439504385362\n",
      "Validation Loss: 0.03752061925526934\n",
      "Epoch 97/100\n",
      "Training Loss: 0.07786299166984116\n",
      "Validation Loss: 0.04032687054276028\n",
      "Epoch 98/100\n",
      "Training Loss: 0.07055927897455137\n",
      "Validation Loss: 0.0679895894934014\n",
      "Epoch 99/100\n",
      "Training Loss: 0.07456726857562282\n",
      "Validation Loss: 0.0604207013571653\n",
      "Epoch 100/100\n",
      "Training Loss: 0.07783194479074805\n",
      "Validation Loss: 0.08124998669286913\n",
      "    Trial 2/2 for combination 43/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.41268753403711883\n",
      "Validation Loss: 0.16280836787294348\n",
      "Epoch 2/100\n",
      "Training Loss: 0.32852513314404497\n",
      "Validation Loss: 0.327787848974418\n",
      "Epoch 3/100\n",
      "Training Loss: 0.21721246819750273\n",
      "Validation Loss: 0.28148722282350513\n",
      "Epoch 4/100\n",
      "Training Loss: 0.2272829147056874\n",
      "Validation Loss: 0.09952185894811391\n",
      "Epoch 5/100\n",
      "Training Loss: 0.19760366231131096\n",
      "Validation Loss: 0.1250731502238424\n",
      "Epoch 6/100\n",
      "Training Loss: 0.21254156738076443\n",
      "Validation Loss: 0.09847742204295803\n",
      "Epoch 7/100\n",
      "Training Loss: 0.16585769772399642\n",
      "Validation Loss: 0.1290335259681707\n",
      "Epoch 8/100\n",
      "Training Loss: 0.15239511636795453\n",
      "Validation Loss: 0.2142650188772798\n",
      "Epoch 9/100\n",
      "Training Loss: 0.13876318566800439\n",
      "Validation Loss: 0.13975470811190221\n",
      "Epoch 10/100\n",
      "Training Loss: 0.18343800187906542\n",
      "Validation Loss: 0.16278017348926557\n",
      "Epoch 11/100\n",
      "Training Loss: 0.15604950765609038\n",
      "Validation Loss: 0.09436036426940875\n",
      "Epoch 12/100\n",
      "Training Loss: 0.16596559397414312\n",
      "Validation Loss: 0.09222499634972486\n",
      "Epoch 13/100\n",
      "Training Loss: 0.13406697308582066\n",
      "Validation Loss: 0.07231079809862559\n",
      "Epoch 14/100\n",
      "Training Loss: 0.15087783213498207\n",
      "Validation Loss: 0.07798140384207826\n",
      "Epoch 15/100\n",
      "Training Loss: 0.14153860111510033\n",
      "Validation Loss: 0.07372230818456231\n",
      "Epoch 16/100\n",
      "Training Loss: 0.12515173984219383\n",
      "Validation Loss: 0.10183094103671457\n",
      "Epoch 17/100\n",
      "Training Loss: 0.10582403130487125\n",
      "Validation Loss: 0.05588894766152752\n",
      "Epoch 18/100\n",
      "Training Loss: 0.1384260046901903\n",
      "Validation Loss: 0.12811125578162347\n",
      "Epoch 19/100\n",
      "Training Loss: 0.13298074343261543\n",
      "Validation Loss: 0.06255967829191039\n",
      "Epoch 20/100\n",
      "Training Loss: 0.11279067776568297\n",
      "Validation Loss: 0.057551443941856996\n",
      "Epoch 21/100\n",
      "Training Loss: 0.10451586486803854\n",
      "Validation Loss: 0.06460793647423345\n",
      "Epoch 22/100\n",
      "Training Loss: 0.10817545264598065\n",
      "Validation Loss: 0.08563137375771995\n",
      "Epoch 23/100\n",
      "Training Loss: 0.10411672214154707\n",
      "Validation Loss: 0.07271030718833434\n",
      "Epoch 24/100\n",
      "Training Loss: 0.09970861706384541\n",
      "Validation Loss: 0.083417916019806\n",
      "Epoch 25/100\n",
      "Training Loss: 0.10231290960727428\n",
      "Validation Loss: 0.1132733023964845\n",
      "Epoch 26/100\n",
      "Training Loss: 0.098166773212684\n",
      "Validation Loss: 0.05935017292150115\n",
      "Epoch 27/100\n",
      "Training Loss: 0.10817653113325619\n",
      "Validation Loss: 0.07290898930064722\n",
      "Epoch 28/100\n",
      "Training Loss: 0.11244265603852757\n",
      "Validation Loss: 0.0704874169680995\n",
      "Epoch 29/100\n",
      "Training Loss: 0.0840584919852818\n",
      "Validation Loss: 0.039170615068602435\n",
      "Epoch 30/100\n",
      "Training Loss: 0.09856499734118478\n",
      "Validation Loss: 0.05291596878772568\n",
      "Epoch 31/100\n",
      "Training Loss: 0.08044010721322405\n",
      "Validation Loss: 0.049155910747067814\n",
      "Epoch 32/100\n",
      "Training Loss: 0.08474226727586452\n",
      "Validation Loss: 0.11989524994053735\n",
      "Epoch 33/100\n",
      "Training Loss: 0.08901751752798345\n",
      "Validation Loss: 0.0603175931308081\n",
      "Epoch 34/100\n",
      "Training Loss: 0.07766353635703971\n",
      "Validation Loss: 0.05511476953111237\n",
      "Epoch 35/100\n",
      "Training Loss: 0.08213952972762767\n",
      "Validation Loss: 0.0673394856648046\n",
      "Epoch 36/100\n",
      "Training Loss: 0.07640412367818952\n",
      "Validation Loss: 0.036374078924989534\n",
      "Epoch 37/100\n",
      "Training Loss: 0.08244210848591275\n",
      "Validation Loss: 0.07631667112651878\n",
      "Epoch 38/100\n",
      "Training Loss: 0.0857509331640217\n",
      "Validation Loss: 0.06160889130934026\n",
      "Epoch 39/100\n",
      "Training Loss: 0.08148552072194694\n",
      "Validation Loss: 0.038316334933852544\n",
      "Epoch 40/100\n",
      "Training Loss: 0.08580463968259479\n",
      "Validation Loss: 0.06360284623651576\n",
      "Epoch 41/100\n",
      "Training Loss: 0.07386745501805088\n",
      "Validation Loss: 0.06315957266079188\n",
      "Epoch 42/100\n",
      "Training Loss: 0.08125472065611662\n",
      "Validation Loss: 0.05834316164073593\n",
      "Epoch 43/100\n",
      "Training Loss: 0.06561905944027402\n",
      "Validation Loss: 0.10708167481252362\n",
      "Epoch 44/100\n",
      "Training Loss: 0.09244481162793591\n",
      "Validation Loss: 0.05503858287839142\n",
      "Epoch 45/100\n",
      "Training Loss: 0.0742873191469122\n",
      "Validation Loss: 0.06591206010981447\n",
      "Epoch 46/100\n",
      "Training Loss: 0.0743347591208235\n",
      "Validation Loss: 0.04255984549980533\n",
      "Epoch 47/100\n",
      "Training Loss: 0.0823285853899646\n",
      "Validation Loss: 0.07183828644516799\n",
      "Epoch 48/100\n",
      "Training Loss: 0.07809134410454627\n",
      "Validation Loss: 0.04480329800972386\n",
      "Epoch 49/100\n",
      "Training Loss: 0.07194198401266368\n",
      "Validation Loss: 0.0424120108113185\n",
      "Epoch 50/100\n",
      "Training Loss: 0.0876114934813724\n",
      "Validation Loss: 0.03739005918338169\n",
      "Epoch 51/100\n",
      "Training Loss: 0.0805050634109266\n",
      "Validation Loss: 0.030473402813339896\n",
      "Epoch 52/100\n",
      "Training Loss: 0.058100621238717426\n",
      "Validation Loss: 0.06560957832595422\n",
      "Epoch 53/100\n",
      "Training Loss: 0.07533770514946464\n",
      "Validation Loss: 0.03773923772387997\n",
      "Epoch 54/100\n",
      "Training Loss: 0.08831090923093288\n",
      "Validation Loss: 0.03979722014435851\n",
      "Epoch 55/100\n",
      "Training Loss: 0.07499216774872905\n",
      "Validation Loss: 0.05297657142732391\n",
      "Epoch 56/100\n",
      "Training Loss: 0.06850273253689222\n",
      "Validation Loss: 0.07169380525718747\n",
      "Epoch 57/100\n",
      "Training Loss: 0.08356365535402142\n",
      "Validation Loss: 0.05224782152458228\n",
      "Epoch 58/100\n",
      "Training Loss: 0.0724221206005586\n",
      "Validation Loss: 0.03192245711021867\n",
      "Epoch 59/100\n",
      "Training Loss: 0.0771192929799299\n",
      "Validation Loss: 0.0763211104439416\n",
      "Epoch 60/100\n",
      "Training Loss: 0.06417259197342949\n",
      "Validation Loss: 0.035502236494420726\n",
      "Epoch 61/100\n",
      "Training Loss: 0.0903772845606721\n",
      "Validation Loss: 0.0447339043377616\n",
      "Epoch 62/100\n",
      "Training Loss: 0.0671185029294572\n",
      "Validation Loss: 0.04419734190330456\n",
      "Epoch 63/100\n",
      "Training Loss: 0.07918360368756357\n",
      "Validation Loss: 0.05506028478545135\n",
      "Epoch 64/100\n",
      "Training Loss: 0.07231714461103494\n",
      "Validation Loss: 0.10547819953055289\n",
      "Epoch 65/100\n",
      "Training Loss: 0.07110021355242654\n",
      "Validation Loss: 0.07432756690455425\n",
      "Epoch 66/100\n",
      "Training Loss: 0.06822317120048124\n",
      "Validation Loss: 0.03586534121354108\n",
      "Epoch 67/100\n",
      "Training Loss: 0.08228352172240871\n",
      "Validation Loss: 0.0819476479532236\n",
      "Epoch 68/100\n",
      "Training Loss: 0.08089679358002021\n",
      "Validation Loss: 0.05318171003470147\n",
      "Epoch 69/100\n",
      "Training Loss: 0.08160336262554216\n",
      "Validation Loss: 0.026396005240002296\n",
      "Epoch 70/100\n",
      "Training Loss: 0.07714694218887064\n",
      "Validation Loss: 0.04165509057753953\n",
      "Epoch 71/100\n",
      "Training Loss: 0.06330217211393237\n",
      "Validation Loss: 0.06618753780001953\n",
      "Epoch 72/100\n",
      "Training Loss: 0.06739657702093639\n",
      "Validation Loss: 0.029728863517664787\n",
      "Epoch 73/100\n",
      "Training Loss: 0.06478644240523782\n",
      "Validation Loss: 0.07233268797090077\n",
      "Epoch 74/100\n",
      "Training Loss: 0.07468836853731527\n",
      "Validation Loss: 0.05300315914437931\n",
      "Epoch 75/100\n",
      "Training Loss: 0.0742170230129836\n",
      "Validation Loss: 0.051408788415573926\n",
      "Epoch 76/100\n",
      "Training Loss: 0.08913909898958464\n",
      "Validation Loss: 0.050391744479286835\n",
      "Epoch 77/100\n",
      "Training Loss: 0.07226698494993329\n",
      "Validation Loss: 0.09866102337028812\n",
      "Epoch 78/100\n",
      "Training Loss: 0.07968660075202043\n",
      "Validation Loss: 0.03143925809093869\n",
      "Epoch 79/100\n",
      "Training Loss: 0.07823869982877413\n",
      "Validation Loss: 0.022893780928332025\n",
      "Epoch 80/100\n",
      "Training Loss: 0.07705561820326202\n",
      "Validation Loss: 0.059812082799811296\n",
      "Epoch 81/100\n",
      "Training Loss: 0.09483857492098718\n",
      "Validation Loss: 0.08943935091312738\n",
      "Epoch 82/100\n",
      "Training Loss: 0.07813844574760773\n",
      "Validation Loss: 0.04599838630898144\n",
      "Epoch 83/100\n",
      "Training Loss: 0.07827086595826538\n",
      "Validation Loss: 0.11008366080602701\n",
      "Epoch 84/100\n",
      "Training Loss: 0.0657194944531446\n",
      "Validation Loss: 0.07267695299471442\n",
      "Epoch 85/100\n",
      "Training Loss: 0.07392405289558412\n",
      "Validation Loss: 0.06305362166751668\n",
      "Epoch 86/100\n",
      "Training Loss: 0.08232766345646349\n",
      "Validation Loss: 0.08090849596334102\n",
      "Epoch 87/100\n",
      "Training Loss: 0.08881928067095585\n",
      "Validation Loss: 0.052476161192810344\n",
      "Epoch 88/100\n",
      "Training Loss: 0.08689474663212018\n",
      "Validation Loss: 0.10968968108201844\n",
      "Epoch 89/100\n",
      "Training Loss: 0.0814320490714391\n",
      "Validation Loss: 0.10054051010548788\n",
      "Epoch 90/100\n",
      "Training Loss: 0.08557306596636524\n",
      "Validation Loss: 0.059421151923851556\n",
      "Epoch 91/100\n",
      "Training Loss: 0.08516725131913071\n",
      "Validation Loss: 0.06189121489560134\n",
      "Epoch 92/100\n",
      "Training Loss: 0.08092785212597486\n",
      "Validation Loss: 0.08094181568585368\n",
      "Epoch 93/100\n",
      "Training Loss: 0.06948945448097059\n",
      "Validation Loss: 0.0324570071001357\n",
      "Epoch 94/100\n",
      "Training Loss: 0.07834956152341827\n",
      "Validation Loss: 0.06188707205114974\n",
      "Epoch 95/100\n",
      "Training Loss: 0.07824849441268869\n",
      "Validation Loss: 0.05683313016023332\n",
      "Epoch 96/100\n",
      "Training Loss: 0.06734765191622298\n",
      "Validation Loss: 0.056464342361758534\n",
      "Epoch 97/100\n",
      "Training Loss: 0.071997008110359\n",
      "Validation Loss: 0.08955719799707888\n",
      "Epoch 98/100\n",
      "Training Loss: 0.07695046422229747\n",
      "Validation Loss: 0.03673752338070183\n",
      "Epoch 99/100\n",
      "Training Loss: 0.08017747627607616\n",
      "Validation Loss: 0.07153325508850773\n",
      "Epoch 100/100\n",
      "Training Loss: 0.07863314259154658\n",
      "Validation Loss: 0.07986433465681884\n",
      "Combination 43: Avg Training Loss = 0.09889568050575709, Avg Validation Loss = 0.07195993301023858\n",
      "Testing combination 44/48: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.001, 'sequence_length': 6, 'dropout_rate': 0.2}\n",
      "Truncated Train Data Shape: (70, 6, 1)\n",
      "Truncated Validation Data Shape: (10, 6, 1)\n",
      "    Trial 1/2 for combination 44/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.40377347361990945\n",
      "Validation Loss: 0.18656089793646566\n",
      "Epoch 2/100\n",
      "Training Loss: 0.2760326053508469\n",
      "Validation Loss: 0.3359831051901584\n",
      "Epoch 3/100\n",
      "Training Loss: 0.23531150660386158\n",
      "Validation Loss: 0.1116139772482784\n",
      "Epoch 4/100\n",
      "Training Loss: 0.23364558034001925\n",
      "Validation Loss: 0.13467589462157564\n",
      "Epoch 5/100\n",
      "Training Loss: 0.1818675010975309\n",
      "Validation Loss: 0.08611530245160638\n",
      "Epoch 6/100\n",
      "Training Loss: 0.15981555951311574\n",
      "Validation Loss: 0.1510372420228387\n",
      "Epoch 7/100\n",
      "Training Loss: 0.15357190716831326\n",
      "Validation Loss: 0.18818886033350282\n",
      "Epoch 8/100\n",
      "Training Loss: 0.11982313068128726\n",
      "Validation Loss: 0.1319578037082621\n",
      "Epoch 9/100\n",
      "Training Loss: 0.13550933222060577\n",
      "Validation Loss: 0.09745753258135222\n",
      "Epoch 10/100\n",
      "Training Loss: 0.13707341634917064\n",
      "Validation Loss: 0.11369423143576463\n",
      "Epoch 11/100\n",
      "Training Loss: 0.15060780583934152\n",
      "Validation Loss: 0.09788595439967067\n",
      "Epoch 12/100\n",
      "Training Loss: 0.13462870420232514\n",
      "Validation Loss: 0.13543484675586576\n",
      "Epoch 13/100\n",
      "Training Loss: 0.1380973249633114\n",
      "Validation Loss: 0.10306917364815064\n",
      "Epoch 14/100\n",
      "Training Loss: 0.12426224288035898\n",
      "Validation Loss: 0.10854070582737567\n",
      "Epoch 15/100\n",
      "Training Loss: 0.11251246596961775\n",
      "Validation Loss: 0.06966132503937138\n",
      "Epoch 16/100\n",
      "Training Loss: 0.11813414453358896\n",
      "Validation Loss: 0.08142752391350924\n",
      "Epoch 17/100\n",
      "Training Loss: 0.10586959872706656\n",
      "Validation Loss: 0.06787499929937618\n",
      "Epoch 18/100\n",
      "Training Loss: 0.11634967071780751\n",
      "Validation Loss: 0.10137108426283532\n",
      "Epoch 19/100\n",
      "Training Loss: 0.10610931657390021\n",
      "Validation Loss: 0.06052451619943178\n",
      "Epoch 20/100\n",
      "Training Loss: 0.10383805675262663\n",
      "Validation Loss: 0.05304769999173202\n",
      "Epoch 21/100\n",
      "Training Loss: 0.11263904988296285\n",
      "Validation Loss: 0.059655734223683844\n",
      "Epoch 22/100\n",
      "Training Loss: 0.09257011802540324\n",
      "Validation Loss: 0.06041509305255668\n",
      "Epoch 23/100\n",
      "Training Loss: 0.09277412731172564\n",
      "Validation Loss: 0.04609196028330473\n",
      "Epoch 24/100\n",
      "Training Loss: 0.09276175895104913\n",
      "Validation Loss: 0.053184668999412155\n",
      "Epoch 25/100\n",
      "Training Loss: 0.08384294248377983\n",
      "Validation Loss: 0.04242069549025102\n",
      "Epoch 26/100\n",
      "Training Loss: 0.0914132360212186\n",
      "Validation Loss: 0.04072777700028589\n",
      "Epoch 27/100\n",
      "Training Loss: 0.09333449674445436\n",
      "Validation Loss: 0.06121328907576077\n",
      "Epoch 28/100\n",
      "Training Loss: 0.08858609363081311\n",
      "Validation Loss: 0.046687276574498095\n",
      "Epoch 29/100\n",
      "Training Loss: 0.0906060432781904\n",
      "Validation Loss: 0.05486962836747061\n",
      "Epoch 30/100\n",
      "Training Loss: 0.07840958808128319\n",
      "Validation Loss: 0.055658420739378114\n",
      "Epoch 31/100\n",
      "Training Loss: 0.08361835175256815\n",
      "Validation Loss: 0.05227814752910792\n",
      "Epoch 32/100\n",
      "Training Loss: 0.08259964068644574\n",
      "Validation Loss: 0.04131448600450871\n",
      "Epoch 33/100\n",
      "Training Loss: 0.07437414301420664\n",
      "Validation Loss: 0.04993049780135581\n",
      "Epoch 34/100\n",
      "Training Loss: 0.07968351564012328\n",
      "Validation Loss: 0.04519448828821685\n",
      "Epoch 35/100\n",
      "Training Loss: 0.08417754089144834\n",
      "Validation Loss: 0.05917019548771183\n",
      "Epoch 36/100\n",
      "Training Loss: 0.07602771833969911\n",
      "Validation Loss: 0.04505549297666077\n",
      "Epoch 37/100\n",
      "Training Loss: 0.08143710160029287\n",
      "Validation Loss: 0.039266641650005436\n",
      "Epoch 38/100\n",
      "Training Loss: 0.07546908019469781\n",
      "Validation Loss: 0.05086650309480477\n",
      "Epoch 39/100\n",
      "Training Loss: 0.07277353128140784\n",
      "Validation Loss: 0.05158512815345484\n",
      "Epoch 40/100\n",
      "Training Loss: 0.08038593808807348\n",
      "Validation Loss: 0.036184165320360374\n",
      "Epoch 41/100\n",
      "Training Loss: 0.07115068479433347\n",
      "Validation Loss: 0.03335812444487796\n",
      "Epoch 42/100\n",
      "Training Loss: 0.07511677405517421\n",
      "Validation Loss: 0.05062106040267898\n",
      "Epoch 43/100\n",
      "Training Loss: 0.06994860542861381\n",
      "Validation Loss: 0.055432981425650285\n",
      "Epoch 44/100\n",
      "Training Loss: 0.07675271231697793\n",
      "Validation Loss: 0.05216274549115246\n",
      "Epoch 45/100\n",
      "Training Loss: 0.07807364481941714\n",
      "Validation Loss: 0.05009618647592355\n",
      "Epoch 46/100\n",
      "Training Loss: 0.0691892839958983\n",
      "Validation Loss: 0.029701214289591805\n",
      "Epoch 47/100\n",
      "Training Loss: 0.07810640326688435\n",
      "Validation Loss: 0.058990476183652454\n",
      "Epoch 48/100\n",
      "Training Loss: 0.0819485941696033\n",
      "Validation Loss: 0.04702356766824799\n",
      "Epoch 49/100\n",
      "Training Loss: 0.07495217294585374\n",
      "Validation Loss: 0.04121808508089305\n",
      "Epoch 50/100\n",
      "Training Loss: 0.06992380180720255\n",
      "Validation Loss: 0.0519333815634697\n",
      "Epoch 51/100\n",
      "Training Loss: 0.08592764061543721\n",
      "Validation Loss: 0.035224448056044064\n",
      "Epoch 52/100\n",
      "Training Loss: 0.06990803797054113\n",
      "Validation Loss: 0.04661852454688008\n",
      "Epoch 53/100\n",
      "Training Loss: 0.07522160804491401\n",
      "Validation Loss: 0.07258170609668477\n",
      "Epoch 54/100\n",
      "Training Loss: 0.0806421282403077\n",
      "Validation Loss: 0.04642688451103126\n",
      "Epoch 55/100\n",
      "Training Loss: 0.06941064955668232\n",
      "Validation Loss: 0.04445968270038787\n",
      "Epoch 56/100\n",
      "Training Loss: 0.07585726614971272\n",
      "Validation Loss: 0.05179315958296045\n",
      "Epoch 57/100\n",
      "Training Loss: 0.06800643833341853\n",
      "Validation Loss: 0.04161246690457494\n",
      "Epoch 58/100\n",
      "Training Loss: 0.07920643919956342\n",
      "Validation Loss: 0.07122882721134767\n",
      "Epoch 59/100\n",
      "Training Loss: 0.07849061161126865\n",
      "Validation Loss: 0.05067331525173002\n",
      "Epoch 60/100\n",
      "Training Loss: 0.0683656954614426\n",
      "Validation Loss: 0.05545480269616824\n",
      "Epoch 61/100\n",
      "Training Loss: 0.07272225711797681\n",
      "Validation Loss: 0.12683886364476105\n",
      "Epoch 62/100\n",
      "Training Loss: 0.0798004193644651\n",
      "Validation Loss: 0.047412531138023545\n",
      "Epoch 63/100\n",
      "Training Loss: 0.0715060571467344\n",
      "Validation Loss: 0.06865751560780577\n",
      "Epoch 64/100\n",
      "Training Loss: 0.06477737251262007\n",
      "Validation Loss: 0.05872923786182278\n",
      "Epoch 65/100\n",
      "Training Loss: 0.0732335280533692\n",
      "Validation Loss: 0.029873812158289537\n",
      "Epoch 66/100\n",
      "Training Loss: 0.08497835164209327\n",
      "Validation Loss: 0.05250122992121953\n",
      "Epoch 67/100\n",
      "Training Loss: 0.07750046612693968\n",
      "Validation Loss: 0.06363201554766225\n",
      "Epoch 68/100\n",
      "Training Loss: 0.07967147194805116\n",
      "Validation Loss: 0.05254572240706592\n",
      "Epoch 69/100\n",
      "Training Loss: 0.07058133631709364\n",
      "Validation Loss: 0.05505650104933711\n",
      "Epoch 70/100\n",
      "Training Loss: 0.06880163548499409\n",
      "Validation Loss: 0.06856751048980422\n",
      "Epoch 71/100\n",
      "Training Loss: 0.07800195875943099\n",
      "Validation Loss: 0.04608020600629767\n",
      "Epoch 72/100\n",
      "Training Loss: 0.08195624700250417\n",
      "Validation Loss: 0.04663892857077163\n",
      "Epoch 73/100\n",
      "Training Loss: 0.07012500231585668\n",
      "Validation Loss: 0.04683259033039029\n",
      "Epoch 74/100\n",
      "Training Loss: 0.07076158636486549\n",
      "Validation Loss: 0.03379418091515232\n",
      "Epoch 75/100\n",
      "Training Loss: 0.08236741092342259\n",
      "Validation Loss: 0.05700412826674731\n",
      "Epoch 76/100\n",
      "Training Loss: 0.06939811967938855\n",
      "Validation Loss: 0.03666620291355432\n",
      "Epoch 77/100\n",
      "Training Loss: 0.06577703512626612\n",
      "Validation Loss: 0.05575014865415515\n",
      "Epoch 78/100\n",
      "Training Loss: 0.07670283619686455\n",
      "Validation Loss: 0.0477925464503397\n",
      "Epoch 79/100\n",
      "Training Loss: 0.07943714608945722\n",
      "Validation Loss: 0.09308201422788345\n",
      "Epoch 80/100\n",
      "Training Loss: 0.07654020337358859\n",
      "Validation Loss: 0.05105529145730795\n",
      "Epoch 81/100\n",
      "Training Loss: 0.08601575645518073\n",
      "Validation Loss: 0.044791151009704294\n",
      "Epoch 82/100\n",
      "Training Loss: 0.07894667530447472\n",
      "Validation Loss: 0.05500453574067311\n",
      "Epoch 83/100\n",
      "Training Loss: 0.08054776184453648\n",
      "Validation Loss: 0.04950690895370493\n",
      "Epoch 84/100\n",
      "Training Loss: 0.06274952739215532\n",
      "Validation Loss: 0.03514424617941246\n",
      "Epoch 85/100\n",
      "Training Loss: 0.07392440092178046\n",
      "Validation Loss: 0.09746797314704524\n",
      "Epoch 86/100\n",
      "Training Loss: 0.07068277707492998\n",
      "Validation Loss: 0.051517678461373895\n",
      "Epoch 87/100\n",
      "Training Loss: 0.08891424143154149\n",
      "Validation Loss: 0.06202848197032844\n",
      "Epoch 88/100\n",
      "Training Loss: 0.060913974684566635\n",
      "Validation Loss: 0.10197927147064889\n",
      "Epoch 89/100\n",
      "Training Loss: 0.0716595545451162\n",
      "Validation Loss: 0.14753202124949438\n",
      "Epoch 90/100\n",
      "Training Loss: 0.09505106992301719\n",
      "Validation Loss: 0.05441859164468312\n",
      "Epoch 91/100\n",
      "Training Loss: 0.07681334985782072\n",
      "Validation Loss: 0.04558797818384741\n",
      "Epoch 92/100\n",
      "Training Loss: 0.06474043824528179\n",
      "Validation Loss: 0.1470232459360279\n",
      "Epoch 93/100\n",
      "Training Loss: 0.07739996108585008\n",
      "Validation Loss: 0.13239718526046917\n",
      "Epoch 94/100\n",
      "Training Loss: 0.07338373643455742\n",
      "Validation Loss: 0.06352837041386453\n",
      "Epoch 95/100\n",
      "Training Loss: 0.05614984049318051\n",
      "Validation Loss: 0.10538942602557144\n",
      "Epoch 96/100\n",
      "Training Loss: 0.06977492802280744\n",
      "Validation Loss: 0.09022934471970381\n",
      "Epoch 97/100\n",
      "Training Loss: 0.06291646352980845\n",
      "Validation Loss: 0.09575974287479706\n",
      "Epoch 98/100\n",
      "Training Loss: 0.08670824615023173\n",
      "Validation Loss: 0.13949085001011763\n",
      "Epoch 99/100\n",
      "Training Loss: 0.07956777791896427\n",
      "Validation Loss: 0.09928773476516012\n",
      "Epoch 100/100\n",
      "Training Loss: 0.08038028714894951\n",
      "Validation Loss: 0.07505669129598903\n",
      "    Trial 2/2 for combination 44/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.3653071103145104\n",
      "Validation Loss: 0.27736304923367794\n",
      "Epoch 2/100\n",
      "Training Loss: 0.2103650948671034\n",
      "Validation Loss: 0.3192611955548247\n",
      "Epoch 3/100\n",
      "Training Loss: 0.20453065139964902\n",
      "Validation Loss: 0.23103722007367478\n",
      "Epoch 4/100\n",
      "Training Loss: 0.2130929021578098\n",
      "Validation Loss: 0.16575477400171973\n",
      "Epoch 5/100\n",
      "Training Loss: 0.2055697839551857\n",
      "Validation Loss: 0.19375921814165492\n",
      "Epoch 6/100\n",
      "Training Loss: 0.165189336212914\n",
      "Validation Loss: 0.13942268808619868\n",
      "Epoch 7/100\n",
      "Training Loss: 0.130412522001214\n",
      "Validation Loss: 0.13473262573249806\n",
      "Epoch 8/100\n",
      "Training Loss: 0.12227818369764853\n",
      "Validation Loss: 0.1464912444646027\n",
      "Epoch 9/100\n",
      "Training Loss: 0.1159148274962988\n",
      "Validation Loss: 0.11458913176135381\n",
      "Epoch 10/100\n",
      "Training Loss: 0.1409066330371993\n",
      "Validation Loss: 0.08201003333958333\n",
      "Epoch 11/100\n",
      "Training Loss: 0.12907572860363933\n",
      "Validation Loss: 0.130912453754235\n",
      "Epoch 12/100\n",
      "Training Loss: 0.12136778660927121\n",
      "Validation Loss: 0.07453648237309006\n",
      "Epoch 13/100\n",
      "Training Loss: 0.10384682404429177\n",
      "Validation Loss: 0.07091047657359013\n",
      "Epoch 14/100\n",
      "Training Loss: 0.10322077385355362\n",
      "Validation Loss: 0.08296115364930687\n",
      "Epoch 15/100\n",
      "Training Loss: 0.11141656514400718\n",
      "Validation Loss: 0.10934103003110147\n",
      "Epoch 16/100\n",
      "Training Loss: 0.09656098612437478\n",
      "Validation Loss: 0.05240446157527825\n",
      "Epoch 17/100\n",
      "Training Loss: 0.09605153788814101\n",
      "Validation Loss: 0.051380467645425476\n",
      "Epoch 18/100\n",
      "Training Loss: 0.09245026681488183\n",
      "Validation Loss: 0.05337038713372694\n",
      "Epoch 19/100\n",
      "Training Loss: 0.08948408386155131\n",
      "Validation Loss: 0.0853996125641969\n",
      "Epoch 20/100\n",
      "Training Loss: 0.0962073254246184\n",
      "Validation Loss: 0.0929941110141588\n",
      "Epoch 21/100\n",
      "Training Loss: 0.08155337448489512\n",
      "Validation Loss: 0.04539869682889673\n",
      "Epoch 22/100\n",
      "Training Loss: 0.09837309919069999\n",
      "Validation Loss: 0.07213201745322242\n",
      "Epoch 23/100\n",
      "Training Loss: 0.07803529943819562\n",
      "Validation Loss: 0.04983332661753765\n",
      "Epoch 24/100\n",
      "Training Loss: 0.09122833661862134\n",
      "Validation Loss: 0.044808123135121454\n",
      "Epoch 25/100\n",
      "Training Loss: 0.08816177995745063\n",
      "Validation Loss: 0.060521637562821404\n",
      "Epoch 26/100\n",
      "Training Loss: 0.09219220362242042\n",
      "Validation Loss: 0.07810256331898432\n",
      "Epoch 27/100\n",
      "Training Loss: 0.08064061221670783\n",
      "Validation Loss: 0.05299438124374657\n",
      "Epoch 28/100\n",
      "Training Loss: 0.0847009439678073\n",
      "Validation Loss: 0.03584946605913709\n",
      "Epoch 29/100\n",
      "Training Loss: 0.08123782777005205\n",
      "Validation Loss: 0.043339934613420096\n",
      "Epoch 30/100\n",
      "Training Loss: 0.07660601669423232\n",
      "Validation Loss: 0.04072812614059871\n",
      "Epoch 31/100\n",
      "Training Loss: 0.07660182215000177\n",
      "Validation Loss: 0.047948822263792504\n",
      "Epoch 32/100\n",
      "Training Loss: 0.07629573197476373\n",
      "Validation Loss: 0.0572270819222642\n",
      "Epoch 33/100\n",
      "Training Loss: 0.07634301360458752\n",
      "Validation Loss: 0.06157300812436724\n",
      "Epoch 34/100\n",
      "Training Loss: 0.08098288764146847\n",
      "Validation Loss: 0.06347639596289334\n",
      "Epoch 35/100\n",
      "Training Loss: 0.07519233640079437\n",
      "Validation Loss: 0.05508605728393421\n",
      "Epoch 36/100\n",
      "Training Loss: 0.07886140718444391\n",
      "Validation Loss: 0.052932678927630226\n",
      "Epoch 37/100\n",
      "Training Loss: 0.06975827674103065\n",
      "Validation Loss: 0.03643563718965465\n",
      "Epoch 38/100\n",
      "Training Loss: 0.07442398188859299\n",
      "Validation Loss: 0.0531663476205646\n",
      "Epoch 39/100\n",
      "Training Loss: 0.0797511887716707\n",
      "Validation Loss: 0.0580282797484607\n",
      "Epoch 40/100\n",
      "Training Loss: 0.06637831781287484\n",
      "Validation Loss: 0.043223525463711734\n",
      "Epoch 41/100\n",
      "Training Loss: 0.07805935593681251\n",
      "Validation Loss: 0.046098992595576566\n",
      "Epoch 42/100\n",
      "Training Loss: 0.06770643177826503\n",
      "Validation Loss: 0.060714089143528674\n",
      "Epoch 43/100\n",
      "Training Loss: 0.07047504394373713\n",
      "Validation Loss: 0.05233414612494118\n",
      "Epoch 44/100\n",
      "Training Loss: 0.07694982085248797\n",
      "Validation Loss: 0.05488470588188055\n",
      "Epoch 45/100\n",
      "Training Loss: 0.08036703364394543\n",
      "Validation Loss: 0.06681119643851535\n",
      "Epoch 46/100\n",
      "Training Loss: 0.07801571771605365\n",
      "Validation Loss: 0.04088836520672205\n",
      "Epoch 47/100\n",
      "Training Loss: 0.06938656485166567\n",
      "Validation Loss: 0.04062911102552248\n",
      "Epoch 48/100\n",
      "Training Loss: 0.07076495135736079\n",
      "Validation Loss: 0.0460969988950832\n",
      "Epoch 49/100\n",
      "Training Loss: 0.06294223428819953\n",
      "Validation Loss: 0.08152306887045659\n",
      "Epoch 50/100\n",
      "Training Loss: 0.0721919264025911\n",
      "Validation Loss: 0.0470563805944619\n",
      "Epoch 51/100\n",
      "Training Loss: 0.08071149262673588\n",
      "Validation Loss: 0.05025296595677466\n",
      "Epoch 52/100\n",
      "Training Loss: 0.07778074246946744\n",
      "Validation Loss: 0.035438789688056076\n",
      "Epoch 53/100\n",
      "Training Loss: 0.06203784348590872\n",
      "Validation Loss: 0.058238098894864795\n",
      "Epoch 54/100\n",
      "Training Loss: 0.07831750046403842\n",
      "Validation Loss: 0.09447565666121892\n",
      "Epoch 55/100\n",
      "Training Loss: 0.07966894881818432\n",
      "Validation Loss: 0.04606437155022737\n",
      "Epoch 56/100\n",
      "Training Loss: 0.07356436992236173\n",
      "Validation Loss: 0.06741622464683847\n",
      "Epoch 57/100\n",
      "Training Loss: 0.06659869709453091\n",
      "Validation Loss: 0.0955643279294401\n",
      "Epoch 58/100\n",
      "Training Loss: 0.0762130153504476\n",
      "Validation Loss: 0.03868191571868178\n",
      "Epoch 59/100\n",
      "Training Loss: 0.08093424509138743\n",
      "Validation Loss: 0.050705953637605605\n",
      "Epoch 60/100\n",
      "Training Loss: 0.07312292412923296\n",
      "Validation Loss: 0.061194314026013684\n",
      "Epoch 61/100\n",
      "Training Loss: 0.06588876977069595\n",
      "Validation Loss: 0.04426009699755802\n",
      "Epoch 62/100\n",
      "Training Loss: 0.07979205527248226\n",
      "Validation Loss: 0.0465244479324729\n",
      "Epoch 63/100\n",
      "Training Loss: 0.07004224856608894\n",
      "Validation Loss: 0.021942231350048975\n",
      "Epoch 64/100\n",
      "Training Loss: 0.07436194266270198\n",
      "Validation Loss: 0.06606835081798923\n",
      "Epoch 65/100\n",
      "Training Loss: 0.07646627719863225\n",
      "Validation Loss: 0.03526185243107064\n",
      "Epoch 66/100\n",
      "Training Loss: 0.06750748755755653\n",
      "Validation Loss: 0.06438195097613503\n",
      "Epoch 67/100\n",
      "Training Loss: 0.07818858792195203\n",
      "Validation Loss: 0.06498905888041419\n",
      "Epoch 68/100\n",
      "Training Loss: 0.06896733035139126\n",
      "Validation Loss: 0.05329630387672442\n",
      "Epoch 69/100\n",
      "Training Loss: 0.08394277908496889\n",
      "Validation Loss: 0.06798141020615056\n",
      "Epoch 70/100\n",
      "Training Loss: 0.07891435044852384\n",
      "Validation Loss: 0.06562939245654126\n",
      "Epoch 71/100\n",
      "Training Loss: 0.06525352178087322\n",
      "Validation Loss: 0.07426906722264433\n",
      "Epoch 72/100\n",
      "Training Loss: 0.07916978006522092\n",
      "Validation Loss: 0.05679521586900722\n",
      "Epoch 73/100\n",
      "Training Loss: 0.0686754936249162\n",
      "Validation Loss: 0.08672922846369718\n",
      "Epoch 74/100\n",
      "Training Loss: 0.07732225278827881\n",
      "Validation Loss: 0.038704972600087026\n",
      "Epoch 75/100\n",
      "Training Loss: 0.06773935314774919\n",
      "Validation Loss: 0.08510267786265291\n",
      "Epoch 76/100\n",
      "Training Loss: 0.06919960356621953\n",
      "Validation Loss: 0.0862864569792505\n",
      "Epoch 77/100\n",
      "Training Loss: 0.0732589385826418\n",
      "Validation Loss: 0.03778942839005898\n",
      "Epoch 78/100\n",
      "Training Loss: 0.06183324731453306\n",
      "Validation Loss: 0.07317881863887729\n",
      "Epoch 79/100\n",
      "Training Loss: 0.07819429085438426\n",
      "Validation Loss: 0.025417956691127792\n",
      "Epoch 80/100\n",
      "Training Loss: 0.08226579087861316\n",
      "Validation Loss: 0.026360648063676956\n",
      "Epoch 81/100\n",
      "Training Loss: 0.07180068911118863\n",
      "Validation Loss: 0.14342636351932764\n",
      "Epoch 82/100\n",
      "Training Loss: 0.07088606584939382\n",
      "Validation Loss: 0.03049509050493625\n",
      "Epoch 83/100\n",
      "Training Loss: 0.07295556784477708\n",
      "Validation Loss: 0.11538124566603569\n",
      "Epoch 84/100\n",
      "Training Loss: 0.09894641423458353\n",
      "Validation Loss: 0.04677456428317088\n",
      "Epoch 85/100\n",
      "Training Loss: 0.063946465466131\n",
      "Validation Loss: 0.046172866830139624\n",
      "Epoch 86/100\n",
      "Training Loss: 0.07953790041493368\n",
      "Validation Loss: 0.10132917158699017\n",
      "Epoch 87/100\n",
      "Training Loss: 0.0621283843494421\n",
      "Validation Loss: 0.1019486989347147\n",
      "Epoch 88/100\n",
      "Training Loss: 0.07682960868510631\n",
      "Validation Loss: 0.04615896308875718\n",
      "Epoch 89/100\n",
      "Training Loss: 0.07562066609622788\n",
      "Validation Loss: 0.08467317583565846\n",
      "Epoch 90/100\n",
      "Training Loss: 0.08017035310997689\n",
      "Validation Loss: 0.14092964196621122\n",
      "Epoch 91/100\n",
      "Training Loss: 0.07442052652493929\n",
      "Validation Loss: 0.021236708461295113\n",
      "Epoch 92/100\n",
      "Training Loss: 0.09190068471887962\n",
      "Validation Loss: 0.1321299641350377\n",
      "Epoch 93/100\n",
      "Training Loss: 0.07291099839041267\n",
      "Validation Loss: 0.09754190794910311\n",
      "Epoch 94/100\n",
      "Training Loss: 0.07428335379754854\n",
      "Validation Loss: 0.07737605754375551\n",
      "Epoch 95/100\n",
      "Training Loss: 0.07546117215325057\n",
      "Validation Loss: 0.06806790037835123\n",
      "Epoch 96/100\n",
      "Training Loss: 0.06330591793357472\n",
      "Validation Loss: 0.1060192220273258\n",
      "Epoch 97/100\n",
      "Training Loss: 0.088810031612963\n",
      "Validation Loss: 0.13779350559801076\n",
      "Epoch 98/100\n",
      "Training Loss: 0.07456504336925639\n",
      "Validation Loss: 0.0802578632786654\n",
      "Epoch 99/100\n",
      "Training Loss: 0.06848112868303051\n",
      "Validation Loss: 0.08176938492655156\n",
      "Epoch 100/100\n",
      "Training Loss: 0.09280304878948341\n",
      "Validation Loss: 0.04022870095748148\n",
      "Combination 44: Avg Training Loss = 0.0921776106361727, Avg Validation Loss = 0.07421672688171821\n",
      "Testing combination 45/48: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.001, 'sequence_length': 20, 'dropout_rate': 0.1}\n",
      "Truncated Train Data Shape: (70, 20, 1)\n",
      "Truncated Validation Data Shape: (10, 20, 1)\n",
      "    Trial 1/2 for combination 45/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.4314314034719974\n",
      "Validation Loss: 0.24971427521104275\n",
      "Epoch 2/100\n",
      "Training Loss: 0.21702184797575488\n",
      "Validation Loss: 0.19805393738233734\n",
      "Epoch 3/100\n",
      "Training Loss: 0.20060422375563539\n",
      "Validation Loss: 0.17801024037321084\n",
      "Epoch 4/100\n",
      "Training Loss: 0.1943007666387718\n",
      "Validation Loss: 0.13842690364572674\n",
      "Epoch 5/100\n",
      "Training Loss: 0.1701292341290881\n",
      "Validation Loss: 0.13416737843467116\n",
      "Epoch 6/100\n",
      "Training Loss: 0.17653607495587526\n",
      "Validation Loss: 0.05959720515139003\n",
      "Epoch 7/100\n",
      "Training Loss: 0.1373637727581363\n",
      "Validation Loss: 0.08413068893202254\n",
      "Epoch 8/100\n",
      "Training Loss: 0.14397001574722237\n",
      "Validation Loss: 0.10745130224578606\n",
      "Epoch 9/100\n",
      "Training Loss: 0.12057493775077095\n",
      "Validation Loss: 0.051888384303982196\n",
      "Epoch 10/100\n",
      "Training Loss: 0.1278752423848514\n",
      "Validation Loss: 0.06688675913569113\n",
      "Epoch 11/100\n",
      "Training Loss: 0.13036882590341248\n",
      "Validation Loss: 0.06309580947829987\n",
      "Epoch 12/100\n",
      "Training Loss: 0.10386668833866866\n",
      "Validation Loss: 0.15174127926047704\n",
      "Epoch 13/100\n",
      "Training Loss: 0.12800546576541585\n",
      "Validation Loss: 0.04309586105855156\n",
      "Epoch 14/100\n",
      "Training Loss: 0.10013587590455977\n",
      "Validation Loss: 0.060392580998869336\n",
      "Epoch 15/100\n",
      "Training Loss: 0.09291244524661584\n",
      "Validation Loss: 0.0567172164729435\n",
      "Epoch 16/100\n",
      "Training Loss: 0.08455398727345374\n",
      "Validation Loss: 0.03637437533081052\n",
      "Epoch 17/100\n",
      "Training Loss: 0.10030140609454388\n",
      "Validation Loss: 0.04348485492663161\n",
      "Epoch 18/100\n",
      "Training Loss: 0.0779502621192725\n",
      "Validation Loss: 0.05570402837728859\n",
      "Epoch 19/100\n",
      "Training Loss: 0.0793200942304055\n",
      "Validation Loss: 0.07685414302706935\n",
      "Epoch 20/100\n",
      "Training Loss: 0.07588412536656436\n",
      "Validation Loss: 0.039739366698942444\n",
      "Epoch 21/100\n",
      "Training Loss: 0.07020420404724392\n",
      "Validation Loss: 0.07158476339851055\n",
      "Epoch 22/100\n",
      "Training Loss: 0.07037709855622487\n",
      "Validation Loss: 0.044778861897329034\n",
      "Epoch 23/100\n",
      "Training Loss: 0.0830839422309417\n",
      "Validation Loss: 0.041093100735372506\n",
      "Epoch 24/100\n",
      "Training Loss: 0.05429660052277306\n",
      "Validation Loss: 0.04179230166022486\n",
      "Epoch 25/100\n",
      "Training Loss: 0.06436656759216551\n",
      "Validation Loss: 0.030311028000537456\n",
      "Epoch 26/100\n",
      "Training Loss: 0.05709421698997445\n",
      "Validation Loss: 0.06951278993652661\n",
      "Epoch 27/100\n",
      "Training Loss: 0.05873008511394941\n",
      "Validation Loss: 0.07880747760557295\n",
      "Epoch 28/100\n",
      "Training Loss: 0.05209775178884668\n",
      "Validation Loss: 0.07499377897637963\n",
      "Epoch 29/100\n",
      "Training Loss: 0.0615210346419703\n",
      "Validation Loss: 0.05826235551037795\n",
      "Epoch 30/100\n",
      "Training Loss: 0.05706827622609098\n",
      "Validation Loss: 0.04135885892187912\n",
      "Epoch 31/100\n",
      "Training Loss: 0.0552581472788757\n",
      "Validation Loss: 0.05251098139551573\n",
      "Epoch 32/100\n",
      "Training Loss: 0.053833062623731325\n",
      "Validation Loss: 0.032941814531651584\n",
      "Epoch 33/100\n",
      "Training Loss: 0.06140139473712281\n",
      "Validation Loss: 0.056603177679903485\n",
      "Epoch 34/100\n",
      "Training Loss: 0.0515798083463689\n",
      "Validation Loss: 0.04487632659476929\n",
      "Epoch 35/100\n",
      "Training Loss: 0.04895159391941409\n",
      "Validation Loss: 0.02468467668042264\n",
      "Epoch 36/100\n",
      "Training Loss: 0.04766279243749847\n",
      "Validation Loss: 0.065810228576312\n",
      "Epoch 37/100\n",
      "Training Loss: 0.05411461559276347\n",
      "Validation Loss: 0.08326918573882693\n",
      "Epoch 38/100\n",
      "Training Loss: 0.06475243072870385\n",
      "Validation Loss: 0.10029187653654639\n",
      "Epoch 39/100\n",
      "Training Loss: 0.05364965353615868\n",
      "Validation Loss: 0.04747870646104221\n",
      "Epoch 40/100\n",
      "Training Loss: 0.054310406227048864\n",
      "Validation Loss: 0.06337089396653084\n",
      "Epoch 41/100\n",
      "Training Loss: 0.0473361430361155\n",
      "Validation Loss: 0.03327245954237929\n",
      "Epoch 42/100\n",
      "Training Loss: 0.049962033212839464\n",
      "Validation Loss: 0.08960142461591376\n",
      "Epoch 43/100\n",
      "Training Loss: 0.0616424967992431\n",
      "Validation Loss: 0.03898581364643307\n",
      "Epoch 44/100\n",
      "Training Loss: 0.052862766544095\n",
      "Validation Loss: 0.07058838955908918\n",
      "Epoch 45/100\n",
      "Training Loss: 0.05475982688620851\n",
      "Validation Loss: 0.052362378994361455\n",
      "Epoch 46/100\n",
      "Training Loss: 0.04791165739343887\n",
      "Validation Loss: 0.04161261192911597\n",
      "Epoch 47/100\n",
      "Training Loss: 0.05080421228480636\n",
      "Validation Loss: 0.04315153203212131\n",
      "Epoch 48/100\n",
      "Training Loss: 0.04621746776143753\n",
      "Validation Loss: 0.05097851250368788\n",
      "Epoch 49/100\n",
      "Training Loss: 0.05448985421876358\n",
      "Validation Loss: 0.08858942622808101\n",
      "Epoch 50/100\n",
      "Training Loss: 0.05788395631029235\n",
      "Validation Loss: 0.07600419141251373\n",
      "Epoch 51/100\n",
      "Training Loss: 0.050287693962596025\n",
      "Validation Loss: 0.06420467240494061\n",
      "Epoch 52/100\n",
      "Training Loss: 0.050904773605169186\n",
      "Validation Loss: 0.030405898480788812\n",
      "Epoch 53/100\n",
      "Training Loss: 0.054304003100294464\n",
      "Validation Loss: 0.0640386363460324\n",
      "Epoch 54/100\n",
      "Training Loss: 0.04412407633746497\n",
      "Validation Loss: 0.06693495021874198\n",
      "Epoch 55/100\n",
      "Training Loss: 0.06762530039360833\n",
      "Validation Loss: 0.061861831774801\n",
      "Epoch 56/100\n",
      "Training Loss: 0.047628833209248884\n",
      "Validation Loss: 0.042856396792536486\n",
      "Epoch 57/100\n",
      "Training Loss: 0.04815542565774459\n",
      "Validation Loss: 0.07285864823713331\n",
      "Epoch 58/100\n",
      "Training Loss: 0.049503485262000685\n",
      "Validation Loss: 0.07983061526079166\n",
      "Epoch 59/100\n",
      "Training Loss: 0.05204230082624558\n",
      "Validation Loss: 0.07629135068948609\n",
      "Epoch 60/100\n",
      "Training Loss: 0.05842007309281454\n",
      "Validation Loss: 0.057479610341056905\n",
      "Epoch 61/100\n",
      "Training Loss: 0.04753989655338457\n",
      "Validation Loss: 0.06534800093432289\n",
      "Epoch 62/100\n",
      "Training Loss: 0.06663677930517423\n",
      "Validation Loss: 0.044371076822034256\n",
      "Epoch 63/100\n",
      "Training Loss: 0.05483646607700898\n",
      "Validation Loss: 0.054206966742948434\n",
      "Epoch 64/100\n",
      "Training Loss: 0.0681027841380204\n",
      "Validation Loss: 0.04467025258383677\n",
      "Epoch 65/100\n",
      "Training Loss: 0.06027113364727143\n",
      "Validation Loss: 0.033659270971866294\n",
      "Epoch 66/100\n",
      "Training Loss: 0.05535417079451301\n",
      "Validation Loss: 0.04146260401014007\n",
      "Epoch 67/100\n",
      "Training Loss: 0.06329531632231696\n",
      "Validation Loss: 0.07217392769882171\n",
      "Epoch 68/100\n",
      "Training Loss: 0.07017992223170293\n",
      "Validation Loss: 0.06893587352312683\n",
      "Epoch 69/100\n",
      "Training Loss: 0.05905579719793182\n",
      "Validation Loss: 0.05776682526888147\n",
      "Epoch 70/100\n",
      "Training Loss: 0.06206524246158095\n",
      "Validation Loss: 0.04151455079345025\n",
      "Epoch 71/100\n",
      "Training Loss: 0.06149322862363099\n",
      "Validation Loss: 0.049819235130818824\n",
      "Epoch 72/100\n",
      "Training Loss: 0.07064490237773298\n",
      "Validation Loss: 0.07375748888651765\n",
      "Epoch 73/100\n",
      "Training Loss: 0.06199176402652589\n",
      "Validation Loss: 0.029827082783975063\n",
      "Epoch 74/100\n",
      "Training Loss: 0.06205832592080984\n",
      "Validation Loss: 0.08594253000175757\n",
      "Epoch 75/100\n",
      "Training Loss: 0.06632748514039616\n",
      "Validation Loss: 0.02902263692566747\n",
      "Epoch 76/100\n",
      "Training Loss: 0.06388535656068556\n",
      "Validation Loss: 0.07502828599968706\n",
      "Epoch 77/100\n",
      "Training Loss: 0.05762988597158739\n",
      "Validation Loss: 0.037674468103729764\n",
      "Epoch 78/100\n",
      "Training Loss: 0.056228523824654576\n",
      "Validation Loss: 0.07355155610496725\n",
      "Epoch 79/100\n",
      "Training Loss: 0.05638095516200895\n",
      "Validation Loss: 0.05643183004526779\n",
      "Epoch 80/100\n",
      "Training Loss: 0.07611817580625369\n",
      "Validation Loss: 0.034125829270721234\n",
      "Epoch 81/100\n",
      "Training Loss: 0.05376079137941082\n",
      "Validation Loss: 0.03918062133792232\n",
      "Epoch 82/100\n",
      "Training Loss: 0.06145666311368513\n",
      "Validation Loss: 0.06320763308173999\n",
      "Epoch 83/100\n",
      "Training Loss: 0.06351176636532811\n",
      "Validation Loss: 0.0398374165565776\n",
      "Epoch 84/100\n",
      "Training Loss: 0.06849897765951996\n",
      "Validation Loss: 0.060217533556184366\n",
      "Epoch 85/100\n",
      "Training Loss: 0.06305213564765821\n",
      "Validation Loss: 0.04879520380567261\n",
      "Epoch 86/100\n",
      "Training Loss: 0.059689463164365984\n",
      "Validation Loss: 0.04171903826280369\n",
      "Epoch 87/100\n",
      "Training Loss: 0.06234059487437749\n",
      "Validation Loss: 0.04070009580105076\n",
      "Epoch 88/100\n",
      "Training Loss: 0.06560905112903881\n",
      "Validation Loss: 0.03181068252420623\n",
      "Epoch 89/100\n",
      "Training Loss: 0.06098116907578294\n",
      "Validation Loss: 0.04942614473623545\n",
      "Epoch 90/100\n",
      "Training Loss: 0.0662158234003401\n",
      "Validation Loss: 0.03410467684793728\n",
      "Epoch 91/100\n",
      "Training Loss: 0.05802205751237954\n",
      "Validation Loss: 0.04242193646488962\n",
      "Epoch 92/100\n",
      "Training Loss: 0.05405371453951583\n",
      "Validation Loss: 0.03489462020157766\n",
      "Epoch 93/100\n",
      "Training Loss: 0.055402189887103895\n",
      "Validation Loss: 0.05109893461499625\n",
      "Epoch 94/100\n",
      "Training Loss: 0.06585924009099044\n",
      "Validation Loss: 0.08827705962814406\n",
      "Epoch 95/100\n",
      "Training Loss: 0.05845874277624682\n",
      "Validation Loss: 0.030124382990147545\n",
      "Epoch 96/100\n",
      "Training Loss: 0.07466916125398847\n",
      "Validation Loss: 0.03236597738398646\n",
      "Epoch 97/100\n",
      "Training Loss: 0.0654274299375477\n",
      "Validation Loss: 0.0457549224671728\n",
      "Epoch 98/100\n",
      "Training Loss: 0.07107407639266185\n",
      "Validation Loss: 0.021132181223842396\n",
      "Epoch 99/100\n",
      "Training Loss: 0.07031696449087908\n",
      "Validation Loss: 0.04366285555692507\n",
      "Epoch 100/100\n",
      "Training Loss: 0.05141303375371109\n",
      "Validation Loss: 0.05802807733176266\n",
      "    Trial 2/2 for combination 45/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.41434666018924043\n",
      "Validation Loss: 0.14539667560676955\n",
      "Epoch 2/100\n",
      "Training Loss: 0.20726934803995598\n",
      "Validation Loss: 0.2241644739336101\n",
      "Epoch 3/100\n",
      "Training Loss: 0.22117382778789824\n",
      "Validation Loss: 0.25080214311477056\n",
      "Epoch 4/100\n",
      "Training Loss: 0.2093114705472945\n",
      "Validation Loss: 0.15015333167278383\n",
      "Epoch 5/100\n",
      "Training Loss: 0.17125014915212114\n",
      "Validation Loss: 0.11839366743563307\n",
      "Epoch 6/100\n",
      "Training Loss: 0.17432668898337025\n",
      "Validation Loss: 0.10874947745137038\n",
      "Epoch 7/100\n",
      "Training Loss: 0.1485527551471633\n",
      "Validation Loss: 0.19944830054837598\n",
      "Epoch 8/100\n",
      "Training Loss: 0.15631089605784268\n",
      "Validation Loss: 0.07463277940647026\n",
      "Epoch 9/100\n",
      "Training Loss: 0.1529120139734538\n",
      "Validation Loss: 0.06792671417766738\n",
      "Epoch 10/100\n",
      "Training Loss: 0.15143460138208834\n",
      "Validation Loss: 0.06455260777797431\n",
      "Epoch 11/100\n",
      "Training Loss: 0.12445100748880325\n",
      "Validation Loss: 0.0955514179227543\n",
      "Epoch 12/100\n",
      "Training Loss: 0.12581470359956012\n",
      "Validation Loss: 0.089284490614347\n",
      "Epoch 13/100\n",
      "Training Loss: 0.11662408491902396\n",
      "Validation Loss: 0.1182603344134665\n",
      "Epoch 14/100\n",
      "Training Loss: 0.11759586938175651\n",
      "Validation Loss: 0.12108569175024475\n",
      "Epoch 15/100\n",
      "Training Loss: 0.09597917332477564\n",
      "Validation Loss: 0.09048172861621766\n",
      "Epoch 16/100\n",
      "Training Loss: 0.07400811518312424\n",
      "Validation Loss: 0.06709776247903546\n",
      "Epoch 17/100\n",
      "Training Loss: 0.09973571839536793\n",
      "Validation Loss: 0.06620029590692252\n",
      "Epoch 18/100\n",
      "Training Loss: 0.08431330943930206\n",
      "Validation Loss: 0.04483438662348912\n",
      "Epoch 19/100\n",
      "Training Loss: 0.07918361222123313\n",
      "Validation Loss: 0.06090632284147751\n",
      "Epoch 20/100\n",
      "Training Loss: 0.08223536859760608\n",
      "Validation Loss: 0.07150854761064024\n",
      "Epoch 21/100\n",
      "Training Loss: 0.08217126675542763\n",
      "Validation Loss: 0.07738294848078157\n",
      "Epoch 22/100\n",
      "Training Loss: 0.06894890359314688\n",
      "Validation Loss: 0.03858191631009551\n",
      "Epoch 23/100\n",
      "Training Loss: 0.059895845311862116\n",
      "Validation Loss: 0.06680570654493241\n",
      "Epoch 24/100\n",
      "Training Loss: 0.0722060790903635\n",
      "Validation Loss: 0.08488755763632837\n",
      "Epoch 25/100\n",
      "Training Loss: 0.0684202771592449\n",
      "Validation Loss: 0.05564069377117451\n",
      "Epoch 26/100\n",
      "Training Loss: 0.06732623522616663\n",
      "Validation Loss: 0.058725809506524854\n",
      "Epoch 27/100\n",
      "Training Loss: 0.06388613839237718\n",
      "Validation Loss: 0.04966328891861456\n",
      "Epoch 28/100\n",
      "Training Loss: 0.05490580253284064\n",
      "Validation Loss: 0.044352510187658775\n",
      "Epoch 29/100\n",
      "Training Loss: 0.06406863128030439\n",
      "Validation Loss: 0.10941036352952031\n",
      "Epoch 30/100\n",
      "Training Loss: 0.05652686922033066\n",
      "Validation Loss: 0.051040792102593456\n",
      "Epoch 31/100\n",
      "Training Loss: 0.06643608185744111\n",
      "Validation Loss: 0.05197676913382378\n",
      "Epoch 32/100\n",
      "Training Loss: 0.059694074865840004\n",
      "Validation Loss: 0.05380236327634713\n",
      "Epoch 33/100\n",
      "Training Loss: 0.05136303904259671\n",
      "Validation Loss: 0.048890672621177425\n",
      "Epoch 34/100\n",
      "Training Loss: 0.060273329219304006\n",
      "Validation Loss: 0.03616330892388207\n",
      "Epoch 35/100\n",
      "Training Loss: 0.05506819734206618\n",
      "Validation Loss: 0.036977004684842545\n",
      "Epoch 36/100\n",
      "Training Loss: 0.04579092467802277\n",
      "Validation Loss: 0.04457515692328551\n",
      "Epoch 37/100\n",
      "Training Loss: 0.04657211379542744\n",
      "Validation Loss: 0.04421067036799953\n",
      "Epoch 38/100\n",
      "Training Loss: 0.05982857396205507\n",
      "Validation Loss: 0.04085433050225849\n",
      "Epoch 39/100\n",
      "Training Loss: 0.05950544799726836\n",
      "Validation Loss: 0.07045275652577544\n",
      "Epoch 40/100\n",
      "Training Loss: 0.058464158640362994\n",
      "Validation Loss: 0.052490790762085016\n",
      "Epoch 41/100\n",
      "Training Loss: 0.05718409781705068\n",
      "Validation Loss: 0.036458589188659206\n",
      "Epoch 42/100\n",
      "Training Loss: 0.05667855076043269\n",
      "Validation Loss: 0.03636545528044388\n",
      "Epoch 43/100\n",
      "Training Loss: 0.058012632412624575\n",
      "Validation Loss: 0.07042289887699434\n",
      "Epoch 44/100\n",
      "Training Loss: 0.05535161282053864\n",
      "Validation Loss: 0.04701020373283795\n",
      "Epoch 45/100\n",
      "Training Loss: 0.05708520220805424\n",
      "Validation Loss: 0.06147395082179703\n",
      "Epoch 46/100\n",
      "Training Loss: 0.049057978889427416\n",
      "Validation Loss: 0.039471027777023826\n",
      "Epoch 47/100\n",
      "Training Loss: 0.04090243987459455\n",
      "Validation Loss: 0.06599503139588561\n",
      "Epoch 48/100\n",
      "Training Loss: 0.054256259212488736\n",
      "Validation Loss: 0.06342180327807317\n",
      "Epoch 49/100\n",
      "Training Loss: 0.04969081755032227\n",
      "Validation Loss: 0.04768129594821693\n",
      "Epoch 50/100\n",
      "Training Loss: 0.04404167116741428\n",
      "Validation Loss: 0.05195850158576502\n",
      "Epoch 51/100\n",
      "Training Loss: 0.051686946321766285\n",
      "Validation Loss: 0.04291460864733716\n",
      "Epoch 52/100\n",
      "Training Loss: 0.05404234023774867\n",
      "Validation Loss: 0.045889735334694946\n",
      "Epoch 53/100\n",
      "Training Loss: 0.0715950366995386\n",
      "Validation Loss: 0.04908331891350731\n",
      "Epoch 54/100\n",
      "Training Loss: 0.047391066353856016\n",
      "Validation Loss: 0.05525624839244094\n",
      "Epoch 55/100\n",
      "Training Loss: 0.05231188373108689\n",
      "Validation Loss: 0.06676630315082693\n",
      "Epoch 56/100\n",
      "Training Loss: 0.057993471561661916\n",
      "Validation Loss: 0.0871397738145997\n",
      "Epoch 57/100\n",
      "Training Loss: 0.048007235776941375\n",
      "Validation Loss: 0.05658213985016246\n",
      "Epoch 58/100\n",
      "Training Loss: 0.06355388621527058\n",
      "Validation Loss: 0.09550732023342452\n",
      "Epoch 59/100\n",
      "Training Loss: 0.05208738714028116\n",
      "Validation Loss: 0.06494368769190631\n",
      "Epoch 60/100\n",
      "Training Loss: 0.052791298746195287\n",
      "Validation Loss: 0.05953869511023572\n",
      "Epoch 61/100\n",
      "Training Loss: 0.05441163094831555\n",
      "Validation Loss: 0.027937651712620602\n",
      "Epoch 62/100\n",
      "Training Loss: 0.06082733646658881\n",
      "Validation Loss: 0.03699715501169988\n",
      "Epoch 63/100\n",
      "Training Loss: 0.05492688999054257\n",
      "Validation Loss: 0.07297167892527985\n",
      "Epoch 64/100\n",
      "Training Loss: 0.05569280188918249\n",
      "Validation Loss: 0.051571387545126766\n",
      "Epoch 65/100\n",
      "Training Loss: 0.056812629244844044\n",
      "Validation Loss: 0.04645357394280125\n",
      "Epoch 66/100\n",
      "Training Loss: 0.0537254662509801\n",
      "Validation Loss: 0.0300450077320873\n",
      "Epoch 67/100\n",
      "Training Loss: 0.06813081319495606\n",
      "Validation Loss: 0.08811311227289684\n",
      "Epoch 68/100\n",
      "Training Loss: 0.06085311023718305\n",
      "Validation Loss: 0.04550376640362573\n",
      "Epoch 69/100\n",
      "Training Loss: 0.05629982782042276\n",
      "Validation Loss: 0.057696088596759634\n",
      "Epoch 70/100\n",
      "Training Loss: 0.06436534173402962\n",
      "Validation Loss: 0.04693025811964889\n",
      "Epoch 71/100\n",
      "Training Loss: 0.062429029795760614\n",
      "Validation Loss: 0.04258566715507224\n",
      "Epoch 72/100\n",
      "Training Loss: 0.055320571863491874\n",
      "Validation Loss: 0.10162289231052828\n",
      "Epoch 73/100\n",
      "Training Loss: 0.06666728076460271\n",
      "Validation Loss: 0.04814548028635236\n",
      "Epoch 74/100\n",
      "Training Loss: 0.062290948432153014\n",
      "Validation Loss: 0.11952353354129445\n",
      "Epoch 75/100\n",
      "Training Loss: 0.0636727946152083\n",
      "Validation Loss: 0.034991856537585915\n",
      "Epoch 76/100\n",
      "Training Loss: 0.061664641543211766\n",
      "Validation Loss: 0.06252937695205371\n",
      "Epoch 77/100\n",
      "Training Loss: 0.062370314242487\n",
      "Validation Loss: 0.08334035839493897\n",
      "Epoch 78/100\n",
      "Training Loss: 0.0555996921417718\n",
      "Validation Loss: 0.06056363344752765\n",
      "Epoch 79/100\n",
      "Training Loss: 0.06396993876953103\n",
      "Validation Loss: 0.051027487333165956\n",
      "Epoch 80/100\n",
      "Training Loss: 0.0683359486895091\n",
      "Validation Loss: 0.05831864746182573\n",
      "Epoch 81/100\n",
      "Training Loss: 0.06077551716439693\n",
      "Validation Loss: 0.033330891999091124\n",
      "Epoch 82/100\n",
      "Training Loss: 0.062104270730040906\n",
      "Validation Loss: 0.06704503281444374\n",
      "Epoch 83/100\n",
      "Training Loss: 0.06507245970804545\n",
      "Validation Loss: 0.03822635574046364\n",
      "Epoch 84/100\n",
      "Training Loss: 0.06367853348105314\n",
      "Validation Loss: 0.05219713856895007\n",
      "Epoch 85/100\n",
      "Training Loss: 0.0570205294716332\n",
      "Validation Loss: 0.019537727439739836\n",
      "Epoch 86/100\n",
      "Training Loss: 0.06519373587546749\n",
      "Validation Loss: 0.04987289341461919\n",
      "Epoch 87/100\n",
      "Training Loss: 0.05659192137696447\n",
      "Validation Loss: 0.03511368918025435\n",
      "Epoch 88/100\n",
      "Training Loss: 0.06018866544565134\n",
      "Validation Loss: 0.03826163031410802\n",
      "Epoch 89/100\n",
      "Training Loss: 0.06436342437869866\n",
      "Validation Loss: 0.04691395127986109\n",
      "Epoch 90/100\n",
      "Training Loss: 0.06113572308664992\n",
      "Validation Loss: 0.04136952153222872\n",
      "Epoch 91/100\n",
      "Training Loss: 0.07352111653064342\n",
      "Validation Loss: 0.03524443946609329\n",
      "Epoch 92/100\n",
      "Training Loss: 0.05735956760789501\n",
      "Validation Loss: 0.04484524708373963\n",
      "Epoch 93/100\n",
      "Training Loss: 0.05617230027159792\n",
      "Validation Loss: 0.026100502897327228\n",
      "Epoch 94/100\n",
      "Training Loss: 0.06747941534077186\n",
      "Validation Loss: 0.05631827550556956\n",
      "Epoch 95/100\n",
      "Training Loss: 0.057374770325391365\n",
      "Validation Loss: 0.05888008956721689\n",
      "Epoch 96/100\n",
      "Training Loss: 0.0572378866363843\n",
      "Validation Loss: 0.05817278898030063\n",
      "Epoch 97/100\n",
      "Training Loss: 0.06746171903602333\n",
      "Validation Loss: 0.027124201460046033\n",
      "Epoch 98/100\n",
      "Training Loss: 0.06215645468097294\n",
      "Validation Loss: 0.03510741973381003\n",
      "Epoch 99/100\n",
      "Training Loss: 0.05571808054483379\n",
      "Validation Loss: 0.038639781274364446\n",
      "Epoch 100/100\n",
      "Training Loss: 0.0630660949283458\n",
      "Validation Loss: 0.023101810882798172\n",
      "Combination 45: Avg Training Loss = 0.07674089145978996, Avg Validation Loss = 0.06335144282846383\n",
      "Testing combination 46/48: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.001, 'sequence_length': 20, 'dropout_rate': 0.2}\n",
      "Truncated Train Data Shape: (70, 20, 1)\n",
      "Truncated Validation Data Shape: (10, 20, 1)\n",
      "    Trial 1/2 for combination 46/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.2974015885651759\n",
      "Validation Loss: 0.21460796863723614\n",
      "Epoch 2/100\n",
      "Training Loss: 0.18396463397868554\n",
      "Validation Loss: 0.1296356845597208\n",
      "Epoch 3/100\n",
      "Training Loss: 0.14842894609279594\n",
      "Validation Loss: 0.12931742082908732\n",
      "Epoch 4/100\n",
      "Training Loss: 0.15304754868811374\n",
      "Validation Loss: 0.10188729657588103\n",
      "Epoch 5/100\n",
      "Training Loss: 0.15496292994778302\n",
      "Validation Loss: 0.07790128041100709\n",
      "Epoch 6/100\n",
      "Training Loss: 0.12238553471563965\n",
      "Validation Loss: 0.12638864868134092\n",
      "Epoch 7/100\n",
      "Training Loss: 0.14158074229186923\n",
      "Validation Loss: 0.10147764153975296\n",
      "Epoch 8/100\n",
      "Training Loss: 0.1169592661835667\n",
      "Validation Loss: 0.0678893435293084\n",
      "Epoch 9/100\n",
      "Training Loss: 0.10523334976115704\n",
      "Validation Loss: 0.08648305854321778\n",
      "Epoch 10/100\n",
      "Training Loss: 0.09913255481965874\n",
      "Validation Loss: 0.08466229371551856\n",
      "Epoch 11/100\n",
      "Training Loss: 0.09567589618796865\n",
      "Validation Loss: 0.06097500021899159\n",
      "Epoch 12/100\n",
      "Training Loss: 0.09527079358970696\n",
      "Validation Loss: 0.04252767702568446\n",
      "Epoch 13/100\n",
      "Training Loss: 0.08072068204642542\n",
      "Validation Loss: 0.06119555479418461\n",
      "Epoch 14/100\n",
      "Training Loss: 0.09144259807005772\n",
      "Validation Loss: 0.04373568574961363\n",
      "Epoch 15/100\n",
      "Training Loss: 0.0815006011904474\n",
      "Validation Loss: 0.06305855185629008\n",
      "Epoch 16/100\n",
      "Training Loss: 0.07829303864463728\n",
      "Validation Loss: 0.06131157448366622\n",
      "Epoch 17/100\n",
      "Training Loss: 0.08007707250543694\n",
      "Validation Loss: 0.036129478864683526\n",
      "Epoch 18/100\n",
      "Training Loss: 0.07896278185907903\n",
      "Validation Loss: 0.05273431690359411\n",
      "Epoch 19/100\n",
      "Training Loss: 0.06037059443637218\n",
      "Validation Loss: 0.13206622530726878\n",
      "Epoch 20/100\n",
      "Training Loss: 0.06101448055923752\n",
      "Validation Loss: 0.14940659032651435\n",
      "Epoch 21/100\n",
      "Training Loss: 0.06057408459054692\n",
      "Validation Loss: 0.1796173688239473\n",
      "Epoch 22/100\n",
      "Training Loss: 0.06452536929974903\n",
      "Validation Loss: 0.07173614848414704\n",
      "Epoch 23/100\n",
      "Training Loss: 0.06990056438896852\n",
      "Validation Loss: 0.06336416468275143\n",
      "Epoch 24/100\n",
      "Training Loss: 0.06446246092887264\n",
      "Validation Loss: 0.08123060077671092\n",
      "Epoch 25/100\n",
      "Training Loss: 0.06208184168474199\n",
      "Validation Loss: 0.09582424182849279\n",
      "Epoch 26/100\n",
      "Training Loss: 0.057981439040225714\n",
      "Validation Loss: 0.1460276162094253\n",
      "Epoch 27/100\n",
      "Training Loss: 0.05206265587948448\n",
      "Validation Loss: 0.06908337583274467\n",
      "Epoch 28/100\n",
      "Training Loss: 0.05957844504098648\n",
      "Validation Loss: 0.10549619828363137\n",
      "Epoch 29/100\n",
      "Training Loss: 0.05283465536617976\n",
      "Validation Loss: 0.07379351448606142\n",
      "Epoch 30/100\n",
      "Training Loss: 0.0641968660262608\n",
      "Validation Loss: 0.1491459064053579\n",
      "Epoch 31/100\n",
      "Training Loss: 0.0543961035651853\n",
      "Validation Loss: 0.06674739128434069\n",
      "Epoch 32/100\n",
      "Training Loss: 0.05345490866853354\n",
      "Validation Loss: 0.09235087966104263\n",
      "Epoch 33/100\n",
      "Training Loss: 0.049854748396267565\n",
      "Validation Loss: 0.12981852472157934\n",
      "Epoch 34/100\n",
      "Training Loss: 0.049691038333568166\n",
      "Validation Loss: 0.08251270939603311\n",
      "Epoch 35/100\n",
      "Training Loss: 0.05820546984423351\n",
      "Validation Loss: 0.07072641453654523\n",
      "Epoch 36/100\n",
      "Training Loss: 0.05348990930508567\n",
      "Validation Loss: 0.07965745320421426\n",
      "Epoch 37/100\n",
      "Training Loss: 0.05884667550148336\n",
      "Validation Loss: 0.0781486770587354\n",
      "Epoch 38/100\n",
      "Training Loss: 0.04872116327849666\n",
      "Validation Loss: 0.1734888453991556\n",
      "Epoch 39/100\n",
      "Training Loss: 0.05542018729950366\n",
      "Validation Loss: 0.06437308295061872\n",
      "Epoch 40/100\n",
      "Training Loss: 0.05861792208822058\n",
      "Validation Loss: 0.0716507990402541\n",
      "Epoch 41/100\n",
      "Training Loss: 0.06711007057886698\n",
      "Validation Loss: 0.13962239982150731\n",
      "Epoch 42/100\n",
      "Training Loss: 0.05311381041133293\n",
      "Validation Loss: 0.07512116351680347\n",
      "Epoch 43/100\n",
      "Training Loss: 0.06565200462221916\n",
      "Validation Loss: 0.10063866962724417\n",
      "Epoch 44/100\n",
      "Training Loss: 0.06126105151528275\n",
      "Validation Loss: 0.09111556118578668\n",
      "Epoch 45/100\n",
      "Training Loss: 0.04712841233053947\n",
      "Validation Loss: 0.09545219080455425\n",
      "Epoch 46/100\n",
      "Training Loss: 0.055383921612082436\n",
      "Validation Loss: 0.1103490589469118\n",
      "Epoch 47/100\n",
      "Training Loss: 0.05392900460907421\n",
      "Validation Loss: 0.15888176886138225\n",
      "Epoch 48/100\n",
      "Training Loss: 0.06122912264503265\n",
      "Validation Loss: 0.15556351654651862\n",
      "Epoch 49/100\n",
      "Training Loss: 0.04739672445556017\n",
      "Validation Loss: 0.09687267650340331\n",
      "Epoch 50/100\n",
      "Training Loss: 0.05854087102343251\n",
      "Validation Loss: 0.0682584944955689\n",
      "Epoch 51/100\n",
      "Training Loss: 0.050207953732901364\n",
      "Validation Loss: 0.06944880109375709\n",
      "Epoch 52/100\n",
      "Training Loss: 0.057833696786010566\n",
      "Validation Loss: 0.13567046967399268\n",
      "Epoch 53/100\n",
      "Training Loss: 0.059704662116575845\n",
      "Validation Loss: 0.14489038698925577\n",
      "Epoch 54/100\n",
      "Training Loss: 0.061148026246121985\n",
      "Validation Loss: 0.09421605262964286\n",
      "Epoch 55/100\n",
      "Training Loss: 0.0544925254105368\n",
      "Validation Loss: 0.0652274429001431\n",
      "Epoch 56/100\n",
      "Training Loss: 0.05531046407302423\n",
      "Validation Loss: 0.08891750246688043\n",
      "Epoch 57/100\n",
      "Training Loss: 0.06641960811283194\n",
      "Validation Loss: 0.0901800806088677\n",
      "Epoch 58/100\n",
      "Training Loss: 0.06089706281403084\n",
      "Validation Loss: 0.11582321182478042\n",
      "Epoch 59/100\n",
      "Training Loss: 0.05809561698208813\n",
      "Validation Loss: 0.07065852596799373\n",
      "Epoch 60/100\n",
      "Training Loss: 0.07037130230458637\n",
      "Validation Loss: 0.07229938091128663\n",
      "Epoch 61/100\n",
      "Training Loss: 0.05112449581227174\n",
      "Validation Loss: 0.09207816220719758\n",
      "Epoch 62/100\n",
      "Training Loss: 0.05578515861125267\n",
      "Validation Loss: 0.12294884938641333\n",
      "Epoch 63/100\n",
      "Training Loss: 0.06512390552045376\n",
      "Validation Loss: 0.07110551101276788\n",
      "Epoch 64/100\n",
      "Training Loss: 0.05885880027994347\n",
      "Validation Loss: 0.04370478141516952\n",
      "Epoch 65/100\n",
      "Training Loss: 0.06881909969339016\n",
      "Validation Loss: 0.06424585451325036\n",
      "Epoch 66/100\n",
      "Training Loss: 0.0539395964781919\n",
      "Validation Loss: 0.06416869756124956\n",
      "Epoch 67/100\n",
      "Training Loss: 0.06420182442011187\n",
      "Validation Loss: 0.08235392090491085\n",
      "Epoch 68/100\n",
      "Training Loss: 0.049170801966378075\n",
      "Validation Loss: 0.055953460156602894\n",
      "Epoch 69/100\n",
      "Training Loss: 0.05472571967716048\n",
      "Validation Loss: 0.04739810144510824\n",
      "Epoch 70/100\n",
      "Training Loss: 0.05587694162201171\n",
      "Validation Loss: 0.05303794863660007\n",
      "Epoch 71/100\n",
      "Training Loss: 0.06049303557839169\n",
      "Validation Loss: 0.08776286712799983\n",
      "Epoch 72/100\n",
      "Training Loss: 0.05249065877225792\n",
      "Validation Loss: 0.0724532316308953\n",
      "Epoch 73/100\n",
      "Training Loss: 0.05965854929431616\n",
      "Validation Loss: 0.07428120713590883\n",
      "Epoch 74/100\n",
      "Training Loss: 0.061826044410401945\n",
      "Validation Loss: 0.06107078978107433\n",
      "Epoch 75/100\n",
      "Training Loss: 0.059293276384494435\n",
      "Validation Loss: 0.07333101774155337\n",
      "Epoch 76/100\n",
      "Training Loss: 0.05342738799120987\n",
      "Validation Loss: 0.058198608525118045\n",
      "Epoch 77/100\n",
      "Training Loss: 0.06496205250003177\n",
      "Validation Loss: 0.08117526348063006\n",
      "Epoch 78/100\n",
      "Training Loss: 0.051377044553803146\n",
      "Validation Loss: 0.0747590491033632\n",
      "Epoch 79/100\n",
      "Training Loss: 0.06193794225702027\n",
      "Validation Loss: 0.10426895877323264\n",
      "Epoch 80/100\n",
      "Training Loss: 0.057204527986770275\n",
      "Validation Loss: 0.09835072013246579\n",
      "Epoch 81/100\n",
      "Training Loss: 0.0675421592336886\n",
      "Validation Loss: 0.08863426854245286\n",
      "Epoch 82/100\n",
      "Training Loss: 0.0620348374871519\n",
      "Validation Loss: 0.06229662518190508\n",
      "Epoch 83/100\n",
      "Training Loss: 0.058632133891958906\n",
      "Validation Loss: 0.0664458960739615\n",
      "Epoch 84/100\n",
      "Training Loss: 0.06098926579019298\n",
      "Validation Loss: 0.02762590627027673\n",
      "Epoch 85/100\n",
      "Training Loss: 0.04822977923343506\n",
      "Validation Loss: 0.0939316617339889\n",
      "Epoch 86/100\n",
      "Training Loss: 0.05811972205397024\n",
      "Validation Loss: 0.13014102220135462\n",
      "Epoch 87/100\n",
      "Training Loss: 0.052984392136049024\n",
      "Validation Loss: 0.05198339361631075\n",
      "Epoch 88/100\n",
      "Training Loss: 0.055754771609298634\n",
      "Validation Loss: 0.09676463783652292\n",
      "Epoch 89/100\n",
      "Training Loss: 0.062000677436428946\n",
      "Validation Loss: 0.07376607120455102\n",
      "Epoch 90/100\n",
      "Training Loss: 0.050173196288683435\n",
      "Validation Loss: 0.08765416956396725\n",
      "Epoch 91/100\n",
      "Training Loss: 0.0577924302067428\n",
      "Validation Loss: 0.09171559331537757\n",
      "Epoch 92/100\n",
      "Training Loss: 0.060031510427272686\n",
      "Validation Loss: 0.057244579672407925\n",
      "Epoch 93/100\n",
      "Training Loss: 0.059846776886714206\n",
      "Validation Loss: 0.0824790077089855\n",
      "Epoch 94/100\n",
      "Training Loss: 0.06397440469235507\n",
      "Validation Loss: 0.08179227886470707\n",
      "Epoch 95/100\n",
      "Training Loss: 0.05895609288485432\n",
      "Validation Loss: 0.04217644800327703\n",
      "Epoch 96/100\n",
      "Training Loss: 0.06556232519865164\n",
      "Validation Loss: 0.032432222606229764\n",
      "Epoch 97/100\n",
      "Training Loss: 0.06277960906994809\n",
      "Validation Loss: 0.07379554402955422\n",
      "Epoch 98/100\n",
      "Training Loss: 0.05972033118127333\n",
      "Validation Loss: 0.06156379368814535\n",
      "Epoch 99/100\n",
      "Training Loss: 0.05247028970791977\n",
      "Validation Loss: 0.04181767835384536\n",
      "Epoch 100/100\n",
      "Training Loss: 0.055220696719495206\n",
      "Validation Loss: 0.1069172760994489\n",
      "    Trial 2/2 for combination 46/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.28747961767357644\n",
      "Validation Loss: 0.20550316685768086\n",
      "Epoch 2/100\n",
      "Training Loss: 0.20948663861837677\n",
      "Validation Loss: 0.10160129210747648\n",
      "Epoch 3/100\n",
      "Training Loss: 0.16281432064329587\n",
      "Validation Loss: 0.17921930886315562\n",
      "Epoch 4/100\n",
      "Training Loss: 0.1422793720952708\n",
      "Validation Loss: 0.11993113329188927\n",
      "Epoch 5/100\n",
      "Training Loss: 0.17931093288518737\n",
      "Validation Loss: 0.14305425078202635\n",
      "Epoch 6/100\n",
      "Training Loss: 0.1268890398385512\n",
      "Validation Loss: 0.12201148121304324\n",
      "Epoch 7/100\n",
      "Training Loss: 0.1336474413848848\n",
      "Validation Loss: 0.11793530047349418\n",
      "Epoch 8/100\n",
      "Training Loss: 0.12639054525592736\n",
      "Validation Loss: 0.06362986653745123\n",
      "Epoch 9/100\n",
      "Training Loss: 0.11020991558838089\n",
      "Validation Loss: 0.08013530430631857\n",
      "Epoch 10/100\n",
      "Training Loss: 0.10162597154702074\n",
      "Validation Loss: 0.12884935776009526\n",
      "Epoch 11/100\n",
      "Training Loss: 0.1140564343761079\n",
      "Validation Loss: 0.0612921528991088\n",
      "Epoch 12/100\n",
      "Training Loss: 0.10049553053428238\n",
      "Validation Loss: 0.04362692402861145\n",
      "Epoch 13/100\n",
      "Training Loss: 0.11328114134724215\n",
      "Validation Loss: 0.0695674084368928\n",
      "Epoch 14/100\n",
      "Training Loss: 0.08671621460663335\n",
      "Validation Loss: 0.04827727052138481\n",
      "Epoch 15/100\n",
      "Training Loss: 0.07458840987727489\n",
      "Validation Loss: 0.07421421936218679\n",
      "Epoch 16/100\n",
      "Training Loss: 0.07733791858791896\n",
      "Validation Loss: 0.05514804365659076\n",
      "Epoch 17/100\n",
      "Training Loss: 0.06845323456218626\n",
      "Validation Loss: 0.08743965303081236\n",
      "Epoch 18/100\n",
      "Training Loss: 0.06868134286022382\n",
      "Validation Loss: 0.1508930501202485\n",
      "Epoch 19/100\n",
      "Training Loss: 0.07851406937823062\n",
      "Validation Loss: 0.1167187740774003\n",
      "Epoch 20/100\n",
      "Training Loss: 0.07366166070352952\n",
      "Validation Loss: 0.08966027303481469\n",
      "Epoch 21/100\n",
      "Training Loss: 0.06972351224780497\n",
      "Validation Loss: 0.06849009910924905\n",
      "Epoch 22/100\n",
      "Training Loss: 0.06952712432153488\n",
      "Validation Loss: 0.043593690892060576\n",
      "Epoch 23/100\n",
      "Training Loss: 0.05662430282633566\n",
      "Validation Loss: 0.06250602441711904\n",
      "Epoch 24/100\n",
      "Training Loss: 0.059823539211494886\n",
      "Validation Loss: 0.14544588848675088\n",
      "Epoch 25/100\n",
      "Training Loss: 0.05994090232469744\n",
      "Validation Loss: 0.1303764882260952\n",
      "Epoch 26/100\n",
      "Training Loss: 0.06080845282227287\n",
      "Validation Loss: 0.11203572615988422\n",
      "Epoch 27/100\n",
      "Training Loss: 0.06224353097945325\n",
      "Validation Loss: 0.06440754111837102\n",
      "Epoch 28/100\n",
      "Training Loss: 0.06653762826177449\n",
      "Validation Loss: 0.0891883259036231\n",
      "Epoch 29/100\n",
      "Training Loss: 0.057608647213621404\n",
      "Validation Loss: 0.09455947697782766\n",
      "Epoch 30/100\n",
      "Training Loss: 0.06235440960841196\n",
      "Validation Loss: 0.061345642930339464\n",
      "Epoch 31/100\n",
      "Training Loss: 0.05497333583849173\n",
      "Validation Loss: 0.0724016378311311\n",
      "Epoch 32/100\n",
      "Training Loss: 0.05512391355585331\n",
      "Validation Loss: 0.1339741483760794\n",
      "Epoch 33/100\n",
      "Training Loss: 0.07632108392785712\n",
      "Validation Loss: 0.06202593437288749\n",
      "Epoch 34/100\n",
      "Training Loss: 0.05170272123028357\n",
      "Validation Loss: 0.07056315300989745\n",
      "Epoch 35/100\n",
      "Training Loss: 0.050545903745950714\n",
      "Validation Loss: 0.13799154450369733\n",
      "Epoch 36/100\n",
      "Training Loss: 0.057643899157297035\n",
      "Validation Loss: 0.11873140599685561\n",
      "Epoch 37/100\n",
      "Training Loss: 0.049038905340130334\n",
      "Validation Loss: 0.09091551850705706\n",
      "Epoch 38/100\n",
      "Training Loss: 0.057176888259494935\n",
      "Validation Loss: 0.040190621289131076\n",
      "Epoch 39/100\n",
      "Training Loss: 0.06678502213265021\n",
      "Validation Loss: 0.04170521983145974\n",
      "Epoch 40/100\n",
      "Training Loss: 0.05242972269807542\n",
      "Validation Loss: 0.12729965147432656\n",
      "Epoch 41/100\n",
      "Training Loss: 0.06507207373050185\n",
      "Validation Loss: 0.11874298152425686\n",
      "Epoch 42/100\n",
      "Training Loss: 0.06523315729641554\n",
      "Validation Loss: 0.07131572696732362\n",
      "Epoch 43/100\n",
      "Training Loss: 0.05017155384114399\n",
      "Validation Loss: 0.11948705773161972\n",
      "Epoch 44/100\n",
      "Training Loss: 0.06439775419058553\n",
      "Validation Loss: 0.1165147001890732\n",
      "Epoch 45/100\n",
      "Training Loss: 0.05290598960003509\n",
      "Validation Loss: 0.1211038195978265\n",
      "Epoch 46/100\n",
      "Training Loss: 0.05312473946418019\n",
      "Validation Loss: 0.12689389389374223\n",
      "Epoch 47/100\n",
      "Training Loss: 0.05912235950703467\n",
      "Validation Loss: 0.06668436335261194\n",
      "Epoch 48/100\n",
      "Training Loss: 0.05150764960552179\n",
      "Validation Loss: 0.08462253555989477\n",
      "Epoch 49/100\n",
      "Training Loss: 0.056722954784421346\n",
      "Validation Loss: 0.06467530482021312\n",
      "Epoch 50/100\n",
      "Training Loss: 0.058778178030951485\n",
      "Validation Loss: 0.04856093188669033\n",
      "Epoch 51/100\n",
      "Training Loss: 0.05930696301464385\n",
      "Validation Loss: 0.11984158683144264\n",
      "Epoch 52/100\n",
      "Training Loss: 0.05905940740601763\n",
      "Validation Loss: 0.08996174629302747\n",
      "Epoch 53/100\n",
      "Training Loss: 0.05414939773794839\n",
      "Validation Loss: 0.052543625165364746\n",
      "Epoch 54/100\n",
      "Training Loss: 0.05735445952634118\n",
      "Validation Loss: 0.09777699820507188\n",
      "Epoch 55/100\n",
      "Training Loss: 0.0493749156202654\n",
      "Validation Loss: 0.1787741413047704\n",
      "Epoch 56/100\n",
      "Training Loss: 0.056812924796664374\n",
      "Validation Loss: 0.104659768985499\n",
      "Epoch 57/100\n",
      "Training Loss: 0.04956818055595246\n",
      "Validation Loss: 0.04295052887267021\n",
      "Epoch 58/100\n",
      "Training Loss: 0.05586217919748679\n",
      "Validation Loss: 0.09910143817945896\n",
      "Epoch 59/100\n",
      "Training Loss: 0.05858696979276096\n",
      "Validation Loss: 0.1523339098148455\n",
      "Epoch 60/100\n",
      "Training Loss: 0.07120798950137908\n",
      "Validation Loss: 0.14113931610463598\n",
      "Epoch 61/100\n",
      "Training Loss: 0.046900694656609426\n",
      "Validation Loss: 0.08218633089609806\n",
      "Epoch 62/100\n",
      "Training Loss: 0.052335113321537174\n",
      "Validation Loss: 0.08850263212642354\n",
      "Epoch 63/100\n",
      "Training Loss: 0.0641926540519815\n",
      "Validation Loss: 0.08108873195906113\n",
      "Epoch 64/100\n",
      "Training Loss: 0.06896264710847322\n",
      "Validation Loss: 0.04571544424159944\n",
      "Epoch 65/100\n",
      "Training Loss: 0.06744875636055112\n",
      "Validation Loss: 0.05488812715133721\n",
      "Epoch 66/100\n",
      "Training Loss: 0.05300175317009524\n",
      "Validation Loss: 0.06616627873380827\n",
      "Epoch 67/100\n",
      "Training Loss: 0.06342115777322019\n",
      "Validation Loss: 0.06650078342583422\n",
      "Epoch 68/100\n",
      "Training Loss: 0.0663719419925638\n",
      "Validation Loss: 0.09703051964698649\n",
      "Epoch 69/100\n",
      "Training Loss: 0.05281860061408683\n",
      "Validation Loss: 0.1147502512332426\n",
      "Epoch 70/100\n",
      "Training Loss: 0.0610411019686703\n",
      "Validation Loss: 0.03457758501820858\n",
      "Epoch 71/100\n",
      "Training Loss: 0.05795570208417792\n",
      "Validation Loss: 0.05382365715777514\n",
      "Epoch 72/100\n",
      "Training Loss: 0.0624151742826536\n",
      "Validation Loss: 0.07337974571943173\n",
      "Epoch 73/100\n",
      "Training Loss: 0.05614496630995912\n",
      "Validation Loss: 0.03247104180581127\n",
      "Epoch 74/100\n",
      "Training Loss: 0.06416525914108662\n",
      "Validation Loss: 0.07352161152061441\n",
      "Epoch 75/100\n",
      "Training Loss: 0.07178580157782233\n",
      "Validation Loss: 0.032482850055161785\n",
      "Epoch 76/100\n",
      "Training Loss: 0.0609633610271396\n",
      "Validation Loss: 0.041356166166983156\n",
      "Epoch 77/100\n",
      "Training Loss: 0.05382827265417759\n",
      "Validation Loss: 0.10195330515167889\n",
      "Epoch 78/100\n",
      "Training Loss: 0.05593865590218791\n",
      "Validation Loss: 0.0854370591127515\n",
      "Epoch 79/100\n",
      "Training Loss: 0.0586383981930504\n",
      "Validation Loss: 0.15602803275117288\n",
      "Epoch 80/100\n",
      "Training Loss: 0.061678230429672444\n",
      "Validation Loss: 0.11051440019052491\n",
      "Epoch 81/100\n",
      "Training Loss: 0.05091484883352484\n",
      "Validation Loss: 0.0798438856784112\n",
      "Epoch 82/100\n",
      "Training Loss: 0.059560391789741866\n",
      "Validation Loss: 0.0583891581710914\n",
      "Epoch 83/100\n",
      "Training Loss: 0.059189786284789034\n",
      "Validation Loss: 0.055676913582615103\n",
      "Epoch 84/100\n",
      "Training Loss: 0.05308121641098751\n",
      "Validation Loss: 0.0521637686265954\n",
      "Epoch 85/100\n",
      "Training Loss: 0.06152851478319694\n",
      "Validation Loss: 0.029695239159578064\n",
      "Epoch 86/100\n",
      "Training Loss: 0.06872262012701842\n",
      "Validation Loss: 0.04862974082425668\n",
      "Epoch 87/100\n",
      "Training Loss: 0.05597989002826791\n",
      "Validation Loss: 0.09537212692148092\n",
      "Epoch 88/100\n",
      "Training Loss: 0.06808435206727344\n",
      "Validation Loss: 0.05297285929713892\n",
      "Epoch 89/100\n",
      "Training Loss: 0.060051571742841606\n",
      "Validation Loss: 0.03527206512506739\n",
      "Epoch 90/100\n",
      "Training Loss: 0.06677633265143769\n",
      "Validation Loss: 0.11044444941282969\n",
      "Epoch 91/100\n",
      "Training Loss: 0.06566800043449171\n",
      "Validation Loss: 0.12260509364156283\n",
      "Epoch 92/100\n",
      "Training Loss: 0.07193724468579514\n",
      "Validation Loss: 0.057915383135477616\n",
      "Epoch 93/100\n",
      "Training Loss: 0.057132666644312366\n",
      "Validation Loss: 0.058797279064159616\n",
      "Epoch 94/100\n",
      "Training Loss: 0.05894937958556372\n",
      "Validation Loss: 0.07155983331273148\n",
      "Epoch 95/100\n",
      "Training Loss: 0.06702902341215175\n",
      "Validation Loss: 0.07324214947446775\n",
      "Epoch 96/100\n",
      "Training Loss: 0.06620606535833991\n",
      "Validation Loss: 0.05917682124809102\n",
      "Epoch 97/100\n",
      "Training Loss: 0.06727855078852736\n",
      "Validation Loss: 0.05967439053131853\n",
      "Epoch 98/100\n",
      "Training Loss: 0.059163596795236424\n",
      "Validation Loss: 0.02966563961862915\n",
      "Epoch 99/100\n",
      "Training Loss: 0.0533203132376995\n",
      "Validation Loss: 0.07722762292677461\n",
      "Epoch 100/100\n",
      "Training Loss: 0.06659560881489555\n",
      "Validation Loss: 0.04630519155343245\n",
      "Combination 46: Avg Training Loss = 0.0709600576667607, Avg Validation Loss = 0.08676181073829098\n",
      "Testing combination 47/48: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.001, 'sequence_length': 50, 'dropout_rate': 0.1}\n",
      "Truncated Train Data Shape: (70, 50, 1)\n",
      "Truncated Validation Data Shape: (10, 50, 1)\n",
      "    Trial 1/2 for combination 47/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.4414360433092494\n",
      "Validation Loss: 0.17457889644394675\n",
      "Epoch 2/100\n",
      "Training Loss: 0.23196864747022344\n",
      "Validation Loss: 0.12688844190220866\n",
      "Epoch 3/100\n",
      "Training Loss: 0.17970495030592443\n",
      "Validation Loss: 0.21478793073063968\n",
      "Epoch 4/100\n",
      "Training Loss: 0.17360929878038173\n",
      "Validation Loss: 0.16221214061904254\n",
      "Epoch 5/100\n",
      "Training Loss: 0.17409798808640164\n",
      "Validation Loss: 0.12901834695696257\n",
      "Epoch 6/100\n",
      "Training Loss: 0.156168572317686\n",
      "Validation Loss: 0.09513640428977929\n",
      "Epoch 7/100\n",
      "Training Loss: 0.14961432927799406\n",
      "Validation Loss: 0.1213978328410658\n",
      "Epoch 8/100\n",
      "Training Loss: 0.12588343046965278\n",
      "Validation Loss: 0.05848979156836894\n",
      "Epoch 9/100\n",
      "Training Loss: 0.10153371849449155\n",
      "Validation Loss: 0.06336331409489031\n",
      "Epoch 10/100\n",
      "Training Loss: 0.10798587343930945\n",
      "Validation Loss: 0.05912333327523216\n",
      "Epoch 11/100\n",
      "Training Loss: 0.0982268202396921\n",
      "Validation Loss: 0.10578465659351129\n",
      "Epoch 12/100\n",
      "Training Loss: 0.09366834509962102\n",
      "Validation Loss: 0.04888222919118697\n",
      "Epoch 13/100\n",
      "Training Loss: 0.07681400662209505\n",
      "Validation Loss: 0.040904181202722614\n",
      "Epoch 14/100\n",
      "Training Loss: 0.08434598269659675\n",
      "Validation Loss: 0.03347630444410145\n",
      "Epoch 15/100\n",
      "Training Loss: 0.07603183868850634\n",
      "Validation Loss: 0.07973017825762381\n",
      "Epoch 16/100\n",
      "Training Loss: 0.06433884452171207\n",
      "Validation Loss: 0.08340459221287122\n",
      "Epoch 17/100\n",
      "Training Loss: 0.07119831383335343\n",
      "Validation Loss: 0.0378360056587543\n",
      "Epoch 18/100\n",
      "Training Loss: 0.06848294271356496\n",
      "Validation Loss: 0.03777378958688985\n",
      "Epoch 19/100\n",
      "Training Loss: 0.06253835069185722\n",
      "Validation Loss: 0.02931669610417917\n",
      "Epoch 20/100\n",
      "Training Loss: 0.05408601186539244\n",
      "Validation Loss: 0.049641773775904535\n",
      "Epoch 21/100\n",
      "Training Loss: 0.05449473864225236\n",
      "Validation Loss: 0.04243996725389026\n",
      "Epoch 22/100\n",
      "Training Loss: 0.05140023052659196\n",
      "Validation Loss: 0.06874750059764403\n",
      "Epoch 23/100\n",
      "Training Loss: 0.05680228430511806\n",
      "Validation Loss: 0.01631992382213566\n",
      "Epoch 24/100\n",
      "Training Loss: 0.05221328980640542\n",
      "Validation Loss: 0.060325883387401566\n",
      "Epoch 25/100\n",
      "Training Loss: 0.0635316150061181\n",
      "Validation Loss: 0.052118099101204164\n",
      "Epoch 26/100\n",
      "Training Loss: 0.05485213272176238\n",
      "Validation Loss: 0.04444393726355387\n",
      "Epoch 27/100\n",
      "Training Loss: 0.05369381332571301\n",
      "Validation Loss: 0.05664170045265461\n",
      "Epoch 28/100\n",
      "Training Loss: 0.057106288516185745\n",
      "Validation Loss: 0.05412799641973727\n",
      "Epoch 29/100\n",
      "Training Loss: 0.04843513661531546\n",
      "Validation Loss: 0.08144567475413095\n",
      "Epoch 30/100\n",
      "Training Loss: 0.04898738730074033\n",
      "Validation Loss: 0.03513328472778896\n",
      "Epoch 31/100\n",
      "Training Loss: 0.0502597697978553\n",
      "Validation Loss: 0.035663268840519506\n",
      "Epoch 32/100\n",
      "Training Loss: 0.050140052320669544\n",
      "Validation Loss: 0.04850495045730878\n",
      "Epoch 33/100\n",
      "Training Loss: 0.06485583065886416\n",
      "Validation Loss: 0.03711997459101846\n",
      "Epoch 34/100\n",
      "Training Loss: 0.04530685346880449\n",
      "Validation Loss: 0.06343507547011455\n",
      "Epoch 35/100\n",
      "Training Loss: 0.04386324375383806\n",
      "Validation Loss: 0.07091361153118542\n",
      "Epoch 36/100\n",
      "Training Loss: 0.04957599253315306\n",
      "Validation Loss: 0.032908317994144294\n",
      "Epoch 37/100\n",
      "Training Loss: 0.056808836349180665\n",
      "Validation Loss: 0.043962701147012784\n",
      "Epoch 38/100\n",
      "Training Loss: 0.04426310040667074\n",
      "Validation Loss: 0.057498123132043444\n",
      "Epoch 39/100\n",
      "Training Loss: 0.052766982823663176\n",
      "Validation Loss: 0.04713451941579831\n",
      "Epoch 40/100\n",
      "Training Loss: 0.06126035838216657\n",
      "Validation Loss: 0.03417868736828829\n",
      "Epoch 41/100\n",
      "Training Loss: 0.054285360132083564\n",
      "Validation Loss: 0.04218995661262091\n",
      "Epoch 42/100\n",
      "Training Loss: 0.06180608527735466\n",
      "Validation Loss: 0.04648562375787517\n",
      "Epoch 43/100\n",
      "Training Loss: 0.06332069785095151\n",
      "Validation Loss: 0.03862572039662589\n",
      "Epoch 44/100\n",
      "Training Loss: 0.053671379624112586\n",
      "Validation Loss: 0.02960857454409376\n",
      "Epoch 45/100\n",
      "Training Loss: 0.05205512428314559\n",
      "Validation Loss: 0.04081937934601964\n",
      "Epoch 46/100\n",
      "Training Loss: 0.055920633964113446\n",
      "Validation Loss: 0.028973649483190516\n",
      "Epoch 47/100\n",
      "Training Loss: 0.05973820302698935\n",
      "Validation Loss: 0.06798092462251194\n",
      "Epoch 48/100\n",
      "Training Loss: 0.0599078133517925\n",
      "Validation Loss: 0.04247364092977291\n",
      "Epoch 49/100\n",
      "Training Loss: 0.05653686376342441\n",
      "Validation Loss: 0.05106723726120009\n",
      "Epoch 50/100\n",
      "Training Loss: 0.05374307331135019\n",
      "Validation Loss: 0.03856768851064059\n",
      "Epoch 51/100\n",
      "Training Loss: 0.05446765127874861\n",
      "Validation Loss: 0.023468980465296538\n",
      "Epoch 52/100\n",
      "Training Loss: 0.06281237765411182\n",
      "Validation Loss: 0.04326075613775895\n",
      "Epoch 53/100\n",
      "Training Loss: 0.04844749504386059\n",
      "Validation Loss: 0.027656628297008733\n",
      "Epoch 54/100\n",
      "Training Loss: 0.05596945125632626\n",
      "Validation Loss: 0.10394667822006323\n",
      "Epoch 55/100\n",
      "Training Loss: 0.04954260980213385\n",
      "Validation Loss: 0.0339595832859399\n",
      "Epoch 56/100\n",
      "Training Loss: 0.05252636015507977\n",
      "Validation Loss: 0.05366715783780675\n",
      "Epoch 57/100\n",
      "Training Loss: 0.06360961819466915\n",
      "Validation Loss: 0.07731230215800501\n",
      "Epoch 58/100\n",
      "Training Loss: 0.057168367115726884\n",
      "Validation Loss: 0.02950726596840093\n",
      "Epoch 59/100\n",
      "Training Loss: 0.0536355517415988\n",
      "Validation Loss: 0.07095738873500647\n",
      "Epoch 60/100\n",
      "Training Loss: 0.05760425230689082\n",
      "Validation Loss: 0.04736547295686323\n",
      "Epoch 61/100\n",
      "Training Loss: 0.05435557042474312\n",
      "Validation Loss: 0.024769899788218273\n",
      "Epoch 62/100\n",
      "Training Loss: 0.0598896071021728\n",
      "Validation Loss: 0.07609885788368187\n",
      "Epoch 63/100\n",
      "Training Loss: 0.05406258232320319\n",
      "Validation Loss: 0.02217657983745417\n",
      "Epoch 64/100\n",
      "Training Loss: 0.04918102074999928\n",
      "Validation Loss: 0.018670905675965955\n",
      "Epoch 65/100\n",
      "Training Loss: 0.0548218547044752\n",
      "Validation Loss: 0.038355984224354454\n",
      "Epoch 66/100\n",
      "Training Loss: 0.05664522765794062\n",
      "Validation Loss: 0.04296039629000037\n",
      "Epoch 67/100\n",
      "Training Loss: 0.05750243975936661\n",
      "Validation Loss: 0.036480848257762814\n",
      "Epoch 68/100\n",
      "Training Loss: 0.07154330317431161\n",
      "Validation Loss: 0.04397072558675993\n",
      "Epoch 69/100\n",
      "Training Loss: 0.06319028176309174\n",
      "Validation Loss: 0.048497807332426694\n",
      "Epoch 70/100\n",
      "Training Loss: 0.04492707663477228\n",
      "Validation Loss: 0.04940740974849551\n",
      "Epoch 71/100\n",
      "Training Loss: 0.051990478761630245\n",
      "Validation Loss: 0.028593567379305785\n",
      "Epoch 72/100\n",
      "Training Loss: 0.05834275085962535\n",
      "Validation Loss: 0.04269722883490777\n",
      "Epoch 73/100\n",
      "Training Loss: 0.06789352357352232\n",
      "Validation Loss: 0.026255514273244955\n",
      "Epoch 74/100\n",
      "Training Loss: 0.05098640420359193\n",
      "Validation Loss: 0.030622822039612908\n",
      "Epoch 75/100\n",
      "Training Loss: 0.05538071023122202\n",
      "Validation Loss: 0.06283070842268011\n",
      "Epoch 76/100\n",
      "Training Loss: 0.049324929321983096\n",
      "Validation Loss: 0.04868275182712604\n",
      "Epoch 77/100\n",
      "Training Loss: 0.06379867964071823\n",
      "Validation Loss: 0.05791589477436789\n",
      "Epoch 78/100\n",
      "Training Loss: 0.05847153926878661\n",
      "Validation Loss: 0.031071605337183627\n",
      "Epoch 79/100\n",
      "Training Loss: 0.05742837167559384\n",
      "Validation Loss: 0.017574349029361926\n",
      "Epoch 80/100\n",
      "Training Loss: 0.06460551140232607\n",
      "Validation Loss: 0.03153659842495112\n",
      "Epoch 81/100\n",
      "Training Loss: 0.04799474980314842\n",
      "Validation Loss: 0.044885458839393294\n",
      "Epoch 82/100\n",
      "Training Loss: 0.05724349274494754\n",
      "Validation Loss: 0.02814192723547022\n",
      "Epoch 83/100\n",
      "Training Loss: 0.06515714136348158\n",
      "Validation Loss: 0.06816120035276516\n",
      "Epoch 84/100\n",
      "Training Loss: 0.06166925642160689\n",
      "Validation Loss: 0.08480078576850983\n",
      "Epoch 85/100\n",
      "Training Loss: 0.06332845586532236\n",
      "Validation Loss: 0.06160238760691158\n",
      "Epoch 86/100\n",
      "Training Loss: 0.05832954453205269\n",
      "Validation Loss: 0.04364422858044112\n",
      "Epoch 87/100\n",
      "Training Loss: 0.05105580536597555\n",
      "Validation Loss: 0.03543157365041222\n",
      "Epoch 88/100\n",
      "Training Loss: 0.06433607221595175\n",
      "Validation Loss: 0.02248789723321965\n",
      "Epoch 89/100\n",
      "Training Loss: 0.044087096744949095\n",
      "Validation Loss: 0.03842333895487403\n",
      "Epoch 90/100\n",
      "Training Loss: 0.053398344480022564\n",
      "Validation Loss: 0.050986313971322104\n",
      "Epoch 91/100\n",
      "Training Loss: 0.060560517169258976\n",
      "Validation Loss: 0.07814121619048\n",
      "Epoch 92/100\n",
      "Training Loss: 0.05222105536688398\n",
      "Validation Loss: 0.046221568924880406\n",
      "Epoch 93/100\n",
      "Training Loss: 0.04921025294863756\n",
      "Validation Loss: 0.040891671616560925\n",
      "Epoch 94/100\n",
      "Training Loss: 0.06279093537534032\n",
      "Validation Loss: 0.05972699140378087\n",
      "Epoch 95/100\n",
      "Training Loss: 0.05587239943409123\n",
      "Validation Loss: 0.06145737609204358\n",
      "Epoch 96/100\n",
      "Training Loss: 0.04429702296280714\n",
      "Validation Loss: 0.022832748794190973\n",
      "Epoch 97/100\n",
      "Training Loss: 0.04644018597103973\n",
      "Validation Loss: 0.06087149871296325\n",
      "Epoch 98/100\n",
      "Training Loss: 0.0585864918596255\n",
      "Validation Loss: 0.03664709949492303\n",
      "Epoch 99/100\n",
      "Training Loss: 0.05484624713044286\n",
      "Validation Loss: 0.054225324964638\n",
      "Epoch 100/100\n",
      "Training Loss: 0.044835912827228666\n",
      "Validation Loss: 0.0512927661864249\n",
      "    Trial 2/2 for combination 47/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.3310758124562718\n",
      "Validation Loss: 0.25632035469796355\n",
      "Epoch 2/100\n",
      "Training Loss: 0.21976050536691194\n",
      "Validation Loss: 0.1567515190548528\n",
      "Epoch 3/100\n",
      "Training Loss: 0.19474606923257526\n",
      "Validation Loss: 0.16242036525475612\n",
      "Epoch 4/100\n",
      "Training Loss: 0.19662659994376422\n",
      "Validation Loss: 0.1357734793640049\n",
      "Epoch 5/100\n",
      "Training Loss: 0.1669539282653671\n",
      "Validation Loss: 0.0584021710504087\n",
      "Epoch 6/100\n",
      "Training Loss: 0.165347030854782\n",
      "Validation Loss: 0.1508727397794621\n",
      "Epoch 7/100\n",
      "Training Loss: 0.13517286917004628\n",
      "Validation Loss: 0.087532678532638\n",
      "Epoch 8/100\n",
      "Training Loss: 0.13499233062927687\n",
      "Validation Loss: 0.0894132299200143\n",
      "Epoch 9/100\n",
      "Training Loss: 0.12337141668404869\n",
      "Validation Loss: 0.051571271260649906\n",
      "Epoch 10/100\n",
      "Training Loss: 0.11826580249070329\n",
      "Validation Loss: 0.051769813006918165\n",
      "Epoch 11/100\n",
      "Training Loss: 0.10244877673601398\n",
      "Validation Loss: 0.05343469070448646\n",
      "Epoch 12/100\n",
      "Training Loss: 0.08735025433832104\n",
      "Validation Loss: 0.06396563846808694\n",
      "Epoch 13/100\n",
      "Training Loss: 0.08901902682323212\n",
      "Validation Loss: 0.05206074800559975\n",
      "Epoch 14/100\n",
      "Training Loss: 0.0748000568564559\n",
      "Validation Loss: 0.08023984384061093\n",
      "Epoch 15/100\n",
      "Training Loss: 0.07504971410856874\n",
      "Validation Loss: 0.039840873070481106\n",
      "Epoch 16/100\n",
      "Training Loss: 0.0775602088971034\n",
      "Validation Loss: 0.051033952928239625\n",
      "Epoch 17/100\n",
      "Training Loss: 0.06604864560475396\n",
      "Validation Loss: 0.03180760868930578\n",
      "Epoch 18/100\n",
      "Training Loss: 0.05379644302158421\n",
      "Validation Loss: 0.06251911874180138\n",
      "Epoch 19/100\n",
      "Training Loss: 0.07446216422338972\n",
      "Validation Loss: 0.028433487455928497\n",
      "Epoch 20/100\n",
      "Training Loss: 0.06428454359250409\n",
      "Validation Loss: 0.059943399901398575\n",
      "Epoch 21/100\n",
      "Training Loss: 0.055900458420204045\n",
      "Validation Loss: 0.03484313441387145\n",
      "Epoch 22/100\n",
      "Training Loss: 0.06432973325751108\n",
      "Validation Loss: 0.02939505563509468\n",
      "Epoch 23/100\n",
      "Training Loss: 0.0564025042122022\n",
      "Validation Loss: 0.08635849022252706\n",
      "Epoch 24/100\n",
      "Training Loss: 0.05667003961095645\n",
      "Validation Loss: 0.05843924713742656\n",
      "Epoch 25/100\n",
      "Training Loss: 0.06272765744957208\n",
      "Validation Loss: 0.06762647677087079\n",
      "Epoch 26/100\n",
      "Training Loss: 0.05696071766386693\n",
      "Validation Loss: 0.044510813404175335\n",
      "Epoch 27/100\n",
      "Training Loss: 0.04734711034889976\n",
      "Validation Loss: 0.07137052782525591\n",
      "Epoch 28/100\n",
      "Training Loss: 0.0629738536497027\n",
      "Validation Loss: 0.05761423846679657\n",
      "Epoch 29/100\n",
      "Training Loss: 0.04856363709115729\n",
      "Validation Loss: 0.05543764970164581\n",
      "Epoch 30/100\n",
      "Training Loss: 0.06286771496414822\n",
      "Validation Loss: 0.06990526408834145\n",
      "Epoch 31/100\n",
      "Training Loss: 0.06237027449271497\n",
      "Validation Loss: 0.050202463076368865\n",
      "Epoch 32/100\n",
      "Training Loss: 0.0561167927531939\n",
      "Validation Loss: 0.028908738831749197\n",
      "Epoch 33/100\n",
      "Training Loss: 0.05151710489431866\n",
      "Validation Loss: 0.04873752553566307\n",
      "Epoch 34/100\n",
      "Training Loss: 0.05015152934400401\n",
      "Validation Loss: 0.024025062616495195\n",
      "Epoch 35/100\n",
      "Training Loss: 0.05920647758643423\n",
      "Validation Loss: 0.04705613612139215\n",
      "Epoch 36/100\n",
      "Training Loss: 0.05725829947344082\n",
      "Validation Loss: 0.04552919700258977\n",
      "Epoch 37/100\n",
      "Training Loss: 0.05944456031527757\n",
      "Validation Loss: 0.027493339723053112\n",
      "Epoch 38/100\n",
      "Training Loss: 0.058467873983568425\n",
      "Validation Loss: 0.06308165639834055\n",
      "Epoch 39/100\n",
      "Training Loss: 0.059726314013248086\n",
      "Validation Loss: 0.044203391222028765\n",
      "Epoch 40/100\n",
      "Training Loss: 0.056694182440005035\n",
      "Validation Loss: 0.03092420557043566\n",
      "Epoch 41/100\n",
      "Training Loss: 0.05529871715942647\n",
      "Validation Loss: 0.040767272729044304\n",
      "Epoch 42/100\n",
      "Training Loss: 0.06172472470754145\n",
      "Validation Loss: 0.07822786173248404\n",
      "Epoch 43/100\n",
      "Training Loss: 0.052939519596545796\n",
      "Validation Loss: 0.05511618983438741\n",
      "Epoch 44/100\n",
      "Training Loss: 0.04993979278554042\n",
      "Validation Loss: 0.04405430864448873\n",
      "Epoch 45/100\n",
      "Training Loss: 0.053905561725673366\n",
      "Validation Loss: 0.04707397553463514\n",
      "Epoch 46/100\n",
      "Training Loss: 0.05163711189886711\n",
      "Validation Loss: 0.06170991311317177\n",
      "Epoch 47/100\n",
      "Training Loss: 0.05423878344518784\n",
      "Validation Loss: 0.03305295873130816\n",
      "Epoch 48/100\n",
      "Training Loss: 0.05338209111942381\n",
      "Validation Loss: 0.04730369500434933\n",
      "Epoch 49/100\n",
      "Training Loss: 0.04948520955272443\n",
      "Validation Loss: 0.082093936757451\n",
      "Epoch 50/100\n",
      "Training Loss: 0.05083753544404697\n",
      "Validation Loss: 0.04924020054411342\n",
      "Epoch 51/100\n",
      "Training Loss: 0.047898037790146605\n",
      "Validation Loss: 0.09704074221537687\n",
      "Epoch 52/100\n",
      "Training Loss: 0.052139464262975894\n",
      "Validation Loss: 0.039629844185532545\n",
      "Epoch 53/100\n",
      "Training Loss: 0.052837324033544755\n",
      "Validation Loss: 0.010470732396769484\n",
      "Epoch 54/100\n",
      "Training Loss: 0.05106229328045842\n",
      "Validation Loss: 0.023618549812150218\n",
      "Epoch 55/100\n",
      "Training Loss: 0.0677387941330116\n",
      "Validation Loss: 0.08345661573265825\n",
      "Epoch 56/100\n",
      "Training Loss: 0.053700526011357164\n",
      "Validation Loss: 0.020512635217759867\n",
      "Epoch 57/100\n",
      "Training Loss: 0.0515638031407831\n",
      "Validation Loss: 0.04739400277677664\n",
      "Epoch 58/100\n",
      "Training Loss: 0.060601869612754114\n",
      "Validation Loss: 0.02473432027750146\n",
      "Epoch 59/100\n",
      "Training Loss: 0.058111100349290595\n",
      "Validation Loss: 0.04376809174217204\n",
      "Epoch 60/100\n",
      "Training Loss: 0.058827277089669505\n",
      "Validation Loss: 0.03460074803548628\n",
      "Epoch 61/100\n",
      "Training Loss: 0.058219033137972\n",
      "Validation Loss: 0.04097632268892788\n",
      "Epoch 62/100\n",
      "Training Loss: 0.05410126045593988\n",
      "Validation Loss: 0.06393481622491372\n",
      "Epoch 63/100\n",
      "Training Loss: 0.06393707107027209\n",
      "Validation Loss: 0.024311695885633068\n",
      "Epoch 64/100\n",
      "Training Loss: 0.05593860941402383\n",
      "Validation Loss: 0.08521747904740443\n",
      "Epoch 65/100\n",
      "Training Loss: 0.06204633187880672\n",
      "Validation Loss: 0.05014538123409233\n",
      "Epoch 66/100\n",
      "Training Loss: 0.06396624354875806\n",
      "Validation Loss: 0.05148321215256031\n",
      "Epoch 67/100\n",
      "Training Loss: 0.059577264464131134\n",
      "Validation Loss: 0.034091251024751676\n",
      "Epoch 68/100\n",
      "Training Loss: 0.06366643749730966\n",
      "Validation Loss: 0.039139961804134774\n",
      "Epoch 69/100\n",
      "Training Loss: 0.06144538332092495\n",
      "Validation Loss: 0.04944975959516793\n",
      "Epoch 70/100\n",
      "Training Loss: 0.06281624639871253\n",
      "Validation Loss: 0.024632831476106724\n",
      "Epoch 71/100\n",
      "Training Loss: 0.06112050772320865\n",
      "Validation Loss: 0.024353007387912736\n",
      "Epoch 72/100\n",
      "Training Loss: 0.05589365120568082\n",
      "Validation Loss: 0.03758861550186678\n",
      "Epoch 73/100\n",
      "Training Loss: 0.0495399184377356\n",
      "Validation Loss: 0.038248091927486655\n",
      "Epoch 74/100\n",
      "Training Loss: 0.06533966420905062\n",
      "Validation Loss: 0.0654075519585718\n",
      "Epoch 75/100\n",
      "Training Loss: 0.05880301099798583\n",
      "Validation Loss: 0.0715790564053433\n",
      "Epoch 76/100\n",
      "Training Loss: 0.04692945906277418\n",
      "Validation Loss: 0.05702968047340702\n",
      "Epoch 77/100\n",
      "Training Loss: 0.06454866541345189\n",
      "Validation Loss: 0.09211032797723846\n",
      "Epoch 78/100\n",
      "Training Loss: 0.04482879334066243\n",
      "Validation Loss: 0.04546705350793266\n",
      "Epoch 79/100\n",
      "Training Loss: 0.0591686720572045\n",
      "Validation Loss: 0.057244977687904056\n",
      "Epoch 80/100\n",
      "Training Loss: 0.054622014348411956\n",
      "Validation Loss: 0.03782893015725358\n",
      "Epoch 81/100\n",
      "Training Loss: 0.05190834419637698\n",
      "Validation Loss: 0.03647807134084299\n",
      "Epoch 82/100\n",
      "Training Loss: 0.06466205849107681\n",
      "Validation Loss: 0.10943351435746569\n",
      "Epoch 83/100\n",
      "Training Loss: 0.05776494470507487\n",
      "Validation Loss: 0.03856271692839646\n",
      "Epoch 84/100\n",
      "Training Loss: 0.06359530963835547\n",
      "Validation Loss: 0.07579087033396603\n",
      "Epoch 85/100\n",
      "Training Loss: 0.06743619672202224\n",
      "Validation Loss: 0.019990914944518396\n",
      "Epoch 86/100\n",
      "Training Loss: 0.05639281128574579\n",
      "Validation Loss: 0.03724872381306796\n",
      "Epoch 87/100\n",
      "Training Loss: 0.06152925435397524\n",
      "Validation Loss: 0.04683619824274747\n",
      "Epoch 88/100\n",
      "Training Loss: 0.0547655940053486\n",
      "Validation Loss: 0.06667740260515528\n",
      "Epoch 89/100\n",
      "Training Loss: 0.054042842879055794\n",
      "Validation Loss: 0.05233552637888798\n",
      "Epoch 90/100\n",
      "Training Loss: 0.05458473052555183\n",
      "Validation Loss: 0.044816356843179174\n",
      "Epoch 91/100\n",
      "Training Loss: 0.05799285380869066\n",
      "Validation Loss: 0.04102025913741186\n",
      "Epoch 92/100\n",
      "Training Loss: 0.06001048900293788\n",
      "Validation Loss: 0.025102503728496085\n",
      "Epoch 93/100\n",
      "Training Loss: 0.05543686986000387\n",
      "Validation Loss: 0.042972186529345494\n",
      "Epoch 94/100\n",
      "Training Loss: 0.05204227590120678\n",
      "Validation Loss: 0.017215143713322074\n",
      "Epoch 95/100\n",
      "Training Loss: 0.06241392827187893\n",
      "Validation Loss: 0.037060434094893104\n",
      "Epoch 96/100\n",
      "Training Loss: 0.047121100242609175\n",
      "Validation Loss: 0.03772269626815029\n",
      "Epoch 97/100\n",
      "Training Loss: 0.06535585785483838\n",
      "Validation Loss: 0.03856277701561107\n",
      "Epoch 98/100\n",
      "Training Loss: 0.06000887759055539\n",
      "Validation Loss: 0.04377485288246609\n",
      "Epoch 99/100\n",
      "Training Loss: 0.06380897244570563\n",
      "Validation Loss: 0.056510581909385224\n",
      "Epoch 100/100\n",
      "Training Loss: 0.05606316903122919\n",
      "Validation Loss: 0.07903488282307733\n",
      "Combination 47: Avg Training Loss = 0.07068933675164729, Avg Validation Loss = 0.05526885643532284\n",
      "Testing combination 48/48: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.001, 'sequence_length': 50, 'dropout_rate': 0.2}\n",
      "Truncated Train Data Shape: (70, 50, 1)\n",
      "Truncated Validation Data Shape: (10, 50, 1)\n",
      "    Trial 1/2 for combination 48/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.2684604140965735\n",
      "Validation Loss: 0.13954954171638223\n",
      "Epoch 2/100\n",
      "Training Loss: 0.17527837812871247\n",
      "Validation Loss: 0.2825533591198749\n",
      "Epoch 3/100\n",
      "Training Loss: 0.19964866539938575\n",
      "Validation Loss: 0.04981953249060583\n",
      "Epoch 4/100\n",
      "Training Loss: 0.1513386963545295\n",
      "Validation Loss: 0.13286459101230763\n",
      "Epoch 5/100\n",
      "Training Loss: 0.14630911098806929\n",
      "Validation Loss: 0.05050583625831745\n",
      "Epoch 6/100\n",
      "Training Loss: 0.13584742279859705\n",
      "Validation Loss: 0.07959862891274384\n",
      "Epoch 7/100\n",
      "Training Loss: 0.12692062176185895\n",
      "Validation Loss: 0.08373004626518456\n",
      "Epoch 8/100\n",
      "Training Loss: 0.11072432012724683\n",
      "Validation Loss: 0.07677365093151332\n",
      "Epoch 9/100\n",
      "Training Loss: 0.10713975570406693\n",
      "Validation Loss: 0.08317461028491178\n",
      "Epoch 10/100\n",
      "Training Loss: 0.09855517198830957\n",
      "Validation Loss: 0.21433791735011662\n",
      "Epoch 11/100\n",
      "Training Loss: 0.0914148160123202\n",
      "Validation Loss: 0.1436721659540147\n",
      "Epoch 12/100\n",
      "Training Loss: 0.07509890638884863\n",
      "Validation Loss: 0.18156920649714026\n",
      "Epoch 13/100\n",
      "Training Loss: 0.0843058710528107\n",
      "Validation Loss: 0.08425035969930778\n",
      "Epoch 14/100\n",
      "Training Loss: 0.06990323679314478\n",
      "Validation Loss: 0.15046342792742667\n",
      "Epoch 15/100\n",
      "Training Loss: 0.07149028847288698\n",
      "Validation Loss: 0.2160896090335485\n",
      "Epoch 16/100\n",
      "Training Loss: 0.06028392403402387\n",
      "Validation Loss: 0.11893767384160056\n",
      "Epoch 17/100\n",
      "Training Loss: 0.05893842205502833\n",
      "Validation Loss: 0.1868288813316677\n",
      "Epoch 18/100\n",
      "Training Loss: 0.06321504946293462\n",
      "Validation Loss: 0.04077648889694095\n",
      "Epoch 19/100\n",
      "Training Loss: 0.05693747171476005\n",
      "Validation Loss: 0.26216960578778264\n",
      "Epoch 20/100\n",
      "Training Loss: 0.056167491447604946\n",
      "Validation Loss: 0.1732655461304355\n",
      "Epoch 21/100\n",
      "Training Loss: 0.05172035692313143\n",
      "Validation Loss: 0.12369139627339021\n",
      "Epoch 22/100\n",
      "Training Loss: 0.05364141825951811\n",
      "Validation Loss: 0.10080350989817474\n",
      "Epoch 23/100\n",
      "Training Loss: 0.06103225200483308\n",
      "Validation Loss: 0.13770261809305728\n",
      "Epoch 24/100\n",
      "Training Loss: 0.05123980106032571\n",
      "Validation Loss: 0.15519610703372547\n",
      "Epoch 25/100\n",
      "Training Loss: 0.059408489834210536\n",
      "Validation Loss: 0.15248151153100747\n",
      "Epoch 26/100\n",
      "Training Loss: 0.054873279677119156\n",
      "Validation Loss: 0.1433388403111188\n",
      "Epoch 27/100\n",
      "Training Loss: 0.052963034429412637\n",
      "Validation Loss: 0.15485467133984915\n",
      "Epoch 28/100\n",
      "Training Loss: 0.04514058155801353\n",
      "Validation Loss: 0.13129866674076177\n",
      "Epoch 29/100\n",
      "Training Loss: 0.04897979316653581\n",
      "Validation Loss: 0.1316064911468227\n",
      "Epoch 30/100\n",
      "Training Loss: 0.047564909945406776\n",
      "Validation Loss: 0.16409696761619813\n",
      "Epoch 31/100\n",
      "Training Loss: 0.04482498878943916\n",
      "Validation Loss: 0.26488939553751784\n",
      "Epoch 32/100\n",
      "Training Loss: 0.05742581902438061\n",
      "Validation Loss: 0.26409277864390523\n",
      "Epoch 33/100\n",
      "Training Loss: 0.06173372641936449\n",
      "Validation Loss: 0.15122443474377253\n",
      "Epoch 34/100\n",
      "Training Loss: 0.05670119083830263\n",
      "Validation Loss: 0.09085312025880259\n",
      "Epoch 35/100\n",
      "Training Loss: 0.05092575288559098\n",
      "Validation Loss: 0.17243009937289558\n",
      "Epoch 36/100\n",
      "Training Loss: 0.05094109918258187\n",
      "Validation Loss: 0.22991910225375856\n",
      "Epoch 37/100\n",
      "Training Loss: 0.061492837654649475\n",
      "Validation Loss: 0.1364992200680774\n",
      "Epoch 38/100\n",
      "Training Loss: 0.04793989019772099\n",
      "Validation Loss: 0.15456553304254653\n",
      "Epoch 39/100\n",
      "Training Loss: 0.06408736465830249\n",
      "Validation Loss: 0.06744682512919101\n",
      "Epoch 40/100\n",
      "Training Loss: 0.0591472341085718\n",
      "Validation Loss: 0.09924561755297179\n",
      "Epoch 41/100\n",
      "Training Loss: 0.058181045033791144\n",
      "Validation Loss: 0.15282423317456423\n",
      "Epoch 42/100\n",
      "Training Loss: 0.0619976865526574\n",
      "Validation Loss: 0.0548725002020667\n",
      "Epoch 43/100\n",
      "Training Loss: 0.0752393800370069\n",
      "Validation Loss: 0.046471908127631814\n",
      "Epoch 44/100\n",
      "Training Loss: 0.07068166577516391\n",
      "Validation Loss: 0.06310728045262812\n",
      "Epoch 45/100\n",
      "Training Loss: 0.0610703146537602\n",
      "Validation Loss: 0.061490905610623356\n",
      "Epoch 46/100\n",
      "Training Loss: 0.06605896936069855\n",
      "Validation Loss: 0.08006264966712494\n",
      "Epoch 47/100\n",
      "Training Loss: 0.0635523366646011\n",
      "Validation Loss: 0.05608466989447432\n",
      "Epoch 48/100\n",
      "Training Loss: 0.06067605373080031\n",
      "Validation Loss: 0.04065972601326213\n",
      "Epoch 49/100\n",
      "Training Loss: 0.05356758335278048\n",
      "Validation Loss: 0.10267428397239924\n",
      "Epoch 50/100\n",
      "Training Loss: 0.06614780069793826\n",
      "Validation Loss: 0.03671111377089216\n",
      "Epoch 51/100\n",
      "Training Loss: 0.06684367989043236\n",
      "Validation Loss: 0.03099577758034388\n",
      "Epoch 52/100\n",
      "Training Loss: 0.06915447198752794\n",
      "Validation Loss: 0.03891214455334724\n",
      "Epoch 53/100\n",
      "Training Loss: 0.05890405252869633\n",
      "Validation Loss: 0.040788105248212206\n",
      "Epoch 54/100\n",
      "Training Loss: 0.05224082733019547\n",
      "Validation Loss: 0.0339718872106939\n",
      "Epoch 55/100\n",
      "Training Loss: 0.05798779140510516\n",
      "Validation Loss: 0.05609098114078008\n",
      "Epoch 56/100\n",
      "Training Loss: 0.059122613703748085\n",
      "Validation Loss: 0.05324544394387307\n",
      "Epoch 57/100\n",
      "Training Loss: 0.06154955057023893\n",
      "Validation Loss: 0.04368341980043145\n",
      "Epoch 58/100\n",
      "Training Loss: 0.06025295663953724\n",
      "Validation Loss: 0.06118521516992178\n",
      "Epoch 59/100\n",
      "Training Loss: 0.05324462169349607\n",
      "Validation Loss: 0.02352396457863415\n",
      "Epoch 60/100\n",
      "Training Loss: 0.05371062756013145\n",
      "Validation Loss: 0.04243312437079893\n",
      "Epoch 61/100\n",
      "Training Loss: 0.05607869255453738\n",
      "Validation Loss: 0.07824065962851903\n",
      "Epoch 62/100\n",
      "Training Loss: 0.05870582212062814\n",
      "Validation Loss: 0.0881110679517704\n",
      "Epoch 63/100\n",
      "Training Loss: 0.06036045853927172\n",
      "Validation Loss: 0.04581572776738212\n",
      "Epoch 64/100\n",
      "Training Loss: 0.05754693006298212\n",
      "Validation Loss: 0.054316636651624506\n",
      "Epoch 65/100\n",
      "Training Loss: 0.0629492275021426\n",
      "Validation Loss: 0.03642302817265068\n",
      "Epoch 66/100\n",
      "Training Loss: 0.049369680023579124\n",
      "Validation Loss: 0.05784330532920232\n",
      "Epoch 67/100\n",
      "Training Loss: 0.05228864070606078\n",
      "Validation Loss: 0.05861224339184924\n",
      "Epoch 68/100\n",
      "Training Loss: 0.06439959292115523\n",
      "Validation Loss: 0.0885449991295553\n",
      "Epoch 69/100\n",
      "Training Loss: 0.0508791462863334\n",
      "Validation Loss: 0.09486450184054865\n",
      "Epoch 70/100\n",
      "Training Loss: 0.04584053286554601\n",
      "Validation Loss: 0.09798078787396222\n",
      "Epoch 71/100\n",
      "Training Loss: 0.06960675446545984\n",
      "Validation Loss: 0.1623041811758888\n",
      "Epoch 72/100\n",
      "Training Loss: 0.056170269314581585\n",
      "Validation Loss: 0.058455761200133495\n",
      "Epoch 73/100\n",
      "Training Loss: 0.06042497289142875\n",
      "Validation Loss: 0.061322837267211085\n",
      "Epoch 74/100\n",
      "Training Loss: 0.06515100301267657\n",
      "Validation Loss: 0.05302205971542071\n",
      "Epoch 75/100\n",
      "Training Loss: 0.0569512172797623\n",
      "Validation Loss: 0.05099835380752217\n",
      "Epoch 76/100\n",
      "Training Loss: 0.058563916731342605\n",
      "Validation Loss: 0.08397740483716873\n",
      "Epoch 77/100\n",
      "Training Loss: 0.06662733996112888\n",
      "Validation Loss: 0.17361493454038163\n",
      "Epoch 78/100\n",
      "Training Loss: 0.06473013894046523\n",
      "Validation Loss: 0.04392353364083827\n",
      "Epoch 79/100\n",
      "Training Loss: 0.053124158214492935\n",
      "Validation Loss: 0.053207643479207145\n",
      "Epoch 80/100\n",
      "Training Loss: 0.05990882701899417\n",
      "Validation Loss: 0.05516189026468299\n",
      "Epoch 81/100\n",
      "Training Loss: 0.06580516088996291\n",
      "Validation Loss: 0.07936108974756587\n",
      "Epoch 82/100\n",
      "Training Loss: 0.0675107343121893\n",
      "Validation Loss: 0.0789395829077859\n",
      "Epoch 83/100\n",
      "Training Loss: 0.06117580821561351\n",
      "Validation Loss: 0.0509002365839902\n",
      "Epoch 84/100\n",
      "Training Loss: 0.056751961053313454\n",
      "Validation Loss: 0.06295177731256622\n",
      "Epoch 85/100\n",
      "Training Loss: 0.05988859838223148\n",
      "Validation Loss: 0.06298195702379569\n",
      "Epoch 86/100\n",
      "Training Loss: 0.05946711603546071\n",
      "Validation Loss: 0.04650086567887579\n",
      "Epoch 87/100\n",
      "Training Loss: 0.05923670725533365\n",
      "Validation Loss: 0.04834021764698334\n",
      "Epoch 88/100\n",
      "Training Loss: 0.06402200033447299\n",
      "Validation Loss: 0.05975247378552466\n",
      "Epoch 89/100\n",
      "Training Loss: 0.06489322835222446\n",
      "Validation Loss: 0.07045885604496331\n",
      "Epoch 90/100\n",
      "Training Loss: 0.06487632464933231\n",
      "Validation Loss: 0.0660617220918345\n",
      "Epoch 91/100\n",
      "Training Loss: 0.05836980981197486\n",
      "Validation Loss: 0.06060014826376805\n",
      "Epoch 92/100\n",
      "Training Loss: 0.06106969902669428\n",
      "Validation Loss: 0.060853949225097625\n",
      "Epoch 93/100\n",
      "Training Loss: 0.06586173031511984\n",
      "Validation Loss: 0.054996309596482815\n",
      "Epoch 94/100\n",
      "Training Loss: 0.06822053889472819\n",
      "Validation Loss: 0.17476680175982567\n",
      "Epoch 95/100\n",
      "Training Loss: 0.05844466610214757\n",
      "Validation Loss: 0.057204870029866864\n",
      "Epoch 96/100\n",
      "Training Loss: 0.05861133501520427\n",
      "Validation Loss: 0.045452574538422566\n",
      "Epoch 97/100\n",
      "Training Loss: 0.06496445966646602\n",
      "Validation Loss: 0.06841288042326686\n",
      "Epoch 98/100\n",
      "Training Loss: 0.0628168051784775\n",
      "Validation Loss: 0.06047299076539845\n",
      "Epoch 99/100\n",
      "Training Loss: 0.06175338335775081\n",
      "Validation Loss: 0.04774273533799839\n",
      "Epoch 100/100\n",
      "Training Loss: 0.06409439456763259\n",
      "Validation Loss: 0.03955302222873119\n",
      "    Trial 2/2 for combination 48/48\n",
      "Epoch 1/100\n",
      "Training Loss: 0.2987008648223607\n",
      "Validation Loss: 0.3647314940467079\n",
      "Epoch 2/100\n",
      "Training Loss: 0.21291222125289216\n",
      "Validation Loss: 0.1447113607849545\n",
      "Epoch 3/100\n",
      "Training Loss: 0.14439615819733886\n",
      "Validation Loss: 0.2822858388956916\n",
      "Epoch 4/100\n",
      "Training Loss: 0.14709301809169392\n",
      "Validation Loss: 0.059385486339329184\n",
      "Epoch 5/100\n",
      "Training Loss: 0.14169387256557173\n",
      "Validation Loss: 0.1792629350389517\n",
      "Epoch 6/100\n",
      "Training Loss: 0.11576187050962981\n",
      "Validation Loss: 0.1512445907852083\n",
      "Epoch 7/100\n",
      "Training Loss: 0.10637836682018638\n",
      "Validation Loss: 0.10962710147829935\n",
      "Epoch 8/100\n",
      "Training Loss: 0.09948485415717646\n",
      "Validation Loss: 0.044950525601764706\n",
      "Epoch 9/100\n",
      "Training Loss: 0.09556680846030885\n",
      "Validation Loss: 0.14477090739313944\n",
      "Epoch 10/100\n",
      "Training Loss: 0.09097391572997848\n",
      "Validation Loss: 0.19427819462708248\n",
      "Epoch 11/100\n",
      "Training Loss: 0.07826676059147139\n",
      "Validation Loss: 0.19965069890278925\n",
      "Epoch 12/100\n",
      "Training Loss: 0.08545702317712774\n",
      "Validation Loss: 0.13384947964116237\n",
      "Epoch 13/100\n",
      "Training Loss: 0.07635266714478542\n",
      "Validation Loss: 0.12118541663106892\n",
      "Epoch 14/100\n",
      "Training Loss: 0.06401985666436497\n",
      "Validation Loss: 0.11613282096620345\n",
      "Epoch 15/100\n",
      "Training Loss: 0.06028498108563184\n",
      "Validation Loss: 0.09075909937215162\n",
      "Epoch 16/100\n",
      "Training Loss: 0.07336432055567631\n",
      "Validation Loss: 0.08014255519671823\n",
      "Epoch 17/100\n",
      "Training Loss: 0.0673800608965326\n",
      "Validation Loss: 0.0895158332544331\n",
      "Epoch 18/100\n",
      "Training Loss: 0.06076542115234517\n",
      "Validation Loss: 0.04625559806927429\n",
      "Epoch 19/100\n",
      "Training Loss: 0.053993598594748574\n",
      "Validation Loss: 0.09444942924814835\n",
      "Epoch 20/100\n",
      "Training Loss: 0.058755712545989436\n",
      "Validation Loss: 0.10742688067555492\n",
      "Epoch 21/100\n",
      "Training Loss: 0.05995825845411128\n",
      "Validation Loss: 0.1450120870730568\n",
      "Epoch 22/100\n",
      "Training Loss: 0.049420218581053055\n",
      "Validation Loss: 0.132254525525043\n",
      "Epoch 23/100\n",
      "Training Loss: 0.05313021261091432\n",
      "Validation Loss: 0.10064754138794345\n",
      "Epoch 24/100\n",
      "Training Loss: 0.054845980686561484\n",
      "Validation Loss: 0.10551353247870787\n",
      "Epoch 25/100\n",
      "Training Loss: 0.05298393308954038\n",
      "Validation Loss: 0.10019095828565994\n",
      "Epoch 26/100\n",
      "Training Loss: 0.05992132533613171\n",
      "Validation Loss: 0.08723734608375314\n",
      "Epoch 27/100\n",
      "Training Loss: 0.05706299571958628\n",
      "Validation Loss: 0.1379655857438505\n",
      "Epoch 28/100\n",
      "Training Loss: 0.04433863857805275\n",
      "Validation Loss: 0.12262465503005408\n",
      "Epoch 29/100\n",
      "Training Loss: 0.050962014091520534\n",
      "Validation Loss: 0.060671505032728903\n",
      "Epoch 30/100\n",
      "Training Loss: 0.04951852906404848\n",
      "Validation Loss: 0.1290338081919175\n",
      "Epoch 31/100\n",
      "Training Loss: 0.05458985314113128\n",
      "Validation Loss: 0.18490488229563265\n",
      "Epoch 32/100\n",
      "Training Loss: 0.04420933462687135\n",
      "Validation Loss: 0.17834291209245862\n",
      "Epoch 33/100\n",
      "Training Loss: 0.060613714240763436\n",
      "Validation Loss: 0.09328512099873472\n",
      "Epoch 34/100\n",
      "Training Loss: 0.051621175257480424\n",
      "Validation Loss: 0.1004634914006826\n",
      "Epoch 35/100\n",
      "Training Loss: 0.05253954005703229\n",
      "Validation Loss: 0.17048336620124208\n",
      "Epoch 36/100\n",
      "Training Loss: 0.05835015700305193\n",
      "Validation Loss: 0.10007511737394234\n",
      "Epoch 37/100\n",
      "Training Loss: 0.05104517937040708\n",
      "Validation Loss: 0.11353638974849381\n",
      "Epoch 38/100\n",
      "Training Loss: 0.049348662617512036\n",
      "Validation Loss: 0.07485816017629565\n",
      "Epoch 39/100\n",
      "Training Loss: 0.05551226861405242\n",
      "Validation Loss: 0.1302051320141948\n",
      "Epoch 40/100\n",
      "Training Loss: 0.05578665009350429\n",
      "Validation Loss: 0.0978280044650848\n",
      "Epoch 41/100\n",
      "Training Loss: 0.05541386298319276\n",
      "Validation Loss: 0.11663167627480349\n",
      "Epoch 42/100\n",
      "Training Loss: 0.05928573127996677\n",
      "Validation Loss: 0.11607566687403628\n",
      "Epoch 43/100\n",
      "Training Loss: 0.05932560717943764\n",
      "Validation Loss: 0.123813069369578\n",
      "Epoch 44/100\n",
      "Training Loss: 0.05365522332860575\n",
      "Validation Loss: 0.09640325943419177\n",
      "Epoch 45/100\n",
      "Training Loss: 0.05820791567273156\n",
      "Validation Loss: 0.09064052919992438\n",
      "Epoch 46/100\n",
      "Training Loss: 0.05074431404901748\n",
      "Validation Loss: 0.04778688108550586\n",
      "Epoch 47/100\n",
      "Training Loss: 0.06369276122083863\n",
      "Validation Loss: 0.08009773346616429\n",
      "Epoch 48/100\n",
      "Training Loss: 0.06403052210803149\n",
      "Validation Loss: 0.06669284107368673\n",
      "Epoch 49/100\n",
      "Training Loss: 0.0671123115892094\n",
      "Validation Loss: 0.038524685327119904\n",
      "Epoch 50/100\n",
      "Training Loss: 0.06270192045252639\n",
      "Validation Loss: 0.08387890168003323\n",
      "Epoch 51/100\n",
      "Training Loss: 0.06435736511691666\n",
      "Validation Loss: 0.07176961567808175\n",
      "Epoch 52/100\n",
      "Training Loss: 0.06387064556119808\n",
      "Validation Loss: 0.08055307147456658\n",
      "Epoch 53/100\n",
      "Training Loss: 0.05987744958442069\n",
      "Validation Loss: 0.07727047451538206\n",
      "Epoch 54/100\n",
      "Training Loss: 0.06543425074222324\n",
      "Validation Loss: 0.07638105930794209\n",
      "Epoch 55/100\n",
      "Training Loss: 0.0644656504304325\n",
      "Validation Loss: 0.052209903336490324\n",
      "Epoch 56/100\n",
      "Training Loss: 0.07211295821654881\n",
      "Validation Loss: 0.055361442875480824\n",
      "Epoch 57/100\n",
      "Training Loss: 0.06721089023685374\n",
      "Validation Loss: 0.05612669978356494\n",
      "Epoch 58/100\n",
      "Training Loss: 0.06855074243345716\n",
      "Validation Loss: 0.09231715989154211\n",
      "Epoch 59/100\n",
      "Training Loss: 0.06018476919438402\n",
      "Validation Loss: 0.0712797978121371\n",
      "Epoch 60/100\n",
      "Training Loss: 0.07210238177182751\n",
      "Validation Loss: 0.06725720818586516\n",
      "Epoch 61/100\n",
      "Training Loss: 0.06712229907671115\n",
      "Validation Loss: 0.05487058975832208\n",
      "Epoch 62/100\n",
      "Training Loss: 0.07007355645758281\n",
      "Validation Loss: 0.21011212035475874\n",
      "Epoch 63/100\n",
      "Training Loss: 0.06277200645497172\n",
      "Validation Loss: 0.04611525816436016\n",
      "Epoch 64/100\n",
      "Training Loss: 0.07280442236832761\n",
      "Validation Loss: 0.041949391340072195\n",
      "Epoch 65/100\n",
      "Training Loss: 0.06173045424673585\n",
      "Validation Loss: 0.05336852052016859\n",
      "Epoch 66/100\n",
      "Training Loss: 0.0659809375921555\n",
      "Validation Loss: 0.06832761837751092\n",
      "Epoch 67/100\n",
      "Training Loss: 0.06896192435497735\n",
      "Validation Loss: 0.046412814793071126\n",
      "Epoch 68/100\n",
      "Training Loss: 0.06089639985306158\n",
      "Validation Loss: 0.032424519961667224\n",
      "Epoch 69/100\n",
      "Training Loss: 0.0621241052908089\n",
      "Validation Loss: 0.02948523384977772\n",
      "Epoch 70/100\n",
      "Training Loss: 0.06326163907702798\n",
      "Validation Loss: 0.10340756623096256\n",
      "Epoch 71/100\n",
      "Training Loss: 0.060909381678160845\n",
      "Validation Loss: 0.05201091320862704\n",
      "Epoch 72/100\n",
      "Training Loss: 0.05693947341725205\n",
      "Validation Loss: 0.08648920855674187\n",
      "Epoch 73/100\n",
      "Training Loss: 0.06045929822679206\n",
      "Validation Loss: 0.04387904496330162\n",
      "Epoch 74/100\n",
      "Training Loss: 0.06836176029962374\n",
      "Validation Loss: 0.014857628998204895\n",
      "Epoch 75/100\n",
      "Training Loss: 0.058006479887080103\n",
      "Validation Loss: 0.0764785470965289\n",
      "Epoch 76/100\n",
      "Training Loss: 0.05489496282105249\n",
      "Validation Loss: 0.09863527295099654\n",
      "Epoch 77/100\n",
      "Training Loss: 0.056148899854776495\n",
      "Validation Loss: 0.10104301326445093\n",
      "Epoch 78/100\n",
      "Training Loss: 0.05932743120274795\n",
      "Validation Loss: 0.06923032169810647\n",
      "Epoch 79/100\n",
      "Training Loss: 0.05620357693314495\n",
      "Validation Loss: 0.07855973723916701\n",
      "Epoch 80/100\n",
      "Training Loss: 0.05979803830939271\n",
      "Validation Loss: 0.175743403384683\n",
      "Epoch 81/100\n",
      "Training Loss: 0.05227046944483275\n",
      "Validation Loss: 0.12033405918939852\n",
      "Epoch 82/100\n",
      "Training Loss: 0.05590536231828025\n",
      "Validation Loss: 0.0875437332979786\n",
      "Epoch 83/100\n",
      "Training Loss: 0.05666841818383089\n",
      "Validation Loss: 0.10546217035482323\n",
      "Epoch 84/100\n",
      "Training Loss: 0.05081663693004238\n",
      "Validation Loss: 0.10319296492854231\n",
      "Epoch 85/100\n",
      "Training Loss: 0.04708489128514405\n",
      "Validation Loss: 0.06968804488112199\n",
      "Epoch 86/100\n",
      "Training Loss: 0.05989085953369861\n",
      "Validation Loss: 0.15126531551234074\n",
      "Epoch 87/100\n",
      "Training Loss: 0.05670930283901529\n",
      "Validation Loss: 0.043234407837937326\n",
      "Epoch 88/100\n",
      "Training Loss: 0.06179361799117187\n",
      "Validation Loss: 0.10670893225519965\n",
      "Epoch 89/100\n",
      "Training Loss: 0.06391037917366844\n",
      "Validation Loss: 0.07353103357860301\n",
      "Epoch 90/100\n",
      "Training Loss: 0.054805534365108675\n",
      "Validation Loss: 0.10856994211930267\n",
      "Epoch 91/100\n",
      "Training Loss: 0.0502350754635875\n",
      "Validation Loss: 0.07106035730523944\n",
      "Epoch 92/100\n",
      "Training Loss: 0.05523875695904283\n",
      "Validation Loss: 0.18930555174575367\n",
      "Epoch 93/100\n",
      "Training Loss: 0.05632676727262772\n",
      "Validation Loss: 0.2992029436761743\n",
      "Epoch 94/100\n",
      "Training Loss: 0.05623632348904535\n",
      "Validation Loss: 0.14479762927933898\n",
      "Epoch 95/100\n",
      "Training Loss: 0.04788934790470641\n",
      "Validation Loss: 0.09041100838745675\n",
      "Epoch 96/100\n",
      "Training Loss: 0.05653768557814741\n",
      "Validation Loss: 0.09511108705261609\n",
      "Epoch 97/100\n",
      "Training Loss: 0.06179404638800591\n",
      "Validation Loss: 0.25802986233261327\n",
      "Epoch 98/100\n",
      "Training Loss: 0.05353062181563847\n",
      "Validation Loss: 0.15415224594383145\n",
      "Epoch 99/100\n",
      "Training Loss: 0.049138769278560555\n",
      "Validation Loss: 0.0667968628236269\n",
      "Epoch 100/100\n",
      "Training Loss: 0.06402778877620934\n",
      "Validation Loss: 0.08374883279676289\n",
      "Combination 48: Avg Training Loss = 0.06872371818595019, Avg Validation Loss = 0.10138186647187343\n",
      "\n",
      "Validation Results for All Combinations:\n",
      "Combination 1/48:\n",
      "  Params: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'sequence_length': 6, 'dropout_rate': 0.1}\n",
      "  Avg Training Loss: 0.20744806017299428\n",
      "  Avg Validation Loss: 0.16159786373801938\n",
      "\n",
      "Combination 2/48:\n",
      "  Params: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'sequence_length': 6, 'dropout_rate': 0.2}\n",
      "  Avg Training Loss: 0.2134673295752186\n",
      "  Avg Validation Loss: 0.1741332931852882\n",
      "\n",
      "Combination 3/48:\n",
      "  Params: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'sequence_length': 20, 'dropout_rate': 0.1}\n",
      "  Avg Training Loss: 0.16732175313861358\n",
      "  Avg Validation Loss: 0.10336155779368646\n",
      "\n",
      "Combination 4/48:\n",
      "  Params: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'sequence_length': 20, 'dropout_rate': 0.2}\n",
      "  Avg Training Loss: 0.17177531775235963\n",
      "  Avg Validation Loss: 0.10505873130570466\n",
      "\n",
      "Combination 5/48:\n",
      "  Params: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'sequence_length': 50, 'dropout_rate': 0.1}\n",
      "  Avg Training Loss: 0.1372454720411989\n",
      "  Avg Validation Loss: 0.08532101999535005\n",
      "\n",
      "Combination 6/48:\n",
      "  Params: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.0001, 'sequence_length': 50, 'dropout_rate': 0.2}\n",
      "  Avg Training Loss: 0.15204191149218665\n",
      "  Avg Validation Loss: 0.08640986345644994\n",
      "\n",
      "Combination 7/48:\n",
      "  Params: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.001, 'sequence_length': 6, 'dropout_rate': 0.1}\n",
      "  Avg Training Loss: 0.09904087161057565\n",
      "  Avg Validation Loss: 0.06186323863396703\n",
      "\n",
      "Combination 8/48:\n",
      "  Params: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.001, 'sequence_length': 6, 'dropout_rate': 0.2}\n",
      "  Avg Training Loss: 0.09796127722488118\n",
      "  Avg Validation Loss: 0.06825993773515206\n",
      "\n",
      "Combination 9/48:\n",
      "  Params: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.001, 'sequence_length': 20, 'dropout_rate': 0.1}\n",
      "  Avg Training Loss: 0.08502566378218827\n",
      "  Avg Validation Loss: 0.056515290404546684\n",
      "\n",
      "Combination 10/48:\n",
      "  Params: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.001, 'sequence_length': 20, 'dropout_rate': 0.2}\n",
      "  Avg Training Loss: 0.08458245649104125\n",
      "  Avg Validation Loss: 0.05984992051841356\n",
      "\n",
      "Combination 11/48:\n",
      "  Params: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.001, 'sequence_length': 50, 'dropout_rate': 0.1}\n",
      "  Avg Training Loss: 0.08158231188874697\n",
      "  Avg Validation Loss: 0.053440353716124454\n",
      "\n",
      "Combination 12/48:\n",
      "  Params: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.001, 'sequence_length': 50, 'dropout_rate': 0.2}\n",
      "  Avg Training Loss: 0.08075489088077717\n",
      "  Avg Validation Loss: 0.057645657640040954\n",
      "\n",
      "Combination 13/48:\n",
      "  Params: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0001, 'sequence_length': 6, 'dropout_rate': 0.1}\n",
      "  Avg Training Loss: 0.1950021817963357\n",
      "  Avg Validation Loss: 0.15448755141911885\n",
      "\n",
      "Combination 14/48:\n",
      "  Params: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0001, 'sequence_length': 6, 'dropout_rate': 0.2}\n",
      "  Avg Training Loss: 0.16276195978562558\n",
      "  Avg Validation Loss: 0.13936861479932644\n",
      "\n",
      "Combination 15/48:\n",
      "  Params: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0001, 'sequence_length': 20, 'dropout_rate': 0.1}\n",
      "  Avg Training Loss: 0.14414373885618387\n",
      "  Avg Validation Loss: 0.09508564609281994\n",
      "\n",
      "Combination 16/48:\n",
      "  Params: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0001, 'sequence_length': 20, 'dropout_rate': 0.2}\n",
      "  Avg Training Loss: 0.14475998500572534\n",
      "  Avg Validation Loss: 0.10350390988866623\n",
      "\n",
      "Combination 17/48:\n",
      "  Params: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0001, 'sequence_length': 50, 'dropout_rate': 0.1}\n",
      "  Avg Training Loss: 0.14448164891813334\n",
      "  Avg Validation Loss: 0.07731621587495192\n",
      "\n",
      "Combination 18/48:\n",
      "  Params: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.0001, 'sequence_length': 50, 'dropout_rate': 0.2}\n",
      "  Avg Training Loss: 0.12521521023134533\n",
      "  Avg Validation Loss: 0.0866919406315913\n",
      "\n",
      "Combination 19/48:\n",
      "  Params: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.001, 'sequence_length': 6, 'dropout_rate': 0.1}\n",
      "  Avg Training Loss: 0.091896528376535\n",
      "  Avg Validation Loss: 0.0627056338643064\n",
      "\n",
      "Combination 20/48:\n",
      "  Params: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.001, 'sequence_length': 6, 'dropout_rate': 0.2}\n",
      "  Avg Training Loss: 0.08830681529728496\n",
      "  Avg Validation Loss: 0.06966802231098689\n",
      "\n",
      "Combination 21/48:\n",
      "  Params: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.001, 'sequence_length': 20, 'dropout_rate': 0.1}\n",
      "  Avg Training Loss: 0.07019774180823159\n",
      "  Avg Validation Loss: 0.05104047214907841\n",
      "\n",
      "Combination 22/48:\n",
      "  Params: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.001, 'sequence_length': 20, 'dropout_rate': 0.2}\n",
      "  Avg Training Loss: 0.06966868661495684\n",
      "  Avg Validation Loss: 0.06834298496772707\n",
      "\n",
      "Combination 23/48:\n",
      "  Params: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.001, 'sequence_length': 50, 'dropout_rate': 0.1}\n",
      "  Avg Training Loss: 0.06263636921104053\n",
      "  Avg Validation Loss: 0.05094609522616325\n",
      "\n",
      "Combination 24/48:\n",
      "  Params: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.001, 'sequence_length': 50, 'dropout_rate': 0.2}\n",
      "  Avg Training Loss: 0.06394109627919309\n",
      "  Avg Validation Loss: 0.07264875781931159\n",
      "\n",
      "Combination 25/48:\n",
      "  Params: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'sequence_length': 6, 'dropout_rate': 0.1}\n",
      "  Avg Training Loss: 0.21412550619611256\n",
      "  Avg Validation Loss: 0.15529703728234243\n",
      "\n",
      "Combination 26/48:\n",
      "  Params: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'sequence_length': 6, 'dropout_rate': 0.2}\n",
      "  Avg Training Loss: 0.21091637878068784\n",
      "  Avg Validation Loss: 0.1737447838571176\n",
      "\n",
      "Combination 27/48:\n",
      "  Params: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'sequence_length': 20, 'dropout_rate': 0.1}\n",
      "  Avg Training Loss: 0.17902381272102716\n",
      "  Avg Validation Loss: 0.11618858609339955\n",
      "\n",
      "Combination 28/48:\n",
      "  Params: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'sequence_length': 20, 'dropout_rate': 0.2}\n",
      "  Avg Training Loss: 0.17493145045713027\n",
      "  Avg Validation Loss: 0.11702069263178866\n",
      "\n",
      "Combination 29/48:\n",
      "  Params: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'sequence_length': 50, 'dropout_rate': 0.1}\n",
      "  Avg Training Loss: 0.15893849127257992\n",
      "  Avg Validation Loss: 0.09147913011072806\n",
      "\n",
      "Combination 30/48:\n",
      "  Params: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.0001, 'sequence_length': 50, 'dropout_rate': 0.2}\n",
      "  Avg Training Loss: 0.1480193833517313\n",
      "  Avg Validation Loss: 0.09296584161990554\n",
      "\n",
      "Combination 31/48:\n",
      "  Params: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.001, 'sequence_length': 6, 'dropout_rate': 0.1}\n",
      "  Avg Training Loss: 0.10498924806162001\n",
      "  Avg Validation Loss: 0.06810783412654836\n",
      "\n",
      "Combination 32/48:\n",
      "  Params: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.001, 'sequence_length': 6, 'dropout_rate': 0.2}\n",
      "  Avg Training Loss: 0.10038231698314205\n",
      "  Avg Validation Loss: 0.06754858448851349\n",
      "\n",
      "Combination 33/48:\n",
      "  Params: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.001, 'sequence_length': 20, 'dropout_rate': 0.1}\n",
      "  Avg Training Loss: 0.0933792634055716\n",
      "  Avg Validation Loss: 0.062431853371336776\n",
      "\n",
      "Combination 34/48:\n",
      "  Params: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.001, 'sequence_length': 20, 'dropout_rate': 0.2}\n",
      "  Avg Training Loss: 0.08582771314373831\n",
      "  Avg Validation Loss: 0.06504373088434365\n",
      "\n",
      "Combination 35/48:\n",
      "  Params: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.001, 'sequence_length': 50, 'dropout_rate': 0.1}\n",
      "  Avg Training Loss: 0.08755716569203209\n",
      "  Avg Validation Loss: 0.05781080189476302\n",
      "\n",
      "Combination 36/48:\n",
      "  Params: {'hidden_size': 256, 'num_layers': 1, 'learning_rate': 0.001, 'sequence_length': 50, 'dropout_rate': 0.2}\n",
      "  Avg Training Loss: 0.08486213758099422\n",
      "  Avg Validation Loss: 0.06334150757110972\n",
      "\n",
      "Combination 37/48:\n",
      "  Params: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0001, 'sequence_length': 6, 'dropout_rate': 0.1}\n",
      "  Avg Training Loss: 0.2109270518371047\n",
      "  Avg Validation Loss: 0.16947763043767558\n",
      "\n",
      "Combination 38/48:\n",
      "  Params: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0001, 'sequence_length': 6, 'dropout_rate': 0.2}\n",
      "  Avg Training Loss: 0.19038746573805015\n",
      "  Avg Validation Loss: 0.1683908636168164\n",
      "\n",
      "Combination 39/48:\n",
      "  Params: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0001, 'sequence_length': 20, 'dropout_rate': 0.1}\n",
      "  Avg Training Loss: 0.15999023065502593\n",
      "  Avg Validation Loss: 0.10267321606371191\n",
      "\n",
      "Combination 40/48:\n",
      "  Params: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0001, 'sequence_length': 20, 'dropout_rate': 0.2}\n",
      "  Avg Training Loss: 0.15211246859237704\n",
      "  Avg Validation Loss: 0.113634778558302\n",
      "\n",
      "Combination 41/48:\n",
      "  Params: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0001, 'sequence_length': 50, 'dropout_rate': 0.1}\n",
      "  Avg Training Loss: 0.13705745924191126\n",
      "  Avg Validation Loss: 0.0824224473266946\n",
      "\n",
      "Combination 42/48:\n",
      "  Params: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.0001, 'sequence_length': 50, 'dropout_rate': 0.2}\n",
      "  Avg Training Loss: 0.12815994019310212\n",
      "  Avg Validation Loss: 0.11139040624920846\n",
      "\n",
      "Combination 43/48:\n",
      "  Params: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.001, 'sequence_length': 6, 'dropout_rate': 0.1}\n",
      "  Avg Training Loss: 0.09889568050575709\n",
      "  Avg Validation Loss: 0.07195993301023858\n",
      "\n",
      "Combination 44/48:\n",
      "  Params: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.001, 'sequence_length': 6, 'dropout_rate': 0.2}\n",
      "  Avg Training Loss: 0.0921776106361727\n",
      "  Avg Validation Loss: 0.07421672688171821\n",
      "\n",
      "Combination 45/48:\n",
      "  Params: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.001, 'sequence_length': 20, 'dropout_rate': 0.1}\n",
      "  Avg Training Loss: 0.07674089145978996\n",
      "  Avg Validation Loss: 0.06335144282846383\n",
      "\n",
      "Combination 46/48:\n",
      "  Params: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.001, 'sequence_length': 20, 'dropout_rate': 0.2}\n",
      "  Avg Training Loss: 0.0709600576667607\n",
      "  Avg Validation Loss: 0.08676181073829098\n",
      "\n",
      "Combination 47/48:\n",
      "  Params: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.001, 'sequence_length': 50, 'dropout_rate': 0.1}\n",
      "  Avg Training Loss: 0.07068933675164729\n",
      "  Avg Validation Loss: 0.05526885643532284\n",
      "\n",
      "Combination 48/48:\n",
      "  Params: {'hidden_size': 256, 'num_layers': 2, 'learning_rate': 0.001, 'sequence_length': 50, 'dropout_rate': 0.2}\n",
      "  Avg Training Loss: 0.06872371818595019\n",
      "  Avg Validation Loss: 0.10138186647187343\n",
      "\n",
      "Best Hyperparameters:\n",
      "  Params: {'hidden_size': 128, 'num_layers': 2, 'learning_rate': 0.001, 'sequence_length': 50, 'dropout_rate': 0.1}\n",
      "  Avg Training Loss: 0.06263636921104053\n",
      "  Avg Validation Loss: 0.05094609522616325\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "param_grid = {\n",
    "    \"hidden_size\": [128, 256],\n",
    "    \"num_layers\": [1, 2],\n",
    "    \"learning_rate\": [0.0001, 0.001],\n",
    "    \"sequence_length\": [6, 20, 50],\n",
    "    \"dropout_rate\": [0.1, 0.2],\n",
    "}\n",
    "\n",
    "# Initialize variables to store results\n",
    "grid_search_results = []\n",
    "total_combinations = len(list(product(*param_grid.values())))\n",
    "num_trials = 2  # Number of trials per hyperparameter combination\n",
    "\n",
    "# Variables to track the best model's losses\n",
    "best_training_losses = None\n",
    "best_validation_losses = None\n",
    "best_model = None\n",
    "\n",
    "# Perform grid search\n",
    "for i, params in enumerate(product(*param_grid.values()), start=1):\n",
    "    hyperparams = dict(zip(param_grid.keys(), params))\n",
    "    print(f\"Testing combination {i}/{total_combinations}: {hyperparams}\")\n",
    "\n",
    "    # Truncate data if necessary for sequence length\n",
    "    seq_length = hyperparams[\"sequence_length\"]\n",
    "    max_length = train_data.shape[1]\n",
    "    if seq_length > max_length:\n",
    "        print(f\"Skipping sequence length {seq_length} exceeding max length {max_length}\")\n",
    "        continue\n",
    "\n",
    "    truncated_train_data = train_data[:, :seq_length]\n",
    "    truncated_val_data = validation_data[:, :seq_length]\n",
    "\n",
    "    # Debug shapes\n",
    "    print(f\"Truncated Train Data Shape: {truncated_train_data.shape}\")\n",
    "    print(f\"Truncated Validation Data Shape: {truncated_val_data.shape}\")\n",
    "\n",
    "    all_training_losses = []\n",
    "    all_validation_losses = []\n",
    "    for trial in range(num_trials):\n",
    "        print(f\"    Trial {trial + 1}/{num_trials} for combination {i}/{total_combinations}\")\n",
    "\n",
    "        try:\n",
    "            # Initialize and train the RNN\n",
    "            rnn = ArithmeticSequenceRNN(\n",
    "                vocab_size=1,\n",
    "                hidden_size=hyperparams[\"hidden_size\"],\n",
    "                sequence_length=seq_length,\n",
    "                learning_rate=hyperparams[\"learning_rate\"],\n",
    "                num_layers=hyperparams[\"num_layers\"],\n",
    "                dropout_rate=hyperparams[\"dropout_rate\"],\n",
    "            )\n",
    "            training_losses, validation_losses = rnn.train(\n",
    "                truncated_train_data,\n",
    "                truncated_train_data,\n",
    "                epochs=100,  # Increased epochs for better analysis\n",
    "                validation_data=(truncated_val_data, truncated_val_data),\n",
    "            )\n",
    "\n",
    "            all_training_losses.append(training_losses)\n",
    "            all_validation_losses.append(validation_losses)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error with combination {hyperparams}, Trial {trial + 1}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not all_validation_losses:\n",
    "        print(f\"No valid trials for hyperparameters: {hyperparams}\")\n",
    "        continue\n",
    "\n",
    "    # Average losses across trials\n",
    "    avg_training_loss = np.mean([np.mean(loss) for loss in all_training_losses])\n",
    "    avg_validation_loss = np.mean([np.mean(loss) for loss in all_validation_losses])\n",
    "    print(f\"Combination {i}: Avg Training Loss = {avg_training_loss}, Avg Validation Loss = {avg_validation_loss}\")\n",
    "\n",
    "    # Check if this combination is the best so far\n",
    "    if best_validation_losses is None or avg_validation_loss < np.mean(best_validation_losses):\n",
    "        best_training_losses = np.mean(all_training_losses, axis=0)  # Epoch-wise average across trials\n",
    "        best_validation_losses = np.mean(all_validation_losses, axis=0)\n",
    "        best_model = rnn\n",
    "\n",
    "    grid_search_results.append({\n",
    "        \"combination\": i,\n",
    "        \"params\": hyperparams,\n",
    "        \"avg_training_loss\": avg_training_loss,\n",
    "        \"avg_validation_loss\": avg_validation_loss,\n",
    "    })\n",
    "\n",
    "# Handle empty grid search results\n",
    "if grid_search_results:\n",
    "    print(\"\\nValidation Results for All Combinations:\")\n",
    "    for result in grid_search_results:\n",
    "        print(f\"Combination {result['combination']}/{total_combinations}:\")\n",
    "        print(f\"  Params: {result['params']}\")\n",
    "        print(f\"  Avg Training Loss: {result['avg_training_loss']}\")\n",
    "        print(f\"  Avg Validation Loss: {result['avg_validation_loss']}\\n\")\n",
    "\n",
    "    best_result = min(grid_search_results, key=lambda x: x[\"avg_validation_loss\"])\n",
    "    print(\"Best Hyperparameters:\")\n",
    "    print(f\"  Params: {best_result['params']}\")\n",
    "    print(f\"  Avg Training Loss: {best_result['avg_training_loss']}\")\n",
    "    print(f\"  Avg Validation Loss: {best_result['avg_validation_loss']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e354ea1",
   "metadata": {},
   "source": [
    "### Error Analysis of the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69708610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training and validation loss trends for the best model\n",
    "if best_training_losses is not None and best_validation_losses is not None:\n",
    "        epochs = range(1, len(best_training_losses) + 1)\n",
    "        plt.plot(epochs, best_training_losses, label=\"Training Loss\")\n",
    "        plt.plot(epochs, best_validation_losses, label=\"Validation Loss\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss Trends for Best Combination\")\n",
    "        plt.legend()\n",
    "        plt.show()    \n",
    "else:\n",
    "    print(\"No valid hyperparameter combinations found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdda21f",
   "metadata": {},
   "source": [
    "### Test the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9dab10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss with Best Hyperparameters: 0.03168078159341003\n"
     ]
    }
   ],
   "source": [
    "# Test the best model with testing data\n",
    "if best_model:\n",
    "    best_seq_length = best_result['params']['sequence_length']  # Corrected key\n",
    "    truncated_test_data = test_data[:, :best_seq_length, :]  # Adjusted slicing for 3D\n",
    "    test_loss = best_model.evaluate(truncated_test_data, truncated_test_data)\n",
    "    print(f\"Test Loss with Best Hyperparameters: {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a25005f",
   "metadata": {},
   "source": [
    "### Save the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86864cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved to rnn_model_for_arithmetic_sequence.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the best model\n",
    "if best_model:\n",
    "    model_save_path = \"rnn_model_for_arithmetic_sequence.pkl\"\n",
    "    with open(model_save_path, \"wb\") as f:\n",
    "        pickle.dump(best_model, f)\n",
    "    print(f\"Best model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b7bc05",
   "metadata": {},
   "source": [
    "### Load and Evaluate the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93ae034c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss with Loaded Model: 0.028092067088428457\n"
     ]
    }
   ],
   "source": [
    "# Load and evaluate the saved model\n",
    "with open(\"rnn_model_for_arithmetic_sequence.pkl\", \"rb\") as f:\n",
    "    best_rnn_model = pickle.load(f)\n",
    "    loaded_test_loss = best_rnn_model.evaluate(truncated_test_data, truncated_test_data)\n",
    "    print(f\"Test Loss with Loaded Model: {loaded_test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53795aee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvflare_env",
   "language": "python",
   "name": "nvflare_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
