{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ce7d82e-065a-4b6f-ae65-12b95e71c806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets saved as 'Reverse_Arithmetic_Sequences.csv' and 'Key_Sequences.csv'.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d8cc05f-09ee-4407-a03b-12ec96477ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([\n",
    "    [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"],\n",
    "    [\"Z\",\"Y\",\"X\",\"W\",\"V\",\"U\",\"T\",\"S\",\"R\",\"Q\",\"P\",\"O\",\"N\",\"M\",\"L\",\"K\",\"J\",\"I\",\"H\",\"G\",\"F\",\"E\",\"D\",\"C\",\"B\",\"A\"],\n",
    "    [\"B\",\"D\",\"F\",\"H\",\"J\",\"L\",\"N\",\"P\",\"R\",\"T\",\"V\",\"X\",\"Z\",\"A\",\"C\",\"E\",\"G\",\"I\",\"K\",\"M\",\"O\",\"Q\",\"S\",\"U\",\"W\",\"Y\"],\n",
    "    [\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\"],\n",
    "    [\"H\",\"G\",\"F\",\"E\",\"D\",\"C\",\"B\",\"A\",\"L\",\"K\",\"J\",\"I\",\"P\",\"O\",\"N\",\"M\",\"U\",\"T\",\"S\",\"R\",\"Q\",\"X\",\"W\",\"V\",\"Z\",\"Y\"]\n",
    "])\n",
    "\n",
    "expected = np.array([\n",
    "    [\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\"],\n",
    "    [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"],\n",
    "    [\"C\",\"E\",\"G\",\"I\",\"K\",\"M\",\"O\",\"Q\",\"S\",\"U\",\"W\",\"Y\",\"A\",\"B\",\"D\",\"F\",\"H\",\"J\",\"L\",\"N\",\"P\",\"R\",\"T\",\"V\",\"X\",\"Z\"], \n",
    "    [\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\"],\n",
    "    [\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\"]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03891cad-14be-4c22-9f04-641cc2b17b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_one_hot(inputs: np.ndarray) -> np.ndarray:\n",
    "    char_to_index = {char: i for i, char in enumerate(string.ascii_uppercase)}\n",
    "\n",
    "    one_hot_inputs = []\n",
    "    for row in inputs:\n",
    "        one_hot_list = []\n",
    "        for char in row:\n",
    "            if char.upper() in char_to_index:\n",
    "                one_hot_vector = np.zeros((len(string.ascii_uppercase), 1))\n",
    "                one_hot_vector[char_to_index[char.upper()]] = 1\n",
    "                one_hot_list.append(one_hot_vector)\n",
    "        one_hot_inputs.append(one_hot_list)\n",
    "\n",
    "    return np.array(one_hot_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "017939be-595c-4e1c-bce3-6c737f84f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputLayer:\n",
    "    inputs: np.ndarray\n",
    "    U: np.ndarray = None\n",
    "    delta_U: np.ndarray = None\n",
    "\n",
    "    def __init__(self, inputs: np.ndarray, hidden_size: int) -> None:\n",
    "        self.inputs = inputs\n",
    "        self.U = np.random.uniform(low=0, high=1, size=(hidden_size, len(inputs[0])))\n",
    "        self.delta_U = np.zeros_like(self.U)\n",
    "\n",
    "    def get_input(self, time_step: int) -> np.ndarray:\n",
    "        return self.inputs[time_step]\n",
    "\n",
    "    def weighted_sum(self, time_step: int) -> np.ndarray:\n",
    "        return self.U @ self.get_input(time_step)\n",
    "\n",
    "    def calculate_deltas_per_step(\n",
    "        self, time_step: int, delta_weighted_sum: np.ndarray\n",
    "    ) -> None:\n",
    "        # (h_dimension, 1) @ (1, input_size) = (h_dimension, input_size)\n",
    "        self.delta_U += delta_weighted_sum @ self.get_input(time_step).T\n",
    "\n",
    "    def update_weights_and_bias(self, learning_rate: float) -> None:\n",
    "        self.U -= learning_rate * self.delta_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56e582c7-e95f-4c12-aad9-efb8a3af3274",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer:\n",
    "    states: np.ndarray = None\n",
    "    W: np.ndarray = None\n",
    "    delta_W: np.ndarray = None\n",
    "    bias: np.ndarray = None\n",
    "    delta_bias: np.ndarray = None\n",
    "    next_delta_activation: np.ndarray = None\n",
    "\n",
    "    def __init__(self, vocab_size: int, size: int) -> None:\n",
    "        self.W = np.random.uniform(low=0, high=1, size=(size, size))\n",
    "        self.bias = np.random.uniform(low=0, high=1, size=(size, 1))\n",
    "        self.states = np.zeros(shape=(vocab_size, size, 1))\n",
    "        self.next_delta_activation = np.zeros(shape=(size, 1))\n",
    "        self.delta_bias = np.zeros_like(self.bias)\n",
    "        self.delta_W = np.zeros_like(self.W)\n",
    "\n",
    "    def get_hidden_state(self, time_step: int) -> np.ndarray:\n",
    "        # If starting out at the beginning of the sequence, a[t-1] will return zeros\n",
    "        if time_step < 0:\n",
    "            return np.zeros_like(self.states[0])\n",
    "        return self.states[time_step]\n",
    "\n",
    "    def set_hidden_state(self, time_step: int, hidden_state: np.ndarray) -> None:\n",
    "        self.states[time_step] = hidden_state\n",
    "\n",
    "    def activate(self, weighted_input: np.ndarray, time_step: int) -> np.ndarray:\n",
    "        previous_hidden_state = self.get_hidden_state(time_step - 1)\n",
    "        # W @ h_prev => (h_dimension, h_dimension) @ (h_dimension, 1) = (h_dimension, 1)\n",
    "        weighted_hidden_state = self.W @ previous_hidden_state\n",
    "        # (h_dimension, 1) + (h_dimension, 1) + (h_dimension, 1) = (h_dimension, 1)\n",
    "        weighted_sum = weighted_input + weighted_hidden_state + self.bias\n",
    "        activation = np.tanh(weighted_sum)  # (h_dimension, 1)\n",
    "        self.set_hidden_state(time_step, activation)\n",
    "        return activation\n",
    "\n",
    "    def calculate_deltas_per_step(\n",
    "        self, time_step: int, delta_output: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        # (h_dimension, 1) + (h_dimension, 1) = (h_dimension, 1)\n",
    "        delta_activation = delta_output + self.next_delta_activation\n",
    "        # (h_dimension, 1) * scalar = (h_dimension, 1)\n",
    "        delta_weighted_sum = delta_activation * (\n",
    "            1 - self.get_hidden_state(time_step) ** 2\n",
    "        )\n",
    "        # (h_dimension, h_dimension) @ (h_dimension, 1) = (h_dimension, 1)\n",
    "        self.next_delta_activation = self.W.T @ delta_weighted_sum\n",
    "\n",
    "        # (h_dimension, 1) @ (1, h_dimension) = (h_dimension, h_dimension)\n",
    "        self.delta_W += delta_weighted_sum @ self.get_hidden_state(time_step - 1).T\n",
    "\n",
    "        # derivative of hidden bias is the same as dL_ds\n",
    "        self.delta_bias += delta_weighted_sum\n",
    "        return delta_weighted_sum\n",
    "\n",
    "    def update_weights_and_bias(self, learning_rate: float) -> None:\n",
    "        self.W -= learning_rate * self.delta_W\n",
    "        self.bias -= learning_rate * self.delta_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1f8df95-9334-4ef7-952f-18ba73a262ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class OutputLayer:\n",
    "    def __init__(self, size: int, hidden_size: int) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the output layer with random weights and biases.\n",
    "        :param size: Output size (vocabulary size for RNN).\n",
    "        :param hidden_size: Hidden layer size.\n",
    "        \"\"\"\n",
    "        self.V = np.random.uniform(low=0, high=1, size=(size, hidden_size))  # Weight matrix\n",
    "        self.bias = np.random.uniform(low=0, high=1, size=(size, 1))  # Bias vector\n",
    "        self.states = np.zeros(shape=(size, size, 1))  # Stores predictions for each time step\n",
    "        self.delta_bias = np.zeros_like(self.bias)  # Gradient accumulator for biases\n",
    "        self.delta_V = np.zeros_like(self.V)  # Gradient accumulator for weights\n",
    "\n",
    "    @staticmethod\n",
    "    def softmax(x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply the softmax function to the input array.\n",
    "        :param x: Input array of logits.\n",
    "        :return: Softmax probabilities.\n",
    "        \"\"\"\n",
    "        exp_x = np.exp(x - np.max(x))  # Stability fix to avoid overflow\n",
    "        return exp_x / np.sum(exp_x, axis=0, keepdims=True)\n",
    "\n",
    "    def predict(self, hidden_state: np.ndarray, time_step: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Make a prediction at the given time step.\n",
    "        :param hidden_state: The hidden state from the hidden layer.\n",
    "        :param time_step: The current time step in the sequence.\n",
    "        :return: Predicted output probabilities.\n",
    "        \"\"\"\n",
    "        output = self.V @ hidden_state + self.bias  # Compute logits\n",
    "        prediction = self.softmax(output)  # Apply softmax\n",
    "        self.set_state(time_step, prediction)  # Save prediction state\n",
    "        return prediction\n",
    "\n",
    "    def get_state(self, time_step: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Retrieve the stored prediction for a specific time step.\n",
    "        :param time_step: Time step index.\n",
    "        :return: Stored prediction.\n",
    "        \"\"\"\n",
    "        return self.states[time_step]\n",
    "\n",
    "    def set_state(self, time_step: int, prediction: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Store the prediction for a specific time step.\n",
    "        :param time_step: Time step index.\n",
    "        :param prediction: Predicted output probabilities.\n",
    "        \"\"\"\n",
    "        self.states[time_step] = prediction\n",
    "\n",
    "    def calculate_deltas_per_step(\n",
    "        self,\n",
    "        expected: np.ndarray,\n",
    "        hidden_state: np.ndarray,\n",
    "        time_step: int,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate gradients for a single time step.\n",
    "        :param expected: Expected output (ground truth one-hot vector).\n",
    "        :param hidden_state: Hidden state from the hidden layer.\n",
    "        :param time_step: The current time step in the sequence.\n",
    "        :return: Gradient of the loss with respect to the hidden state.\n",
    "        \"\"\"\n",
    "        delta_output = self.get_state(time_step) - expected  # Error at the output layer\n",
    "\n",
    "        # Accumulate gradients for weights and biases\n",
    "        self.delta_V += delta_output @ hidden_state.T\n",
    "        self.delta_bias += delta_output\n",
    "\n",
    "        # Return the gradient of the loss with respect to the hidden state\n",
    "        return self.V.T @ delta_output\n",
    "\n",
    "    def update_weights_and_bias(self, learning_rate: float) -> None:\n",
    "        \"\"\"\n",
    "        Update the weights and biases using accumulated gradients.\n",
    "        :param learning_rate: Learning rate for gradient descent.\n",
    "        \"\"\"\n",
    "        self.V -= learning_rate * self.delta_V\n",
    "        self.bias -= learning_rate * self.delta_bias\n",
    "\n",
    "        # Reset gradients after the update\n",
    "        self.delta_V = np.zeros_like(self.V)\n",
    "        self.delta_bias = np.zeros_like(self.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68de1559-ca92-4ace-9b3f-ef7f8bb3a720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "class VanillaRNN:\n",
    "    hidden_layer: HiddenLayer\n",
    "    output_layer: OutputLayer\n",
    "    alpha: float  # learning rate\n",
    "    input_layer: InputLayer = None\n",
    "\n",
    "    def __init__(self, vocab_size: int, hidden_size: int, alpha: float) -> None:\n",
    "        self.hidden_layer = HiddenLayer(vocab_size, hidden_size)\n",
    "        self.output_layer = OutputLayer(vocab_size, hidden_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def feed_forward(self, inputs: np.ndarray) -> OutputLayer:\n",
    "        self.input_layer = InputLayer(inputs, self.hidden_size)\n",
    "        for step in range(len(inputs)):\n",
    "            weighted_input = self.input_layer.weighted_sum(step)\n",
    "            activation = self.hidden_layer.activate(weighted_input, step)\n",
    "            self.output_layer.predict(activation, step)\n",
    "        return self.output_layer\n",
    "\n",
    "    def backpropagation(self, expected: np.ndarray) -> None:\n",
    "        for step_number in reversed(range(len(expected))):\n",
    "            delta_output = self.output_layer.calculate_deltas_per_step(\n",
    "                expected[step_number],\n",
    "                self.hidden_layer.get_hidden_state(step_number),\n",
    "                step_number,\n",
    "            )\n",
    "            delta_weighted_sum = self.hidden_layer.calculate_deltas_per_step(\n",
    "                step_number, delta_output\n",
    "            )\n",
    "            self.input_layer.calculate_deltas_per_step(step_number, delta_weighted_sum)\n",
    "\n",
    "        self.output_layer.update_weights_and_bias(self.alpha)\n",
    "        self.hidden_layer.update_weights_and_bias(self.alpha)\n",
    "        self.input_layer.update_weights_and_bias(self.alpha)\n",
    "\n",
    "    def loss(self, y_hat: List[np.ndarray], y: List[np.ndarray]) -> float:\n",
    "        \"\"\"\n",
    "        Cross-entropy loss function - Calculating difference between 2 probability distributions.\n",
    "        First, calculate cross-entropy loss for each time step with np.sum, which returns a numpy array\n",
    "        Then, sum across individual losses of all time steps with sum() to get a scalar value.\n",
    "        :param y_hat: predicted value\n",
    "        :param y: expected value - true label\n",
    "        :return: total loss\n",
    "        \"\"\"\n",
    "        return sum(-np.sum(y[i] * np.log(y_hat[i]) for i in range(len(y))))\n",
    "\n",
    "    def train(self, inputs: np.ndarray, expected: np.ndarray, epochs: int) -> None:\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"epoch={epoch}\")\n",
    "            for idx, input in enumerate(inputs):\n",
    "                y_hats = self.feed_forward(input)\n",
    "                self.backpropagation(expected[idx])\n",
    "                print(\n",
    "                    f\"Loss round: {self.loss([y for y in y_hats.states], expected[idx])}\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec7f5fd1-1361-4187-a8bb-7a56e3278c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0\n",
      "Loss round: [154.71751088]\n",
      "Loss round: [151.77378383]\n",
      "Loss round: [148.97863057]\n",
      "Loss round: [147.92767954]\n",
      "Loss round: [146.77982045]\n",
      "epoch=1\n",
      "Loss round: [145.3443987]\n",
      "Loss round: [143.0978177]\n",
      "Loss round: [139.52377741]\n",
      "Loss round: [141.017251]\n",
      "Loss round: [139.53211161]\n",
      "epoch=2\n",
      "Loss round: [137.29369607]\n",
      "Loss round: [135.90495058]\n",
      "Loss round: [133.16774936]\n",
      "Loss round: [134.44779077]\n",
      "Loss round: [133.53347645]\n",
      "epoch=3\n",
      "Loss round: [132.555209]\n",
      "Loss round: [131.35037706]\n",
      "Loss round: [129.0839957]\n",
      "Loss round: [130.02912165]\n",
      "Loss round: [129.07171531]\n",
      "epoch=4\n",
      "Loss round: [128.29974701]\n",
      "Loss round: [126.56014473]\n",
      "Loss round: [124.43907088]\n",
      "Loss round: [126.15327391]\n",
      "Loss round: [125.82829161]\n",
      "epoch=5\n",
      "Loss round: [125.60084973]\n",
      "Loss round: [123.35948764]\n",
      "Loss round: [120.7862178]\n",
      "Loss round: [122.95441777]\n",
      "Loss round: [122.64752867]\n",
      "epoch=6\n",
      "Loss round: [121.58779385]\n",
      "Loss round: [120.20770913]\n",
      "Loss round: [118.5504378]\n",
      "Loss round: [120.60329066]\n",
      "Loss round: [120.40633136]\n",
      "epoch=7\n",
      "Loss round: [119.92733294]\n",
      "Loss round: [118.29904176]\n",
      "Loss round: [116.79524715]\n",
      "Loss round: [116.60098743]\n",
      "Loss round: [117.77999146]\n",
      "epoch=8\n",
      "Loss round: [116.92427235]\n",
      "Loss round: [115.518952]\n",
      "Loss round: [114.74384531]\n",
      "Loss round: [115.24388034]\n",
      "Loss round: [115.50232572]\n",
      "epoch=9\n",
      "Loss round: [114.14776152]\n",
      "Loss round: [113.88430072]\n",
      "Loss round: [112.02546482]\n",
      "Loss round: [114.32085218]\n",
      "Loss round: [113.59584626]\n",
      "3\n",
      "D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1284129/3890183113.py:49: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  return sum(-np.sum(y[i] * np.log(y_hat[i]) for i in range(len(y))))\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "if __name__ == \"__main__\":\n",
    "  inputs = np.array([\n",
    "      [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"],\n",
    "      [\"Z\",\"Y\",\"X\",\"W\",\"V\",\"U\",\"T\",\"S\",\"R\",\"Q\",\"P\",\"O\",\"N\",\"M\",\"L\",\"K\",\"J\",\"I\",\"H\",\"G\",\"F\",\"E\",\"D\",\"C\",\"B\",\"A\"],\n",
    "      [\"B\",\"D\",\"F\",\"H\",\"J\",\"L\",\"N\",\"P\",\"R\",\"T\",\"V\",\"X\",\"Z\",\"A\",\"C\",\"E\",\"G\",\"I\",\"K\",\"M\",\"O\",\"Q\",\"S\",\"U\",\"W\",\"Y\"],\n",
    "      [\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\"],\n",
    "      [\"H\",\"G\",\"F\",\"E\",\"D\",\"C\",\"B\",\"A\",\"L\",\"K\",\"J\",\"I\",\"P\",\"O\",\"N\",\"M\",\"U\",\"T\",\"S\",\"R\",\"Q\",\"X\",\"W\",\"V\",\"Z\",\"Y\"]\n",
    "  ])\n",
    "\n",
    "  expected = np.array([\n",
    "      [\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\"],\n",
    "      [\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"],\n",
    "      [\"C\",\"E\",\"G\",\"I\",\"K\",\"M\",\"O\",\"Q\",\"S\",\"U\",\"W\",\"Y\",\"A\",\"B\",\"D\",\"F\",\"H\",\"J\",\"L\",\"N\",\"P\",\"R\",\"T\",\"V\",\"X\",\"Z\"], \n",
    "      [\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\"],\n",
    "      [\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\"]\n",
    "  ])\n",
    "  \n",
    "  one_hot_inputs = string_to_one_hot(inputs)\n",
    "  one_hot_expected = string_to_one_hot(expected)\n",
    "\n",
    "  # Forward pass through time, no gradient clipping yet so there will be gradient exploding problem\n",
    "  # https://stackoverflow.com/a/33980220\n",
    "  # https://stackoverflow.com/a/72494516\n",
    "  rnn = VanillaRNN(vocab_size=len(string.ascii_uppercase), hidden_size=128, alpha=0.0001)\n",
    "  rnn.train(one_hot_inputs, one_hot_expected, epochs=10)\n",
    "\n",
    "  new_inputs = np.array([[\"B\", \"C\", \"D\"]])\n",
    "  for input in string_to_one_hot(new_inputs):\n",
    "      predictions = rnn.feed_forward(input)\n",
    "      output = np.argmax(predictions.states[-1])\n",
    "      print(output) # index of the one-hot value of prediction\n",
    "      print(string.ascii_uppercase[output]) # mapping one hot to character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd0fd3c-05a7-4226-8a5c-094efa9c678c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
